{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49031,
     "status": "ok",
     "timestamp": 1717658104173,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "2eSvM9zX_2d3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" #\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Home Directory: ['/workspace/.cache/huggingface/']\n",
      "Updated PATH: ['/workspace/anaconda3/envs/autotrain/bin:/workspace/anaconda3/envs/autotrain/bin:/workspace/anaconda3/bin:/workspace/anaconda3/bin:/workspace/anaconda3/condabin:/workspace/anaconda3/bin:/usr/local/cuda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin']\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface_hub in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: filelock in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.6.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0musage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: --version\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /workspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the PIP_CACHE_DIR environment variable\n",
    "os.environ['PIP_CACHE_DIR'] = '/workspace/.pip/'\n",
    "# Set the HF_HOME environment variable to the desired directory\n",
    "os.environ['HF_HOME'] = '/workspace/.cache/huggingface/'\n",
    "\n",
    "# Add /workspace/anaconda3/envs/autotrain/bin/ to the PATH environment variable\n",
    "os.environ['PATH'] = '/workspace/anaconda3/envs/autotrain/bin:' + os.environ['PATH']\n",
    "\n",
    "# Verify the changes\n",
    "hf_home_dir = !echo $HF_HOME\n",
    "path_dirs = !echo $PATH\n",
    "print(\"Hugging Face Home Directory:\", hf_home_dir)\n",
    "print(\"Updated PATH:\", path_dirs)\n",
    "\n",
    "# Install huggingface-cli\n",
    "!pip install huggingface_hub\n",
    "\n",
    "# Verify the installation\n",
    "!huggingface-cli --version\n",
    "\n",
    "# Test the Hugging Face CLI\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  4 16:03:42 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              62W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Delete model and other training-related objects\n",
    "try:\n",
    "    #del model\n",
    "    del trainer\n",
    "    #del tokenizer\n",
    "    #del train_dataset\n",
    "except NameError:\n",
    "    pass  # Ignore if they are not defined\n",
    "\n",
    "# Invoke garbage collector multiple times\n",
    "for _ in range(10):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Reset the CUDA runtime\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Optionally, you can use nvidia-smi command to kill all processes using GPU\n",
    "import os\n",
    "os.system('nvidia-smi')\n",
    "\n",
    "# Optionally, restart the notebook kernel (uncomment if necessary)\n",
    "# import IPython\n",
    "# IPython.display.clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    PeftModelForCausalLM,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, get_last_checkpoint\n",
    "from transformers.utils import send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /workspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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 --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/.cache/huggingface/\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367,
     "referenced_widgets": [
      "e185802b926d44cfaeadf604733d4a5d",
      "4aeae8a8d48a4d2b8ba1df3c7d3a0309",
      "05eca118bd194c2c9d5f7d825bf0493f",
      "27ea6ee950254229ae3493c862fabab7",
      "3d1a950d2faf4a11bcd41af7fd1af8eb",
      "1d5689dd39d7414dbeb92976c9a682c3",
      "aa63039256a643b38ff77fe394a6832f",
      "987d83a2779c42fd99b862fead155f57",
      "943df89e90eb4def9bb8414ecfffc681",
      "d49a277941384dfeb7907b9017d16ecf",
      "efd751b6c2a642ff94845df115af4ed7",
      "4806cee42b644a868e29a19ede8aad49",
      "5380ca6400a1401e8fa1cad69af5c97c",
      "0d920b9593ea41a3ae099ee0c3eb6dbf",
      "cee00c75e5cb405bbd928ff7759c52b8",
      "0cc5119ca0244fd59e2b5b3cb4715005",
      "94fa0935dfb84f61b61af5e27396ab65",
      "6488ca56a5784cad9195c6eb302b0839",
      "a55b27398caf4ae39ac5900d22677220",
      "f7187ecc5d974ff18fc70470d1a48464",
      "c6bab59f1a624d21a6a09f94fbe99e58",
      "ce88a214d07e42399d9edd549175b760",
      "be4f34efcc81414585f78d57ae025a16",
      "29d5c06b48d04bfa99cefe38030f277c",
      "ac0fdfbd66024fcfb7d84d752b8b65de",
      "5eb4b0b839e94e53adffd6a5a28781b1",
      "704e93e3978140e1864b84cf19507b58",
      "4e9990fc07da480a96223afe3c85be2f",
      "abd2784d68354f6388adafe797feb706",
      "16937b1d13dc433ca1605982da3f7587",
      "ae325732051f41ed8949a6026a113e95",
      "baee5199c98e443697d179bf127b3798",
      "25350d0725054c33b7e2e792190bf039",
      "d62c059e650c44ed9bdfd9ea259724ef",
      "b168622cd5844e95be35c6476b3c3847",
      "2a73eec6178640ccac73682222f36a56",
      "0c3a85239fa94b8a85545c8eec38d887",
      "689ab31dd3994630a248b642096bee22",
      "5c9bdfe89d304e7ca23a9ad7ff0302f2",
      "a5ff362daae4430ba27be07304270123",
      "1dd43f29b71042df88ff4c49af908ce3",
      "777c1f497f364f04bb3cfc97d937de0a",
      "e66a1bb087944ceca9cbe6517ac19292",
      "38725355a21844458da20ebf9eaf7d21",
      "2a0095c0f676406c9799779a33225860",
      "91476a34dbb14bde981fdd7743fb860f",
      "2a68f7129c5a4e3bb75c4469a8eda942",
      "b87ddb145c414e53b425345c652699d1",
      "fdad79e64e214bfbb4abda9b3eee7eb6",
      "90924adbead14a25bdbc8e0779e6360a",
      "310f05c91ee14e4796bfa18897f81e26",
      "d208453e5609482c9c5113d3b3c84a6d",
      "b9b86ece90ea4666be175d68bc26a648",
      "fbc15025bfb441638bf4fe58cb7d08c1",
      "f4e2e1c634ab4a07af5483b8dccf8b52",
      "10a7046d9e1c4ada876b2b65e4169de9",
      "026e66bb6fb64494b389198e3f64504b",
      "a893e20bedc6404fb2ab9002d7da7090",
      "1df6a414771544528de945d997ea18a5",
      "ed92ddd32f6e4301a46e0178f12e5a8d",
      "33a88395eec948cdbc9cd038a986729d",
      "2ba358a81db741b7963d4e14136a6bff",
      "8e41f5233d7945469c4099b7b1f6cba9",
      "469da5951d744acdb7e5472e3fae2cb2",
      "eaf2f4d59814487aa8ea1abbf8831da1",
      "5f1cbf83549943cfa3b568ccd5cdb34e",
      "939a4d107645466a8c6633285471ae40",
      "fab0a887002e467786f3f6980aa57d0a",
      "4f8d9958f6694a2cb0d55308f6653cbc",
      "f1e10f0376324720bb85d554f677f5f7",
      "35a82e3cee404c0eb8c4a99d48fde443",
      "44d377128d6748d4b871d2efd101dd19",
      "c4228273bcdc4d619d23112c5c9a14f0",
      "897760bb1650484bbbd2a604d886b68a",
      "ea0292273e364ecfb256158407eda9fc",
      "9656d55a0d3c4fa1bb7a04f5c17b7a84",
      "108675c4393b4b678a5cfa2f73ce9f4f"
     ]
    },
    "executionInfo": {
     "elapsed": 72427,
     "status": "ok",
     "timestamp": 1717658176589,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "4807874d-7225-42ac-e0d3-fe7e3f2de8cd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "[codecarbon INFO @ 23:40:05] Energy consumed for RAM : 15.275533 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:40:05] Energy consumed for RAM : 15.236813 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:40:05] Energy consumed for all GPUs : 7.600023 kWh. Total GPU Power : 66.48391989084017 W\n",
      "[codecarbon INFO @ 23:40:05] Energy consumed for all CPUs : 2.848356 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:40:05] 25.723912 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:40:05] Energy consumed for all GPUs : 7.595575 kWh. Total GPU Power : 66.9349624738624 W\n",
      "[codecarbon INFO @ 23:40:05] Energy consumed for all CPUs : 2.841050 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:40:05] 25.673438 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:40:20] Energy consumed for RAM : 15.278622 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:40:20] Energy consumed for all GPUs : 7.600297 kWh. Total GPU Power : 66.81713134031068 W\n",
      "[codecarbon INFO @ 23:40:20] Energy consumed for all CPUs : 2.848936 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:40:20] 25.727855 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:40:20] Energy consumed for RAM : 15.239922 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:40:20] Energy consumed for all GPUs : 7.595852 kWh. Total GPU Power : 66.40289616858833 W\n",
      "[codecarbon INFO @ 23:40:20] Energy consumed for all CPUs : 2.841633 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:40:20] 25.677408 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:40:35] Energy consumed for RAM : 15.281753 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:40:35] Energy consumed for RAM : 15.243052 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:40:35] Energy consumed for all GPUs : 7.596131 kWh. Total GPU Power : 66.99871054833511 W\n",
      "[codecarbon INFO @ 23:40:35] Energy consumed for all GPUs : 7.600580 kWh. Total GPU Power : 67.8494394514439 W\n",
      "[codecarbon INFO @ 23:40:35] Energy consumed for all CPUs : 2.842220 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:40:35] Energy consumed for all CPUs : 2.849523 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:40:35] 25.681403 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:40:35] 25.731857 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:40:50] Energy consumed for RAM : 15.284842 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:40:50] Energy consumed for all GPUs : 7.600855 kWh. Total GPU Power : 66.85112768290797 W\n",
      "[codecarbon INFO @ 23:40:50] Energy consumed for all CPUs : 2.850099 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:40:50] 25.735796 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:40:50] Energy consumed for RAM : 15.246161 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:40:50] Energy consumed for all GPUs : 7.596407 kWh. Total GPU Power : 66.87824202662392 W\n",
      "[codecarbon INFO @ 23:40:50] Energy consumed for all CPUs : 2.842804 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:40:50] 25.685372 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e24d6326a3413a9e48e9c947109856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:41:05] Energy consumed for RAM : 15.287995 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:41:05] Energy consumed for RAM : 15.249276 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:41:05] Energy consumed for all GPUs : 7.596687 kWh. Total GPU Power : 67.3390320641158 W\n",
      "[codecarbon INFO @ 23:41:05] Energy consumed for all CPUs : 2.843385 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:41:05] 25.689348 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:41:05] Energy consumed for all GPUs : 7.601137 kWh. Total GPU Power : 66.94177877530271 W\n",
      "[codecarbon INFO @ 23:41:05] Energy consumed for all CPUs : 2.850688 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:41:05] 25.739820 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:41:20] Energy consumed for RAM : 15.291112 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:41:20] Energy consumed for RAM : 15.252401 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:41:20] Energy consumed for all GPUs : 7.601414 kWh. Total GPU Power : 66.84709561257718 W\n",
      "[codecarbon INFO @ 23:41:20] Energy consumed for all CPUs : 2.851269 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:41:20] 25.743795 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:41:20] Energy consumed for all GPUs : 7.596964 kWh. Total GPU Power : 66.68683089752152 W\n",
      "[codecarbon INFO @ 23:41:20] Energy consumed for all CPUs : 2.843967 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:41:20] 25.693332 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:41:35] Energy consumed for RAM : 15.294241 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:41:35] Energy consumed for RAM : 15.255530 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:41:35] Energy consumed for all GPUs : 7.601691 kWh. Total GPU Power : 66.45358383190866 W\n",
      "[codecarbon INFO @ 23:41:35] Energy consumed for all CPUs : 2.851852 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:41:35] 25.747784 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:41:35] Energy consumed for all GPUs : 7.597241 kWh. Total GPU Power : 66.46089949340333 W\n",
      "[codecarbon INFO @ 23:41:35] Energy consumed for all CPUs : 2.844551 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:41:35] 25.697321 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:41:50] Energy consumed for RAM : 15.297370 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:41:50] Energy consumed for RAM : 15.258659 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:41:50] Energy consumed for all GPUs : 7.601968 kWh. Total GPU Power : 66.52403092280649 W\n",
      "[codecarbon INFO @ 23:41:50] Energy consumed for all CPUs : 2.852436 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:41:50] 25.751773 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:41:50] Energy consumed for all GPUs : 7.597518 kWh. Total GPU Power : 66.52761924938787 W\n",
      "[codecarbon INFO @ 23:41:50] Energy consumed for all CPUs : 2.845134 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:41:50] 25.701311 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:42:05] Energy consumed for RAM : 15.300499 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:42:05] Energy consumed for RAM : 15.261788 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:42:05] Energy consumed for all GPUs : 7.602246 kWh. Total GPU Power : 66.9039356563281 W\n",
      "[codecarbon INFO @ 23:42:05] Energy consumed for all CPUs : 2.853019 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:42:05] 25.755764 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:42:05] Energy consumed for all GPUs : 7.597797 kWh. Total GPU Power : 66.90638780463422 W\n",
      "[codecarbon INFO @ 23:42:05] Energy consumed for all CPUs : 2.845717 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:42:05] 25.705302 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:42:20] Energy consumed for RAM : 15.303628 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:42:20] Energy consumed for RAM : 15.264917 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:42:20] Energy consumed for all GPUs : 7.602523 kWh. Total GPU Power : 66.44172876978188 W\n",
      "[codecarbon INFO @ 23:42:20] Energy consumed for all CPUs : 2.853602 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:42:20] 25.759753 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:42:20] Energy consumed for all GPUs : 7.598074 kWh. Total GPU Power : 66.43749660373376 W\n",
      "[codecarbon INFO @ 23:42:20] Energy consumed for all CPUs : 2.846300 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:42:20] 25.709291 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:42:35] Energy consumed for RAM : 15.306757 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:42:35] Energy consumed for RAM : 15.268046 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:42:35] Energy consumed for all GPUs : 7.602800 kWh. Total GPU Power : 66.50521961019545 W\n",
      "[codecarbon INFO @ 23:42:35] Energy consumed for all CPUs : 2.854186 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:42:35] 25.763743 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:42:35] Energy consumed for all GPUs : 7.598351 kWh. Total GPU Power : 66.51258258336013 W\n",
      "[codecarbon INFO @ 23:42:35] Energy consumed for all CPUs : 2.846884 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:42:35] 25.713281 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:42:50] Energy consumed for RAM : 15.309887 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:42:50] Energy consumed for RAM : 15.271176 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:42:50] Energy consumed for all GPUs : 7.603077 kWh. Total GPU Power : 66.60783163016431 W\n",
      "[codecarbon INFO @ 23:42:50] Energy consumed for all CPUs : 2.854769 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:42:50] 25.767733 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:42:50] Energy consumed for all GPUs : 7.598628 kWh. Total GPU Power : 66.60400819569283 W\n",
      "[codecarbon INFO @ 23:42:50] Energy consumed for all CPUs : 2.847467 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:42:50] 25.717271 kWh of electricity used since the beginning.\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "loading file tokenizer.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /workspace/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[codecarbon INFO @ 23:43:05] Energy consumed for RAM : 15.313016 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:43:05] Energy consumed for RAM : 15.274306 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:43:05] Energy consumed for all GPUs : 7.603357 kWh. Total GPU Power : 67.04339503133163 W\n",
      "[codecarbon INFO @ 23:43:05] Energy consumed for all CPUs : 2.855352 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:43:05] 25.771725 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:43:05] Energy consumed for all GPUs : 7.598907 kWh. Total GPU Power : 67.04400260145131 W\n",
      "[codecarbon INFO @ 23:43:05] Energy consumed for all CPUs : 2.848050 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:43:05] 25.721263 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:43:20] Energy consumed for RAM : 15.316146 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:43:20] Energy consumed for RAM : 15.277435 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:43:20] Energy consumed for all GPUs : 7.603634 kWh. Total GPU Power : 66.45623819104938 W\n",
      "[codecarbon INFO @ 23:43:20] Energy consumed for all CPUs : 2.855935 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:43:20] 25.775715 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:43:20] Energy consumed for all GPUs : 7.599184 kWh. Total GPU Power : 66.45987101896183 W\n",
      "[codecarbon INFO @ 23:43:20] Energy consumed for all CPUs : 2.848634 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:43:20] 25.725253 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:43:35] Energy consumed for RAM : 15.319275 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:43:35] Energy consumed for RAM : 15.280564 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:43:35] Energy consumed for all GPUs : 7.603910 kWh. Total GPU Power : 66.4590684322677 W\n",
      "[codecarbon INFO @ 23:43:35] Energy consumed for all CPUs : 2.856519 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:43:35] 25.779704 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:43:35] Energy consumed for all GPUs : 7.599461 kWh. Total GPU Power : 66.45629439282543 W\n",
      "[codecarbon INFO @ 23:43:35] Energy consumed for all CPUs : 2.849217 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:43:35] 25.729242 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:43:50] Energy consumed for RAM : 15.322404 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:43:50] Energy consumed for RAM : 15.283694 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:43:50] Energy consumed for all GPUs : 7.604189 kWh. Total GPU Power : 66.86789531517141 W\n",
      "[codecarbon INFO @ 23:43:50] Energy consumed for all CPUs : 2.857102 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:43:50] 25.783694 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:43:50] Energy consumed for all GPUs : 7.599739 kWh. Total GPU Power : 66.86724454070398 W\n",
      "[codecarbon INFO @ 23:43:50] Energy consumed for all CPUs : 2.849800 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:43:50] 25.733233 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_orig, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    #model_name = \"BanglaLLM/BanglaLLama-3-8b-BnWiki-Base\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"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\" # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/logs/*\n"
     ]
    }
   ],
   "source": [
    "log_file_base = \"/workspace/logs/\"\n",
    "existing_log_files = log_file_base + \"*\"\n",
    "print(existing_log_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file should be at: /workspace/logs/BanglaLLM_Llama8B_Uonlp_CulturaX_2024-07-03_22:51:18.431741.log\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback, TrainingArguments, logging as hf_logging\n",
    "from unsloth import is_bfloat16_supported, UnslothTrainer, UnslothTrainingArguments\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Setup the log file name\n",
    "log_file_base = \"/workspace/logs/BanglaLLM_Llama8B_Uonlp_CulturaX_\"\n",
    "log_file = log_file_base + \"_\".join(str(datetime.datetime.now()).split()) + \".log\"\n",
    "\n",
    "existing_log_files = log_file_base + \"*\"\n",
    "#!rm -rf $existing_log_files\n",
    "\n",
    "# Create a dedicated logger\n",
    "logger = logging.getLogger('BanglaLLMLogger')\n",
    "logger.setLevel(logging.DEBUG)  # Set to DEBUG to capture all levels of logs\n",
    "\n",
    "# Create handlers\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters and add it to the handlers\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Redirect Hugging Face Transformers logging to our logger\n",
    "transformers_logger = hf_logging.get_logger()\n",
    "transformers_logger.setLevel(logging.DEBUG)\n",
    "for handler in logger.handlers:\n",
    "    transformers_logger.addHandler(handler)\n",
    "\n",
    "print(f\"Log file should be at: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "\n",
    "We also add `embed_tokens` and `lm_head` to allow the model to learn out of distribution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15122,
     "status": "ok",
     "timestamp": 1717658191701,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "1f0fc2ba-b073-45e5-eba1-7fc51ff64c8f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): ModulesToSaveWrapper(\n",
       "          (original_module): Embedding(128256, 4096)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Embedding(128256, 4096)\n",
       "          )\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a015bbfe004a4216b89004f27f614eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/30.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70aad84bbaa46388798b1109f764b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ee0ec407144e58b80be94a0035b308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/18 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'uonlp/CulturaX' #french novels\n",
    "dataset = load_dataset(dataset_name, \"bn\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'timestamp', 'url', 'source'],\n",
       "        num_rows: 12436596\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/'\n",
    "data_cache_dir = '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00001-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00003-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00000-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00004-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00002-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00005-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00007-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00006-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00065-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00008-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00009-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00015-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00011-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00010-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00018-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00021-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00012-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00022-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00014-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00013-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00017-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00016-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00019-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00020-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00023-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00032-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00033-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00038-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00025-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00035-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00027-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00026-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00024-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00029-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00034-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00047-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00031-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00040-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00030-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00028-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00037-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00046-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00049-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00051-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00039-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00036-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00048-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00042-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00041-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00054-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00050-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00044-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00053-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00052-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00060-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00043-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00057-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00059-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00045-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00055-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00058-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00056-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00062-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00070-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00068-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00067-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00063-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00069-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00061-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00066-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00064-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00072-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00078-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00081-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00073-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00074-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00080-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00071-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00075-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00076-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00086-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00077-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00079-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00083-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00082-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00085-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00084-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00088-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00087-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00089-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00090-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00093-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00092-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00091-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00100-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00094-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00108-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00104-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00096-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00095-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00098-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00107-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00097-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00110-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00101-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00099-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00109-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00105-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00106-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00114-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00113-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00115-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00102-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00117-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00111-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00116-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00112-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00122-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00103-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00120-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00118-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00121-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00125-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00123-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00124-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00127-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00130-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00131-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00132-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00119-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00126-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00129-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00128-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00133-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00134-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00135-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00136-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00138-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00140-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00142-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00144-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00137-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00145-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00139-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00141-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00147-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00149-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00146-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00143-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00148-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00150-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00152-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00156-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00157-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00153-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00154-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00158-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00151-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00159-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00160-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00155-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00161-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00162-of-00163.parquet']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(dataset_dir)\n",
    "parquet_files = [str(path) + \"/\" + file.name for file in path.glob(\"*.parquet\")]\n",
    "parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb575e289f0482da5efebb7e5b1a4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bb41f6e23a457e96fc8f5e3745c2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('parquet', data_files=parquet_files, split='train', keep_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'timestamp', 'url', 'source'],\n",
       "    num_rows: 12436596\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset_dir = \"/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/parquet_files/\"\n",
    "\n",
    "# # Convert the Hugging Face dataset to a pandas DataFrame\n",
    "# df = pd.DataFrame(dataset)\n",
    "\n",
    "# # Set the number of chunks equal to the number of files\n",
    "# num_chunks = len(text_files)\n",
    "\n",
    "# # Calculate the chunk size\n",
    "# chunk_size = len(df) // num_chunks\n",
    "\n",
    "# # Save the DataFrame as multiple Parquet files\n",
    "# output_dir = dataset_dir + \"parquet_files\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# for i in range(num_chunks):\n",
    "#     print(f\"processing text_files {text_files[0]}\")\n",
    "#     start_idx = i * chunk_size\n",
    "#     end_idx = (i + 1) * chunk_size if i < num_chunks - 1 else len(df)\n",
    "#     chunk_df = df[start_idx:end_idx]\n",
    "#     output_parquet_path = os.path.join(output_dir, f\"output_dataset_part_{i + 1}.parquet\")\n",
    "#     chunk_df.to_parquet(output_parquet_path, index=False)\n",
    "#     print(f\"Saved chunk {i + 1} as {output_parquet_path}\")\n",
    "\n",
    "# print(\"Dataset saved as multiple Parquet files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = \"/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/\"\n",
    "dataset_dir = dataset_base_dir + \"parquet_files/\"\n",
    "data_cache_dir = dataset_dir + \"uonlp_cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: /workspace/data/Bangla2B+/shards/parquet_files/cache//*\n"
     ]
    }
   ],
   "source": [
    "#!rm -rf $data_cache_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "if block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        logger.warning(\n",
    "            \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n",
    "            \" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n",
    "            \" override this default with `--block_size xxx`.\"\n",
    "        )\n",
    "        block_size = 1024\n",
    "else:\n",
    "    if block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(block_size, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "# since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "tok_logger = transformers.utils.logging.get_logger(\n",
    "    \"transformers.tokenization_utils_base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[\"text\"])\n",
    "    # clm input could be much much longer than block_size\n",
    "    # if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "    #     tok_logger.warning(\n",
    "    #         \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "    #         \" before being passed to the model.\"\n",
    "    #     )\n",
    "    return output\n",
    "\n",
    "# # Example tokenizer function that does not truncate\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True, max_length=block_size)\n",
    "\n",
    "def tokenize_and_check(examples):\n",
    "    result = tokenize_function(examples)\n",
    "    print(f\"Average tokens per text: {sum(len(ids) for ids in result['input_ids']) / len(result['input_ids'])}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_check(examples):\n",
    "    result = tokenize_function(examples)\n",
    "    print(f\"Average tokens per text: {sum(len(ids) for ids in result['input_ids']) / len(result['input_ids'])}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def combine_short_sequences(examples, max_length=256, absolute_max_length=4096):\n",
    "    combined = {k: [] for k in examples.keys()}\n",
    "    current_sequence = {k: [] for k in examples.keys()}\n",
    "    current_length = 0\n",
    "    newline_token_ids = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "    \n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        sequence_length = len(examples['input_ids'][i])\n",
    "        if current_length + sequence_length > max_length: # or current_length + sequence_length > absolute_max_length:\n",
    "            # Add the current combined sequence and start a new one\n",
    "            for k in combined.keys():\n",
    "                combined[k].append(list(chain(*current_sequence[k])))\n",
    "                current_sequence[k] = []\n",
    "            current_length = 0\n",
    "        \n",
    "        # Add the current sequence\n",
    "        for k in examples.keys():\n",
    "            if k == 'input_ids':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append(newline_token_ids)\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            elif k == 'attention_mask':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append([1] * len(newline_token_ids))\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            else:\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "        \n",
    "        current_length += sequence_length\n",
    "        if current_sequence['input_ids']:\n",
    "            current_length += len(newline_token_ids)\n",
    "    \n",
    "    # Add the last combined sequence\n",
    "    for k in combined.keys():\n",
    "        if current_sequence[k]:\n",
    "            combined[k].append(list(chain(*current_sequence[k])))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def group_texts_dynamic(examples):\n",
    "    # Use this before group_texts\n",
    "    examples = combine_short_sequences(examples)\n",
    "\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    block_size = 512\n",
    "    # More granular block size selection\n",
    "    if total_length < 512:\n",
    "        block_size = 512\n",
    "    if total_length < 1024:\n",
    "        block_size = 1024\n",
    "    elif total_length < 2048:\n",
    "        block_size = 2048\n",
    "    elif total_length < 4096:\n",
    "        block_size = 4096\n",
    "    else:\n",
    "        block_size = 8192  # Increase maximum block size, ensure your model can handle this\n",
    "    \n",
    "    # Adaptive padding\n",
    "    padding_unit = 128  # or another suitable value\n",
    "    if total_length < block_size:\n",
    "        block_size = ((total_length + padding_unit - 1) // padding_unit) * padding_unit\n",
    "    \n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "    result = {k: [] for k in concatenated_examples.keys()}\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        block_start = i * block_size\n",
    "        block_end = min((i + 1) * block_size, total_length)\n",
    "        for k, t in concatenated_examples.items():\n",
    "            block = t[block_start:block_end]\n",
    "            if len(block) < block_size:\n",
    "                padding_length = block_size - len(block)\n",
    "                if k == 'input_ids':\n",
    "                    block = block + [tokenizer.pad_token_id] * padding_length\n",
    "                elif k == 'attention_mask':\n",
    "                    block = block + [0] * padding_length\n",
    "            result[k].append(block)\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # Calculate how many blocks we'll create\n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "\n",
    "    # Split by chunks of block_size, allowing the last chunk to be smaller\n",
    "    result = {\n",
    "        k: [t[i * block_size : (i + 1) * block_size] for i in range(num_blocks)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # Create labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'timestamp', 'url', 'source'],\n",
       "    num_rows: 12436596\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00054-of-00163_text_512\n",
      "cultura_x-train-00055-of-00163_512\n",
      "cultura_x-train-00055-of-00163_text_512\n",
      "cultura_x-train-00056-of-00163_512\n",
      "cultura_x-train-00056-of-00163_text_512\n",
      "cultura_x-train-00057-of-00163_512\n",
      "cultura_x-train-00057-of-00163_text_512\n",
      "cultura_x-train-00058-of-00163_512\n",
      "cultura_x-train-00058-of-00163_text_512\n",
      "cultura_x-train-00059-of-00163_512\n",
      "cultura_x-train-00059-of-00163_text_512\n",
      "cultura_x-train-00060-of-00163_512\n",
      "cultura_x-train-00060-of-00163_text_512\n",
      "cultura_x-train-00061-of-00163_512\n",
      "cultura_x-train-00061-of-00163_text_512\n",
      "cultura_x-train-00062-of-00163_512\n",
      "cultura_x-train-00062-of-00163_text_512\n",
      "cultura_x-train-00063-of-00163_512\n",
      "cultura_x-train-00063-of-00163_text_512\n",
      "cultura_x-train-00064-of-00163_512\n",
      "cultura_x-train-00064-of-00163_text_512\n",
      "cultura_x-train-00065-of-00163_512\n",
      "cultura_x-train-00065-of-00163_text_512\n",
      "cultura_x-train-00066-of-00163_512\n",
      "cultura_x-train-00066-of-00163_text_512\n",
      "cultura_x-train-00067-of-00163_512\n",
      "cultura_x-train-00067-of-00163_text_512\n",
      "cultura_x-train-00068-of-00163_512\n",
      "cultura_x-train-00068-of-00163_text_512\n",
      "cultura_x-train-00069-of-00163_512\n",
      "cultura_x-train-00069-of-00163_text_512\n",
      "cultura_x-train-00070-of-00163_512\n",
      "cultura_x-train-00070-of-00163_text_512\n",
      "cultura_x-train-00071-of-00163_512\n",
      "cultura_x-train-00071-of-00163_text_512\n",
      "cultura_x-train-00072-of-00163_512\n",
      "cultura_x-train-00072-of-00163_text_512\n",
      "cultura_x-train-00073-of-00163_512\n",
      "cultura_x-train-00073-of-00163_text_512\n",
      "cultura_x-train-00074-of-00163_512\n",
      "cultura_x-train-00074-of-00163_text_512\n",
      "cultura_x-train-00075-of-00163_512\n",
      "cultura_x-train-00075-of-00163_text_512\n",
      "cultura_x-train-00076-of-00163_512\n",
      "cultura_x-train-00076-of-00163_text_512\n",
      "cultura_x-train-00077-of-00163_512\n",
      "cultura_x-train-00077-of-00163_text_512\n",
      "cultura_x-train-00078-of-00163_512\n",
      "cultura_x-train-00078-of-00163_text_512\n",
      "cultura_x-train-00079-of-00163_512\n",
      "cultura_x-train-00079-of-00163_text_512\n",
      "cultura_x-train-00080-of-00163_512\n",
      "cultura_x-train-00080-of-00163_text_512\n",
      "cultura_x-train-00081-of-00163_512\n",
      "cultura_x-train-00081-of-00163_text_512\n",
      "cultura_x-train-00082-of-00163_512\n",
      "cultura_x-train-00082-of-00163_text_512\n",
      "cultura_x-train-00083-of-00163_512\n",
      "cultura_x-train-00083-of-00163_text_512\n",
      "cultura_x-train-00084-of-00163_512\n",
      "cultura_x-train-00084-of-00163_text_512\n",
      "cultura_x-train-00085-of-00163_512\n",
      "cultura_x-train-00085-of-00163_text_512\n",
      "cultura_x-train-00086-of-00163_512\n",
      "cultura_x-train-00086-of-00163_text_512\n",
      "cultura_x-train-00087-of-00163_512\n",
      "cultura_x-train-00087-of-00163_text_512\n",
      "cultura_x-train-00088-of-00163_512\n",
      "cultura_x-train-00088-of-00163_text_512\n",
      "cultura_x-train-00089-of-00163_512\n",
      "cultura_x-train-00089-of-00163_text_512\n",
      "cultura_x-train-00090-of-00163_512\n",
      "cultura_x-train-00090-of-00163_text_512\n",
      "cultura_x-train-00091-of-00163_512\n",
      "cultura_x-train-00091-of-00163_text_512\n",
      "cultura_x-train-00092-of-00163_512\n",
      "cultura_x-train-00092-of-00163_text_512\n",
      "cultura_x-train-00093-of-00163_512\n",
      "cultura_x-train-00093-of-00163_text_512\n",
      "cultura_x-train-00094-of-00163_512\n",
      "cultura_x-train-00094-of-00163_text_512\n",
      "cultura_x-train-00095-of-00163_512\n",
      "cultura_x-train-00095-of-00163_text_512\n",
      "cultura_x-train-00096-of-00163_512\n",
      "cultura_x-train-00096-of-00163_text_512\n",
      "cultura_x-train-00097-of-00163_512\n",
      "cultura_x-train-00097-of-00163_text_512\n",
      "cultura_x-train-00098-of-00163_512\n",
      "cultura_x-train-00098-of-00163_text_512\n",
      "cultura_x-train-00099-of-00163_512\n",
      "cultura_x-train-00099-of-00163_text_512\n",
      "cultura_x-train-00100-of-00163_512\n",
      "cultura_x-train-00100-of-00163_text_512\n",
      "cultura_x-train-00101-of-00163_512\n",
      "cultura_x-train-00101-of-00163_text_512\n",
      "cultura_x-train-00102-of-00163_512\n",
      "cultura_x-train-00102-of-00163_text_512\n",
      "cultura_x-train-00103-of-00163_512\n",
      "cultura_x-train-00103-of-00163_text_512\n",
      "cultura_x-train-00104-of-00163_512\n",
      "cultura_x-train-00104-of-00163_text_512\n",
      "cultura_x-train-00105-of-00163_512\n",
      "cultura_x-train-00105-of-00163_text_512\n",
      "cultura_x-train-00106-of-00163_512\n",
      "cultura_x-train-00106-of-00163_text_512\n",
      "cultura_x-train-00107-of-00163_512\n",
      "cultura_x-train-00107-of-00163_text_512\n",
      "cultura_x-train-00108-of-00163_512\n",
      "cultura_x-train-00108-of-00163_text_512\n",
      "cultura_x-train-00109-of-00163_512\n",
      "cultura_x-train-00109-of-00163_text_512\n",
      "cultura_x-train-00110-of-00163_512\n",
      "cultura_x-train-00110-of-00163_text_512\n",
      "cultura_x-train-00111-of-00163_512\n",
      "cultura_x-train-00111-of-00163_text_512\n",
      "cultura_x-train-00112-of-00163_512\n",
      "cultura_x-train-00112-of-00163_text_512\n",
      "cultura_x-train-00113-of-00163_512\n",
      "cultura_x-train-00113-of-00163_text_512\n",
      "cultura_x-train-00114-of-00163_512\n",
      "cultura_x-train-00114-of-00163_text_512\n",
      "cultura_x-train-00115-of-00163_512\n",
      "cultura_x-train-00115-of-00163_text_512\n",
      "cultura_x-train-00116-of-00163_512\n",
      "cultura_x-train-00116-of-00163_text_512\n",
      "cultura_x-train-00117-of-00163_512\n",
      "cultura_x-train-00117-of-00163_text_512\n",
      "cultura_x-train-00118-of-00163_512\n",
      "cultura_x-train-00118-of-00163_text_512\n",
      "cultura_x-train-00119-of-00163_512\n",
      "cultura_x-train-00119-of-00163_text_512\n",
      "cultura_x-train-00120-of-00163_512\n",
      "cultura_x-train-00120-of-00163_text_512\n",
      "cultura_x-train-00121-of-00163_512\n",
      "cultura_x-train-00121-of-00163_text_512\n",
      "cultura_x-train-00122-of-00163_512\n",
      "cultura_x-train-00122-of-00163_text_512\n",
      "cultura_x-train-00123-of-00163_512\n",
      "cultura_x-train-00123-of-00163_text_512\n",
      "cultura_x-train-00124-of-00163_512\n",
      "cultura_x-train-00124-of-00163_text_512\n",
      "cultura_x-train-00125-of-00163_512\n",
      "cultura_x-train-00125-of-00163_text_512\n",
      "cultura_x-train-00126-of-00163_512\n",
      "cultura_x-train-00126-of-00163_text_512\n",
      "cultura_x-train-00127-of-00163_512\n",
      "cultura_x-train-00127-of-00163_text_512\n",
      "cultura_x-train-00128-of-00163_512\n",
      "cultura_x-train-00128-of-00163_text_512\n",
      "cultura_x-train-00129-of-00163_512\n",
      "cultura_x-train-00129-of-00163_text_512\n",
      "cultura_x-train-00130-of-00163_512\n",
      "cultura_x-train-00130-of-00163_text_512\n",
      "cultura_x-train-00131-of-00163_512\n",
      "cultura_x-train-00131-of-00163_text_512\n",
      "cultura_x-train-00132-of-00163_512\n",
      "cultura_x-train-00132-of-00163_text_512\n",
      "cultura_x-train-00133-of-00163_512\n",
      "cultura_x-train-00133-of-00163_text_512\n",
      "cultura_x-train-00134-of-00163_512\n",
      "cultura_x-train-00134-of-00163_text_512\n",
      "cultura_x-train-00135-of-00163_512\n",
      "cultura_x-train-00135-of-00163_text_512\n",
      "cultura_x-train-00136-of-00163_512\n",
      "cultura_x-train-00136-of-00163_text_512\n",
      "cultura_x-train-00137-of-00163_512\n",
      "cultura_x-train-00137-of-00163_text_512\n",
      "cultura_x-train-00138-of-00163_512\n",
      "cultura_x-train-00138-of-00163_text_512\n",
      "cultura_x-train-00140-of-00163_512\n",
      "cultura_x-train-00140-of-00163_parquet_512\n",
      "cultura_x-train-00140-of-00163_text_512\n",
      "cultura_x-train-00142-of-00163_512\n",
      "cultura_x-train-00142-of-00163_text_512\n",
      "cultura_x-train-00144-of-00163_512\n",
      "cultura_x-train-00144-of-00163_text_512\n",
      "tmp_cache\n"
     ]
    }
   ],
   "source": [
    "!ls $data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: /workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache//*\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $data_cache_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"        SSTV\\nHome /  /       \\n                  ()          ,      '           -                        \\n ,                          ()              :\\n   ,                       ,                 ,                      (            - )                    \\n , '    ,      '\\n       ()      .   , '                        '\\n    '                             \\n                                                      \\n                                  \\n                                                                \\n         -              \\n                      \\n   '                     \\n                               \\nPrevious        \\nNext     \\n               \",\n",
       " 'timestamp': '2021/08/05 06:44:57',\n",
       " 'url': 'https://www.sstvbd.net/%E0%A6%AF%E0%A7%87-%E0%A6%9B%E0%A7%9F-%E0%A6%95%E0%A6%BE%E0%A6%B0%E0%A6%A3%E0%A7%87-%E0%A6%98%E0%A7%81%E0%A6%B0%E0%A7%87-%E0%A6%A6%E0%A6%BE%E0%A6%81%E0%A7%9C%E0%A6%BF%E0%A7%9F%E0%A7%87%E0%A6%9B/',\n",
       " 'source': 'mC4'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00001-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00001-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f534a366c80e43d88353a5d90aa0e26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca445e84110440aab5285823b226467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10382 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10348 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10904 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14530 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16483 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13772 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11607 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20566 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8250 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93872dc0d4ac474598e87ba8fd62b931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f432b7d4bf45b8b921b80860f001d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00003-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00003-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0625008a7e7a4d0db4fd0a573d9943b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d83a9bae4e4c038dd53ca6a1bcbb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9661 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8247 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9028 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9727 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8443 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11633 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14168 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15866 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10758 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11823 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10737 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9053 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15988 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12065 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892ed87527a4420687232d008877f903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e630a82b5f4e7c9c0464e5862d4039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00000-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00000-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72d8c3f1b9d477798a35c1121819dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6492be4cdda45dca3e4ad93dc5a5f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8464 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9185 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11758 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13701 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (105300 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64225 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13725 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022c8f99bc864cb3aa051fdd3891e31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c0856812b344e5851a3c793e607c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00004-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00004-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a18170a58f4d818c9662b587c55fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21806f4a5904cf692d18f0a755fbb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13882 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8892 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23059 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8665 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9034 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17577 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9199 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9440 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31047 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31758 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18112 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11448 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12517 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23507 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b60be3d3c4444d48051bce78303d9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde7df20ca7e47deb94a0cd690313da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00002-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00002-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa342118de64a3eb709da4b6832d221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f911a661fa94fbeb86309f92867f328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17533 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18747 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9217 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15130 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12279 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14116 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19635 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10914 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13990 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10790 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17783 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e68584dbf074bce86c269d4ba57b6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cb5d1ab3ca4fd9a1a9f1eaacbaccf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00005-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00005-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671a8d67d4ce42ec8db2bd796bf5a009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b63eb7ab944834b9883b461797ec56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10109 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10074 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (94334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9350 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14450 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13511 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10726 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16153 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23183 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9651 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177cd3725de74ba8a1918c57a59aee13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d6d918fe8947c89367f0858f86f82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00007-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00007-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a646d5271d5d44bd8cde4b05398361e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271bd6c13d4449449c42d85cfade3165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15015 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20854 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25985 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8601 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10184 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13087 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9015 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12746 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (51196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9736 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10702 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9306 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775187de1d1841aca40317266db47781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d4e4fdd9284d0193e158c6ac0bb8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00006-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00006-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb6ec84019f4450b61dabad3564ac0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98722eab0484f078d48ff55d0d7ab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8988 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8465 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9897 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9747 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9719 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24078 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13944 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12582 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (50634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9254 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8447 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117c6fa64cc647688954bb2574638a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005a023ab03b42be9bf53a065aeb3034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00065-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00065-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd7bb6f33814c2fb6274dfb67924dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2718b51fa6ec40d5bc90e515f7850142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8358 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13425 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8562 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9430 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8883 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17950 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33075 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11773 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10119 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc641e7f8294881a10c87189a98ff46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde8dbd860824023b62bf4dcad0bd083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00008-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00008-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a50b11ac6e84130b8b107cf1589c071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a093170a734ecf95e91819a13f24f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13305 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9222 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11006 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8679 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9168 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9040 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8991 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11453 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14617 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16302 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10541 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11235 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14359 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1541d43974b4ca3a91b5ad407413773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a416b35beb14dd283757d4166032a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00009-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00009-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69a8f84c95d499492cf1d3f12f3548e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1943a635744444069f1a6b7978d3ff2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8727 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12983 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9630 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14938 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20748 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11804 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18448 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18eb68816d8c448f960a4f9f9c4754a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0fe9b077074f72b570f275074169ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00015-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00015-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42a806cdc0145a4b2ea027621a647ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f200e75fd94d49dbb54ee1129c23ca84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8581 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9743 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8655 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20097 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9828 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30656 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10591 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14481 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17809 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8635d737e94b09a5c1f506149e38ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8233329bd7d2418896dc635351027763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00011-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00011-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a19e78d858a43ec8e2d09c43c1e3503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1bbeed4ad441a29eb732b3b74fd559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8341 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38202 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32220 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15012 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48299 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41474 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8944 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10292 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14793 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8738 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcda3ba776214bc1ae4e931ae17b8c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b518d2d2f447e89c3b1b4592e5a205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00010-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00010-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63482e084ae741219161b0bef1b03f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac139d1b29847c28b0ff8885ae87434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38017 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8822 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9981 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11588 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33085 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12904 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31461 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9897 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9363 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10906 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12708 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14450 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12550 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d153dfdb44f4c89ab0152457ffe6deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61230173ce80465393d961295a904e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00018-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00018-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6c71c302a047ac8a45a5693066a1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70c9b1a81e9459380cc464f792f56a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8220 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12537 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10784 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41743 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13691 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11517 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10703 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806d1b7aebf14887a6b2d43baebe556a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d1c02d088f484f9dab728e49f47b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00021-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00021-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51714a9fa3d042888c82848294838e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e479533b7441ff8056878762981edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12046 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8835 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13547 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9322 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9984 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22341 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12248 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10137 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17073 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (68324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24515 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081cd36ea1414ef19d05fd4f25a44b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935a2cfa2b2c4434ae815bb3fa850bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00012-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00012-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6711aa8ab7f04b13bc5b6d9b8b9dc5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3377acd63cf64aa2b682420a2b7cd848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8346 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8596 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (50544 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8759 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11266 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11880 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8895 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9505 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8212 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65c07df39f44ef5bdbd2668e4413af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be70e05609b43b39f1b7c0890ccf30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00022-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00022-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35af61a3690a4baea8e4c25bcf3f1195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690b3bd319464d64a902828336ccb8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10403 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14466 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8370 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10101 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18641 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15485 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11599 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24167 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34338 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28996 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1b18998335483f963cdddc1953811b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0039ec50b0a34dc2a3a668898d841dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00014-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00014-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce542398b928414f869cc8b3e920c206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa01bf71e2bd4e2f81aad807b98a67bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10211 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9577 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11928 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8894 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8562 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22492 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9659 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10320 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17667 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12553 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14897 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8542 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10319 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa49fc2adee64943b4ad219574e5b6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352e5f5bc3764c8790820d1f25fbb07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00013-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00013-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152d9b03231844de999ff4f6bec62daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd22b1ed1c804da3a2e212810324394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (99908 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13708 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13968 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13921 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8594 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10214 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11566 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11107 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64053 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21922 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12338 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4c295dd1d645258cbff23454e91238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0d27d297d04e14827b4bab431c7af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00017-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00017-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffad6a95d4342cba1308a7756850a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4498a56db5ab4425ab1686b50a4a9094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8491 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8426 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16018 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20027 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16097 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14142 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26498 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23095 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39238 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9812 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5ace6539c045d58541bdfdc71b2579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42598a5d676a489382da76cf0e29fe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00016-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00016-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc940afbcaa4b9e89511bf065fdb710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d3523fc38946ad98e7fa13e56ba6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9263 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11621 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8842 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16851 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15461 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15550 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (149750 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b02fa54e7744c7817a33bc170bfc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b24281980149188afee00b070de35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00019-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00019-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630b3272c8054d56a258b3cd037884e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c369fff605d4970b6794416877d76b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13443 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9193 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9319 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12808 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27004 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9553 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16937 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8426 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8932 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14428 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13649 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57f53e69efd43f9828bf9067dfb7388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6292baaa1a943408351b4361f4bf408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00020-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00020-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51ef81c4387423992bb0ebcb7382ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02db96e722454ad18277c4e1bb6a6d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18061 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13895 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16349 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8605 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9485 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13488 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (75690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9846 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18004 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8597 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10127 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11906 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11023 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12735 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91466596bc5f428d9c36fcfbfdd32d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04593a99527241e0858f51a6e244394d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00023-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00023-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f984609287e449cbad191c0b86df2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6e04c06fdb473fb7b8b1a7dd4bce9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10413 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29904 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8856 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11849 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19878 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13320 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12363 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35594 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99499677ba2349db9146784e9c8795d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afe6ae1130f470cba55a03feb56d0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00032-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00032-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf0049b26fe47099ab946cf4146a7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc3e3cf1fab46d79f5478da66057a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9673 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12937 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9859 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8211 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (85142 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9142 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20674 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10186 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30258 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6b0709360f4647a6328c88cf67c40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a272c7b6b658498d94528e0610a891a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00033-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00033-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c73660f6811496a986ce7d4f2549157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fda097033a493695ce1802069b92f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13805 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10915 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11890 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22854 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26748 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43108 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11718 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7492cb15938541da8b34f114fd0adf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e96a5aee2db44c79e74e4877a4b3e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00038-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00038-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f360fdae21a450980b74cd809aea16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b5e54fabca4b44b7153841f5ec14bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10071 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12546 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14087 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21532 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19495 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9039 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30416 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14601 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12981 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17160 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12572 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8419 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12927 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fc48232bbf43e9b0efc07e3482e8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fbd832fef34064b527cdbd39aadee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00025-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00025-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b803d8b4c917477faef9f134fe3acae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30dbc0f0e544cf99d9504154820135d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13227 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11263 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10702 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (130718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23817 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9078 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9895 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20605 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b664d052c9423ab281db6197115bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a031b77b2f429a99cfc03adcd678c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00035-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00035-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262b0cea66c24b0fb8ab35b6d5db55ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04b810fc51143598e7ae9a95afe1a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9549 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8601 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11880 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9233 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8950 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9127 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23608 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8482 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217a05c161974859aee7a482815ca93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1a1fd711c44ff0ae5c65bc1b3f8cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00027-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00027-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2de7e7a7b7647588fc1462710474150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dc7ca09f764f278bea6e6554d92255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10979 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10017 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17866 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9191 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10158 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10110 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16378 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9719 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11673 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6562ebc645432eb36db42b758b5f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c87290bfd24572940af7839aeaac80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00026-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00026-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3740e105c6cf4c27b9adb3315670c31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919ff466c8e44d7fafda2e94277a78c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9715 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9137 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12623 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12621 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9700 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10015 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13303 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24388 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8594 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8375 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7724d954219546208eb102c6d2b7e5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2726cc9352ee400db2279d71d4eb7f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00024-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00024-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6747e91e39134b188d5443bc43cc4636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfc55feb7454114832b9801a67a1f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14251 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9134 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8932 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9413 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17012 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18542 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16909 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11683 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18944 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9977 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9924 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d74e98ebede452698a614b623364cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76c5b46d7c34d589f5e37df1a206994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00029-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00029-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff9415f5cb741788f59770c0d1335d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca81e0d2b8634b4cbc05f9d591659500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9562 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8454 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9507 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11762 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12047 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37857 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10150 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8337 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9370 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9963 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20279 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11920 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10533 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abd7b8a6e7d4aea8d90cd3a5c1bdbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0226b8de33c8445d9f1985adfc7ad944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00034-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00034-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c295851aca44afb9fe55806a124e49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abfb92382e94ea4abf74233a938914d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26295 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16020 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8974 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10477 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15480 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10796 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10158 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8439 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (154951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11647 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35584 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c56528d8a294e58b23608b969ad9542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a8560716e74a93b9461cd951de0f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00047-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00047-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf29052c7d94b07b3361abce8c869c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0708d221b58e4551b73f19953def5c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16193 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32023 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8617 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10015 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8440 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11247 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19922 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9053 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8898 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8979 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15318 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa87b128d3f34314993348be6e17f4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448960de088c4df78e9b4cbf15caebc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00031-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00031-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b03ba1805b4f2a9d5a60e15d023b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4844da7b0645bba40ba06b61af044b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8198 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17535 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8521 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11456 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36397 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13027 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8320 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10630 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11435 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276ef49e40e04c59a23a1ec30d118216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cfd10265734556a00eec847a341b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00040-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00040-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7493fd279234bff95da688dd901a20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97fc6c927ed42d79a0c192e29cb951f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12078 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8330 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16633 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10112 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8405 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11076 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13834 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13766 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10203 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14529 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (90239 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c388f6ff4b4544ae07b50884084796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803b416dfc7442598df181764ba14fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00030-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00030-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50fc4d0302e459f965f4d689f550caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12c643f5d32481dbbe89028d2d19600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16036 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17450 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28153 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39648 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8588 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (155996 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10081 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24790 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab06f2435add4740b1d888c3457ca6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de5484551cd45149711493f8d468468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00028-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00028-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18364345d3c04190abfdc05a9ec23531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fec23d837740eaaa0e491cd2e452ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8303 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8464 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11421 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10829 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10836 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9911 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10097 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a54558fdc043ca85fb5723e20bc9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e305e6eb0049478c9adf128c23e89cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00037-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00037-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4050f4b772264163902a4e6cd83e1edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3efa96902f4426a63e379ea08d9bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15511 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11498 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15097 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13743 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11440 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24388 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (150295 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20360 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2542585df91749e1bd6d8512f7cd02e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde9309018fd40b5afdeb64d13250b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00046-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00046-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba68f8254a1478c819d77342f7444fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f297f0abe0f4dcaa8e9a2fee32d44b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18269 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8906 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10233 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8726 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9537 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9254 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28115 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11998 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13549 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10159 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd95c9fdf554db3a974b2a637f4f7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6e45ddff9a408ab4d5d716304daaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00049-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00049-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7687824887074c178e63a46140eac018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20865b4d3bd549d9aa619ebe50319be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15006 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8708 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11488 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14154 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9030 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8842 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12101 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10171 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15298 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8605 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49648 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9984420935425c8ec57faef0b5d825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256f92d6240e4c538460d4f22011839c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00051-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00051-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b719a65eb74905b3c6920deddcaabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b505a5ccc59043e4bef87e233598569d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34382 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8461 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11678 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15076 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12337 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12008 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17785 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12673 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14271 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9877 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851322325f1b433baca8605e6c9c00a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599ba48e408042c497f5b4a1307c6ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00039-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00039-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e258fd8126da46adbddc1444fe49ddc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7d8c402332449d9e4387c2fc6fb33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12562 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10168 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13605 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12891 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (124505 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9631 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10928 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47032 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bcb3d3fbf44f81abdf399458cc1b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c622afb992545798cc993a16c6636ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00036-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00036-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68bfebc3b20406d89847124a1f7f64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b243134d4c4d9e982a42d851c38018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9456 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12578 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13348 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9339 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23505 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8274 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9091 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (86611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18274 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13554 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9434 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64734ce291204dfaa0ec84c7a2a6a790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d2c0caf50542fd8568693dec6e4afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00048-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00048-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8fcfddb75748db9f9a94984885cb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011e468865d34d59a8482665c22490c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8498 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13805 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (63243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (65720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11575 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14792 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (176826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24646 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a239d1f697224f6f8eaf5bc3144db768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab40dd4297a4282863aef4f1f6027e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00042-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00042-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d067c987df47ed983715abca7fa9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014d5956105c44c0aa11cace7609d966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8898 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9921 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8503 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10089 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10240 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8798 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9816 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10963 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8993 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9527 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32020 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11518 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8218 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affa69123811435f942c099e41e7fd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86628595946c475fabb0fabba6cb5dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00041-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00041-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664aa08616654e439cc0a8d576d60c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bbf7ddfa0d48d4bec007e892bd7c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14679 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10426 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9918 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11691 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12535 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8400 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12382 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (60741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8439 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22698 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (61189 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe4d30f2bef4b8e9233b865391fdbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3335788bb5554e7cab9cc680b724d39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00054-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00054-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8299f5c823934c0dafb192aa096228e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d849fad85f554365a05f23f9f1883a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23683 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31199 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10578 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8728 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9908 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9497 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15570 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56636 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9437 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8724 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15685 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8962 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19541 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8623 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e05d41b1f14962aeec50f2f6482b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94305040c6d04c7592e30d09872dceb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00050-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00050-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af19bebb989468cb4f0ade9b8aaf007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4166ecc3abfe40b49cb140125d41aebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15321 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9866 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10423 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10914 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25517 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10784 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23273 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13477 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14341 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (165172 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c52274b9502421c9520553c67165277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcdb3a437fd424f94bed97f895d5ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00044-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00044-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e94ede3196a4c448ea1ea7318f7b813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a67b61d7ea740159877bdf1acbde768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9189 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9003 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9123 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10938 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17547 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14259 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10804 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10944 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58de074852704188bc09fbbd6eb46071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a8dea3990f42789c82bce81acae6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00053-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00053-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5732b8bd39543db8cd465cca8894d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc98dde4ab0e4d2c958f44ec9c4e6f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12026 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21453 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10403 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11086 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8377 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11807 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12181 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9886 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18941 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8420 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8854 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15016 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36005 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9319 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9b724fbdf341ed960dd9882bc22bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48de4e1563c14ae486a90619cd7cb840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00052-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00052-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6cf3d0e3304f9ca1601c5365c2a734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351a72ea96c148d89c444e1ec1db42a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12497 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13914 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8937 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10802 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14189 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19005 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11736 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10116 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11816 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8995 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9085 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0037b2e52a44219b926fc30313a7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cda31bc4cb4d7dacfd6e4e84af36e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00060-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00060-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aa963da01847c09ada450590909b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ce51da722a44718670d69db7d9a023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11423 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12572 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24216 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22453 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8253 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18191 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10038 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (98468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19221 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10804 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10895 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149ba9bd39ce4e828728ca0ba00f1652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81149bb56da140ce845bd60432b50fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00043-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00043-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261485f1006c4244b06dd99b0d673dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a02fbc2be884d398d107e415f2e4551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42448 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11998 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9554 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8578 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9989 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10154 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16881 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11436 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e5f28c9bb14cf1b81f6b0afe41eaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7059e898b07f416f8d6b0d4aeceddf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00057-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00057-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a00b67d9b214b3f97d32b6ad125e750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d54a3677e5420bba686af920efe3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14123 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14108 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11403 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12542 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10722 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8891 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8346 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10268 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d3e41e0ba34ac9b9e5fee18a71703e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83b402070c546ec8eb18b5dfc24a4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00059-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00059-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d32e8d668446f2997c0760aa2ffdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff3308a844e42be88885ddbf0b58e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23946 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11903 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9647 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (96037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16123 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14122 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14804 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8937 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11558 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10605 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8829 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10211 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556138fc64194b4c8a5c74b46efdb2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0058f407ed409a9abc27b414c1477d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00045-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00045-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70aeaaa80e214aae846c252aaf567236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e3285aa9744830b96532adde9d26ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20497 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18483 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10016 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11535 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9271 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15475 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16160 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12722 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffb040d31de4904a52488a9474759ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4369545c50244c6398d143d67f811a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00055-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00055-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca262afe6e644ad7b01ab2d7e2d0b191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b6652fd05b4e469336ea31fa02c131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9792 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (57199 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21238 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9808 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8909 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16952 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9370 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13984 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10795 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15303 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9423 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17298 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10662 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28b9ebf54e84d99a8cc1af1345a9b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5394915707464db278de2450df2a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00058-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00058-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383a197b7b094fcc9eec53ec7e5d8b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef831e3c2e6148638ea1459e81ef9a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10150 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11232 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8467 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12097 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24655 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14491 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18071 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8768 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44502 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21485 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8382 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10264 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38701 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9942 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981c0805385244f2a108b403c5b4b9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566a4515e59c4aada8cd47e973690f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00056-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00056-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a402407e379644e5aa8fc50edbac5555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d1a918d66b433088ea80c24e685d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14366 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14197 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8542 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19135 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10750 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12977 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9583 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (103213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8626 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10490 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (71514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (112838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8651 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55788e5dd02a4d02aba8c4c3952b2ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393ee73f3ea14b9186993ad8b2720292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00062-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00062-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e281990dd91412ebbb430d10a472438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca1c3a1af1b4a3496ef918f19113cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9599 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9729 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8748 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11939 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12529 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8665 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16107 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15828 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20083 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13932 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8527 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10026 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8970 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10692 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b88a4e421943e494c3f91870464cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb42482156b4427beaa1244af23f171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00070-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00070-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe676fa725441ab9ea169a0640f666f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190c63a3e62e4ecb9c69b1ee90064269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8303 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12871 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14251 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62700 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18659 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25292 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29225 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24183 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11030 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1eb96b25334fb9adb1762cb8c013b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba147bc60a84fa39f6072a631c81ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00068-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00068-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3218739fbe8e46239644b14edd8e898a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acff8d21e03456ab230944d1d050966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12413 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9321 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9282 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8762 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9808 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10259 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10626 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (59722 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9974 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9022 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d0792fe29f438bb5b86de7492585a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36820002a81a4ff4a33b0b20ccc65645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00067-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00067-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20424cb9df424fe597655280babc4e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc63251acc54f8483980e4d5bfc5f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14122 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15023 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9144 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11344 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (61613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10130 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12005 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10967 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11467 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (186663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (82837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33454 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4bd695d2f04869af4597fddaabe62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bfd50b363b44dfadca1aec635771a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00063-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00063-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8a9e0aa0c6414c89bc768addcbf0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4c1b6528d342c3a8073daf6013c83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12903 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10858 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9397 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10623 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14337 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16558 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8322 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21536 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8580 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14005 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9107 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20537 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9939 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8859 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8233 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e79b92f7234927b82069197b30a50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348b3c66b0de4288bd4fdaad2b248e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00069-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00069-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08acbae950914038b7d78ffb4be180ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050d1b842a674d2da513941e5d83cef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12143 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12050 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14393 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9167 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12269 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10828 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18834 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d44f7c6e3234f7f9be16c25854781b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b4b8363bda49e38bfd0839a62391ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00061-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00061-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d81b490b364e28921ef6880252ced4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65887cc52c714dd5b6674a04ccfd091e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9626 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11746 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10993 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22989 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (98754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9443 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10951 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6db8c77a6149d6883007578881286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7a420e5661436ba2f80dae5a954a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00066-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00066-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18ce7260f834b86b30f099867afc14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11dc1a7d7444cda88616387f88ddea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19882 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16582 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9998 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8580 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16490 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8859 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19030 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8580 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17071 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3634cb6f35f40ecb51e62f748b46262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c22fd59de9b4b64adbab93765acc6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00064-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00064-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143adcccfee149bbabc833382038072a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f898281bffd142c3b1d1923b4fca8cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9227 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9292 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9473 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9532 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (140834 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9312 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bdcacc4155424da681acc19d5f530f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacc0ac6fb5c4da8baba86547c2ae845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00072-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00072-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3a5630c34e40209d5b3b06d0c575bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effa5a1614e74c50850fa893297bc2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9144 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14181 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8983 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11106 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11475 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18075 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11295 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8656 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3db0639b104a21988e3ab4daf770c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b8c80d273f4090a3cb2b4ad6a7faa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00078-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00078-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8534a5ff9e8544fba2837284a15f092a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e7245b3009412392167ca2929833db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10765 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9187 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13968 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9667 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8914 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23889 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (68951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10419 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9202 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9254 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8492 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10489 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2405755e395d4bab8c6943dc5320547e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e7354425242dea00b9db17816b97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00081-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00081-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b5d2983d454779bf103d9b7b746050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73af9755a9074e1fa3da9f2490f13033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8858 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10851 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9059 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35228 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10674 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8977 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11260 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11202 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8881 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10818 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce56449985704b7eaf204f0b8b42ad4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4763410ec2aa4addbc6a872c6248bead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00073-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00073-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9ca085f32a423fac22ecd8b7fe3a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c513b4a75df74f649e9f9cbab71c4791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18289 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14924 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16926 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10240 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9597 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16798 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ca6d295c4347bea52c34513b44cb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ad2d1da7604c6faeccd0cf80f84960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00074-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00074-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c67ee569b64f149b6302f61744647e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb9c6ef5f82479aa57b5a8731e21bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13068 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9388 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13507 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12655 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16459 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23517 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36679 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9483 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15101 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10909 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10960 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12317 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09102218ae0646c8a02027b29a3bb6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffb4afbd7f745108b4a33f2c137ddf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00080-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00080-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a5d6b435ff42dcbca03517fac37661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32de6f78e179495997c8f05906d45306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17583 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27647 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9701 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11465 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8672 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10179 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9305 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9255ae37f21d4275a16163415be43ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3471333094408c9a391a92c18eb394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00071-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00071-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1912ede644464b893694e11cac0fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445d5c13c72e4776834c24969d79bfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12763 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8470 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39677 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8392 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12012 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9891 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9932 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12529 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9308 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249d41161e9f4d0daf0f5f6b12f4cb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096768035f5f40c7bddaf2f0b80da5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00075-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00075-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed33f16452e42b49100d016d8d783b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61408217ed6b4bd196dcdcc368cc5ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11004 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10530 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9047 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13515 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16001 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13594 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13006 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9491 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9677 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10979 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bde70ac54c94b5f83e8dc38d42bd7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb818bbb9d4408daba7724bd174c049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00076-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00076-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642321fd2ebb4e3da3aa94a5501ee94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133fb0ebdf2f47d4b3d7309a9c101663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33746 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12469 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14168 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14686 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10007 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16607 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9542 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27bad5c3d804710a3d9011339dda7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e9a404f6ca4b8f9e075399264f05d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00086-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00086-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58aa745281ee44fabb4f3a7a4e9766a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d3369cabe6401092f96744a306de27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9154 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12101 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15071 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16473 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11118 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea359c7138145f58f15f902a6426854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86695ce3efc24c20a5fe36a368a4e011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00077-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00077-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c8251635544f87915055c9045d098c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00a1203d636469eb14b58673cfe8e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10480 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17938 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10835 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9400 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9397 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8228 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41589 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8880 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13039 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9072 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32163 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37048 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75fb229394c4aaa8dc5f1f0f60955ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdddfef842046eba355970b258d1a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00079-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00079-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad3ed7044e5418aae0d9b668e799643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32472bc52d654b69907701ce302c3dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8237 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8547 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (138923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18127 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10137 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13869 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16137 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11266 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25553 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14993 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11195 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b3a1f816d34accbf4dbb46097bb03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ea55facaab41e8aa2005353ea2e32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00083-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00083-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa36ff0e08034a70bebc8da51c449c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a804d72cbcf5457d933490a813247ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11928 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9282 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10122 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11071 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10247 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9836 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14974 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20829 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8319 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10171 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16128 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93560c2b1b54b66a0470315a8969d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d527fd9e7ff4820a6aa48d0409cb007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00082-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00082-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94f8d1534734716b3edf5c8b2c54270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e31bec575da45928b1d0d44a6623eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11991 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8927 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8915 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20153 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10472 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15214 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9154 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34157 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21698 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9470 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13140 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10768 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9326 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf782ef7108438aae74a0af8bd860da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ea98ba11304c379fb143f6e1e47d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00085-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00085-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751a2e1408fe43f4abb0b9a01c8e36b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad29beb59a214b4ab737a5a69a33d7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9857 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36085 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8282 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9456 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10047 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23298 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (54490 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9348 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7c49f364d2467da2ca9908cab8acdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1683394cde54697bcbc6ea4df803756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00084-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00084-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb827545b4b40caa2a6e200566fa217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781144f2903b4d28960e9c977b130692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11414 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8661 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12962 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13020 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (67625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9530 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f58e639e77414c89f4b852b7046dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9126f1c237bd43178ac0ce175a475002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00088-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00088-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1d0e1cff2d4fdbbac6fcd657a47b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338292271fc2473f8f4854e0b29756fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8349 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17778 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8221 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14894 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10636 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29423 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12113 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f9c6048b4d476b8fe37546c8ca0099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d564ce28f0c842ef90bd60db235918d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00087-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00087-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26832f24239c4014bd10bc15b363d21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007bcc86330d475aa61b6b2199b754e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9507 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11456 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23338 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9996 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19618 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10567 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8898 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12724 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8762 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8373 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3850de523e9a4e8fb4cd95d508e27785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ac5fb02112453981515f979a6f747b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00089-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00089-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ffc71ac34f42aa83a06b81368bc17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15206cd38c66467f99ca4403c9020dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10339 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10016 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14282 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12846 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16121 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12544 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13185 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12120 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8470 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8706 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18906 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c03df50ac6b42ce93608af2bc39555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76d5f9a0389428da4e96773647ee72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00090-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00090-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388cfe5e92884e3da151e351c35a3a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b6534b5c06498c9ad68311a21986d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9129 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8988 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9273 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17293 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10151 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18798 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11091 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10728 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19781 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18453 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16637 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5202311684a4a10a71f334eb796c113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae01d17bb6074049a4d77dc077658919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00093-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00093-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966dc09a3dbe4f44828109e543c404d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a474b2590b8a45929e35b5d2710bea89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16225 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9163 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11728 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14882 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8214 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (90751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12772 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26170 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8493 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4232225c0ac2494b81159d22e27d2092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63912ff3af0d44f8bf504c79383fa3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00092-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00092-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069f82f499fc4698bf11b9982237b738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d448c31482734e5d878c4c2713264bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13321 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12750 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13210 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9591 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10787 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10465 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10050 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16452 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd9ad6e2d43472288a67ba9609dfb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4504a9413634628826713534dd82cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00091-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00091-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37a8016cb4e4a9289282c41e8b35541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d257b8020744b24b772d9506db65e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17835 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8822 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8858 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40784 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8484 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41186 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15260 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9975 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13703 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaa9730e80245b1be6d5faf1cf166d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f89dcf6b244da08fd98136de269280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00100-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00100-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e4c66f9df041f3a24c270fef6da96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1038a57fd094fba839a1adf52698f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39481 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25527 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13477 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10889 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40679 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16211 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8816 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10834 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9144 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841c1b52396a4b5f9e7a6fab131b1781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c8b5b1d19f4bb6bed815a57820419e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00094-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00094-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46512c8a44d5472b93f228eef5bb5956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56dea201bae543b4b6834180f02afe72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10722 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9040 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13171 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16392 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16207 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17921 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8594 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27095 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8515 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35217 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33393 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26946 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11be80372e8c47e2a51814fcf78d418a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc68c3a52d64ffca3bc22e45404b4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00108-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00108-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51df1f5d38440e49b5453602d8e3405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2b7af64801403d9b4a74c4337f4437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10233 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9984 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8437 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9488 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14979 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32638 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9320 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2defea3c96c740ec80eeb80ec61ebefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1f1d8512714154a52ad4d470d8b54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00104-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00104-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f36781452f04ac2b3726587a5e10e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede5fbab43e84d33a72c84d38e7d55cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12550 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13349 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (122212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26078 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11995 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15106 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11350 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23903 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12087 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b606286eb08460f90c9dfa25c29074e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3968dd0955cb436e9f6baae1c0d51bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00096-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00096-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45163107b9d145d291f485fccdd34002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b805372083d6471c886fb983bea728fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26990 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18073 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10496 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8674 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20849 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16254 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20008 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10778 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8572 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8633 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27588 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a0c04f3e494fec9ae978efb9f7e467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42af97d1906d4c5d900ebdb7ee34feb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00095-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00095-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3834c52ac3ef4a9ea01069db55abb80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7115031174d42f9bd9d85a3d8445808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16530 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20996 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17500 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8405 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9474 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10575 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8967 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11729 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18623 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35028 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18071 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61663f0a20714ab685646b4e8cad00bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a01ef0fdad4764a7f131829b70e84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00098-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00098-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ea9d9a9107422da869c494d70156cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b2a648d0aa4b2384087e71115d594d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11781 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13607 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (70190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8235 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (123155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8521 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24572 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10585 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8349 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8430 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eded7e3dbc147e297552413ac4959be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bd52a40eda4adea09ac0c592204985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00107-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00107-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cf48e93a484a99b33841d5b8367c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef714dded1448dda758ebc16d1ef47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9496 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8811 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10309 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9521 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9631 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13302 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (53471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10081 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20621 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d3699ad7e948278652a1f59d2e0ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2de367b795492eb5697784e1eb0ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00097-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00097-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2f785ea27045849653dd54b4dd2526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1604d500dd714e69b60bfe40688b9d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11703 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (46208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16995 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (66693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9509 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10197 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22798 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8509 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9157 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9004 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13320 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9591 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8515 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ead6e96352d4e6a922edb83ef3a3023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354830c1f2004310a4ae4ccbb310432d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00110-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00110-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf23751adc954382aedb6c042f645f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4db4cccf7ad425f8a622cf6958cee51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8630 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36747 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32626 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9763 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32618 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14140 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27116 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12823 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8430 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9095 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11869 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10214 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9079 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11112 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (112714 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24361c254c314e0d95d122e85c713067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e0bf2eb17c4917b106b240bc076b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00101-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00101-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6e193c6a0046bbadba15e0337b1379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a165eed4616d4cd6b386495d99653ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9108 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8339 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20350 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8460 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41722 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18419 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9924 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18672 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464f0ebff6b344b99e15080cb5a97543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190994bf9ec24c908635553330411ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00099-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00099-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f07a85f2d434570b18cb64813509bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ff8402b84b468783e7aa885aee6549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10091 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10820 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13886 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8195 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dd84931b7f49abb54c9d23645521ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f87c9163864fbba8b6f23645831772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00109-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00109-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7e6898be8547a39a2ffb92c402cbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abf637236b8487fa60ef1cc271bb362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18573 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9715 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8785 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10046 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8890 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11582 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19235 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8344 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15440 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8621 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12577 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8248 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8998 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15871 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14758 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37779 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72c3fdd8f4440a0b79852735bd4e89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e38dd257ca4576a1ee46416787460a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00105-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00105-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6015d44123c4395bdd0bcb954db3bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6375bdda6ce74a3c93bb41620ba3046e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22582 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19985 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14969 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10129 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18856 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11796 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9253 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10563 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18391 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8197 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8265 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10436 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba09094b153447a951687deb77ad6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405e995e8cb04af8b7de7613941fb5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00106-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00106-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9e910e2f664907abee104b2129e7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25c83384b2e43609cdc2385caef2f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11880 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8397 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9538 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8510 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10073 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14460 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (66116 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22012 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8890 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45645 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8742ad9a214c168615f664029a9839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcd36cf47c24406ae54e90b4e043746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00114-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00114-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd2184d50de4137ad9926d5d3223be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e327d8321ca4b0fb49daedaaaa3fade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9635 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12510 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9521 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21999 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9377 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9179 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21979 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9456 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad8e4de549b4aed806b17b1070ce191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a51a4ac202f4ca4ac8929f7839c9e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00113-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00113-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81a2f737dc143d8adf95e494269ef37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3337d3a3b1d44165be837beb4641f7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16469 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8448 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14302 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9121 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16350 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30129 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10092 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1106232e504f4f80bc7a69a2f69c2689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea454275e1d4ff3ba441311fbe1c8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00115-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00115-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e975dcc91924d18a3dfe39739154139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7dbf370e6d442f8323d9f479fc2e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15519 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9321 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17949 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17121 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23242 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9911 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9189 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (76734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38528 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8558 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9223 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1e97a0ca244e6c95c64ad7d9965685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7a00c03fc4437f986f9f2a98302fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00102-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00102-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d3a31e3918432c8f6df7bb329e8bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01e1432daa449f594efd02b650348a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14567 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11829 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13515 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10126 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9240 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9089 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11403 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8503 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9038 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29121 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12544 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16759 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30003 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11891 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8750 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9126 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f8fb6e353f410688e05ae04949b71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a84e97194646dca2b4d900c8914851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00117-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00117-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cedad141d0c4db9a2dd5554e8dca23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68536f2ee83a4850acebdbbf283590e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22001 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11126 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9585 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12467 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9050 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11484 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15862 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d830e447dc374bdcb3f7616a09348bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe93188538fc45d1822661f8ec6494e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00111-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00111-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6d7d755fbb440ab892be1f2b1811b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce71af360a614394a3d44ad85d17b35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42686 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11768 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10581 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33358 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19926 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11719 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13941 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20228 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8477 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10527 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9975 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12104 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ace3ef4c744c00b7168aa207a58c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03abe7d025a44ca9adf5a99242fb15e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00116-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00116-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b480520ac6634b4bb54db436a8118d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34b1aaf70a7407a8934ccd7668dd41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11053 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16105 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18098 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12895 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15530 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10112 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14307 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11210 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8989 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8237 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15609 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7346863f38714ffd89975fa01f4cbeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd951083b7f4f9fae3e954b18be3358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00112-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00112-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbecf592f914a7d90fee7a22333c308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae12f373e8c44908855dcfc9aa436b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30163 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8363 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8778 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8263 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9820 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8797 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (54243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36080 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16608 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9215 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7373eeb489c41c186b042d05c0b8f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1784da3bcb844449fc2b9f293136cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00122-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00122-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e321c906ca44b00aea787612cb8184d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026c1080996b44fd8ca0a4aa5f165d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37018 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15984 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14061 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11260 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8537 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (65684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20295 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55631 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10203 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27072 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18662 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6567a67dc6b641f180aeabd446d1a5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96644bdc57404150b034c8792d0f6f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00103-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00103-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dffe6eed804915ae52a2ca8ca30dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd91b5184fe34f1d99d7a785864cea43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8222 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12525 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (78142 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9253 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17234 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28498 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10087 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21797 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9091 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (152917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9964 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9628 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da482673ad44e6fa64d3473a702390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f9852c2684481ba5b17d2165ed8158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00120-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00120-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0e88b874fd411bb42760def6a9de88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ee3b2fa1b344cc9e1e88518c8c52c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15469 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23203 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12144 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8203 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9302 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8904 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14307 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11797 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14105 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17110 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8727 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9181 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8638 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26478 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a0efddf77e49769c07425594a215a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef81dff65a0a41d8b57f9ade40fb887f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00118-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00118-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923eccf9988a4e2596b5c42d6e64647f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d3a969b1e942c8994e79e25a7b65ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8946 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10001 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8737 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16469 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26473 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14785 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9026 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13748 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffe1b44490747ef887705bf8cf0b828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10aa8adeb7854d68b818391d30354e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00121-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00121-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3667c7f8e1aa48f2b42b5dad1af0d022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9ac35f939b45e0ae12dd7625ffaeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17796 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14854 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12228 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16784 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9428 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8918 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11981 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8617 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16127 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61182880246144449f5230a36ef226e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26cab76ba9c493b979a83c2d9452470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00125-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00125-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a7ae4ab48a432abe5163e87433d0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734cfea36c8b4c189650bdb8ce9adef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23392 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14538 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11976 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12461 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14963 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10999 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11085 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19950 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8915 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27414 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (105166 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2817bd730f014fe7b06b0616aafbd193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c98b854e29049fb8fbcd9607124d8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00123-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00123-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f59d49ed05042f6b83ba371b6d22fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b6fdf002dc4a7b914132e016b707b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14083 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31849 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12797 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24216 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8763 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12152 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10861 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10072 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9920 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13513 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224ea7582a9642acaa2227a056d877dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2dc75c1bf04feba6c72838edafe095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00124-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00124-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a56d288dcc454cabc868cf0b2edd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31234ce5f3b94d449d086f6b871b693a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9728 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9862 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9171 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16086 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8307 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12946 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8907 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62ae1ecb50c43118724868f1f92338c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a790522f71704acf8765e3399c684c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00127-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00127-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e11e850f074eafa241b34d26c182d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8abbf4f74fc4fb48c2e0831461b0026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14502 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10430 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11481 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9563 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13763 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14081 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (59726 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10526 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8817 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (89673 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8533 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34186 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9204 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a8353229124fbb9d2f470c981759b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b654160da34e9cb12d32225ef5e443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00130-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00130-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3504da4d52b84a528440fae919983cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d3332480f14bc1b1636d9092992791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10046 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9862 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9426 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14151 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11474 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10098 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9820 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21836 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13990 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9685 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (60058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14588 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44944 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (159393 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe3cba9e31342098bd3618459370b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b3a937cae7436893e93881cd45a7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00131-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00131-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9354ea245ff94c2b9e4463f60c8840ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dabd28d39144b495787f40e26c1bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20811 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14344 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10975 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20920 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8793 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18708 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8851 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8536 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (83859 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2678d60fce9740b1a8b615f8ddc4c5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e8025d5a204aabadaeaa31cef88db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00132-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00132-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5e8fe3d2204303911de6977d1c5ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3507bc04f54dd797f74a47870ee4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26046 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10186 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8659 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12536 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8549 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10828 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10993 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32871 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9778 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19330 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (88208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10337 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bd02d59f934b50bc02e6974910447e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75827a98467b4919b2f7c98a5a695138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00119-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00119-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b93de97eac42128d0baf7add3a06a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f22f65fe3974a2392aed66a45d30ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10886 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10061 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10279 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8519 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10264 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11307 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21007 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15746 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9378 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (190204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18153 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fe7794a71546a6b1132b9fc78f1bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3797c179954d7b85227236096efb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00126-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00126-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38042fe4038408c89960ceaf7e2919a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e08c633cd040a6ad302d08dc94fe0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16665 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13939 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14772 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8507 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39089 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11087 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12798 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18617 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9158 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16962 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9939 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8601 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13225 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12313 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79be34f8df04ef1b63686b384204cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aad1e3c57d4386b34bcc5ce26fc345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00129-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00129-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574b6fabc2cc435e96a26cfa7840698d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88973617b15f4c2c831ade61ec589611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13003 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14970 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13299 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8266 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12110 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12928 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8511 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8773 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33877 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7148dcfc514a467585e53712a2aa1772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08f4e0bda3c49d5bfe437dc35d9c5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00128-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00128-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2b4e3d3c534a38ad33aecdc156ce94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240006153aca46868ee367d867258867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13143 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18028 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30485 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10748 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9983 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17152 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14511 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20525 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (75903 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9908 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10559 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2384f2305c884f7b869143b728fd1669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3524a8ea0d8a4b6fbf647f24840b5e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00133-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00133-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b7f8230f574dc3bcb89a4a56673b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff1f45073754582b089407d5ece2c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40470 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9027 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (54193 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10970 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9227 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14110 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11795 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38497 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12088 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12388 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8866 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21761 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21698 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16856 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8274 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6844dc6776a045b7a012c5020e9ec56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7e2e5d40c546708704618c2f7b7700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00134-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00134-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71b6b82e5c946a1baa6074842164a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83d63614560471ea0858cdeab4dd710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23792 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12220 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9088 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9074 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10685 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10500 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9949 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (46427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8217 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dae54f81444288acb38e5967cc4bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad9161c677c4b29910482533b9cf393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00135-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00135-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6632e368a94357a4f7772e6c9cb2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b27cefa5934403953a5a8bb8b249cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/54922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (46268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11921 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8630 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15319 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8920 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24737 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (145971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8911 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15677 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15511 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3dd37238054efc94f24443f8370c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/54922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab21d9592270448b9914c8efcf09ee79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00136-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00136-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df784cf903724e2196bcf68260b3024a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8428b072ea4b5dabdee47ae4c90b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12018 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10475 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18068 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13698 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34968 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17210 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13348 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10909 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (170011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (50479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11271 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11820 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3270c8087af64dfc8cb45c178bf64b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d66d7832c8a4a078040ff15fc3b0554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00138-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00138-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc0197d0f8441cd9306a0225fab39db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f07d12dd0d743d184c2433d3d92132c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8856 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14702 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32251 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21199 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8908 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24211 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9074 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9017 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15702 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (46755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10106 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8509 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8816 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11312 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cce58de9f4f438f90a1340cbe128f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5cdca54d254d9d9570c0d7e60f4152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00140-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00140-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4633319431e34a678e1e9704d17853a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428d9b8f0cce47d09be16ceeb22022ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11264 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21983 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10183 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8217 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10279 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8871 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26500 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8299 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38072 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16304 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d47835c396a4a9389da08ed53ae1884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9575ece4add34325af0d7b5dc0bef8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00142-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00142-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd34f5e381a149d9a70b36debd258c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c0ac0c3dd447568bcbffc8807559d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15500 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15222 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15135 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9488 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8366 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41703 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8835 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (78817 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11115 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17242 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8674 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19445 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50312907228545c4878c948a25ba636f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63a7c9e5238426ebb51d516ad00d916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00144-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00144-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88374acf59d14791a115d5629ef63f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364799b4c887451799ac01717308163b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11319 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17428 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12703 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18420 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9583 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9960 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9795 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11185 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21544 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15459 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11805 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11025 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357c011cf37542019e1a403a2e9ea3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a557af58df4fec86e323e0d9e08b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00137-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00137-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e276be791d5a456483518883972fc82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d2fe51b1004be78cefdfb049639529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11346 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21036 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (72352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10465 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10240 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8573 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20122 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14995 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14183 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22647 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32541 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8878 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6009cf3eccac4203b7e8e388891dda85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb7f19f71d444578f0d84ddb19ef765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00145-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00145-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17886d6adf0b48b198c705aaf8dd6d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d026654b43c4407887b4fb4183fc909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/64000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14881 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8259 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9567 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10726 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12583 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12991 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26059 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8264 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15546 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c100b2b84e1476694212ca6af8c5008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/64000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2056eccc51d4c5b931e6a48625e7fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00139-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00139-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c6954caddb4e35a2d790afe0f39c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdff16f77bfa4ddabb25cd53dba8488b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15016 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8221 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9437 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (68170 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13886 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20633 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8724 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15567 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15553 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18492 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8421 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (145205 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b211845c01e4adda0d8ce37846603ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d287f67782c24c40b580e1d5938cb814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00141-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00141-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925c10a6cff046e1b43a1b31871719a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7976730ba61142f48b30079eff6d7152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13007 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30309 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9883 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14706 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9460 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15259 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9552 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11220 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9176 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19056 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730c7783bba248718ccf782c88484756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8732d496f04c4917a46dafff74eda0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00147-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00147-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c02f0e0b5146ccb8cd6e6acbb49711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b75c097fc344e129d944353f528846b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/67922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9484 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9768 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (116431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8805 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11273 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10007 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11305 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (82771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9170 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12158 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2e13e3616a410b9e5ac41cf5711961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/67922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cb0522abe14557986339554fed25f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00149-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00149-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71065b0d5cd74408be05c72cafbaacfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299c89ecfd4e449facca52cf9052c580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/102000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10126 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18807 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42014 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8393 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9050 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9989 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41115 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12421 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12104 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db71719ba82a435eb8b0563f5ecb9a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/102000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c24f93424de426cb6c4808400741352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00146-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00146-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e36f8610e4c4b0182eea2a3be3fc55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589429c22b46402eaa7e526e74ee728b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/66000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15263 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9265 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13253 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28363 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15550 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10859 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9157 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (70740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8274 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16298 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15597 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11150 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12950 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99f6d402f7f4b1cab8009ee61bbe5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/66000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c28c0711c34b1195032097dd5a466b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00143-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00143-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97aa76c162974d14bdccac86cb720ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c759ba23f49043f89de093f5d86b2026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10638 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8635 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18346 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8836 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64242 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12621 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (61367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14425 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12413 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9643 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c358dc8826c446098b7085c80066006a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1259f43cfb5943458553cf450abea274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00148-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00148-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fbabd2c3464179b73d272e260feb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e46d2549194fdeb4a7ffb041b1ac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/68000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10089 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12475 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8344 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10266 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8322 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18742 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1471ce163b824809844af012c33c1e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/68000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391881cc434f4d35b67eece345996a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00150-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00150-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4321e8bebf324ca6a418501ed596d1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b50f3c6db1b41f8884b6007dfa9f867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/121000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9762 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38846 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16305 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14495 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8533 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10563 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12352 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ec4d033ca84f69804def1d2f5a1966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/121000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991694f70bcb4031bd02c86917659847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/608 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00152-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00152-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dda75bb7684b1fb700e9f48a67116e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4956af1c08944e38e675325fdab5726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/114000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13198 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13490 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33293 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9430 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9405 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23846 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15242 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39578 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9991 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (144305 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27849 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9568 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e0ed846e6b4d8fb94025b61343a2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/114000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18ca2f8c7434aeaa6993f572be12ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00156-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00156-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf4ea33ab734befbdaafdf8ea2870ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4e221faa394bc1b28c4032948ee651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15554 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8327 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8570 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9184 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12918 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13176 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17289 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38074 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9352 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8315a08c57eb4da8a131989e6680f64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ec740223d141558f0e9a159dc45a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards): 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct<timestamp: list<item: string>, url: list<item: string>, source: list<item: string>, input_ids: list<item: int32>, attention_mask: list<item: int8>, labels: list<item: int64>>\n",
      "struct<timestamp: string, url: string, source: string, input_ids: list<item: int32>, attention_mask: list<item: int8>>\n",
      "Converting to match types\n",
      "Before transformation: {'timestamp': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transform_record' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 75\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting to match types\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore transformation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, processed_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m---> 75\u001b[0m processed_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m processed_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[43mtransform_record\u001b[49m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter transformation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, processed_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transform_record' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "lm_datasets = []\n",
    "path = Path(dataset_dir)\n",
    "files = [file.name for file in path.glob(\"*.parquet\")]\n",
    "files\n",
    "# \\\n",
    "for idx, file in enumerate(files):\n",
    "    data_file = os.path.join(path, file)\n",
    "    filename = \"\".join(file.split(\".\")[:-1])\n",
    "    print(filename)\n",
    "    cache_path = os.path.join(\n",
    "        data_cache_dir, filename + f\"_{block_size}\"\n",
    "    )\n",
    "    print(cache_path)\n",
    "    os.makedirs(cache_path, exist_ok=True)\n",
    "    try:\n",
    "        processed_dataset = datasets.load_from_disk(\n",
    "            cache_path, keep_in_memory=False\n",
    "        )\n",
    "        logger.info(f\"training datasets-{filename} has been loaded from disk\")\n",
    "    except Exception:\n",
    "        cache_dir = os.path.join(\n",
    "            data_cache_dir, filename + f\"_text_{block_size}\"\n",
    "        )\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        raw_dataset = load_dataset(\n",
    "            \"parquet\",\n",
    "            data_files=data_file,\n",
    "            cache_dir=cache_dir,\n",
    "            keep_in_memory=False,\n",
    "        )\n",
    "        logger.info(f\"{file} has been loaded\")\n",
    "        tokenized_dataset = raw_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=32, #preprocessing_num_workers,\n",
    "            remove_columns=\"text\",\n",
    "            load_from_cache_file=True,\n",
    "            keep_in_memory=False,\n",
    "            cache_file_names={\n",
    "                k: os.path.join(cache_dir, \"tokenized.arrow\")\n",
    "                for k in raw_dataset\n",
    "            },\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "        grouped_datasets = tokenized_dataset.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=32, #preprocessing_num_workers,\n",
    "            load_from_cache_file=True,\n",
    "            keep_in_memory=False,\n",
    "            cache_file_names={\n",
    "                k: os.path.join(cache_dir, f\"grouped_{i}.arrow\")\n",
    "                for k in tokenized_chunk\n",
    "            },\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "        )\n",
    "        processed_dataset = grouped_datasets\n",
    "        processed_dataset.save_to_disk(cache_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if idx == 0:\n",
    "        lm_datasets = processed_dataset[\"train\"]\n",
    "    else:\n",
    "        if lm_datasets.features.type != processed_dataset[\"train\"].features.type:\n",
    "          print(lm_datasets.features.type)\n",
    "          print(processed_dataset[\"train\"].features.type)\n",
    "\n",
    "          print(\"Converting to match types\")\n",
    "\n",
    "\n",
    "          print(\"Before transformation:\", processed_dataset[\"train\"].features)\n",
    "          processed_dataset['train'] = processed_dataset['train'].map(transform_record)\n",
    "          print(\"After transformation:\", processed_dataset[\"train\"].features)\n",
    "\n",
    "          continue\n",
    "          # assert (\n",
    "          #     lm_datasets.features.type\n",
    "          #     == processed_dataset[\"train\"].features.type\n",
    "          # )\n",
    "        lm_datasets = concatenate_datasets(\n",
    "            [lm_datasets, processed_dataset[\"train\"]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 57664\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = lm_datasets.train_test_split(\n",
    "    test_size=validation_split_percentage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data tokenization/grouping Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data_cache_dir = data_cache_dir + 'tmp_cache2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/tmp_cache2/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $tmp_data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:34:48] Energy consumed for RAM : 15.171420 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:48] Energy consumed for RAM : 15.209956 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:48] Energy consumed for all GPUs : 7.589710 kWh. Total GPU Power : 66.39070048466797 W\n",
      "[codecarbon INFO @ 23:34:48] Energy consumed for all CPUs : 2.828756 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:48] 25.589886 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:48] Energy consumed for all GPUs : 7.594160 kWh. Total GPU Power : 66.38837280417593 W\n",
      "[codecarbon INFO @ 23:34:48] Energy consumed for all CPUs : 2.836026 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:48] 25.640142 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:35:03] Energy consumed for RAM : 15.174549 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:03] Energy consumed for RAM : 15.213086 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:03] Energy consumed for all GPUs : 7.589987 kWh. Total GPU Power : 66.41549202147394 W\n",
      "[codecarbon INFO @ 23:35:03] Energy consumed for all CPUs : 2.829339 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:03] 25.593875 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:35:03] Energy consumed for all GPUs : 7.594437 kWh. Total GPU Power : 66.39995661989083 W\n",
      "[codecarbon INFO @ 23:35:03] Energy consumed for all CPUs : 2.836609 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:03] 25.644132 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "def combine_short_sequences(examples, max_length=256, absolute_max_length=4096):\n",
    "    combined = {k: [] for k in examples.keys()}\n",
    "    current_sequence = {k: [] for k in examples.keys()}\n",
    "    current_length = 0\n",
    "    newline_token_ids = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "    \n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        sequence_length = len(examples['input_ids'][i])\n",
    "        if current_length + sequence_length > max_length: # or current_length + sequence_length > absolute_max_length:\n",
    "            # Add the current combined sequence and start a new one\n",
    "            for k in combined.keys():\n",
    "                combined[k].append(list(chain(*current_sequence[k])))\n",
    "                current_sequence[k] = []\n",
    "            current_length = 0\n",
    "        \n",
    "        # Add the current sequence\n",
    "        for k in examples.keys():\n",
    "            if k == 'input_ids':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append(newline_token_ids)\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            elif k == 'attention_mask':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append([1] * len(newline_token_ids))\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            else:\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "        \n",
    "        current_length += sequence_length\n",
    "        if current_sequence['input_ids']:\n",
    "            current_length += len(newline_token_ids)\n",
    "    \n",
    "    # Add the last combined sequence\n",
    "    for k in combined.keys():\n",
    "        if current_sequence[k]:\n",
    "            combined[k].append(list(chain(*current_sequence[k])))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Use this before group_texts\n",
    "    examples = combine_short_sequences(examples)\n",
    "\n",
    "    print(f\"Input keys: {examples.keys()}\")\n",
    "    print(f\"Number of examples: {len(examples[list(examples.keys())[0]])}\")\n",
    "    # Concatenate all texts\n",
    "    print(f\"Input length: {len(examples['input_ids'])}\")\n",
    "    \n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples['input_ids'])\n",
    "\n",
    "    print(f\"Concatenated keys: {concatenated_examples.keys()}\")\n",
    "    \n",
    "    print(f\"Actual total length: {sum(len(ids) for ids in examples['input_ids'])}\")\n",
    "    print(f\"Total length after concatenation: {total_length}\")\n",
    "\n",
    "\n",
    "    block_size = 512\n",
    "    # More granular block size selection\n",
    "    if total_length < 512:\n",
    "        block_size = 512\n",
    "    if total_length < 1024:\n",
    "        block_size = 1024\n",
    "    elif total_length < 2048:\n",
    "        block_size = 2048\n",
    "    else: #if total_length < 4096:\n",
    "        block_size = 4096\n",
    "    # else:\n",
    "    #     block_size = 8192  # Increase maximum block size, ensure your model can handle this\n",
    "\n",
    "    print(f\"Block size: {block_size}\")\n",
    "\n",
    "    # Adaptive padding\n",
    "    padding_unit = 128  # or another suitable value\n",
    "    if total_length < block_size:\n",
    "        block_size = ((total_length + padding_unit - 1) // padding_unit) * padding_unit\n",
    "    \n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "\n",
    "    print(f\"Number of blocks to be created: {num_blocks}\")\n",
    "\n",
    "    result = {k: [] for k in concatenated_examples.keys()}\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        block_start = i * block_size\n",
    "        block_end = min((i + 1) * block_size, total_length)\n",
    "        for k, t in concatenated_examples.items():\n",
    "            block = t[block_start:block_end]\n",
    "            if len(block) < block_size:\n",
    "                padding_length = block_size - len(block)\n",
    "                if k == 'input_ids':\n",
    "                    block = block + [tokenizer.pad_token_id] * padding_length\n",
    "                elif k == 'attention_mask':\n",
    "                    block = block + [0] * padding_length\n",
    "            result[k].append(block)\n",
    "\n",
    "\n",
    "    print(f\"Result keys after splitting: {result.keys()}\")\n",
    "    print(f\"Number of blocks created: {len(result[list(result.keys())[0]])}\")\n",
    "\n",
    "    # Create labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    print(f\"Final result keys: {result.keys()}\")\n",
    "    print(f\"Final number of examples: {len(result['labels'])}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def group_texts_old(examples):\n",
    "    print(f\"Input keys: {examples.keys()}\")\n",
    "    print(f\"Number of examples: {len(examples[list(examples.keys())[0]])}\")\n",
    "    print(f\"Block size: {block_size}\")\n",
    "\n",
    "    # Concatenate all texts\n",
    "    print(f\"Input length: {len(examples['input_ids'])}\")\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    \n",
    "    \n",
    "    print(f\"Concatenated keys: {concatenated_examples.keys()}\")\n",
    "    total_length = len(concatenated_examples['input_ids'])\n",
    "    \n",
    "    print(f\"Actual total length: {sum(len(ids) for ids in examples['input_ids'])}\")\n",
    "    print(f\"Total length after concatenation: {total_length}\")\n",
    "\n",
    "    # Calculate how many blocks we'll create\n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "    print(f\"Number of blocks to be created: {num_blocks}\")\n",
    "\n",
    "    # Split by chunks of block_size, allowing the last chunk to be smaller\n",
    "    result = {\n",
    "        k: [t[i * block_size : (i + 1) * block_size] for i in range(num_blocks)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    print(f\"Result keys after splitting: {result.keys()}\")\n",
    "    print(f\"Number of blocks created: {len(result[list(result.keys())[0]])}\")\n",
    "\n",
    "    # Create labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    print(f\"Final result keys: {result.keys()}\")\n",
    "    print(f\"Final number of examples: {len(result['labels'])}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $tmp_data_cache_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls $tmp_data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_chunk['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[\"text\"], truncation=False)\n",
    "    # clm input could be much much longer than block_size\n",
    "    # if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "    #     tok_logger.warning(\n",
    "    #         \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "    #         \" before being passed to the model.\"\n",
    "    #     )\n",
    "    return output\n",
    "\n",
    "# # Example tokenizer function that does not truncate\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True, max_length=block_size)\n",
    "\n",
    "def tokenize_and_check(examples):\n",
    "    result = tokenize_function(examples)\n",
    "    print(f\"Average tokens per text: {sum(len(ids) for ids in result['input_ids']) / len(result['input_ids'])}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:35:21] Energy consumed for RAM : 15.178067 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:21] Energy consumed for RAM : 15.216835 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:22] Energy consumed for all GPUs : 7.590331 kWh. Total GPU Power : 66.73665050980664 W\n",
      "[codecarbon INFO @ 23:35:22] Energy consumed for all CPUs : 2.830096 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:23] Energy consumed for all GPUs : 7.594797 kWh. Total GPU Power : 66.63260887164563 W\n",
      "[codecarbon INFO @ 23:35:23] 25.598493 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:35:23] Energy consumed for all CPUs : 2.837403 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:24] 25.649034 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk = DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'timestamp', 'url', 'source'],\n",
      "        num_rows: 76000\n",
      "    })\n",
      "})\n",
      "tokenized_chunk = DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 76000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97514d8d6864e7ea3e8211bc2113edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of dynamic block_size (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:35:35] Energy consumed for RAM : 15.219193 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:35] Energy consumed for RAM : 15.180426 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:35] Energy consumed for all GPUs : 7.595027 kWh. Total GPU Power : 73.30418645994425 W\n",
      "[codecarbon INFO @ 23:35:35] Energy consumed for all CPUs : 2.837842 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:35] 25.652063 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:35:35] Energy consumed for all GPUs : 7.590577 kWh. Total GPU Power : 78.59936409585177 W\n",
      "[codecarbon INFO @ 23:35:35] Energy consumed for all CPUs : 2.830536 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:35] 25.601539 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "\n",
      "Number of examples: 1001Input length: 1001\n",
      "\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "\n",
      "\n",
      "Number of examples: 1001Number of examples: 1001\n",
      "Input length: 1001Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Input length: 1001\n",
      "\n",
      "\n",
      "Number of examples: 1001Number of examples: 1001\n",
      "\n",
      "Input length: 1001Input length: 1001\n",
      "\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "\n",
      "Number of examples: 1001Input length: 1001\n",
      "\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 1001\n",
      "\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of examples: 1001\n",
      "Input length: 1001\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "\n",
      "Number of examples: 1001Input length: 1001Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 1001\n",
      "\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of examples: 1001\n",
      "\n",
      "Actual total length: 2712695\n",
      "Input length: 1001Total length after concatenation: 2712695\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 663\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2882021\n",
      "Total length after concatenation: 2882021\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 704\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2780859\n",
      "Total length after concatenation: 2780859\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 679Actual total length: 2771840\n",
      "\n",
      "Total length after concatenation: 2771840\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 677\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2747548\n",
      "Total length after concatenation: 2747548\n",
      "Block size: 4096Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks to be created: 671\n",
      "Actual total length: 2802270\n",
      "Total length after concatenation: 2802270\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 685\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Actual total length: 2824777\n",
      "\n",
      "\n",
      "Actual total length: 2981320Actual total length: 2875976Total length after concatenation: 2824777\n",
      "\n",
      "Total length after concatenation: 2981320\n",
      "Block size: 4096\n",
      "Total length after concatenation: 2875976Block size: 4096\n",
      "\n",
      "\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096Number of blocks to be created: 690Number of blocks to be created: 728\n",
      "\n",
      "\n",
      "\n",
      "Number of blocks to be created: 703\n",
      "Actual total length: 2868648\n",
      "Total length after concatenation: 2868648Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Block size: 4096Actual total length: 2789787\n",
      "\n",
      "Number of blocks to be created: 701Total length after concatenation: 2789787\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 682Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2908091\n",
      "Total length after concatenation: 2908091\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 710\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Actual total length: 2919952\n",
      "\n",
      "\n",
      "Total length after concatenation: 2919952Actual total length: 2892831Number of examples: 1001\n",
      "\n",
      "\n",
      "Total length after concatenation: 2892831Block size: 4096Input length: 1001\n",
      "\n",
      "\n",
      "Number of blocks to be created: 713Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 707\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3011602\n",
      "Total length after concatenation: 3011602\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 736\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3043824Actual total length: 2893176\n",
      "\n",
      "Total length after concatenation: 3043824Total length after concatenation: 2893176\n",
      "\n",
      "Block size: 4096Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 707Number of blocks to be created: 744\n",
      "\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3001808\n",
      "Total length after concatenation: 3001808Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Block size: 4096Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks to be created: 733Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks created: 704\n",
      "\n",
      "\n",
      "\n",
      "Number of examples: 1001Number of examples: 1001Actual total length: 2881988Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "\n",
      "\n",
      "Input length: 1001Final number of examples: 704Total length after concatenation: 2881988Input length: 1001\n",
      "\n",
      "\n",
      "\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096\n",
      "\n",
      "Actual total length: 2894589Number of blocks to be created: 704Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Total length after concatenation: 2894589\n",
      "Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 707Number of examples: 1001\n",
      "\n",
      "Input length: 1001Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2982828\n",
      "Total length after concatenation: 2982828\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 729Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2858420\n",
      "Total length after concatenation: 2858420\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 698\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 663\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 663\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3027040\n",
      "Total length after concatenation: 3027040\n",
      "Block size: 4096\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks to be created: 740\n",
      "\n",
      "Number of blocks created: 679\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Final number of examples: 679\n",
      "\n",
      "Actual total length: 3077641\n",
      "Total length after concatenation: 3077641\n",
      "Block size: 4096Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks to be created: 752\n",
      "Actual total length: 3039541\n",
      "Number of blocks created: 728\n",
      "\n",
      "Total length after concatenation: 3039541Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 728Block size: 4096\n",
      "\n",
      "\n",
      "Number of examples: 1001Number of blocks to be created: 743\n",
      "\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 671\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 671\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 703\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 703\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 677\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 677\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 710\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 710\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2886820\n",
      "Total length after concatenation: 2886820\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 705\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 707\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 707\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 685\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Final number of examples: 685\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of examples: 1001\n",
      "Actual total length: 3012596\n",
      "Number of blocks created: 713\n",
      "Total length after concatenation: 3012596Input length: 1001\n",
      "\n",
      "Block size: 4096\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Number of blocks to be created: 736Final number of examples: 713\n",
      "\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 733\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 733\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 704\n",
      "Number of examples: 1001\n",
      "\n",
      "Number of blocks created: 682Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Input length: 1001\n",
      "\n",
      "\n",
      "Final number of examples: 704\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final number of examples: 682Number of blocks created: 690\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 690\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 701\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 701\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks created: 736\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Actual total length: 3022643\n",
      "\n",
      "Final number of examples: 736Total length after concatenation: 3022643\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 738\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 707\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 707\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 744\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 744\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 729\n",
      "Actual total length: 2860693Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 729\n",
      "Total length after concatenation: 2860693\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 699\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 752\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 752\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 698\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 698\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2814520\n",
      "Total length after concatenation: 2814520\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 688\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 705\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Final number of examples: 705\n",
      "\n",
      "Number of blocks created: 707\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 707\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2811655\n",
      "Total length after concatenation: 2811655\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 687\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 740Actual total length: 2985598\n",
      "\n",
      "Total length after concatenation: 2985598Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 740Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 729\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 743Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Number of blocks created: 736\n",
      "\n",
      "Final number of examples: 743Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 736\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 738\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 738\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 699\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 699\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 687\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 687\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 688\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 688\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 729\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:35:50] Energy consumed for RAM : 15.222317 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:50] Energy consumed for RAM : 15.183554 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:50] Energy consumed for all GPUs : 7.595304 kWh. Total GPU Power : 66.43220716778957 W\n",
      "[codecarbon INFO @ 23:35:50] Energy consumed for all CPUs : 2.838425 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:50] 25.656046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:35:50] Energy consumed for all GPUs : 7.590854 kWh. Total GPU Power : 66.42381201592785 W\n",
      "[codecarbon INFO @ 23:35:50] Energy consumed for all CPUs : 2.831119 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:50] 25.605527 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2838164\n",
      "Total length after concatenation: 2838164\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 693\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 693\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 693\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 1001Number of examples: 1001\n",
      "\n",
      "Input length: 1001Input length: 1001\n",
      "\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2771585\n",
      "Total length after concatenation: 2771585\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 677\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2858621\n",
      "Total length after concatenation: 2858621\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Block size: 4096Actual total length: 3135931\n",
      "\n",
      "Number of blocks to be created: 698Total length after concatenation: 3135931\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 766\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Actual total length: 2797422\n",
      "Number of examples: 1001\n",
      "Actual total length: 2878274\n",
      "\n",
      "Total length after concatenation: 2797422Input length: 1001Total length after concatenation: 2878274\n",
      "\n",
      "\n",
      "Block size: 4096Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 683Number of blocks to be created: 703\n",
      "\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 677\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 677\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2942703\n",
      "Total length after concatenation: 2942703\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 719\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Input length: 1001\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 698\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Number of blocks created: 766Final number of examples: 698\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 766\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2857487\n",
      "Total length after concatenation: 2857487\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 698\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2783807\n",
      "Total length after concatenation: 2783807\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 680\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 703\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 703Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2671359\n",
      "Total length after concatenation: 2671359\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 653\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 683Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Actual total length: 2997309\n",
      "\n",
      "Final number of examples: 683Total length after concatenation: 2997309\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 732\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 1001\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 719\n",
      "\n",
      "Number of blocks created: 698Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Final number of examples: 719\n",
      "\n",
      "Final number of examples: 698\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 1001Actual total length: 2859723\n",
      "\n",
      "Input length: 1001Total length after concatenation: 2859723\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 699\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 1001\n",
      "\n",
      "Actual total length: 2896836\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 2896836\n",
      "\n",
      "Number of blocks created: 680Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 708Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 680\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2917693\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 2917693Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "Block size: 4096Number of examples: 1001Number of examples: 1001\n",
      "\n",
      "\n",
      "Number of blocks to be created: 713Input length: 1001Input length: 1001\n",
      "\n",
      "\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 653\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 653\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2852597\n",
      "Total length after concatenation: 2852597\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 697\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 1001\n",
      "\n",
      "Actual total length: 3006933Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Total length after concatenation: 3006933\n",
      "Block size: 4096\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 732Actual total length: 3264494Number of blocks to be created: 735\n",
      "\n",
      "\n",
      "Total length after concatenation: 3264494Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Block size: 4096Final number of examples: 732\n",
      "\n",
      "Number of blocks to be created: 797\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 699\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 699\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 708\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 708\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2781495\n",
      "Total length after concatenation: 2781495Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Block size: 4096\n",
      "Number of examples: 1001\n",
      "Number of blocks created: 713Number of blocks to be created: 680\n",
      "\n",
      "\n",
      "Input length: 1001Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 713\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2838615\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 2838615\n",
      "\n",
      "Block size: 4096Actual total length: 2918165\n",
      "\n",
      "Number of blocks to be created: 694Total length after concatenation: 2918165\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 713\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3143791Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Total length after concatenation: 3143791Number of blocks created: 697\n",
      "\n",
      "Block size: 4096Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 697Number of blocks to be created: 768\n",
      "\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 735\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final number of examples: 735\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "\n",
      "Number of blocks created: 680Number of blocks created: 797\n",
      "Input length: 1001\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 680Final number of examples: 797\n",
      "\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001Number of blocks created: 694\n",
      "Input length: 1001\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 694\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2871752Number of examples: 1001\n",
      "\n",
      "Total length after concatenation: 2871752Input length: 1001\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 702\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2903367\n",
      "Total length after concatenation: 2903367\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 709\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 1001\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 713\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 713\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 768Number of examples: 1001\n",
      "\n",
      "Input length: 1001Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 768\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2875881\n",
      "Total length after concatenation: 2875881\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 703\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 702\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 702\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3070846\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 3070846\n",
      "\n",
      "Block size: 4096Actual total length: 2832262\n",
      "\n",
      "Number of blocks to be created: 750Total length after concatenation: 2832262\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 692\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2909200\n",
      "Total length after concatenation: 2909200\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 711\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 709\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Final number of examples: 709\n",
      "\n",
      "Actual total length: 2791641\n",
      "Total length after concatenation: 2791641Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Block size: 4096\n",
      "Actual total length: 2941909Number of blocks to be created: 682\n",
      "\n",
      "Total length after concatenation: 2941909\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 719\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2860537\n",
      "Total length after concatenation: 2860537\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 699\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 703\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 703\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3166582\n",
      "Total length after concatenation: 3166582\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 774\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2899245\n",
      "Total length after concatenation: 2899245\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 708\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 692\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 692\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 711\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 711\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 750\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 750\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 682\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 682\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 699\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 699\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 719\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 719\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 774\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 774\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 708\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 708\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1075597\n",
      "Total length after concatenation: 1075597\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 263\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 263\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 263\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 376\n",
      "Actual total length: 1086382Input length: 376\n",
      "\n",
      "Total length after concatenation: 1086382\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 266Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 1022683\n",
      "Total length after concatenation: 1022683\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 250\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1037889\n",
      "Total length after concatenation: 1037889\n",
      "Block size: 4096Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks to be created: 254Number of examples: 376\n",
      "\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Actual total length: 1141705\n",
      "\n",
      "\n",
      "Actual total length: 1091989Number of blocks created: 266Total length after concatenation: 1141705\n",
      "\n",
      "Total length after concatenation: 1091989\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Block size: 4096\n",
      "Block size: 4096\n",
      "Final number of examples: 266Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks to be created: 279\n",
      "\n",
      "Number of blocks to be created: 267\n",
      "\n",
      "Number of blocks created: 250\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 250\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 254\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 254\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks created: 267Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Actual total length: 1099253Number of examples: 376\n",
      "\n",
      "\n",
      "Final number of examples: 267Total length after concatenation: 1099253Input length: 376\n",
      "\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 269\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 279\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 279\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1109323\n",
      "Total length after concatenation: 1109323\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 271\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 269\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 269\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1102957\n",
      "Total length after concatenation: 1102957\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 270\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks created: 271\n",
      "\n",
      "Actual total length: 1092550Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "Total length after concatenation: 1092550Final number of examples: 271Number of examples: 376\n",
      "\n",
      "\n",
      "Block size: 4096Input length: 376\n",
      "\n",
      "Number of blocks to be created: 267\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 270\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 270\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 267\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 267\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1065929\n",
      "Total length after concatenation: 1065929\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 261\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1032169\n",
      "Total length after concatenation: 1032169\n",
      "Block size: 4096Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks to be created: 252Number of examples: 376\n",
      "\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 261\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 261\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 252\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 252\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1029039\n",
      "Total length after concatenation: 1029039Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096Actual total length: 1068244\n",
      "\n",
      "\n",
      "Actual total length: 1059793Number of blocks to be created: 252Total length after concatenation: 1068244\n",
      "\n",
      "\n",
      "Total length after concatenation: 1059793Block size: 4096\n",
      "\n",
      "Block size: 4096Number of blocks to be created: 261\n",
      "\n",
      "Number of blocks to be created: 259\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1015755\n",
      "Total length after concatenation: 1015755\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 248\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks created: 252\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Number of examples: 376\n",
      "\n",
      "Final number of examples: 252Input length: 376\n",
      "\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 259\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 259\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 261\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 261\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 248\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 248\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1104521\n",
      "Total length after concatenation: 1104521\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 270\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 270\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 270\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 959903\n",
      "Total length after concatenation: 959903\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 235\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 376\n",
      "\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1071472\n",
      "Total length after concatenation: 1071472\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 262\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 376\n",
      "\n",
      "Number of blocks created: 235\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Final number of examples: 235\n",
      "\n",
      "Actual total length: 1539642\n",
      "Total length after concatenation: 1539642\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 376\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Actual total length: 1120606\n",
      "\n",
      "Number of examples: 376Total length after concatenation: 1120606\n",
      "\n",
      "Input length: 376Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 274\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1007326\n",
      "Total length after concatenation: 1007326\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096\n",
      "\n",
      "Number of blocks created: 262Number of blocks to be created: 246\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 262\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1178901Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Total length after concatenation: 1178901\n",
      "Number of blocks created: 274Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096\n",
      "\n",
      "\n",
      "Actual total length: 1117180Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Number of blocks to be created: 288\n",
      "\n",
      "\n",
      "Total length after concatenation: 1117180Final number of examples: 274\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 273\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1037375Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Total length after concatenation: 1037375Number of blocks created: 376\n",
      "\n",
      "Block size: 4096\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks to be created: 254\n",
      "\n",
      "\n",
      "Final number of examples: 376Number of examples: 376\n",
      "\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 246\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 246\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1045654\n",
      "Total length after concatenation: 1045654\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 256\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 288Number of blocks created: 273\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 288Final number of examples: 273\n",
      "\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1066498\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 1066498\n",
      "\n",
      "Number of examples: 376Block size: 4096\n",
      "\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 376Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks to be created: 261\n",
      "\n",
      "\n",
      "\n",
      "Number of blocks created: 254Number of blocks created: 256Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 254Final number of examples: 256\n",
      "\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1035905\n",
      "Total length after concatenation: 1035905Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Block size: 4096Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of examples: 376\n",
      "\n",
      "Number of blocks to be created: 253\n",
      "Actual total length: 1081130\n",
      "\n",
      "Input length: 376Total length after concatenation: 1081130\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 264\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 261\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 261\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 253\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 253\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 264\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Final number of examples: 264\n",
      "\n",
      "Actual total length: 1231667\n",
      "Total length after concatenation: 1231667\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 301\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1144995\n",
      "Total length after concatenation: 1144995\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 280\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 301\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 301\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 280\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 280\n",
      "Input keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1206993\n",
      "Total length after concatenation: 1206993\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 295\n",
      "Result keys after splitting: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 295\n",
      "Final result keys: dict_keys(['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:36:05] Energy consumed for RAM : 15.225447 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:36:05] Energy consumed for RAM : 15.186684 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:36:05] Energy consumed for all GPUs : 7.595580 kWh. Total GPU Power : 66.40100097315062 W\n",
      "[codecarbon INFO @ 23:36:05] Energy consumed for all CPUs : 2.839009 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:36:05] 25.660035 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:36:05] Energy consumed for all GPUs : 7.591131 kWh. Total GPU Power : 66.40396972768774 W\n",
      "[codecarbon INFO @ 23:36:05] Energy consumed for all CPUs : 2.831702 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:36:05] 25.609517 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_chunk = DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 54020\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:36:20] Energy consumed for RAM : 15.228576 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:36:20] Energy consumed for RAM : 15.189813 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:36:20] Energy consumed for all GPUs : 7.595857 kWh. Total GPU Power : 66.39853745398017 W\n",
      "[codecarbon INFO @ 23:36:20] Energy consumed for all CPUs : 2.839592 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:36:20] 25.664025 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:36:20] Energy consumed for all GPUs : 7.591407 kWh. Total GPU Power : 66.40491335462424 W\n",
      "[codecarbon INFO @ 23:36:20] Energy consumed for all CPUs : 2.832286 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:36:20] 25.613506 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:36:35] Energy consumed for RAM : 15.231705 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:36:35] Energy consumed for RAM : 15.192943 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:36:35] Energy consumed for all GPUs : 7.596135 kWh. Total GPU Power : 66.8639441261687 W\n",
      "[codecarbon INFO @ 23:36:35] Energy consumed for all CPUs : 2.840175 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:36:35] 25.668016 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:36:35] Energy consumed for all GPUs : 7.591686 kWh. Total GPU Power : 66.86030935434972 W\n",
      "[codecarbon INFO @ 23:36:35] Energy consumed for all CPUs : 2.832869 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:36:35] 25.617497 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:36:50] Energy consumed for RAM : 15.234835 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:36:50] Energy consumed for RAM : 15.196072 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:36:50] Energy consumed for all GPUs : 7.596412 kWh. Total GPU Power : 66.41385129300411 W\n",
      "[codecarbon INFO @ 23:36:50] Energy consumed for all CPUs : 2.840759 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:36:50] 25.672005 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:36:50] Energy consumed for all GPUs : 7.591962 kWh. Total GPU Power : 66.41129680351143 W\n",
      "[codecarbon INFO @ 23:36:50] Energy consumed for all CPUs : 2.833452 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:36:50] 25.621487 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:37:05] Energy consumed for RAM : 15.237964 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:37:05] Energy consumed for RAM : 15.199201 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:37:05] Energy consumed for all GPUs : 7.596688 kWh. Total GPU Power : 66.39041294983741 W\n",
      "[codecarbon INFO @ 23:37:05] Energy consumed for all CPUs : 2.841342 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:37:05] 25.675994 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:37:05] Energy consumed for all GPUs : 7.592239 kWh. Total GPU Power : 66.39023138064836 W\n",
      "[codecarbon INFO @ 23:37:05] Energy consumed for all CPUs : 2.834036 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:37:05] 25.625476 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:37:20] Energy consumed for RAM : 15.241093 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:37:20] Energy consumed for RAM : 15.202331 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:37:20] Energy consumed for all GPUs : 7.596967 kWh. Total GPU Power : 66.87260010672863 W\n",
      "[codecarbon INFO @ 23:37:20] Energy consumed for all CPUs : 2.841925 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:37:20] 25.679985 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:37:20] Energy consumed for all GPUs : 7.592517 kWh. Total GPU Power : 66.86940366870662 W\n",
      "[codecarbon INFO @ 23:37:20] Energy consumed for all CPUs : 2.834619 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:37:20] 25.629467 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:37:35] Energy consumed for RAM : 15.244222 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:37:35] Energy consumed for RAM : 15.205460 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:37:35] Energy consumed for all GPUs : 7.597244 kWh. Total GPU Power : 66.43290568539265 W\n",
      "[codecarbon INFO @ 23:37:35] Energy consumed for all CPUs : 2.842509 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:37:35] 25.683974 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:37:35] Energy consumed for all GPUs : 7.592794 kWh. Total GPU Power : 66.42000674687776 W\n",
      "[codecarbon INFO @ 23:37:35] Energy consumed for all CPUs : 2.835202 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:37:35] 25.633457 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:37:50] Energy consumed for RAM : 15.247351 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:37:50] Energy consumed for RAM : 15.208589 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:37:50] Energy consumed for all GPUs : 7.597520 kWh. Total GPU Power : 66.47400009029703 W\n",
      "[codecarbon INFO @ 23:37:50] Energy consumed for all CPUs : 2.843092 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:37:50] 25.687963 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:37:50] Energy consumed for all GPUs : 7.593071 kWh. Total GPU Power : 66.46348336949038 W\n",
      "[codecarbon INFO @ 23:37:50] Energy consumed for all CPUs : 2.835786 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:37:50] 25.637446 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:38:05] Energy consumed for RAM : 15.250480 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:38:05] Energy consumed for RAM : 15.211718 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:38:05] Energy consumed for all GPUs : 7.597799 kWh. Total GPU Power : 66.83877390859062 W\n",
      "[codecarbon INFO @ 23:38:05] Energy consumed for all CPUs : 2.843675 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:38:05] 25.691954 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:38:05] Energy consumed for all GPUs : 7.593349 kWh. Total GPU Power : 66.83350899326031 W\n",
      "[codecarbon INFO @ 23:38:05] Energy consumed for all CPUs : 2.836369 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:38:05] 25.641436 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:38:20] Energy consumed for RAM : 15.253609 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:38:20] Energy consumed for RAM : 15.214848 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:38:20] Energy consumed for all GPUs : 7.598075 kWh. Total GPU Power : 66.40739964562167 W\n",
      "[codecarbon INFO @ 23:38:20] Energy consumed for all CPUs : 2.844258 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:38:20] 25.695943 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:38:20] Energy consumed for all GPUs : 7.593626 kWh. Total GPU Power : 66.39757492293968 W\n",
      "[codecarbon INFO @ 23:38:20] Energy consumed for all CPUs : 2.836952 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:38:20] 25.645426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:38:35] Energy consumed for RAM : 15.256738 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:38:35] Energy consumed for RAM : 15.217977 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:38:35] Energy consumed for all GPUs : 7.598352 kWh. Total GPU Power : 66.40319900494997 W\n",
      "[codecarbon INFO @ 23:38:35] Energy consumed for all CPUs : 2.844842 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:38:35] 25.699932 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:38:35] Energy consumed for all GPUs : 7.593902 kWh. Total GPU Power : 66.39596628138172 W\n",
      "[codecarbon INFO @ 23:38:35] Energy consumed for all CPUs : 2.837535 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:38:35] 25.649415 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:38:50] Energy consumed for RAM : 15.259868 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:38:50] Energy consumed for RAM : 15.221106 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:38:50] Energy consumed for all GPUs : 7.598630 kWh. Total GPU Power : 66.86599497040775 W\n",
      "[codecarbon INFO @ 23:38:50] Energy consumed for all CPUs : 2.845425 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:38:50] 25.703923 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:38:50] Energy consumed for all GPUs : 7.594181 kWh. Total GPU Power : 66.85394439073279 W\n",
      "[codecarbon INFO @ 23:38:50] Energy consumed for all CPUs : 2.838119 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:38:50] 25.653406 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:39:05] Energy consumed for RAM : 15.262997 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:39:05] Energy consumed for all GPUs : 7.598907 kWh. Total GPU Power : 66.46649191080552 W\n",
      "[codecarbon INFO @ 23:39:05] Energy consumed for RAM : 15.224236 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:39:05] Energy consumed for all CPUs : 2.846008 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:39:05] 25.707912 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:39:05] Energy consumed for all GPUs : 7.594458 kWh. Total GPU Power : 66.45843764451928 W\n",
      "[codecarbon INFO @ 23:39:05] Energy consumed for all CPUs : 2.838702 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:39:05] 25.657395 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "lm_datasets = []\n",
    "block_size=512\n",
    "\n",
    "\n",
    "path = Path(dataset_dir)\n",
    "files = [file.name for file in path.glob(\"*.parquet\")]\n",
    "\n",
    "file = files[48]\n",
    "\n",
    "data_file = os.path.join(path, file)\n",
    "filename = \"\".join(file.split(\".\")[:-1])\n",
    "\n",
    "cache_dir = os.path.join(\n",
    "    tmp_data_cache_dir, filename + f\"_parquet_maxblocksize_{block_size}\"\n",
    ")\n",
    "\n",
    "raw_dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=data_file,\n",
    "    cache_dir=cache_dir,\n",
    "    keep_in_memory=False,\n",
    ")\n",
    "\n",
    "# Process in chunks similar to Dataset1 file size\n",
    "chunk_size = len(raw_dataset['train'])\n",
    "for i in range(0, len(raw_dataset['train']), chunk_size):\n",
    "    cur_i = 0\n",
    "    i = cur_i\n",
    "    #logger.info(f\"{file} has been loaded\")\n",
    "    \n",
    "    # Create a chunk that's a DatasetDict, just like raw_dataset\n",
    "    chunk = DatasetDict({\n",
    "        'train': Dataset.from_dict(\n",
    "            raw_dataset['train'].select(range(i, min(i+chunk_size, len(raw_dataset['train'])))).to_dict()\n",
    "        )\n",
    "    })\n",
    "    print(f\"chunk = {chunk}\")\n",
    "    tokenized_chunk = chunk.map(\n",
    "        tokenize_and_check,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=32, #data_args.preprocessing_num_workers,\n",
    "        #remove_columns=[\"text\"],\n",
    "        load_from_cache_file=True,\n",
    "        keep_in_memory=False,\n",
    "        cache_file_names={\n",
    "            k: os.path.join(cache_dir, f\"tokenized_{filename}_{block_size}_{i}.arrow\")\n",
    "            for k in chunk\n",
    "        },\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    print(f\"tokenized_chunk = {tokenized_chunk}\")\n",
    "    \n",
    "    grouped_chunk = tokenized_chunk.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=32, #data_args.preprocessing_num_workers,\n",
    "        load_from_cache_file=False,\n",
    "        keep_in_memory=False,\n",
    "        cache_file_names={\n",
    "            k: os.path.join(cache_dir, f\"grouped_{filename}_{block_size}_{i}.arrow\")\n",
    "            for k in tokenized_chunk\n",
    "        },\n",
    "        desc=f\"Grouping texts in chunks of dynamic block_size\",\n",
    "    )\n",
    "\n",
    "    print(f\"grouped_chunk = {grouped_chunk}\")\n",
    "    \n",
    "    if i == cur_i:\n",
    "        processed_dataset = grouped_chunk\n",
    "    else:\n",
    "        processed_dataset = concatenate_datasets([processed_dataset, grouped_chunk])\n",
    "    \n",
    "    break\n",
    "\n",
    "idx = 0\n",
    "if idx == 0:\n",
    "    lm_datasets = processed_dataset\n",
    "else:\n",
    "    lm_datasets = concatenate_datasets([lm_datasets, processed_dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count total tokens in the dataset\n",
    "from tqdm import tqdm\n",
    "def count_total_tokens(dataset, tokenizer, text_column=\"text\"):\n",
    "    total_text = 0\n",
    "    total_tokens = 0\n",
    "    for example in tqdm(dataset[\"train\"], desc=\"Counting texts and tokens length\"):\n",
    "        text = example[text_column]\n",
    "        tokens = tokenizer(text)[\"input_ids\"]\n",
    "        total_text += len(text)\n",
    "        total_tokens += len(tokens)\n",
    "    return total_text, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'timestamp', 'url', 'source'],\n",
       "        num_rows: 76000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 54020\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:39:20] Energy consumed for RAM : 15.266126 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:39:20] Energy consumed for RAM : 15.227365 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:39:20] Energy consumed for all GPUs : 7.599185 kWh. Total GPU Power : 66.6243837813797 W\n",
      "[codecarbon INFO @ 23:39:20] Energy consumed for all CPUs : 2.846591 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:39:20] 25.711902 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:39:20] Energy consumed for all GPUs : 7.594735 kWh. Total GPU Power : 66.60474971643254 W\n",
      "[codecarbon INFO @ 23:39:20] Energy consumed for all CPUs : 2.839285 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:39:20] 25.661386 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tokens:   0%|                                             | 0/76000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (40136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Counting tokens: 100%|| 76000/76000 [03:50<00:00, 329.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of texts: 183765478 \n",
      " tokens in the dataset: 220508397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total tokens in the dataset\n",
    "total_text, total_tokens = count_total_tokens(raw_dataset, tokenizer)\n",
    "print(f\"Total number of texts: {total_text} \\n tokens in the dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split_percentage = 0.1\n",
    "lm_datasets = lm_datasets['train'].train_test_split(\n",
    "        test_size=validation_split_percentage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'timestamp', 'url', 'source'],\n",
       "        num_rows: 77000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 48618\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5402\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1717658191701,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "# Wikipedia provides a title and an article text.\n",
    "# Use https://translate.google.com!\n",
    "alpaca_prompt = \"\"\"    ,        ,                     \n",
    "### Instruction: {}\n",
    "\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "# becomes:\n",
    "wikipedia_prompt = \"\"\" \n",
    "### : {}\n",
    "\n",
    "### :\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(convo[0][\"value\"], convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw0tuebw4Ppe"
   },
   "source": [
    "We only use 1% of the dataset to speed things up! Use more for longer runs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Continued Pretraining\n",
    "Now let's use Unsloth's `UnslothTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 20 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
    "\n",
    "Also set `embedding_learning_rate` to be a learning rate at least 2x or 10x smaller than `learning_rate` to make continual pretraining work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  38839 MiB |  38839 MiB |  39165 GiB |  39127 GiB |\n",
      "|       from large pool |  38804 MiB |  38804 MiB |  39040 GiB |  39002 GiB |\n",
      "|       from small pool |     35 MiB |     35 MiB |    125 GiB |    125 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  38839 MiB |  38839 MiB |  39165 GiB |  39127 GiB |\n",
      "|       from large pool |  38804 MiB |  38804 MiB |  39040 GiB |  39002 GiB |\n",
      "|       from small pool |     35 MiB |     35 MiB |    125 GiB |    125 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  38809 MiB |  38809 MiB |  39156 GiB |  39118 GiB |\n",
      "|       from large pool |  38774 MiB |  38774 MiB |  39030 GiB |  38992 GiB |\n",
      "|       from small pool |     35 MiB |     35 MiB |    125 GiB |    125 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  42624 MiB |  42624 MiB |    983 GiB |    942 GiB |\n",
      "|       from large pool |  42584 MiB |  42584 MiB |    981 GiB |    940 GiB |\n",
      "|       from small pool |     40 MiB |     40 MiB |      1 GiB |      1 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   3784 MiB |   3784 MiB |  61625 GiB |  61621 GiB |\n",
      "|       from large pool |   3779 MiB |   3779 MiB |  61498 GiB |  61495 GiB |\n",
      "|       from small pool |      4 MiB |      4 MiB |    126 GiB |    126 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     821    |     821    |  458496    |  457675    |\n",
      "|       from large pool |     682    |     682    |  277817    |  277135    |\n",
      "|       from small pool |     139    |     139    |  180679    |  180540    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     821    |     821    |  458496    |  457675    |\n",
      "|       from large pool |     682    |     682    |  277817    |  277135    |\n",
      "|       from small pool |     139    |     139    |  180679    |  180540    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     284    |     284    |    1373    |    1089    |\n",
      "|       from large pool |     264    |     264    |     366    |     102    |\n",
      "|       from small pool |      20    |      20    |    1007    |     987    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      17    |      17    |  236828    |  236811    |\n",
      "|       from large pool |       5    |       5    |  111071    |  111066    |\n",
      "|       from small pool |      12    |      12    |  125757    |  125745    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:34:18] Energy consumed for RAM : 0.250753 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:18] Energy consumed for all GPUs : 0.055731 kWh. Total GPU Power : 67.26548683104527 W\n",
      "[codecarbon INFO @ 14:34:18] Energy consumed for all CPUs : 0.046815 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:18] 0.353298 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:25] Energy consumed for RAM : 0.071979 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:25] Energy consumed for all GPUs : 0.006863 kWh. Total GPU Power : 67.39344048417644 W\n",
      "[codecarbon INFO @ 14:34:25] Energy consumed for all CPUs : 0.013416 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:25] 0.092258 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:27] Energy consumed for RAM : 0.379717 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:27] Energy consumed for all GPUs : 0.075252 kWh. Total GPU Power : 66.28695029699956 W\n",
      "[codecarbon INFO @ 14:34:27] Energy consumed for all CPUs : 0.070786 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:27] 0.525756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for RAM : 0.207504 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for RAM : 0.188861 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for all GPUs : 0.044835 kWh. Total GPU Power : 66.73695977463593 W\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for all CPUs : 0.038681 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:29] 0.291020 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for all GPUs : 0.042685 kWh. Total GPU Power : 66.74786190979862 W\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for all CPUs : 0.035208 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:29] 0.266754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:33] Energy consumed for RAM : 0.253882 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:33] Energy consumed for all GPUs : 0.056008 kWh. Total GPU Power : 66.72205427835652 W\n",
      "[codecarbon INFO @ 14:34:33] Energy consumed for all CPUs : 0.047398 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:33] 0.357289 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:40] Energy consumed for RAM : 0.075108 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:40] Energy consumed for all GPUs : 0.007139 kWh. Total GPU Power : 66.29611396649406 W\n",
      "[codecarbon INFO @ 14:34:40] Energy consumed for all CPUs : 0.013999 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:40] 0.096247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:42] Energy consumed for RAM : 0.382847 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:42] Energy consumed for all GPUs : 0.075529 kWh. Total GPU Power : 66.28275228288952 W\n",
      "[codecarbon INFO @ 14:34:42] Energy consumed for all CPUs : 0.071370 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:42] 0.529745 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingCallback(TrainerCallback):\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        logger.info(f\"Starting step {state.global_step}\")\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        logger.info(f\"Finished step {state.global_step}\")\n",
    "        logger.info(f\"Learning rate: {args.learning_rate}\")\n",
    "        logger.info(f\"GPU memory allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "        logger.info(f\"GPU memory reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            logger.info(f\"Logs: {logs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:47:05] Energy consumed for RAM : 15.324352 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:47:05] Energy consumed for all GPUs : 7.603364 kWh. Total GPU Power : 66.5408339601439 W\n",
      "[codecarbon INFO @ 23:47:05] Energy consumed for all CPUs : 2.857382 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:47:05] 25.785099 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:47:05] Energy consumed for RAM : 15.363052 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:47:05] Energy consumed for all GPUs : 7.607814 kWh. Total GPU Power : 66.54931010242503 W\n",
      "[codecarbon INFO @ 23:47:05] Energy consumed for all CPUs : 2.864688 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:47:05] 25.835554 kWh of electricity used since the beginning.\n",
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  3 23:47:06 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              66W / 400W |  43339MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:47:20] Energy consumed for RAM : 15.327482 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:47:20] Energy consumed for all GPUs : 7.603652 kWh. Total GPU Power : 69.19188334651376 W\n",
      "[codecarbon INFO @ 23:47:20] Energy consumed for all CPUs : 2.857965 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:47:20] 25.789100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:47:20] Energy consumed for RAM : 15.366182 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:47:20] Energy consumed for all GPUs : 7.608102 kWh. Total GPU Power : 69.18405126125727 W\n",
      "[codecarbon INFO @ 23:47:20] Energy consumed for all CPUs : 2.865271 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:47:20] 25.839555 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Invoke garbage collector multiple times\n",
    "for _ in range(10):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if trainer:\n",
    "    del trainer\n",
    "    time.sleep(10)\n",
    "# Reset the CUDA runtime\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "#clear_gpu_memory()\n",
    "\n",
    "# Optionally, you can use nvidia-smi command to kill all processes using GPU\n",
    "import os\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split_percentage = 0.9\n",
    "lm_datasets = lm_datasets['train'].train_test_split(\n",
    "        test_size=validation_split_percentage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    ,    ,      - everyone said i need to improve my foot movement but nobody had the answers how, sais virender sehwag, Bangla News\\n  >  >   ,    ,     \\n  ,    ,     \\n   . Updated: 10 Jun 2021, 09:32 PM IST  Abhisake Koley\\n           ,    \\nVirender Sehwag Footwork Former India Opener Mansoor Ali Khan Pataudi Sunil Gavaskar Krishnamachari Srikkanth\\n'             '              ,              \\n                                   -            '       \\nCricuru app-    , '          ,          ,     '\\n   ,    ,           , '   ,        -         ' ,       | BJP leader Mukul Roy says there should be EVM in Municipal Election - Bengali Oneindia\\n| Updated: Thu, Feb 20, 2020, 15:15 [IST]\\n   ,                                     ,      \\n   \\n                   ,               ,   ,       \\n  , -     \\n-       -                                \\n    \\n-     ,                  -                -   ,     -         -        -     ,      \\n \\n   ,           -  ,            \\n .-          \\n    '  '!       ,   \\nbjp mukul roy election commission municipal election      politics\\nBJP leader Mukul Roy says t\""
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:48:35] Energy consumed for RAM : 15.343129 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:48:35] Energy consumed for all GPUs : 7.605041 kWh. Total GPU Power : 66.4159630866111 W\n",
      "[codecarbon INFO @ 23:48:35] Energy consumed for all CPUs : 2.860882 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:48:35] 25.809053 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:48:35] Energy consumed for RAM : 15.381831 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:48:35] Energy consumed for all GPUs : 7.609491 kWh. Total GPU Power : 66.41365140997195 W\n",
      "[codecarbon INFO @ 23:48:35] Energy consumed for all CPUs : 2.868188 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:48:35] 25.859509 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "\"\".join(lm_datasets['train'][5]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5402\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 48618\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:45:20] Energy consumed for RAM : 15.302446 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:45:20] Energy consumed for all GPUs : 7.601413 kWh. Total GPU Power : 66.51066019111632 W\n",
      "[codecarbon INFO @ 23:45:20] Energy consumed for all CPUs : 2.853299 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:45:20] 25.757158 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:45:20] Energy consumed for RAM : 15.341145 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:45:20] Energy consumed for all GPUs : 7.605863 kWh. Total GPU Power : 66.49996245149546 W\n",
      "[codecarbon INFO @ 23:45:20] Energy consumed for all CPUs : 2.860605 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:45:20] 25.807613 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:44:05] Energy consumed for RAM : 15.325533 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:44:05] Energy consumed for RAM : 15.286823 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:44:05] Energy consumed for all GPUs : 7.604471 kWh. Total GPU Power : 67.72879759726379 W\n",
      "[codecarbon INFO @ 23:44:05] Energy consumed for all CPUs : 2.857685 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:44:05] 25.787689 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:44:05] Energy consumed for all GPUs : 7.600021 kWh. Total GPU Power : 67.71571479162614 W\n",
      "[codecarbon INFO @ 23:44:05] Energy consumed for all CPUs : 2.850384 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:44:05] 25.737228 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:44:20] Energy consumed for RAM : 15.328662 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:44:20] Energy consumed for RAM : 15.289968 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:44:20] Energy consumed for all GPUs : 7.604752 kWh. Total GPU Power : 67.18920432179715 W\n",
      "[codecarbon INFO @ 23:44:20] Energy consumed for all CPUs : 2.858271 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:44:20] 25.791686 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:44:20] Energy consumed for all GPUs : 7.600303 kWh. Total GPU Power : 67.20349353004173 W\n",
      "[codecarbon INFO @ 23:44:20] Energy consumed for all CPUs : 2.850970 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:44:20] 25.741240 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:44:35] Energy consumed for RAM : 15.293078 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:44:35] Energy consumed for RAM : 15.331794 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:44:35] Energy consumed for all GPUs : 7.600580 kWh. Total GPU Power : 66.92871645773425 W\n",
      "[codecarbon INFO @ 23:44:35] Energy consumed for all CPUs : 2.851553 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:44:35] 25.745211 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:44:35] Energy consumed for all GPUs : 7.605031 kWh. Total GPU Power : 66.54374210637181 W\n",
      "[codecarbon INFO @ 23:44:36] Energy consumed for all CPUs : 2.858862 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:44:36] 25.795688 kWh of electricity used since the beginning.\n",
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Not an error, but Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2024.6 patched 32 layers with 0 QKV layers, 32 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:44:50] Energy consumed for RAM : 15.296187 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:44:50] Energy consumed for all GPUs : 7.600859 kWh. Total GPU Power : 67.51341245615288 W\n",
      "[codecarbon INFO @ 23:44:50] Energy consumed for all CPUs : 2.852133 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:44:50] 25.749179 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:44:50] Energy consumed for RAM : 15.334885 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:44:50] Energy consumed for all GPUs : 7.605309 kWh. Total GPU Power : 67.4706399052205 W\n",
      "[codecarbon INFO @ 23:44:50] Energy consumed for all CPUs : 2.859438 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:44:50] 25.799633 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:45:05] Energy consumed for RAM : 15.299317 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:45:05] Energy consumed for all GPUs : 7.601136 kWh. Total GPU Power : 66.48528620507932 W\n",
      "[codecarbon INFO @ 23:45:05] Energy consumed for all CPUs : 2.852716 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:45:05] 25.753169 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:45:05] Energy consumed for RAM : 15.338015 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:45:05] Energy consumed for all GPUs : 7.605586 kWh. Total GPU Power : 66.47564749027838 W\n",
      "[codecarbon INFO @ 23:45:05] Energy consumed for all CPUs : 2.860022 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:45:05] 25.803623 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model_orig,\n",
    "    r = 16,  # Reduced from 128 to balance between capacity and efficiency\n",
    "    target_modules = [\"q_proj\", \"v_proj\", \"o_proj\", #, \"k_proj\"\n",
    "                      \"gate_proj\", \"up_proj\"], # , \"down_proj\"\n",
    "    modules_to_save = [\"embed_tokens\", \"lm_head\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,080,557,568 || all params: 9,110,818,816 || trainable%: 11.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:45:35] Energy consumed for RAM : 15.305575 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:45:35] Energy consumed for all GPUs : 7.601691 kWh. Total GPU Power : 66.57877679055093 W\n",
      "[codecarbon INFO @ 23:45:35] Energy consumed for all CPUs : 2.853883 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:45:35] 25.761148 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:45:35] Energy consumed for RAM : 15.344275 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:45:35] Energy consumed for all GPUs : 7.606142 kWh. Total GPU Power : 67.02896417392036 W\n",
      "[codecarbon INFO @ 23:45:35] Energy consumed for all CPUs : 2.861188 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:45:35] 25.811605 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Both warmup_ratio and warmup_steps given, warmup_steps will override any effect of warmup_ratio during training\n",
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "[codecarbon INFO @ 23:48:48] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 23:48:48] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 23:48:48] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 23:48:48] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 23:48:48] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 23:48:50] CPU Model on constant consumption mode: AMD EPYC 7763 64-Core Processor\n",
      "[codecarbon INFO @ 23:48:50] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 23:48:50]   Platform system: Linux-5.4.0-162-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 23:48:50]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 23:48:50]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 23:48:50]   Available RAM : 2003.876 GB\n",
      "[codecarbon INFO @ 23:48:50]   CPU count: 255\n",
      "[codecarbon INFO @ 23:48:50]   CPU model: AMD EPYC 7763 64-Core Processor\n",
      "[codecarbon INFO @ 23:48:50]   GPU count: 1\n",
      "[codecarbon INFO @ 23:48:50]   GPU model: 1 x NVIDIA A100-SXM4-80GB\n",
      "[codecarbon INFO @ 23:48:50] Energy consumed for RAM : 15.346259 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:48:50] Energy consumed for all GPUs : 7.605318 kWh. Total GPU Power : 66.46416770275292 W\n",
      "[codecarbon INFO @ 23:48:50] Energy consumed for all CPUs : 2.861465 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:48:50] 25.813042 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:48:50] Energy consumed for RAM : 15.384961 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:48:50] Energy consumed for all GPUs : 7.609768 kWh. Total GPU Power : 66.4657421583953 W\n",
      "[codecarbon INFO @ 23:48:50] Energy consumed for all CPUs : 2.868771 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:48:50] 25.863499 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run the trainer with the custom callback\n",
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    #eval_dataset=train_dataset['test'][0:100],\n",
    "\n",
    "    packing=True,\n",
    "    dataset_text_field = None, #\"text\",\n",
    "    \n",
    "    \n",
    "    #max_seq_length = 512, #block_size, #max_seq_length,\n",
    "    dataset_num_proc=32,\n",
    "    \n",
    "    args=UnslothTrainingArguments(\n",
    "        per_device_train_batch_size=8,  # Increased batch size to leverage GPU memory\n",
    "        gradient_accumulation_steps=16,   # Adjusted gradient accumulation\n",
    "        \n",
    "        #max_steps = 120,\n",
    "        warmup_steps=100,                 # More warmup steps for stability with larger batch sizes\n",
    "        warmup_ratio=0.05,\n",
    "        num_train_epochs=1,               # Increase epochs for better training (adjust as needed)\n",
    "        \n",
    "        learning_rate=1e-4,               # Adjusted learning rate for larger batch size\n",
    "        embedding_learning_rate=2e-5,     # Adjusted embedding learning rate\n",
    "        \n",
    "        fp16 = False, #not is_bfloat16_supported(),\n",
    "        bf16 = True, #is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        evaluation_strategy=\"steps\",  # Added\n",
    "        eval_steps=25,  # Added\n",
    "        save_strategy=\"steps\",  # Added\n",
    "        save_steps=25,  # Added\n",
    "    ),\n",
    "    callbacks=[LoggingCallback()]  # Add the custom logging callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1717658245568,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "4eaa7bd1-f800-49d3-a8f5-58c918f339cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-80GB. Max memory = 79.151 GB.\n",
      "41.744 GB of memory reserved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:49:05] Energy consumed for RAM : 15.349389 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:05] Energy consumed for all GPUs : 7.605597 kWh. Total GPU Power : 66.9227688973394 W\n",
      "[codecarbon INFO @ 23:49:05] Energy consumed for all CPUs : 2.862048 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:05] 25.817034 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:49:05] Energy consumed for RAM : 15.388091 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:05] Energy consumed for all GPUs : 7.610046 kWh. Total GPU Power : 66.91306114142832 W\n",
      "[codecarbon INFO @ 23:49:05] Energy consumed for all CPUs : 2.869354 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:05] 25.867491 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "import torch\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5239701,
     "status": "ok",
     "timestamp": 1717663485260,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "c60e3467-a197-48e1-8d52-c9999b59efc7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afaf2c0c62a34990aae41e57dae2ce79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting untrained tokens:   0%|          | 0/5402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:49:21] Energy consumed for RAM : 15.391395 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:21] Energy consumed for RAM : 15.352694 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:21] Energy consumed for all GPUs : 7.605891 kWh. Total GPU Power : 66.55759836939502 W\n",
      "[codecarbon INFO @ 23:49:21] Energy consumed for all CPUs : 2.862667 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:23] 25.821252 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:49:23] Energy consumed for all GPUs : 7.610340 kWh. Total GPU Power : 66.57308152156793 W\n",
      "[codecarbon INFO @ 23:49:23] Energy consumed for all CPUs : 2.870042 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:23] 25.871776 kWh of electricity used since the beginning.\n",
      "Unsloth: Setting embed_tokens & lm_head untrained tokens to mean(trained) to counteract NaNs during training.\n",
      "Currently training with a batch size of: 8\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: source, url, timestamp, text. If source, url, timestamp, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 5,402 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 16\n",
      "\\        /    Total batch size = 128 | Total steps = 42\n",
      " \"-____-\"     Number of trainable parameters = 1,080,557,568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 2.00e-05 instead of 1.00e-04 for embed_tokens.\n",
      "Unsloth: Setting lr = 2.00e-05 instead of 1.00e-04 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:49:36] Energy consumed for RAM : 15.394141 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:36] Energy consumed for RAM : 15.355442 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:36] Energy consumed for all GPUs : 7.611331 kWh. Total GPU Power : 271.1596547318406 W\n",
      "[codecarbon INFO @ 23:49:36] Energy consumed for all CPUs : 2.870554 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:36] 25.876026 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:49:36] Energy consumed for all GPUs : 7.606882 kWh. Total GPU Power : 271.0811474452329 W\n",
      "[codecarbon INFO @ 23:49:36] Energy consumed for all CPUs : 2.863179 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:36] 25.825503 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:49:43] Energy consumed for RAM : 0.003132 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:43] Energy consumed for all GPUs : 0.001546 kWh. Total GPU Power : 370.79580064343884 W\n",
      "[codecarbon INFO @ 23:49:43] Energy consumed for all CPUs : 0.000584 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:43] 0.005261 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:49:51] Energy consumed for RAM : 15.397270 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:51] Energy consumed for RAM : 15.358570 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:51] Energy consumed for all GPUs : 7.612892 kWh. Total GPU Power : 374.52769421426063 W\n",
      "[codecarbon INFO @ 23:49:51] Energy consumed for all CPUs : 2.871137 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:51] 25.881298 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:49:51] Energy consumed for all GPUs : 7.608442 kWh. Total GPU Power : 374.5819683690834 W\n",
      "[codecarbon INFO @ 23:49:51] Energy consumed for all CPUs : 2.863763 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:51] 25.830775 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:49:59] Energy consumed for RAM : 0.006563 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:49:59] Energy consumed for all GPUs : 0.003266 kWh. Total GPU Power : 376.5591932828497 W\n",
      "[codecarbon INFO @ 23:49:59] Energy consumed for all CPUs : 0.001223 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:49:59] 0.011052 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:06] Energy consumed for RAM : 15.361698 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:06] Energy consumed for RAM : 15.400399 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:06] Energy consumed for all GPUs : 7.610021 kWh. Total GPU Power : 379.1695230441488 W\n",
      "[codecarbon INFO @ 23:50:06] Energy consumed for all CPUs : 2.864346 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:06] 25.836065 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:06] Energy consumed for all GPUs : 7.614470 kWh. Total GPU Power : 379.10232286322116 W\n",
      "[codecarbon INFO @ 23:50:06] Energy consumed for all CPUs : 2.871720 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:06] 25.886589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:14] Energy consumed for RAM : 0.009693 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:14] Energy consumed for all GPUs : 0.004848 kWh. Total GPU Power : 379.9575504263451 W\n",
      "[codecarbon INFO @ 23:50:14] Energy consumed for all CPUs : 0.001807 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:14] 0.016347 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:21] Energy consumed for RAM : 15.364827 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:21] Energy consumed for RAM : 15.403527 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:21] Energy consumed for all GPUs : 7.611582 kWh. Total GPU Power : 374.9368027819367 W\n",
      "[codecarbon INFO @ 23:50:21] Energy consumed for all CPUs : 2.864929 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:21] 25.841338 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:21] Energy consumed for all GPUs : 7.616032 kWh. Total GPU Power : 374.9756555919524 W\n",
      "[codecarbon INFO @ 23:50:21] Energy consumed for all CPUs : 2.872304 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:21] 25.891863 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:29] Energy consumed for RAM : 0.012822 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:29] Energy consumed for all GPUs : 0.006392 kWh. Total GPU Power : 370.6370798992881 W\n",
      "[codecarbon INFO @ 23:50:29] Energy consumed for all CPUs : 0.002390 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:29] 0.021604 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:37] Energy consumed for RAM : 15.368176 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:37] Energy consumed for RAM : 15.406875 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:37] Energy consumed for all GPUs : 7.613239 kWh. Total GPU Power : 371.77514405981816 W\n",
      "[codecarbon INFO @ 23:50:37] Energy consumed for all CPUs : 2.865553 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:37] 25.846968 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:37] Energy consumed for all GPUs : 7.617689 kWh. Total GPU Power : 371.8972228791255 W\n",
      "[codecarbon INFO @ 23:50:37] Energy consumed for all CPUs : 2.872928 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:37] 25.897492 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:44] Energy consumed for RAM : 0.015952 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:44] Energy consumed for all GPUs : 0.007973 kWh. Total GPU Power : 379.6879166545794 W\n",
      "[codecarbon INFO @ 23:50:44] Energy consumed for all CPUs : 0.002973 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:44] 0.026898 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:52] Energy consumed for RAM : 15.371304 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:52] Energy consumed for RAM : 15.410002 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:52] Energy consumed for all GPUs : 7.614824 kWh. Total GPU Power : 380.5226135986511 W\n",
      "[codecarbon INFO @ 23:50:52] Energy consumed for all CPUs : 2.866137 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:52] 25.852264 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:52] Energy consumed for all GPUs : 7.619273 kWh. Total GPU Power : 380.5958103495687 W\n",
      "[codecarbon INFO @ 23:50:52] Energy consumed for all CPUs : 2.873511 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:52] 25.902787 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:50:59] Energy consumed for RAM : 0.019081 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:50:59] Energy consumed for all GPUs : 0.009532 kWh. Total GPU Power : 374.3088177620369 W\n",
      "[codecarbon INFO @ 23:50:59] Energy consumed for all CPUs : 0.003557 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:50:59] 0.032170 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:07] Energy consumed for RAM : 15.413130 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:07] Energy consumed for RAM : 15.374432 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:07] Energy consumed for all GPUs : 7.620837 kWh. Total GPU Power : 375.5813117432877 W\n",
      "[codecarbon INFO @ 23:51:07] Energy consumed for all CPUs : 2.874094 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:07] 25.908061 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:07] Energy consumed for all GPUs : 7.616388 kWh. Total GPU Power : 375.4943617823853 W\n",
      "[codecarbon INFO @ 23:51:07] Energy consumed for all CPUs : 2.866720 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:07] 25.857540 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:15] Energy consumed for RAM : 0.022394 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:15] Energy consumed for all GPUs : 0.011190 kWh. Total GPU Power : 375.96736336239206 W\n",
      "[codecarbon INFO @ 23:51:15] Energy consumed for all CPUs : 0.004174 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:15] 0.037759 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:22] Energy consumed for RAM : 15.416259 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:22] Energy consumed for RAM : 15.377560 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:22] Energy consumed for all GPUs : 7.622410 kWh. Total GPU Power : 377.61739634076184 W\n",
      "[codecarbon INFO @ 23:51:22] Energy consumed for all CPUs : 2.874678 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:22] 25.913347 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:22] Energy consumed for all GPUs : 7.617960 kWh. Total GPU Power : 377.7344059369111 W\n",
      "[codecarbon INFO @ 23:51:22] Energy consumed for all CPUs : 2.867303 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:22] 25.862824 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:30] Energy consumed for RAM : 0.025523 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:30] Energy consumed for all GPUs : 0.012777 kWh. Total GPU Power : 381.07848212930156 W\n",
      "[codecarbon INFO @ 23:51:30] Energy consumed for all CPUs : 0.004758 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:30] 0.043058 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:37] Energy consumed for RAM : 15.419388 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:37] Energy consumed for RAM : 15.380689 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:37] Energy consumed for all GPUs : 7.623972 kWh. Total GPU Power : 375.1794030857313 W\n",
      "[codecarbon INFO @ 23:51:37] Energy consumed for all CPUs : 2.875261 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:37] 25.918620 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:37] Energy consumed for all GPUs : 7.619523 kWh. Total GPU Power : 375.2012504850554 W\n",
      "[codecarbon INFO @ 23:51:37] Energy consumed for all CPUs : 2.867887 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:37] 25.868098 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:45] Energy consumed for RAM : 0.028652 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:45] Energy consumed for all GPUs : 0.014329 kWh. Total GPU Power : 372.6382671669529 W\n",
      "[codecarbon INFO @ 23:51:45] Energy consumed for all CPUs : 0.005341 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:45] 0.048322 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:53] Energy consumed for RAM : 15.422675 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:53] Energy consumed for RAM : 15.383975 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:51:53] Energy consumed for all GPUs : 7.625614 kWh. Total GPU Power : 375.23563780476155 W\n",
      "[codecarbon INFO @ 23:51:53] Energy consumed for all CPUs : 2.875874 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:53] 25.924164 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:51:53] Energy consumed for all GPUs : 7.621165 kWh. Total GPU Power : 375.28792256887095 W\n",
      "[codecarbon INFO @ 23:51:53] Energy consumed for all CPUs : 2.868500 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:51:53] 25.873641 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/42 1:00:48 < 40:32, 0.01 it/s, Epoch 0.59/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:52:00] Energy consumed for RAM : 0.031779 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:00] Energy consumed for all GPUs : 0.015895 kWh. Total GPU Power : 376.2965444938798 W\n",
      "[codecarbon INFO @ 23:52:00] Energy consumed for all CPUs : 0.005924 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:00] 0.053599 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:08] Energy consumed for RAM : 15.425799 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:08] Energy consumed for RAM : 15.387098 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:08] Energy consumed for all GPUs : 7.627190 kWh. Total GPU Power : 378.96870357021163 W\n",
      "[codecarbon INFO @ 23:52:08] Energy consumed for all CPUs : 2.876457 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:08] 25.929446 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:08] Energy consumed for all GPUs : 7.622741 kWh. Total GPU Power : 379.0545312281602 W\n",
      "[codecarbon INFO @ 23:52:08] Energy consumed for all CPUs : 2.869083 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:08] 25.878922 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:15] Energy consumed for RAM : 0.034909 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:15] Energy consumed for all GPUs : 0.017462 kWh. Total GPU Power : 376.1414450990481 W\n",
      "[codecarbon INFO @ 23:52:15] Energy consumed for all CPUs : 0.006507 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:15] 0.058878 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:23] Energy consumed for RAM : 15.428928 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:23] Energy consumed for RAM : 15.390227 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:23] Energy consumed for all GPUs : 7.628745 kWh. Total GPU Power : 373.270214418334 W\n",
      "[codecarbon INFO @ 23:52:23] Energy consumed for all CPUs : 2.877040 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:23] 25.934713 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:23] Energy consumed for all GPUs : 7.624296 kWh. Total GPU Power : 373.3158360849881 W\n",
      "[codecarbon INFO @ 23:52:23] Energy consumed for all CPUs : 2.869666 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:23] 25.884189 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:31] Energy consumed for RAM : 0.038222 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:31] Energy consumed for all GPUs : 0.019116 kWh. Total GPU Power : 374.9548392986136 W\n",
      "[codecarbon INFO @ 23:52:31] Energy consumed for all CPUs : 0.007125 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:31] 0.064463 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:38] Energy consumed for RAM : 15.432056 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:38] Energy consumed for RAM : 15.393355 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:38] Energy consumed for all GPUs : 7.630316 kWh. Total GPU Power : 377.3197105666703 W\n",
      "[codecarbon INFO @ 23:52:38] Energy consumed for all CPUs : 2.877623 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:38] 25.939996 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:38] Energy consumed for all GPUs : 7.625867 kWh. Total GPU Power : 377.34136600458123 W\n",
      "[codecarbon INFO @ 23:52:38] Energy consumed for all CPUs : 2.870249 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:38] 25.889470 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:46] Energy consumed for RAM : 0.041351 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:46] Energy consumed for all GPUs : 0.020695 kWh. Total GPU Power : 379.26743413250534 W\n",
      "[codecarbon INFO @ 23:52:46] Energy consumed for all CPUs : 0.007708 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:46] 0.069754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:53] Energy consumed for RAM : 15.435185 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:53] Energy consumed for RAM : 15.396483 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:52:53] Energy consumed for all GPUs : 7.631890 kWh. Total GPU Power : 377.81954075975744 W\n",
      "[codecarbon INFO @ 23:52:53] Energy consumed for all CPUs : 2.878207 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:53] 25.945282 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:52:53] Energy consumed for all GPUs : 7.627440 kWh. Total GPU Power : 377.89802338027124 W\n",
      "[codecarbon INFO @ 23:52:53] Energy consumed for all CPUs : 2.870833 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:52:53] 25.894756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:01] Energy consumed for RAM : 0.044480 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:01] Energy consumed for all GPUs : 0.022258 kWh. Total GPU Power : 375.27392918599276 W\n",
      "[codecarbon INFO @ 23:53:01] Energy consumed for all CPUs : 0.008292 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:01] 0.075030 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:09] Energy consumed for RAM : 15.438486 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:09] Energy consumed for RAM : 15.399784 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:09] Energy consumed for all GPUs : 7.633532 kWh. Total GPU Power : 373.6855593895029 W\n",
      "[codecarbon INFO @ 23:53:09] Energy consumed for all CPUs : 2.878822 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:09] 25.950841 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:09] Energy consumed for all GPUs : 7.629083 kWh. Total GPU Power : 373.7529109085337 W\n",
      "[codecarbon INFO @ 23:53:09] Energy consumed for all CPUs : 2.871448 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:09] 25.900314 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:16] Energy consumed for RAM : 0.047609 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:16] Energy consumed for all GPUs : 0.023833 kWh. Total GPU Power : 378.2482863716745 W\n",
      "[codecarbon INFO @ 23:53:16] Energy consumed for all CPUs : 0.008875 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:16] 0.080317 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:24] Energy consumed for RAM : 15.441614 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:24] Energy consumed for RAM : 15.402910 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:24] Energy consumed for all GPUs : 7.635121 kWh. Total GPU Power : 381.74588037013797 W\n",
      "[codecarbon INFO @ 23:53:24] Energy consumed for all CPUs : 2.879405 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:24] 25.956141 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:24] Energy consumed for all GPUs : 7.630672 kWh. Total GPU Power : 381.9718400756521 W\n",
      "[codecarbon INFO @ 23:53:24] Energy consumed for all CPUs : 2.872031 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:24] 25.905613 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:31] Energy consumed for RAM : 0.050738 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:31] Energy consumed for all GPUs : 0.025387 kWh. Total GPU Power : 373.12868934955935 W\n",
      "[codecarbon INFO @ 23:53:31] Energy consumed for all CPUs : 0.009458 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:31] 0.085583 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:39] Energy consumed for RAM : 15.444743 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:39] Energy consumed for RAM : 15.406039 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:39] Energy consumed for all GPUs : 7.636671 kWh. Total GPU Power : 372.01642781273733 W\n",
      "[codecarbon INFO @ 23:53:39] Energy consumed for all CPUs : 2.879989 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:39] 25.961403 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:39] Energy consumed for all GPUs : 7.632221 kWh. Total GPU Power : 371.99942558904655 W\n",
      "[codecarbon INFO @ 23:53:39] Energy consumed for all CPUs : 2.872615 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:39] 25.910875 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:47] Energy consumed for RAM : 0.054020 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:47] Energy consumed for all GPUs : 0.027030 kWh. Total GPU Power : 375.93308110811984 W\n",
      "[codecarbon INFO @ 23:53:47] Energy consumed for all CPUs : 0.010070 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:47] 0.091120 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:54] Energy consumed for RAM : 15.447872 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:54] Energy consumed for RAM : 15.409168 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:53:54] Energy consumed for all GPUs : 7.638242 kWh. Total GPU Power : 377.2164879090269 W\n",
      "[codecarbon INFO @ 23:53:54] Energy consumed for all CPUs : 2.880572 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:54] 25.966685 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:53:54] Energy consumed for all GPUs : 7.633792 kWh. Total GPU Power : 377.2120608436941 W\n",
      "[codecarbon INFO @ 23:53:54] Energy consumed for all CPUs : 2.873198 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:53:54] 25.916158 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:02] Energy consumed for RAM : 0.057149 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:02] Energy consumed for all GPUs : 0.028604 kWh. Total GPU Power : 377.9778946517154 W\n",
      "[codecarbon INFO @ 23:54:02] Energy consumed for all CPUs : 0.010653 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:02] 0.096406 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:09] Energy consumed for RAM : 15.450999 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:09] Energy consumed for RAM : 15.412295 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:09] Energy consumed for all GPUs : 7.639810 kWh. Total GPU Power : 376.80708283531067 W\n",
      "[codecarbon INFO @ 23:54:09] Energy consumed for all CPUs : 2.881155 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:09] 25.971965 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:09] Energy consumed for all GPUs : 7.635361 kWh. Total GPU Power : 376.819798336204 W\n",
      "[codecarbon INFO @ 23:54:09] Energy consumed for all CPUs : 2.873781 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:09] 25.921437 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:17] Energy consumed for RAM : 0.060278 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:17] Energy consumed for all GPUs : 0.030166 kWh. Total GPU Power : 375.07061164739065 W\n",
      "[codecarbon INFO @ 23:54:17] Energy consumed for all CPUs : 0.011236 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:17] 0.101681 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:25] Energy consumed for RAM : 15.454290 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:25] Energy consumed for RAM : 15.415585 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:25] Energy consumed for all GPUs : 7.636996 kWh. Total GPU Power : 373.34685602345644 W\n",
      "[codecarbon INFO @ 23:54:25] Energy consumed for all CPUs : 2.874395 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:25] 25.926976 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:25] Energy consumed for all GPUs : 7.641445 kWh. Total GPU Power : 373.26455597358887 W\n",
      "[codecarbon INFO @ 23:54:25] Energy consumed for all CPUs : 2.881770 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:25] 25.977505 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:32] Energy consumed for RAM : 0.063408 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:32] Energy consumed for all GPUs : 0.031724 kWh. Total GPU Power : 374.1346171936134 W\n",
      "[codecarbon INFO @ 23:54:32] Energy consumed for all CPUs : 0.011820 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:32] 0.106951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:40] Energy consumed for RAM : 15.457411 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:40] Energy consumed for RAM : 15.418712 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:40] Energy consumed for all GPUs : 7.643013 kWh. Total GPU Power : 377.2773072154336 W\n",
      "[codecarbon INFO @ 23:54:40] Energy consumed for all CPUs : 2.882352 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:40] 25.982775 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:40] Energy consumed for all GPUs : 7.638563 kWh. Total GPU Power : 376.60388546714665 W\n",
      "[codecarbon INFO @ 23:54:40] Energy consumed for all CPUs : 2.874978 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:40] 25.932253 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:47] Energy consumed for RAM : 0.066537 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:47] Energy consumed for all GPUs : 0.033285 kWh. Total GPU Power : 374.6655267459106 W\n",
      "[codecarbon INFO @ 23:54:47] Energy consumed for all CPUs : 0.012403 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:47] 0.112225 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:55] Energy consumed for RAM : 15.460539 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:55] Energy consumed for RAM : 15.421840 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:54:55] Energy consumed for all GPUs : 7.644566 kWh. Total GPU Power : 373.03035819753467 W\n",
      "[codecarbon INFO @ 23:54:55] Energy consumed for all CPUs : 2.882935 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:55] 25.988040 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:54:55] Energy consumed for all GPUs : 7.640117 kWh. Total GPU Power : 373.1018753522985 W\n",
      "[codecarbon INFO @ 23:54:55] Energy consumed for all CPUs : 2.875561 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:54:55] 25.937518 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:03] Energy consumed for RAM : 0.069861 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:03] Energy consumed for all GPUs : 0.034947 kWh. Total GPU Power : 375.7786412008205 W\n",
      "[codecarbon INFO @ 23:55:03] Energy consumed for all CPUs : 0.013023 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:03] 0.117831 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:10] Energy consumed for RAM : 15.463668 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:10] Energy consumed for RAM : 15.424969 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:10] Energy consumed for all GPUs : 7.646154 kWh. Total GPU Power : 381.2341197842665 W\n",
      "[codecarbon INFO @ 23:55:10] Energy consumed for all CPUs : 2.883518 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:10] 25.993340 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:10] Energy consumed for all GPUs : 7.641704 kWh. Total GPU Power : 381.2425955190745 W\n",
      "[codecarbon INFO @ 23:55:10] Energy consumed for all CPUs : 2.876144 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:10] 25.942818 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:18] Energy consumed for RAM : 0.072987 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:18] Energy consumed for all GPUs : 0.036531 kWh. Total GPU Power : 380.6229302610176 W\n",
      "[codecarbon INFO @ 23:55:18] Energy consumed for all CPUs : 0.013606 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:18] 0.123124 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:25] Energy consumed for RAM : 15.466796 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:25] Energy consumed for RAM : 15.428097 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:25] Energy consumed for all GPUs : 7.647719 kWh. Total GPU Power : 376.00109260891924 W\n",
      "[codecarbon INFO @ 23:55:25] Energy consumed for all CPUs : 2.884102 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:25] 25.998617 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:25] Energy consumed for all GPUs : 7.643270 kWh. Total GPU Power : 376.0524920956333 W\n",
      "[codecarbon INFO @ 23:55:25] Energy consumed for all CPUs : 2.876728 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:25] 25.948094 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:33] Energy consumed for RAM : 0.076115 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:33] Energy consumed for all GPUs : 0.038103 kWh. Total GPU Power : 377.5714997843524 W\n",
      "[codecarbon INFO @ 23:55:33] Energy consumed for all CPUs : 0.014189 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:33] 0.128408 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:41] Energy consumed for RAM : 15.431441 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:41] Energy consumed for RAM : 15.470141 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:41] Energy consumed for all GPUs : 7.644945 kWh. Total GPU Power : 376.38763767897944 W\n",
      "[codecarbon INFO @ 23:55:41] Energy consumed for all CPUs : 2.877351 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:41] 25.953737 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:41] Energy consumed for all GPUs : 7.649394 kWh. Total GPU Power : 376.2527360220685 W\n",
      "[codecarbon INFO @ 23:55:41] Energy consumed for all CPUs : 2.884725 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:41] 26.004261 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:48] Energy consumed for RAM : 0.079245 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:48] Energy consumed for all GPUs : 0.039672 kWh. Total GPU Power : 376.7172372480552 W\n",
      "[codecarbon INFO @ 23:55:48] Energy consumed for all CPUs : 0.014772 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:48] 0.133689 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:56] Energy consumed for RAM : 15.434570 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:56] Energy consumed for RAM : 15.473269 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:56] Energy consumed for all GPUs : 7.646520 kWh. Total GPU Power : 378.0252077374714 W\n",
      "[codecarbon INFO @ 23:55:56] Energy consumed for all CPUs : 2.877934 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:56] 25.959024 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:56] Energy consumed for all GPUs : 7.650969 kWh. Total GPU Power : 378.0898974837375 W\n",
      "[codecarbon INFO @ 23:55:56] Energy consumed for all CPUs : 2.885309 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:56] 26.009547 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:03] Energy consumed for RAM : 0.082373 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:03] Energy consumed for all GPUs : 0.041232 kWh. Total GPU Power : 374.75823140558515 W\n",
      "[codecarbon INFO @ 23:56:03] Energy consumed for all CPUs : 0.015356 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:03] 0.138961 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:11] Energy consumed for RAM : 15.437698 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:11] Energy consumed for RAM : 15.476397 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:11] Energy consumed for all GPUs : 7.652526 kWh. Total GPU Power : 373.88757787359776 W\n",
      "[codecarbon INFO @ 23:56:11] Energy consumed for all CPUs : 2.885892 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:11] 26.014814 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:11] Energy consumed for all GPUs : 7.648076 kWh. Total GPU Power : 373.7744736941894 W\n",
      "[codecarbon INFO @ 23:56:11] Energy consumed for all CPUs : 2.878518 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:11] 25.964292 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:19] Energy consumed for RAM : 0.085696 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:19] Energy consumed for all GPUs : 0.042893 kWh. Total GPU Power : 375.3748697406961 W\n",
      "[codecarbon INFO @ 23:56:19] Energy consumed for all CPUs : 0.015975 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:19] 0.144564 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:26] Energy consumed for RAM : 15.440826 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:26] Energy consumed for RAM : 15.479526 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:26] Energy consumed for all GPUs : 7.649649 kWh. Total GPU Power : 377.8103320271093 W\n",
      "[codecarbon INFO @ 23:56:26] Energy consumed for all CPUs : 2.879101 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:26] 25.969576 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:26] Energy consumed for all GPUs : 7.654099 kWh. Total GPU Power : 377.6889608655398 W\n",
      "[codecarbon INFO @ 23:56:26] Energy consumed for all CPUs : 2.886475 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:26] 26.020100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:34] Energy consumed for RAM : 0.088825 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:34] Energy consumed for all GPUs : 0.044472 kWh. Total GPU Power : 379.31147653657183 W\n",
      "[codecarbon INFO @ 23:56:34] Energy consumed for all CPUs : 0.016559 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:34] 0.149855 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:41] Energy consumed for RAM : 15.443956 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:41] Energy consumed for RAM : 15.482654 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:41] Energy consumed for all GPUs : 7.651219 kWh. Total GPU Power : 376.7091473027789 W\n",
      "[codecarbon INFO @ 23:56:41] Energy consumed for all CPUs : 2.879684 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:41] 25.974858 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:41] Energy consumed for all GPUs : 7.655668 kWh. Total GPU Power : 376.8767064586604 W\n",
      "[codecarbon INFO @ 23:56:41] Energy consumed for all CPUs : 2.887059 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:41] 26.025381 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:49] Energy consumed for RAM : 0.091954 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:49] Energy consumed for all GPUs : 0.046037 kWh. Total GPU Power : 375.77737039821045 W\n",
      "[codecarbon INFO @ 23:56:49] Energy consumed for all CPUs : 0.017142 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:49] 0.155133 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:57] Energy consumed for RAM : 15.485979 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:57] Energy consumed for RAM : 15.447281 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:56:57] Energy consumed for all GPUs : 7.657322 kWh. Total GPU Power : 373.7457770448441 W\n",
      "[codecarbon INFO @ 23:56:57] Energy consumed for all CPUs : 2.887678 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:57] 26.030980 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:56:57] Energy consumed for all GPUs : 7.652873 kWh. Total GPU Power : 373.62748810599084 W\n",
      "[codecarbon INFO @ 23:56:57] Energy consumed for all CPUs : 2.880304 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:56:57] 25.980458 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:04] Energy consumed for RAM : 0.095084 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:04] Energy consumed for all GPUs : 0.047594 kWh. Total GPU Power : 373.6358487807332 W\n",
      "[codecarbon INFO @ 23:57:04] Energy consumed for all CPUs : 0.017725 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:04] 0.160403 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:12] Energy consumed for RAM : 15.489108 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:12] Energy consumed for RAM : 15.450409 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:12] Energy consumed for all GPUs : 7.658893 kWh. Total GPU Power : 377.2185561525419 W\n",
      "[codecarbon INFO @ 23:57:12] Energy consumed for all CPUs : 2.888262 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:12] 26.036263 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:12] Energy consumed for all GPUs : 7.654444 kWh. Total GPU Power : 377.28971563798746 W\n",
      "[codecarbon INFO @ 23:57:12] Energy consumed for all CPUs : 2.880888 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:12] 25.985741 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:19] Energy consumed for RAM : 0.098213 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:19] Energy consumed for all GPUs : 0.049146 kWh. Total GPU Power : 372.5390426375663 W\n",
      "[codecarbon INFO @ 23:57:19] Energy consumed for all CPUs : 0.018309 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:19] 0.165667 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:27] Energy consumed for RAM : 15.453537 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:27] Energy consumed for RAM : 15.492238 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:27] Energy consumed for all GPUs : 7.655984 kWh. Total GPU Power : 369.6896546049219 W\n",
      "[codecarbon INFO @ 23:57:27] Energy consumed for all GPUs : 7.660433 kWh. Total GPU Power : 369.5067144276918 W\n",
      "[codecarbon INFO @ 23:57:27] Energy consumed for all CPUs : 2.881471 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:27] 25.990992 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:27] Energy consumed for all CPUs : 2.888846 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:27] 26.041517 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:35] Energy consumed for RAM : 0.101593 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:35] Energy consumed for all GPUs : 0.050829 kWh. Total GPU Power : 374.257401909728 W\n",
      "[codecarbon INFO @ 23:57:35] Energy consumed for all CPUs : 0.018939 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:35] 0.171361 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:42] Energy consumed for RAM : 15.456660 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:42] Energy consumed for RAM : 15.495360 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:42] Energy consumed for all GPUs : 7.657556 kWh. Total GPU Power : 378.2455675974925 W\n",
      "[codecarbon INFO @ 23:57:42] Energy consumed for all CPUs : 2.882054 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:42] 25.996270 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:42] Energy consumed for all GPUs : 7.662006 kWh. Total GPU Power : 378.4084664095367 W\n",
      "[codecarbon INFO @ 23:57:42] Energy consumed for all CPUs : 2.889428 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:42] 26.046793 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:50] Energy consumed for RAM : 0.104722 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:50] Energy consumed for all GPUs : 0.052413 kWh. Total GPU Power : 380.27451243562734 W\n",
      "[codecarbon INFO @ 23:57:50] Energy consumed for all CPUs : 0.019522 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:50] 0.176657 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:57] Energy consumed for RAM : 15.459790 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:57] Energy consumed for RAM : 15.498489 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:57:57] Energy consumed for all GPUs : 7.659124 kWh. Total GPU Power : 376.4461808348231 W\n",
      "[codecarbon INFO @ 23:57:57] Energy consumed for all CPUs : 2.882637 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:57] 26.001550 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:57:57] Energy consumed for all GPUs : 7.663573 kWh. Total GPU Power : 376.298679005315 W\n",
      "[codecarbon INFO @ 23:57:57] Energy consumed for all CPUs : 2.890011 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:57:57] 26.052074 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:05] Energy consumed for RAM : 0.107852 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:05] Energy consumed for all GPUs : 0.053964 kWh. Total GPU Power : 372.36384259526415 W\n",
      "[codecarbon INFO @ 23:58:05] Energy consumed for all CPUs : 0.020105 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:05] 0.181921 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:13] Energy consumed for RAM : 15.501859 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:13] Energy consumed for RAM : 15.463161 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:13] Energy consumed for all GPUs : 7.665249 kWh. Total GPU Power : 373.5960064218547 W\n",
      "[codecarbon INFO @ 23:58:13] Energy consumed for all CPUs : 2.890640 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:13] 26.057748 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:13] Energy consumed for all GPUs : 7.660800 kWh. Total GPU Power : 373.36037447483164 W\n",
      "[codecarbon INFO @ 23:58:13] Energy consumed for all CPUs : 2.883266 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:13] 26.007227 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:20] Energy consumed for RAM : 0.110981 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:20] Energy consumed for all GPUs : 0.055535 kWh. Total GPU Power : 377.19409688841847 W\n",
      "[codecarbon INFO @ 23:58:20] Energy consumed for all CPUs : 0.020689 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:20] 0.187204 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:28] Energy consumed for RAM : 15.504988 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:28] Energy consumed for RAM : 15.466288 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:28] Energy consumed for all GPUs : 7.666835 kWh. Total GPU Power : 380.80347014597845 W\n",
      "[codecarbon INFO @ 23:58:28] Energy consumed for all CPUs : 2.891223 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:28] 26.063046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:28] Energy consumed for all GPUs : 7.662386 kWh. Total GPU Power : 381.0476655532037 W\n",
      "[codecarbon INFO @ 23:58:28] Energy consumed for all CPUs : 2.883849 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:28] 26.012523 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:35] Energy consumed for RAM : 0.114110 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:35] Energy consumed for all GPUs : 0.057095 kWh. Total GPU Power : 374.5143019007798 W\n",
      "[codecarbon INFO @ 23:58:35] Energy consumed for all CPUs : 0.021272 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:35] 0.192477 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:43] Energy consumed for RAM : 15.508117 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:43] Energy consumed for RAM : 15.469417 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:43] Energy consumed for all GPUs : 7.668389 kWh. Total GPU Power : 373.01530968466415 W\n",
      "[codecarbon INFO @ 23:58:43] Energy consumed for all CPUs : 2.891806 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:43] 26.068312 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:43] Energy consumed for all GPUs : 7.663939 kWh. Total GPU Power : 373.031753095462 W\n",
      "[codecarbon INFO @ 23:58:43] Energy consumed for all CPUs : 2.884432 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:43] 26.017788 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:51] Energy consumed for RAM : 0.117419 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:51] Energy consumed for all GPUs : 0.058749 kWh. Total GPU Power : 375.5417060710423 W\n",
      "[codecarbon INFO @ 23:58:51] Energy consumed for all CPUs : 0.021889 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:51] 0.198057 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:58] Energy consumed for RAM : 15.511245 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:58] Energy consumed for RAM : 15.472546 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:58:58] Energy consumed for all GPUs : 7.669956 kWh. Total GPU Power : 376.36596845422065 W\n",
      "[codecarbon INFO @ 23:58:58] Energy consumed for all CPUs : 2.892389 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:58] 26.073590 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:58:58] Energy consumed for all GPUs : 7.665506 kWh. Total GPU Power : 376.39028455163 W\n",
      "[codecarbon INFO @ 23:58:58] Energy consumed for all CPUs : 2.885015 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:58:58] 26.023067 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:06] Energy consumed for RAM : 0.120548 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:06] Energy consumed for all GPUs : 0.060316 kWh. Total GPU Power : 376.3050510438537 W\n",
      "[codecarbon INFO @ 23:59:06] Energy consumed for all CPUs : 0.022472 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:06] 0.203336 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:13] Energy consumed for RAM : 15.514374 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:13] Energy consumed for RAM : 15.475674 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:13] Energy consumed for all GPUs : 7.671512 kWh. Total GPU Power : 373.8008790506068 W\n",
      "[codecarbon INFO @ 23:59:13] Energy consumed for all CPUs : 2.892973 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:13] 26.078858 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:13] Energy consumed for all GPUs : 7.667063 kWh. Total GPU Power : 373.7840959158752 W\n",
      "[codecarbon INFO @ 23:59:13] Energy consumed for all CPUs : 2.885599 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:13] 26.028336 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:21] Energy consumed for RAM : 0.123678 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:21] Energy consumed for all GPUs : 0.061881 kWh. Total GPU Power : 375.3878202859059 W\n",
      "[codecarbon INFO @ 23:59:21] Energy consumed for all CPUs : 0.023056 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:21] 0.208614 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:29] Energy consumed for RAM : 15.517693 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:29] Energy consumed for RAM : 15.478993 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:29] Energy consumed for all GPUs : 7.673180 kWh. Total GPU Power : 377.3989758329913 W\n",
      "[codecarbon INFO @ 23:59:29] Energy consumed for all CPUs : 2.893592 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:29] 26.084465 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:29] Energy consumed for all GPUs : 7.668730 kWh. Total GPU Power : 377.4306321420461 W\n",
      "[codecarbon INFO @ 23:59:29] Energy consumed for all CPUs : 2.886218 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:29] 26.033942 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:36] Energy consumed for RAM : 0.126806 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:36] Energy consumed for all GPUs : 0.063456 kWh. Total GPU Power : 378.42236895188296 W\n",
      "[codecarbon INFO @ 23:59:36] Energy consumed for all CPUs : 0.023638 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:36] 0.213900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:44] Energy consumed for RAM : 15.520817 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:44] Energy consumed for RAM : 15.482116 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:44] Energy consumed for all GPUs : 7.674759 kWh. Total GPU Power : 379.71533417406886 W\n",
      "[codecarbon INFO @ 23:59:44] Energy consumed for all CPUs : 2.894175 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:44] 26.089751 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:44] Energy consumed for all GPUs : 7.670309 kWh. Total GPU Power : 379.80997543656457 W\n",
      "[codecarbon INFO @ 23:59:44] Energy consumed for all CPUs : 2.886801 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:44] 26.039226 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:51] Energy consumed for RAM : 0.129935 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:51] Energy consumed for all GPUs : 0.065020 kWh. Total GPU Power : 375.61005689437417 W\n",
      "[codecarbon INFO @ 23:59:51] Energy consumed for all CPUs : 0.024222 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:51] 0.219177 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:59] Energy consumed for RAM : 15.485245 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:59] Energy consumed for RAM : 15.523947 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:59:59] Energy consumed for all GPUs : 7.671864 kWh. Total GPU Power : 373.28386068191645 W\n",
      "[codecarbon INFO @ 23:59:59] Energy consumed for all CPUs : 2.887384 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:59] 26.044493 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:59:59] Energy consumed for all GPUs : 7.676313 kWh. Total GPU Power : 373.16798266198674 W\n",
      "[codecarbon INFO @ 23:59:59] Energy consumed for all CPUs : 2.894758 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:59:59] 26.095018 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:07] Energy consumed for RAM : 0.133283 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:07] Energy consumed for all GPUs : 0.066693 kWh. Total GPU Power : 375.44483400130935 W\n",
      "[codecarbon INFO @ 00:00:07] Energy consumed for all CPUs : 0.024846 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:07] 0.224823 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:14] Energy consumed for RAM : 15.488374 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:14] Energy consumed for RAM : 15.527075 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:14] Energy consumed for all GPUs : 7.673436 kWh. Total GPU Power : 377.43910354469364 W\n",
      "[codecarbon INFO @ 00:00:14] Energy consumed for all CPUs : 2.887967 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:14] 26.049777 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:14] Energy consumed for all GPUs : 7.677886 kWh. Total GPU Power : 377.50456736472296 W\n",
      "[codecarbon INFO @ 00:00:14] Energy consumed for all CPUs : 2.895341 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:14] 26.100302 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:22] Energy consumed for RAM : 0.136412 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:22] Energy consumed for all GPUs : 0.068269 kWh. Total GPU Power : 378.385315015573 W\n",
      "[codecarbon INFO @ 00:00:22] Energy consumed for all CPUs : 0.025429 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:22] 0.230110 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:29] Energy consumed for RAM : 15.530204 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:29] Energy consumed for RAM : 15.491503 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:29] Energy consumed for all GPUs : 7.679455 kWh. Total GPU Power : 376.98470168412155 W\n",
      "[codecarbon INFO @ 00:00:29] Energy consumed for all CPUs : 2.895924 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:29] 26.105583 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:29] Energy consumed for all GPUs : 7.675006 kWh. Total GPU Power : 376.8803793807209 W\n",
      "[codecarbon INFO @ 00:00:29] Energy consumed for all CPUs : 2.888550 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:29] 26.055059 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:37] Energy consumed for RAM : 0.139541 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:37] Energy consumed for all GPUs : 0.069830 kWh. Total GPU Power : 374.7394748990021 W\n",
      "[codecarbon INFO @ 00:00:37] Energy consumed for all CPUs : 0.026012 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:37] 0.235383 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:45] Energy consumed for RAM : 15.494838 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:45] Energy consumed for RAM : 15.533540 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:45] Energy consumed for all GPUs : 7.676672 kWh. Total GPU Power : 375.40224026560594 W\n",
      "[codecarbon INFO @ 00:00:45] Energy consumed for all CPUs : 2.889172 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:45] 26.060683 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:45] Energy consumed for all GPUs : 7.681122 kWh. Total GPU Power : 375.2556655354551 W\n",
      "[codecarbon INFO @ 00:00:45] Energy consumed for all CPUs : 2.896546 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:45] 26.111208 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:00:52] Energy consumed for RAM : 0.142671 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:00:52] Energy consumed for all GPUs : 0.071403 kWh. Total GPU Power : 377.787862699362 W\n",
      "[codecarbon INFO @ 00:00:52] Energy consumed for all CPUs : 0.026596 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:00:52] 0.240670 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:00] Energy consumed for RAM : 15.497967 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:00] Energy consumed for RAM : 15.536668 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:00] Energy consumed for all GPUs : 7.678252 kWh. Total GPU Power : 379.31303561344964 W\n",
      "[codecarbon INFO @ 00:01:00] Energy consumed for all CPUs : 2.889755 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:00] 26.065974 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:00] Energy consumed for all GPUs : 7.682701 kWh. Total GPU Power : 379.3135423826412 W\n",
      "[codecarbon INFO @ 00:01:00] Energy consumed for all CPUs : 2.897130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:00] 26.116499 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:07] Energy consumed for RAM : 0.145800 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:07] Energy consumed for all GPUs : 0.072974 kWh. Total GPU Power : 377.20160479930473 W\n",
      "[codecarbon INFO @ 00:01:07] Energy consumed for all CPUs : 0.027179 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:07] 0.245953 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:15] Energy consumed for RAM : 15.501096 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:15] Energy consumed for RAM : 15.539797 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:15] Energy consumed for all GPUs : 7.679804 kWh. Total GPU Power : 372.5564304812359 W\n",
      "[codecarbon INFO @ 00:01:15] Energy consumed for all CPUs : 2.890339 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:15] 26.071238 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:15] Energy consumed for all GPUs : 7.684265 kWh. Total GPU Power : 375.5732568681283 W\n",
      "[codecarbon INFO @ 00:01:15] Energy consumed for all CPUs : 2.897713 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:15] 26.121775 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:23] Energy consumed for RAM : 0.149119 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:23] Energy consumed for all GPUs : 0.074628 kWh. Total GPU Power : 374.1717233688924 W\n",
      "[codecarbon INFO @ 00:01:23] Energy consumed for all CPUs : 0.027798 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:23] 0.251545 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:30] Energy consumed for RAM : 15.504224 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:30] Energy consumed for RAM : 15.542925 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:30] Energy consumed for all GPUs : 7.681389 kWh. Total GPU Power : 380.70488361089787 W\n",
      "[codecarbon INFO @ 00:01:30] Energy consumed for all CPUs : 2.890922 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:30] 26.076535 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:30] Energy consumed for all GPUs : 7.685838 kWh. Total GPU Power : 377.8286731420698 W\n",
      "[codecarbon INFO @ 00:01:30] Energy consumed for all CPUs : 2.898296 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:30] 26.127059 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:38] Energy consumed for RAM : 0.152248 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:38] Energy consumed for all GPUs : 0.076201 kWh. Total GPU Power : 377.7386792891657 W\n",
      "[codecarbon INFO @ 00:01:38] Energy consumed for all CPUs : 0.028381 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:38] 0.256830 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:45] Energy consumed for RAM : 15.507354 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:45] Energy consumed for RAM : 15.546054 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:45] Energy consumed for all GPUs : 7.682949 kWh. Total GPU Power : 374.5427795338952 W\n",
      "[codecarbon INFO @ 00:01:45] Energy consumed for all CPUs : 2.891505 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:45] 26.081808 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:45] Energy consumed for all GPUs : 7.687398 kWh. Total GPU Power : 374.57882017359515 W\n",
      "[codecarbon INFO @ 00:01:45] Energy consumed for all CPUs : 2.898880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:45] 26.132332 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:53] Energy consumed for RAM : 0.155377 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:53] Energy consumed for all GPUs : 0.077773 kWh. Total GPU Power : 377.54345899809 W\n",
      "[codecarbon INFO @ 00:01:53] Energy consumed for all CPUs : 0.028964 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:53] 0.262115 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:01] Energy consumed for RAM : 15.510670 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:01] Energy consumed for RAM : 15.549370 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:01] Energy consumed for all GPUs : 7.689056 kWh. Total GPU Power : 375.56405895431243 W\n",
      "[codecarbon INFO @ 00:02:01] Energy consumed for all CPUs : 2.899498 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:01] 26.137924 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:01] Energy consumed for all GPUs : 7.684607 kWh. Total GPU Power : 375.4581351910246 W\n",
      "[codecarbon INFO @ 00:02:01] Energy consumed for all CPUs : 2.892124 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:01] 26.087401 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:08] Energy consumed for RAM : 0.158506 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:08] Energy consumed for all GPUs : 0.079343 kWh. Total GPU Power : 376.939351938329 W\n",
      "[codecarbon INFO @ 00:02:08] Energy consumed for all CPUs : 0.029547 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:08] 0.267397 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:16] Energy consumed for RAM : 15.513798 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:16] Energy consumed for RAM : 15.552499 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:16] Energy consumed for all GPUs : 7.686182 kWh. Total GPU Power : 378.50659190465075 W\n",
      "[codecarbon INFO @ 00:02:16] Energy consumed for all CPUs : 2.892707 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:16] 26.092687 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:16] Energy consumed for all GPUs : 7.690632 kWh. Total GPU Power : 378.36429173274263 W\n",
      "[codecarbon INFO @ 00:02:16] Energy consumed for all CPUs : 2.900081 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:16] 26.143211 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:23] Energy consumed for RAM : 0.161636 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:23] Energy consumed for all GPUs : 0.080904 kWh. Total GPU Power : 374.78845895732377 W\n",
      "[codecarbon INFO @ 00:02:23] Energy consumed for all CPUs : 0.030131 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:23] 0.272671 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:31] Energy consumed for RAM : 15.516928 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:31] Energy consumed for RAM : 15.555629 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:31] Energy consumed for all GPUs : 7.687742 kWh. Total GPU Power : 374.4054384679642 W\n",
      "[codecarbon INFO @ 00:02:31] Energy consumed for all CPUs : 2.893290 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:31] 26.097960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:31] Energy consumed for all GPUs : 7.692191 kWh. Total GPU Power : 374.3896109653082 W\n",
      "[codecarbon INFO @ 00:02:31] Energy consumed for all CPUs : 2.900665 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:31] 26.148484 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:39] Energy consumed for RAM : 0.164979 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:39] Energy consumed for all GPUs : 0.082568 kWh. Total GPU Power : 373.8455017166057 W\n",
      "[codecarbon INFO @ 00:02:39] Energy consumed for all CPUs : 0.030754 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:39] 0.278302 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:46] Energy consumed for RAM : 15.520055 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:46] Energy consumed for RAM : 15.558756 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:46] Energy consumed for all GPUs : 7.689310 kWh. Total GPU Power : 376.6777705319332 W\n",
      "[codecarbon INFO @ 00:02:46] Energy consumed for all CPUs : 2.893873 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:46] 26.103239 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:46] Energy consumed for all GPUs : 7.693759 kWh. Total GPU Power : 376.7877800155077 W\n",
      "[codecarbon INFO @ 00:02:46] Energy consumed for all CPUs : 2.901248 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:46] 26.153762 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:02:54] Energy consumed for RAM : 0.168107 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:02:54] Energy consumed for all GPUs : 0.084153 kWh. Total GPU Power : 380.706627373412 W\n",
      "[codecarbon INFO @ 00:02:54] Energy consumed for all CPUs : 0.031337 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:02:54] 0.283597 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:01] Energy consumed for RAM : 15.523184 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:01] Energy consumed for RAM : 15.561885 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:01] Energy consumed for all GPUs : 7.690878 kWh. Total GPU Power : 376.6202664932029 W\n",
      "[codecarbon INFO @ 00:03:01] Energy consumed for all CPUs : 2.894457 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:01] 26.108519 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:01] Energy consumed for all GPUs : 7.695328 kWh. Total GPU Power : 376.62453074814863 W\n",
      "[codecarbon INFO @ 00:03:01] Energy consumed for all CPUs : 2.901831 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:01] 26.159044 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:09] Energy consumed for RAM : 0.171236 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:09] Energy consumed for all GPUs : 0.085710 kWh. Total GPU Power : 373.9579814982599 W\n",
      "[codecarbon INFO @ 00:03:09] Energy consumed for all CPUs : 0.031920 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:09] 0.288867 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:17] Energy consumed for RAM : 15.526532 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:17] Energy consumed for RAM : 15.565231 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:17] Energy consumed for all GPUs : 7.697003 kWh. Total GPU Power : 376.08867550092737 W\n",
      "[codecarbon INFO @ 00:03:17] Energy consumed for all CPUs : 2.902455 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:17] 26.164689 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:17] Energy consumed for all GPUs : 7.692554 kWh. Total GPU Power : 375.93119011609406 W\n",
      "[codecarbon INFO @ 00:03:17] Energy consumed for all CPUs : 2.895081 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:17] 26.114166 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:24] Energy consumed for RAM : 0.174365 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:24] Energy consumed for all GPUs : 0.087284 kWh. Total GPU Power : 377.7826447976365 W\n",
      "[codecarbon INFO @ 00:03:24] Energy consumed for all CPUs : 0.032504 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:24] 0.294153 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:32] Energy consumed for RAM : 15.568359 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:32] Energy consumed for RAM : 15.529660 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:32] Energy consumed for all GPUs : 7.698579 kWh. Total GPU Power : 378.35714090339155 W\n",
      "[codecarbon INFO @ 00:03:32] Energy consumed for all CPUs : 2.903038 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:32] 26.169976 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:32] Energy consumed for all GPUs : 7.694129 kWh. Total GPU Power : 378.45286242283146 W\n",
      "[codecarbon INFO @ 00:03:32] Energy consumed for all CPUs : 2.895665 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:32] 26.119453 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:39] Energy consumed for RAM : 0.177495 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:39] Energy consumed for all GPUs : 0.088849 kWh. Total GPU Power : 375.762421900938 W\n",
      "[codecarbon INFO @ 00:03:39] Energy consumed for all CPUs : 0.033087 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:39] 0.299430 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:47] Energy consumed for RAM : 15.571486 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:47] Energy consumed for RAM : 15.532786 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:47] Energy consumed for all GPUs : 7.700131 kWh. Total GPU Power : 372.9622185360938 W\n",
      "[codecarbon INFO @ 00:03:47] Energy consumed for all CPUs : 2.903621 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:47] 26.175239 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:47] Energy consumed for all GPUs : 7.695682 kWh. Total GPU Power : 373.071266571272 W\n",
      "[codecarbon INFO @ 00:03:47] Energy consumed for all CPUs : 2.896247 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:47] 26.124715 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:03:55] Energy consumed for RAM : 0.180803 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:55] Energy consumed for all GPUs : 0.090497 kWh. Total GPU Power : 374.16329526913046 W\n",
      "[codecarbon INFO @ 00:03:55] Energy consumed for all CPUs : 0.033704 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:55] 0.305004 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:02] Energy consumed for RAM : 15.574615 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:02] Energy consumed for RAM : 15.535915 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:02] Energy consumed for all GPUs : 7.701704 kWh. Total GPU Power : 377.64173723148957 W\n",
      "[codecarbon INFO @ 00:04:02] Energy consumed for all CPUs : 2.904205 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:02] 26.180523 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:02] Energy consumed for all GPUs : 7.697254 kWh. Total GPU Power : 377.660097210575 W\n",
      "[codecarbon INFO @ 00:04:02] Energy consumed for all CPUs : 2.896831 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:02] 26.130000 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:10] Energy consumed for RAM : 0.183932 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:10] Energy consumed for all GPUs : 0.092082 kWh. Total GPU Power : 380.70888510407417 W\n",
      "[codecarbon INFO @ 00:04:10] Energy consumed for all CPUs : 0.034287 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:10] 0.310300 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:17] Energy consumed for RAM : 15.577744 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:17] Energy consumed for RAM : 15.539043 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:17] Energy consumed for all GPUs : 7.703278 kWh. Total GPU Power : 377.8970445923792 W\n",
      "[codecarbon INFO @ 00:04:17] Energy consumed for all CPUs : 2.904788 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:17] 26.185810 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:17] Energy consumed for all GPUs : 7.698828 kWh. Total GPU Power : 377.98426592519155 W\n",
      "[codecarbon INFO @ 00:04:17] Energy consumed for all CPUs : 2.897414 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:17] 26.135285 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:25] Energy consumed for RAM : 0.187060 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:25] Energy consumed for all GPUs : 0.093641 kWh. Total GPU Power : 374.31923652524983 W\n",
      "[codecarbon INFO @ 00:04:25] Energy consumed for all CPUs : 0.034870 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:25] 0.315571 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:33] Energy consumed for RAM : 15.581057 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:33] Energy consumed for RAM : 15.542356 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:33] Energy consumed for all GPUs : 7.704930 kWh. Total GPU Power : 374.80262701985845 W\n",
      "[codecarbon INFO @ 00:04:33] Energy consumed for all CPUs : 2.905405 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:33] 26.191392 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:33] Energy consumed for all GPUs : 7.700481 kWh. Total GPU Power : 374.81321165495825 W\n",
      "[codecarbon INFO @ 00:04:33] Energy consumed for all CPUs : 2.898031 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:33] 26.140868 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:40] Energy consumed for RAM : 0.190189 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:40] Energy consumed for all GPUs : 0.095201 kWh. Total GPU Power : 374.63051028920523 W\n",
      "[codecarbon INFO @ 00:04:40] Energy consumed for all CPUs : 0.035454 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:40] 0.320844 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:48] Energy consumed for RAM : 15.584185 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:48] Energy consumed for RAM : 15.545484 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:48] Energy consumed for all GPUs : 7.706500 kWh. Total GPU Power : 376.81313383436503 W\n",
      "[codecarbon INFO @ 00:04:48] Energy consumed for all CPUs : 2.905989 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:48] 26.196674 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:48] Energy consumed for all GPUs : 7.702050 kWh. Total GPU Power : 376.87483364754905 W\n",
      "[codecarbon INFO @ 00:04:48] Energy consumed for all CPUs : 2.898615 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:48] 26.146149 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:55] Energy consumed for RAM : 0.193318 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:55] Energy consumed for all GPUs : 0.096769 kWh. Total GPU Power : 376.5239382506738 W\n",
      "[codecarbon INFO @ 00:04:55] Energy consumed for all CPUs : 0.036037 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:55] 0.326124 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:03] Energy consumed for RAM : 15.587314 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:03] Energy consumed for RAM : 15.548613 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:03] Energy consumed for all GPUs : 7.708049 kWh. Total GPU Power : 372.1330644934643 W\n",
      "[codecarbon INFO @ 00:05:03] Energy consumed for all CPUs : 2.906572 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:03] 26.201935 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:03] Energy consumed for all GPUs : 7.703600 kWh. Total GPU Power : 372.10155108096126 W\n",
      "[codecarbon INFO @ 00:05:03] Energy consumed for all CPUs : 2.899198 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:03] 26.151411 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:11] Energy consumed for RAM : 0.196663 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:11] Energy consumed for all GPUs : 0.098431 kWh. Total GPU Power : 373.2984206461893 W\n",
      "[codecarbon INFO @ 00:05:11] Energy consumed for all CPUs : 0.036660 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:11] 0.331754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:18] Energy consumed for RAM : 15.590442 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:18] Energy consumed for RAM : 15.551741 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:18] Energy consumed for all GPUs : 7.709625 kWh. Total GPU Power : 378.33382606961715 W\n",
      "[codecarbon INFO @ 00:05:18] Energy consumed for all CPUs : 2.907155 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:18] 26.207222 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:18] Energy consumed for all GPUs : 7.705175 kWh. Total GPU Power : 378.3730964608617 W\n",
      "[codecarbon INFO @ 00:05:18] Energy consumed for all CPUs : 2.899781 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:18] 26.156698 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:26] Energy consumed for RAM : 0.199790 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:26] Energy consumed for all GPUs : 0.100015 kWh. Total GPU Power : 380.55048096231116 W\n",
      "[codecarbon INFO @ 00:05:26] Energy consumed for all CPUs : 0.037243 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:26] 0.337048 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:33] Energy consumed for RAM : 15.593571 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:33] Energy consumed for RAM : 15.554870 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:33] Energy consumed for all GPUs : 7.711200 kWh. Total GPU Power : 378.2334351180277 W\n",
      "[codecarbon INFO @ 00:05:33] Energy consumed for all CPUs : 2.907738 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:33] 26.212510 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:33] Energy consumed for all GPUs : 7.706750 kWh. Total GPU Power : 378.2541311168874 W\n",
      "[codecarbon INFO @ 00:05:33] Energy consumed for all CPUs : 2.900364 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:33] 26.161985 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:41] Energy consumed for RAM : 0.202919 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:41] Energy consumed for all GPUs : 0.101578 kWh. Total GPU Power : 375.39066213040957 W\n",
      "[codecarbon INFO @ 00:05:41] Energy consumed for all CPUs : 0.037827 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:41] 0.342324 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:49] Energy consumed for RAM : 15.596913 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:49] Energy consumed for RAM : 15.558212 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:49] Energy consumed for all GPUs : 7.712865 kWh. Total GPU Power : 374.24171223294064 W\n",
      "[codecarbon INFO @ 00:05:49] Energy consumed for all CPUs : 2.908361 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:49] 26.218139 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:49] Energy consumed for all GPUs : 7.708415 kWh. Total GPU Power : 374.2829119957878 W\n",
      "[codecarbon INFO @ 00:05:49] Energy consumed for all CPUs : 2.900987 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:49] 26.167615 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:05:56] Energy consumed for RAM : 0.206048 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:05:56] Energy consumed for all GPUs : 0.103146 kWh. Total GPU Power : 376.61248376837966 W\n",
      "[codecarbon INFO @ 00:05:56] Energy consumed for all CPUs : 0.038410 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:05:56] 0.347604 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:04] Energy consumed for RAM : 15.561340 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:04] Energy consumed for RAM : 15.600042 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:04] Energy consumed for all GPUs : 7.709992 kWh. Total GPU Power : 378.6042688022515 W\n",
      "[codecarbon INFO @ 00:06:04] Energy consumed for all CPUs : 2.901571 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:04] 26.172902 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:04] Energy consumed for all GPUs : 7.714441 kWh. Total GPU Power : 378.5154292539367 W\n",
      "[codecarbon INFO @ 00:06:04] Energy consumed for all CPUs : 2.908945 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:04] 26.223428 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:11] Energy consumed for RAM : 0.209177 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:11] Energy consumed for all GPUs : 0.104708 kWh. Total GPU Power : 374.9978014989028 W\n",
      "[codecarbon INFO @ 00:06:11] Energy consumed for all CPUs : 0.038993 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:11] 0.352878 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:19] Energy consumed for RAM : 15.564468 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:19] Energy consumed for RAM : 15.603170 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:19] Energy consumed for all GPUs : 7.711552 kWh. Total GPU Power : 374.78225886788647 W\n",
      "[codecarbon INFO @ 00:06:19] Energy consumed for all CPUs : 2.902154 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:19] 26.178174 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:19] Energy consumed for all GPUs : 7.716002 kWh. Total GPU Power : 374.81544260813695 W\n",
      "[codecarbon INFO @ 00:06:19] Energy consumed for all CPUs : 2.909528 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:19] 26.228700 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:27] Energy consumed for RAM : 0.212498 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:27] Energy consumed for all GPUs : 0.106371 kWh. Total GPU Power : 375.89233704820157 W\n",
      "[codecarbon INFO @ 00:06:27] Energy consumed for all CPUs : 0.039613 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:27] 0.358482 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:34] Energy consumed for RAM : 15.567598 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:34] Energy consumed for RAM : 15.606300 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:34] Energy consumed for all GPUs : 7.713127 kWh. Total GPU Power : 378.0733573944677 W\n",
      "[codecarbon INFO @ 00:06:34] Energy consumed for all CPUs : 2.902737 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:34] 26.183462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:34] Energy consumed for all GPUs : 7.717577 kWh. Total GPU Power : 378.060438313919 W\n",
      "[codecarbon INFO @ 00:06:34] Energy consumed for all CPUs : 2.910111 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:34] 26.233988 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:42] Energy consumed for RAM : 0.215624 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:42] Energy consumed for all GPUs : 0.107954 kWh. Total GPU Power : 380.42170988905946 W\n",
      "[codecarbon INFO @ 00:06:42] Energy consumed for all CPUs : 0.040196 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:42] 0.363773 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:49] Energy consumed for RAM : 15.570727 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:49] Energy consumed for RAM : 15.609428 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:49] Energy consumed for all GPUs : 7.714698 kWh. Total GPU Power : 377.0716769018265 W\n",
      "[codecarbon INFO @ 00:06:49] Energy consumed for all CPUs : 2.903320 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:49] 26.188745 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:49] Energy consumed for all GPUs : 7.719147 kWh. Total GPU Power : 377.2190758621974 W\n",
      "[codecarbon INFO @ 00:06:49] Energy consumed for all CPUs : 2.910694 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:49] 26.239270 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:06:57] Energy consumed for RAM : 0.218752 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:06:57] Energy consumed for all GPUs : 0.109507 kWh. Total GPU Power : 372.95992359319314 W\n",
      "[codecarbon INFO @ 00:06:57] Energy consumed for all CPUs : 0.040779 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:06:57] 0.369038 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:04] Energy consumed for RAM : 15.574052 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:04] Energy consumed for RAM : 15.612752 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:05] Energy consumed for all GPUs : 7.716355 kWh. Total GPU Power : 374.42822942028215 W\n",
      "[codecarbon INFO @ 00:07:05] Energy consumed for all CPUs : 2.903941 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:05] 26.194348 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:05] Energy consumed for all GPUs : 7.720804 kWh. Total GPU Power : 374.4789551554822 W\n",
      "[codecarbon INFO @ 00:07:05] Energy consumed for all CPUs : 2.911315 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:05] 26.244872 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:12] Energy consumed for RAM : 0.221881 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:12] Energy consumed for all GPUs : 0.111082 kWh. Total GPU Power : 378.24222673112763 W\n",
      "[codecarbon INFO @ 00:07:12] Energy consumed for all CPUs : 0.041362 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:12] 0.374326 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:19] Energy consumed for RAM : 15.615876 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:19] Energy consumed for RAM : 15.577178 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:20] Energy consumed for all GPUs : 7.722368 kWh. Total GPU Power : 375.95075885932397 W\n",
      "[codecarbon INFO @ 00:07:20] Energy consumed for all CPUs : 2.911898 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:20] 26.250141 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:20] Energy consumed for all GPUs : 7.717918 kWh. Total GPU Power : 375.70131204656246 W\n",
      "[codecarbon INFO @ 00:07:20] Energy consumed for all CPUs : 2.904524 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:20] 26.199620 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:27] Energy consumed for RAM : 0.225011 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:27] Energy consumed for all GPUs : 0.112632 kWh. Total GPU Power : 372.0054076309537 W\n",
      "[codecarbon INFO @ 00:07:27] Energy consumed for all CPUs : 0.041945 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:27] 0.379588 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:34] Energy consumed for RAM : 15.619003 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:34] Energy consumed for RAM : 15.580305 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:35] Energy consumed for all GPUs : 7.723925 kWh. Total GPU Power : 374.1940856914228 W\n",
      "[codecarbon INFO @ 00:07:35] Energy consumed for all CPUs : 2.912481 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:35] 26.255410 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:35] Energy consumed for all GPUs : 7.719476 kWh. Total GPU Power : 374.21772909873454 W\n",
      "[codecarbon INFO @ 00:07:35] Energy consumed for all CPUs : 2.905107 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:35] 26.204888 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:43] Energy consumed for RAM : 0.228371 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:43] Energy consumed for all GPUs : 0.114309 kWh. Total GPU Power : 375.18281146106153 W\n",
      "[codecarbon INFO @ 00:07:43] Energy consumed for all CPUs : 0.042572 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:43] 0.385252 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:49] Energy consumed for RAM : 15.622132 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:49] Energy consumed for RAM : 15.583434 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:50] Energy consumed for all GPUs : 7.725498 kWh. Total GPU Power : 377.6611859095635 W\n",
      "[codecarbon INFO @ 00:07:50] Energy consumed for all CPUs : 2.913064 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:50] 26.260695 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:50] Energy consumed for all GPUs : 7.721049 kWh. Total GPU Power : 377.7317107811591 W\n",
      "[codecarbon INFO @ 00:07:50] Energy consumed for all CPUs : 2.905690 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:50] 26.210173 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:07:58] Energy consumed for RAM : 0.231500 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:07:58] Energy consumed for all GPUs : 0.115899 kWh. Total GPU Power : 381.5653227117692 W\n",
      "[codecarbon INFO @ 00:07:58] Energy consumed for all CPUs : 0.043155 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:07:58] 0.390553 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:04] Energy consumed for RAM : 15.625261 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:08:04] Energy consumed for RAM : 15.586561 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:08:05] Energy consumed for all GPUs : 7.727062 kWh. Total GPU Power : 375.4544794994278 W\n",
      "[codecarbon INFO @ 00:08:05] Energy consumed for all CPUs : 2.913647 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:08:05] 26.265970 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:05] Energy consumed for all GPUs : 7.722612 kWh. Total GPU Power : 375.5703343312421 W\n",
      "[codecarbon INFO @ 00:08:05] Energy consumed for all CPUs : 2.906273 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:08:05] 26.215447 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:13] Energy consumed for RAM : 0.234628 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:08:13] Energy consumed for all GPUs : 0.117454 kWh. Total GPU Power : 373.4303093661632 W\n",
      "[codecarbon INFO @ 00:08:13] Energy consumed for all CPUs : 0.043738 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:08:13] 0.395820 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:21] Energy consumed for RAM : 15.589918 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:08:21] Energy consumed for RAM : 15.628618 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:08:21] Energy consumed for all GPUs : 7.724290 kWh. Total GPU Power : 375.44520215115847 W\n",
      "[codecarbon INFO @ 00:08:21] Energy consumed for all CPUs : 2.906899 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:08:21] 26.221107 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:21] Energy consumed for all GPUs : 7.728739 kWh. Total GPU Power : 375.3554400611712 W\n",
      "[codecarbon INFO @ 00:08:21] Energy consumed for all CPUs : 2.914273 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:08:21] 26.271631 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:28] Energy consumed for RAM : 0.237758 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:08:28] Energy consumed for all GPUs : 0.119027 kWh. Total GPU Power : 377.650633989012 W\n",
      "[codecarbon INFO @ 00:08:28] Energy consumed for all CPUs : 0.044322 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:08:28] 0.401106 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:36] Energy consumed for RAM : 15.593047 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:08:36] Energy consumed for RAM : 15.631747 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:08:36] Energy consumed for all GPUs : 7.725879 kWh. Total GPU Power : 381.550071447176 W\n",
      "[codecarbon INFO @ 00:08:36] Energy consumed for all CPUs : 2.907482 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:08:36] 26.226408 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:36] Energy consumed for all GPUs : 7.730328 kWh. Total GPU Power : 381.5966541908303 W\n",
      "[codecarbon INFO @ 00:08:36] Energy consumed for all CPUs : 2.914856 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:08:36] 26.276931 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:08:43] Energy consumed for RAM : 0.240887 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:08:43] Energy consumed for all GPUs : 0.120593 kWh. Total GPU Power : 376.14936963960224 W\n",
      "[codecarbon INFO @ 00:08:43] Energy consumed for all CPUs : 0.044905 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:08:43] 0.406384 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:10:15] Energy consumed for RAM : 0.260082 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:10:15] Energy consumed for all GPUs : 0.130218 kWh. Total GPU Power : 376.9460824128826 W\n",
      "[codecarbon INFO @ 00:10:15] Energy consumed for all CPUs : 0.048483 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:10:15] 0.438782 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:10:21] Energy consumed for RAM : 15.653833 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:10:21] Energy consumed for RAM : 15.615129 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:10:21] Energy consumed for all GPUs : 7.741408 kWh. Total GPU Power : 380.4579970183667 W\n",
      "[codecarbon INFO @ 00:10:21] Energy consumed for all CPUs : 2.918974 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:10:21] 26.314215 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:10:21] Energy consumed for all GPUs : 7.736958 kWh. Total GPU Power : 380.4684871584324 W\n",
      "[codecarbon INFO @ 00:10:21] Energy consumed for all CPUs : 2.911600 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:10:22] 26.263687 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:10:30] Energy consumed for RAM : 0.263211 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:10:30] Energy consumed for all GPUs : 0.131795 kWh. Total GPU Power : 378.6135296132975 W\n",
      "[codecarbon INFO @ 00:10:30] Energy consumed for all CPUs : 0.049066 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:10:30] 0.444071 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:10:36] Energy consumed for RAM : 15.656962 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:10:36] Energy consumed for RAM : 15.618258 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:10:36] Energy consumed for all GPUs : 7.742970 kWh. Total GPU Power : 375.03951856728025 W\n",
      "[codecarbon INFO @ 00:10:36] Energy consumed for all CPUs : 2.919558 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:10:36] 26.319490 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:10:37] Energy consumed for all GPUs : 7.738521 kWh. Total GPU Power : 375.0840474830676 W\n",
      "[codecarbon INFO @ 00:10:37] Energy consumed for all CPUs : 2.912183 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:10:37] 26.268962 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:10:45] Energy consumed for RAM : 0.266340 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:10:45] Energy consumed for all GPUs : 0.133353 kWh. Total GPU Power : 374.20090514928415 W\n",
      "[codecarbon INFO @ 00:10:45] Energy consumed for all CPUs : 0.049649 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:10:45] 0.449342 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:10:53] Energy consumed for RAM : 15.621622 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:10:53] Energy consumed for RAM : 15.660327 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:10:53] Energy consumed for all GPUs : 7.740200 kWh. Total GPU Power : 375.0561081079343 W\n",
      "[codecarbon INFO @ 00:10:53] Energy consumed for all CPUs : 2.912810 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:10:53] 26.274632 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:10:53] Energy consumed for all GPUs : 7.744649 kWh. Total GPU Power : 374.98498598677423 W\n",
      "[codecarbon INFO @ 00:10:53] Energy consumed for all CPUs : 2.920185 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:10:53] 26.325161 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:00] Energy consumed for RAM : 0.269469 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:00] Energy consumed for all GPUs : 0.134939 kWh. Total GPU Power : 380.79444797574945 W\n",
      "[codecarbon INFO @ 00:11:00] Energy consumed for all CPUs : 0.050232 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:00] 0.454640 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:08] Energy consumed for RAM : 15.624750 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:08] Energy consumed for RAM : 15.663454 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:08] Energy consumed for all GPUs : 7.741790 kWh. Total GPU Power : 381.86094529926487 W\n",
      "[codecarbon INFO @ 00:11:08] Energy consumed for all CPUs : 2.913393 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:08] 26.279934 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:08] Energy consumed for all GPUs : 7.746240 kWh. Total GPU Power : 381.9406280411259 W\n",
      "[codecarbon INFO @ 00:11:08] Energy consumed for all CPUs : 2.920768 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:08] 26.330462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:15] Energy consumed for RAM : 0.272598 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:15] Energy consumed for all GPUs : 0.136500 kWh. Total GPU Power : 374.6548938820036 W\n",
      "[codecarbon INFO @ 00:11:15] Energy consumed for all CPUs : 0.050816 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:15] 0.459914 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:23] Energy consumed for RAM : 15.627879 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:23] Energy consumed for RAM : 15.666583 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:23] Energy consumed for all GPUs : 7.743341 kWh. Total GPU Power : 372.3291788712541 W\n",
      "[codecarbon INFO @ 00:11:23] Energy consumed for all CPUs : 2.913977 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:23] 26.285196 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:23] Energy consumed for all GPUs : 7.747790 kWh. Total GPU Power : 372.3426432193185 W\n",
      "[codecarbon INFO @ 00:11:23] Energy consumed for all CPUs : 2.921351 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:23] 26.335725 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:31] Energy consumed for RAM : 0.275926 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:31] Energy consumed for all GPUs : 0.138153 kWh. Total GPU Power : 373.27164015565563 W\n",
      "[codecarbon INFO @ 00:11:31] Energy consumed for all CPUs : 0.051436 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:31] 0.465515 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:38] Energy consumed for RAM : 15.669712 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:38] Energy consumed for RAM : 15.631008 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:38] Energy consumed for all GPUs : 7.749354 kWh. Total GPU Power : 375.4678049175991 W\n",
      "[codecarbon INFO @ 00:11:38] Energy consumed for all CPUs : 2.921935 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:38] 26.341001 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:38] Energy consumed for all GPUs : 7.744904 kWh. Total GPU Power : 375.34652367691376 W\n",
      "[codecarbon INFO @ 00:11:38] Energy consumed for all CPUs : 2.914560 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:38] 26.290473 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:46] Energy consumed for RAM : 0.279055 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:46] Energy consumed for all GPUs : 0.139728 kWh. Total GPU Power : 378.2640659220362 W\n",
      "[codecarbon INFO @ 00:11:46] Energy consumed for all CPUs : 0.052019 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:46] 0.470802 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:53] Energy consumed for RAM : 15.672841 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:53] Energy consumed for RAM : 15.634136 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:11:53] Energy consumed for all GPUs : 7.750918 kWh. Total GPU Power : 375.4681364134983 W\n",
      "[codecarbon INFO @ 00:11:53] Energy consumed for all CPUs : 2.922518 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:53] 26.346276 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:11:53] Energy consumed for all GPUs : 7.746468 kWh. Total GPU Power : 375.6054033148069 W\n",
      "[codecarbon INFO @ 00:11:53] Energy consumed for all CPUs : 2.915143 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:11:53] 26.295747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:01] Energy consumed for RAM : 0.282184 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:01] Energy consumed for all GPUs : 0.141288 kWh. Total GPU Power : 374.4919568415067 W\n",
      "[codecarbon INFO @ 00:12:01] Energy consumed for all CPUs : 0.052603 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:01] 0.476075 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:09] Energy consumed for RAM : 15.637461 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:09] Energy consumed for RAM : 15.676167 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:09] Energy consumed for all GPUs : 7.748135 kWh. Total GPU Power : 376.66867367093374 W\n",
      "[codecarbon INFO @ 00:12:09] Energy consumed for all CPUs : 2.915763 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:09] 26.301359 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:09] Energy consumed for all GPUs : 7.752585 kWh. Total GPU Power : 376.5675792633533 W\n",
      "[codecarbon INFO @ 00:12:09] Energy consumed for all CPUs : 2.923138 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:09] 26.351889 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:16] Energy consumed for RAM : 0.285314 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:16] Energy consumed for all GPUs : 0.142869 kWh. Total GPU Power : 379.5849519905304 W\n",
      "[codecarbon INFO @ 00:12:16] Energy consumed for all CPUs : 0.053186 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:16] 0.481369 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:24] Energy consumed for RAM : 15.679296 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:24] Energy consumed for RAM : 15.640591 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:24] Energy consumed for all GPUs : 7.754161 kWh. Total GPU Power : 378.46751142474244 W\n",
      "[codecarbon INFO @ 00:12:24] Energy consumed for all CPUs : 2.923721 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:24] 26.357178 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:24] Energy consumed for all GPUs : 7.749711 kWh. Total GPU Power : 378.3837542910572 W\n",
      "[codecarbon INFO @ 00:12:24] Energy consumed for all CPUs : 2.916347 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:24] 26.306649 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:31] Energy consumed for RAM : 0.288444 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:31] Energy consumed for all GPUs : 0.144435 kWh. Total GPU Power : 375.9907995222828 W\n",
      "[codecarbon INFO @ 00:12:31] Energy consumed for all CPUs : 0.053769 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:31] 0.486648 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:39] Energy consumed for RAM : 15.682425 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:39] Energy consumed for RAM : 15.643719 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:39] Energy consumed for all GPUs : 7.755725 kWh. Total GPU Power : 375.6527498156238 W\n",
      "[codecarbon INFO @ 00:12:39] Energy consumed for all CPUs : 2.924304 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:39] 26.362454 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:39] Energy consumed for all GPUs : 7.751276 kWh. Total GPU Power : 375.73494599008677 W\n",
      "[codecarbon INFO @ 00:12:39] Energy consumed for all CPUs : 2.916930 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:39] 26.311925 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:47] Energy consumed for RAM : 0.291795 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:47] Energy consumed for all GPUs : 0.146102 kWh. Total GPU Power : 373.7075860132468 W\n",
      "[codecarbon INFO @ 00:12:47] Energy consumed for all CPUs : 0.054394 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:47] 0.492291 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:54] Energy consumed for RAM : 15.685554 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:54] Energy consumed for RAM : 15.646848 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:12:54] Energy consumed for all GPUs : 7.752845 kWh. Total GPU Power : 376.78731133266353 W\n",
      "[codecarbon INFO @ 00:12:54] Energy consumed for all CPUs : 2.917513 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:54] 26.317206 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:12:54] Energy consumed for all GPUs : 7.757295 kWh. Total GPU Power : 376.7225018327751 W\n",
      "[codecarbon INFO @ 00:12:54] Energy consumed for all CPUs : 2.924888 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:12:54] 26.367736 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:02] Energy consumed for RAM : 0.294924 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:02] Energy consumed for all GPUs : 0.147686 kWh. Total GPU Power : 380.30343303098255 W\n",
      "[codecarbon INFO @ 00:13:02] Energy consumed for all CPUs : 0.054977 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:02] 0.497587 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:09] Energy consumed for RAM : 15.649977 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:09] Energy consumed for RAM : 15.688682 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:09] Energy consumed for all GPUs : 7.758859 kWh. Total GPU Power : 375.6925369929862 W\n",
      "[codecarbon INFO @ 00:13:09] Energy consumed for all CPUs : 2.925471 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:09] 26.373013 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:09] Energy consumed for all GPUs : 7.754410 kWh. Total GPU Power : 375.62456323181084 W\n",
      "[codecarbon INFO @ 00:13:09] Energy consumed for all CPUs : 2.918097 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:09] 26.322483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:17] Energy consumed for RAM : 0.298053 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:17] Energy consumed for all GPUs : 0.149242 kWh. Total GPU Power : 373.49018840502436 W\n",
      "[codecarbon INFO @ 00:13:17] Energy consumed for all CPUs : 0.055560 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:17] 0.502855 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:25] Energy consumed for RAM : 15.653322 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:25] Energy consumed for RAM : 15.692029 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:25] Energy consumed for all GPUs : 7.756086 kWh. Total GPU Power : 376.37286962687705 W\n",
      "[codecarbon INFO @ 00:13:25] Energy consumed for all CPUs : 2.918721 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:25] 26.328128 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:25] Energy consumed for all GPUs : 7.760535 kWh. Total GPU Power : 376.19225451456515 W\n",
      "[codecarbon INFO @ 00:13:25] Energy consumed for all CPUs : 2.926096 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:25] 26.378659 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:32] Energy consumed for RAM : 0.301183 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:32] Energy consumed for all GPUs : 0.150821 kWh. Total GPU Power : 379.0914060052535 W\n",
      "[codecarbon INFO @ 00:13:32] Energy consumed for all CPUs : 0.056144 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:32] 0.508147 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:40] Energy consumed for RAM : 15.656448 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:40] Energy consumed for RAM : 15.695154 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:40] Energy consumed for all GPUs : 7.757670 kWh. Total GPU Power : 380.62645837643976 W\n",
      "[codecarbon INFO @ 00:13:40] Energy consumed for all CPUs : 2.919303 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:40] 26.333421 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:40] Energy consumed for all GPUs : 7.762119 kWh. Total GPU Power : 380.8267707168179 W\n",
      "[codecarbon INFO @ 00:13:40] Energy consumed for all CPUs : 2.926678 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:40] 26.383951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:47] Energy consumed for RAM : 0.304312 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:47] Energy consumed for all GPUs : 0.152396 kWh. Total GPU Power : 378.1850299945605 W\n",
      "[codecarbon INFO @ 00:13:47] Energy consumed for all CPUs : 0.056727 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:47] 0.513435 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:55] Energy consumed for RAM : 15.659577 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:55] Energy consumed for RAM : 15.698282 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:13:55] Energy consumed for all GPUs : 7.759231 kWh. Total GPU Power : 374.86740106721874 W\n",
      "[codecarbon INFO @ 00:13:55] Energy consumed for all CPUs : 2.919887 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:55] 26.338694 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:13:55] Energy consumed for all GPUs : 7.763680 kWh. Total GPU Power : 374.87084696169023 W\n",
      "[codecarbon INFO @ 00:13:55] Energy consumed for all CPUs : 2.927261 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:13:55] 26.389224 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:03] Energy consumed for RAM : 0.307631 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:03] Energy consumed for all GPUs : 0.154052 kWh. Total GPU Power : 374.9157042413554 W\n",
      "[codecarbon INFO @ 00:14:03] Energy consumed for all CPUs : 0.057346 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:03] 0.519028 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:10] Energy consumed for RAM : 15.662706 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:10] Energy consumed for RAM : 15.701412 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:10] Energy consumed for all GPUs : 7.760804 kWh. Total GPU Power : 377.82824502661316 W\n",
      "[codecarbon INFO @ 00:14:10] Energy consumed for all CPUs : 2.920470 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:10] 26.343981 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:10] Energy consumed for all GPUs : 7.765254 kWh. Total GPU Power : 377.8749889874435 W\n",
      "[codecarbon INFO @ 00:14:10] Energy consumed for all CPUs : 2.927845 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:10] 26.394510 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:18] Energy consumed for RAM : 0.310760 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:18] Energy consumed for all GPUs : 0.155630 kWh. Total GPU Power : 379.03723837518754 W\n",
      "[codecarbon INFO @ 00:14:18] Energy consumed for all CPUs : 0.057929 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:18] 0.524319 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:25] Energy consumed for RAM : 15.665835 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:25] Energy consumed for RAM : 15.704540 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:25] Energy consumed for all GPUs : 7.762379 kWh. Total GPU Power : 378.2141397284743 W\n",
      "[codecarbon INFO @ 00:14:25] Energy consumed for all CPUs : 2.921053 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:25] 26.349267 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:25] Energy consumed for all GPUs : 7.766829 kWh. Total GPU Power : 378.1561125005947 W\n",
      "[codecarbon INFO @ 00:14:25] Energy consumed for all CPUs : 2.928428 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:25] 26.399798 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:33] Energy consumed for RAM : 0.313888 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:33] Energy consumed for all GPUs : 0.157187 kWh. Total GPU Power : 373.85401150660476 W\n",
      "[codecarbon INFO @ 00:14:33] Energy consumed for all CPUs : 0.058512 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:33] 0.529587 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:41] Energy consumed for RAM : 15.669163 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:41] Energy consumed for RAM : 15.707867 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:41] Energy consumed for all GPUs : 7.764034 kWh. Total GPU Power : 373.4516912530899 W\n",
      "[codecarbon INFO @ 00:14:41] Energy consumed for all CPUs : 2.921674 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:41] 26.354871 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:41] Energy consumed for all GPUs : 7.768483 kWh. Total GPU Power : 373.5646053983693 W\n",
      "[codecarbon INFO @ 00:14:41] Energy consumed for all CPUs : 2.929049 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:41] 26.405399 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:48] Energy consumed for RAM : 0.317018 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:48] Energy consumed for all GPUs : 0.158763 kWh. Total GPU Power : 378.34237109330326 W\n",
      "[codecarbon INFO @ 00:14:48] Energy consumed for all CPUs : 0.059095 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:48] 0.534876 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:56] Energy consumed for RAM : 15.672292 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:56] Energy consumed for RAM : 15.710995 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:14:56] Energy consumed for all GPUs : 7.765609 kWh. Total GPU Power : 378.1132887945641 W\n",
      "[codecarbon INFO @ 00:14:56] Energy consumed for all CPUs : 2.922257 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:56] 26.360157 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:14:56] Energy consumed for all GPUs : 7.770058 kWh. Total GPU Power : 378.21504071043705 W\n",
      "[codecarbon INFO @ 00:14:56] Energy consumed for all CPUs : 2.929632 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:14:56] 26.410685 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:03] Energy consumed for RAM : 0.320147 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:03] Energy consumed for all GPUs : 0.160322 kWh. Total GPU Power : 374.3599775357839 W\n",
      "[codecarbon INFO @ 00:15:03] Energy consumed for all CPUs : 0.059679 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:03] 0.540148 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:11] Energy consumed for RAM : 15.675421 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:11] Energy consumed for RAM : 15.714123 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:11] Energy consumed for all GPUs : 7.767163 kWh. Total GPU Power : 373.2690634005385 W\n",
      "[codecarbon INFO @ 00:15:11] Energy consumed for all CPUs : 2.922840 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:11] 26.365424 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:11] Energy consumed for all GPUs : 7.771613 kWh. Total GPU Power : 373.4224022106991 W\n",
      "[codecarbon INFO @ 00:15:11] Energy consumed for all CPUs : 2.930215 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:11] 26.415951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:19] Energy consumed for RAM : 0.323529 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:19] Energy consumed for all GPUs : 0.161998 kWh. Total GPU Power : 372.37044777688993 W\n",
      "[codecarbon INFO @ 00:15:19] Energy consumed for all CPUs : 0.060309 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:19] 0.545837 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:26] Energy consumed for RAM : 15.678550 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:26] Energy consumed for RAM : 15.717253 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:26] Energy consumed for all GPUs : 7.768730 kWh. Total GPU Power : 375.9604979651178 W\n",
      "[codecarbon INFO @ 00:15:26] Energy consumed for all CPUs : 2.923424 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:26] 26.370703 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:26] Energy consumed for all GPUs : 7.773179 kWh. Total GPU Power : 376.03322539216197 W\n",
      "[codecarbon INFO @ 00:15:26] Energy consumed for all CPUs : 2.930798 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:26] 26.421230 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:34] Energy consumed for RAM : 0.326658 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:34] Energy consumed for all GPUs : 0.163577 kWh. Total GPU Power : 379.0613533955892 W\n",
      "[codecarbon INFO @ 00:15:34] Energy consumed for all CPUs : 0.060892 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:34] 0.551128 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:41] Energy consumed for RAM : 15.681679 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:41] Energy consumed for RAM : 15.720381 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:41] Energy consumed for all GPUs : 7.770286 kWh. Total GPU Power : 373.688264454544 W\n",
      "[codecarbon INFO @ 00:15:41] Energy consumed for all CPUs : 2.924007 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:41] 26.375971 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:41] Energy consumed for all GPUs : 7.774745 kWh. Total GPU Power : 376.1308091361093 W\n",
      "[codecarbon INFO @ 00:15:41] Energy consumed for all CPUs : 2.931382 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:41] 26.426508 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:49] Energy consumed for RAM : 0.329787 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:49] Energy consumed for all GPUs : 0.165137 kWh. Total GPU Power : 374.6029293318203 W\n",
      "[codecarbon INFO @ 00:15:49] Energy consumed for all CPUs : 0.061476 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:49] 0.556400 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:57] Energy consumed for RAM : 15.685085 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:57] Energy consumed for RAM : 15.723787 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:15:57] Energy consumed for all GPUs : 7.771990 kWh. Total GPU Power : 375.822301715638 W\n",
      "[codecarbon INFO @ 00:15:57] Energy consumed for all CPUs : 2.924642 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:57] 26.381717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:15:57] Energy consumed for all GPUs : 7.776439 kWh. Total GPU Power : 373.6202984507161 W\n",
      "[codecarbon INFO @ 00:15:57] Energy consumed for all CPUs : 2.932017 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:15:57] 26.432243 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:04] Energy consumed for RAM : 0.332916 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:04] Energy consumed for all GPUs : 0.166711 kWh. Total GPU Power : 377.8481764497889 W\n",
      "[codecarbon INFO @ 00:16:04] Energy consumed for all CPUs : 0.062059 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:04] 0.561686 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:12] Energy consumed for RAM : 15.688214 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:12] Energy consumed for RAM : 15.726915 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:12] Energy consumed for all GPUs : 7.778022 kWh. Total GPU Power : 380.2391729239933 W\n",
      "[codecarbon INFO @ 00:16:12] Energy consumed for all CPUs : 2.932600 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:12] 26.437537 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:12] Energy consumed for all GPUs : 7.773573 kWh. Total GPU Power : 380.1546724825564 W\n",
      "[codecarbon INFO @ 00:16:12] Energy consumed for all CPUs : 2.925225 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:12] 26.387012 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:19] Energy consumed for RAM : 0.336046 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:19] Energy consumed for all GPUs : 0.168272 kWh. Total GPU Power : 374.9178796384115 W\n",
      "[codecarbon INFO @ 00:16:19] Energy consumed for all CPUs : 0.062642 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:19] 0.566960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:27] Energy consumed for RAM : 15.691342 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:27] Energy consumed for RAM : 15.730044 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:27] Energy consumed for all GPUs : 7.775124 kWh. Total GPU Power : 372.48532428707773 W\n",
      "[codecarbon INFO @ 00:16:27] Energy consumed for all CPUs : 2.925808 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:27] 26.392274 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:27] Energy consumed for all GPUs : 7.779573 kWh. Total GPU Power : 372.37441312482133 W\n",
      "[codecarbon INFO @ 00:16:27] Energy consumed for all CPUs : 2.933183 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:27] 26.442801 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:35] Energy consumed for RAM : 0.339401 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:35] Energy consumed for all GPUs : 0.169948 kWh. Total GPU Power : 375.20830340601333 W\n",
      "[codecarbon INFO @ 00:16:35] Energy consumed for all CPUs : 0.063268 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:35] 0.572616 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:42] Energy consumed for RAM : 15.694471 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:42] Energy consumed for RAM : 15.733173 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:42] Energy consumed for all GPUs : 7.776706 kWh. Total GPU Power : 379.8137981150648 W\n",
      "[codecarbon INFO @ 00:16:42] Energy consumed for all CPUs : 2.926392 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:42] 26.397569 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:42] Energy consumed for all GPUs : 7.781155 kWh. Total GPU Power : 379.81807407672284 W\n",
      "[codecarbon INFO @ 00:16:42] Energy consumed for all CPUs : 2.933767 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:42] 26.448095 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:50] Energy consumed for RAM : 0.342529 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:50] Energy consumed for all GPUs : 0.171532 kWh. Total GPU Power : 380.36491554084296 W\n",
      "[codecarbon INFO @ 00:16:50] Energy consumed for all CPUs : 0.063851 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:50] 0.577912 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:57] Energy consumed for RAM : 15.697599 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:57] Energy consumed for RAM : 15.736301 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:16:57] Energy consumed for all GPUs : 7.778270 kWh. Total GPU Power : 375.6418481072144 W\n",
      "[codecarbon INFO @ 00:16:57] Energy consumed for all CPUs : 2.926975 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:57] 26.402844 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:16:57] Energy consumed for all GPUs : 7.782719 kWh. Total GPU Power : 375.70606406670055 W\n",
      "[codecarbon INFO @ 00:16:57] Energy consumed for all CPUs : 2.934350 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:16:57] 26.453370 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:05] Energy consumed for RAM : 0.345659 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:05] Energy consumed for all GPUs : 0.173087 kWh. Total GPU Power : 373.30133971809556 W\n",
      "[codecarbon INFO @ 00:17:05] Energy consumed for all CPUs : 0.064434 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:05] 0.583179 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:13] Energy consumed for RAM : 15.739624 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:13] Energy consumed for RAM : 15.700923 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:13] Energy consumed for all GPUs : 7.784373 kWh. Total GPU Power : 373.9505775581318 W\n",
      "[codecarbon INFO @ 00:17:13] Energy consumed for all CPUs : 2.934970 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:13] Energy consumed for all GPUs : 7.779924 kWh. Total GPU Power : 373.8782723311352 W\n",
      "[codecarbon INFO @ 00:17:13] 26.458968 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:13] Energy consumed for all CPUs : 2.927596 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:13] 26.408443 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:20] Energy consumed for RAM : 0.348787 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:20] Energy consumed for all GPUs : 0.174651 kWh. Total GPU Power : 375.62113254862095 W\n",
      "[codecarbon INFO @ 00:17:20] Energy consumed for all CPUs : 0.065017 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:20] 0.588455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:28] Energy consumed for RAM : 15.742745 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:28] Energy consumed for RAM : 15.704043 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:28] Energy consumed for all GPUs : 7.785951 kWh. Total GPU Power : 379.8186309920248 W\n",
      "[codecarbon INFO @ 00:17:28] Energy consumed for all CPUs : 2.935552 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:28] 26.464249 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:28] Energy consumed for all GPUs : 7.781502 kWh. Total GPU Power : 379.86932151720254 W\n",
      "[codecarbon INFO @ 00:17:28] Energy consumed for all CPUs : 2.928178 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:28] 26.413723 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:35] Energy consumed for RAM : 0.351917 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:35] Energy consumed for all GPUs : 0.176225 kWh. Total GPU Power : 378.07632178448824 W\n",
      "[codecarbon INFO @ 00:17:35] Energy consumed for all CPUs : 0.065601 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:35] 0.593743 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:43] Energy consumed for RAM : 15.745874 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:43] Energy consumed for RAM : 15.707172 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:43] Energy consumed for all GPUs : 7.787509 kWh. Total GPU Power : 373.94486191388944 W\n",
      "[codecarbon INFO @ 00:17:43] Energy consumed for all CPUs : 2.936136 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:43] 26.469518 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:43] Energy consumed for all GPUs : 7.783059 kWh. Total GPU Power : 373.9619697522784 W\n",
      "[codecarbon INFO @ 00:17:43] Energy consumed for all CPUs : 2.928761 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:43] 26.418992 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:51] Energy consumed for RAM : 0.355249 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:51] Energy consumed for all GPUs : 0.177889 kWh. Total GPU Power : 374.99127503503786 W\n",
      "[codecarbon INFO @ 00:17:51] Energy consumed for all CPUs : 0.066222 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:51] 0.599360 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:58] Energy consumed for RAM : 15.749003 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:58] Energy consumed for RAM : 15.710300 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:17:58] Energy consumed for all GPUs : 7.789093 kWh. Total GPU Power : 380.3381090241596 W\n",
      "[codecarbon INFO @ 00:17:58] Energy consumed for all CPUs : 2.936719 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:58] 26.474815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:17:58] Energy consumed for all GPUs : 7.784644 kWh. Total GPU Power : 380.40394171222704 W\n",
      "[codecarbon INFO @ 00:17:58] Energy consumed for all CPUs : 2.929344 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:17:58] 26.424288 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:06] Energy consumed for RAM : 0.358378 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:06] Energy consumed for all GPUs : 0.179468 kWh. Total GPU Power : 379.29961337507945 W\n",
      "[codecarbon INFO @ 00:18:06] Energy consumed for all CPUs : 0.066805 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:06] 0.604652 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:13] Energy consumed for RAM : 15.713427 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:13] Energy consumed for RAM : 15.752131 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:13] Energy consumed for all GPUs : 7.786204 kWh. Total GPU Power : 374.97397340723563 W\n",
      "[codecarbon INFO @ 00:18:13] Energy consumed for all CPUs : 2.929927 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:13] 26.429559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:13] Energy consumed for all GPUs : 7.790654 kWh. Total GPU Power : 374.7694008561571 W\n",
      "[codecarbon INFO @ 00:18:13] Energy consumed for all CPUs : 2.937302 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:13] 26.480088 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:21] Energy consumed for RAM : 0.361507 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:21] Energy consumed for all GPUs : 0.181020 kWh. Total GPU Power : 372.4606875862004 W\n",
      "[codecarbon INFO @ 00:18:21] Energy consumed for all CPUs : 0.067388 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:21] 0.609916 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:29] Energy consumed for RAM : 15.755462 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:29] Energy consumed for RAM : 15.716759 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:29] Energy consumed for all GPUs : 7.792312 kWh. Total GPU Power : 373.8677678480674 W\n",
      "[codecarbon INFO @ 00:18:29] Energy consumed for all CPUs : 2.937924 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:29] 26.485697 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:29] Energy consumed for all GPUs : 7.787862 kWh. Total GPU Power : 373.72497210234803 W\n",
      "[codecarbon INFO @ 00:18:29] Energy consumed for all CPUs : 2.930549 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:29] 26.435170 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:36] Energy consumed for RAM : 0.364637 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:36] Energy consumed for all GPUs : 0.182591 kWh. Total GPU Power : 377.2149538588 W\n",
      "[codecarbon INFO @ 00:18:36] Energy consumed for all CPUs : 0.067972 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:36] 0.615199 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:44] Energy consumed for RAM : 15.719885 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:44] Energy consumed for RAM : 15.758589 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:44] Energy consumed for all GPUs : 7.789442 kWh. Total GPU Power : 379.65856234152363 W\n",
      "[codecarbon INFO @ 00:18:44] Energy consumed for all CPUs : 2.931132 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:44] 26.440458 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:44] Energy consumed for all GPUs : 7.793891 kWh. Total GPU Power : 379.51330155484965 W\n",
      "[codecarbon INFO @ 00:18:44] Energy consumed for all CPUs : 2.938507 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:44] 26.490987 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:51] Energy consumed for RAM : 0.367766 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:51] Energy consumed for all GPUs : 0.184163 kWh. Total GPU Power : 377.6339187495435 W\n",
      "[codecarbon INFO @ 00:18:51] Energy consumed for all CPUs : 0.068555 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:51] 0.620484 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:59] Energy consumed for RAM : 15.723013 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:59] Energy consumed for RAM : 15.761716 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:18:59] Energy consumed for all GPUs : 7.790989 kWh. Total GPU Power : 371.6230016801789 W\n",
      "[codecarbon INFO @ 00:18:59] Energy consumed for all CPUs : 2.931715 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:59] 26.445717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:59] Energy consumed for all GPUs : 7.795438 kWh. Total GPU Power : 371.63854377196384 W\n",
      "[codecarbon INFO @ 00:18:59] Energy consumed for all CPUs : 2.939090 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:18:59] 26.496245 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:07] Energy consumed for RAM : 0.371102 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:07] Energy consumed for all GPUs : 0.185821 kWh. Total GPU Power : 373.304385298284 W\n",
      "[codecarbon INFO @ 00:19:07] Energy consumed for all CPUs : 0.069177 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:07] 0.626100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:14] Energy consumed for RAM : 15.726142 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:14] Energy consumed for RAM : 15.764844 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:14] Energy consumed for all GPUs : 7.792575 kWh. Total GPU Power : 380.6984955083752 W\n",
      "[codecarbon INFO @ 00:19:14] Energy consumed for all CPUs : 2.932298 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:14] 26.451015 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:14] Energy consumed for all GPUs : 7.797024 kWh. Total GPU Power : 380.89026168704225 W\n",
      "[codecarbon INFO @ 00:19:14] Energy consumed for all CPUs : 2.939673 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:14] 26.501542 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:22] Energy consumed for RAM : 0.374230 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:22] Energy consumed for all GPUs : 0.187398 kWh. Total GPU Power : 378.72758399794907 W\n",
      "[codecarbon INFO @ 00:19:22] Energy consumed for all CPUs : 0.069760 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:22] 0.631389 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:29] Energy consumed for RAM : 15.729271 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:29] Energy consumed for RAM : 15.767973 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:29] Energy consumed for all GPUs : 7.794136 kWh. Total GPU Power : 374.894426437187 W\n",
      "[codecarbon INFO @ 00:19:29] Energy consumed for all CPUs : 2.932882 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:29] 26.456289 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:29] Energy consumed for all GPUs : 7.798586 kWh. Total GPU Power : 374.92063574243934 W\n",
      "[codecarbon INFO @ 00:19:29] Energy consumed for all CPUs : 2.940257 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:29] 26.506815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:37] Energy consumed for RAM : 0.377360 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:37] Energy consumed for all GPUs : 0.188959 kWh. Total GPU Power : 374.6916393683995 W\n",
      "[codecarbon INFO @ 00:19:37] Energy consumed for all CPUs : 0.070343 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:37] 0.636662 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:45] Energy consumed for RAM : 15.732618 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:45] Energy consumed for RAM : 15.771320 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:45] Energy consumed for all GPUs : 7.795807 kWh. Total GPU Power : 374.8320041794577 W\n",
      "[codecarbon INFO @ 00:19:45] Energy consumed for all CPUs : 2.933506 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:45] 26.461930 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:45] Energy consumed for all GPUs : 7.800259 kWh. Total GPU Power : 375.61204607257577 W\n",
      "[codecarbon INFO @ 00:19:45] Energy consumed for all CPUs : 2.940881 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:45] 26.512460 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:52] Energy consumed for RAM : 0.380489 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:19:52] Energy consumed for all GPUs : 0.190536 kWh. Total GPU Power : 378.7253421833011 W\n",
      "[codecarbon INFO @ 00:19:52] Energy consumed for all CPUs : 0.070927 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:19:52] 0.641952 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:00] Energy consumed for RAM : 15.735746 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:00] Energy consumed for RAM : 15.774447 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:00] Energy consumed for all GPUs : 7.797386 kWh. Total GPU Power : 379.4304733134744 W\n",
      "[codecarbon INFO @ 00:20:00] Energy consumed for all CPUs : 2.934089 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:00] 26.467221 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:00] Energy consumed for all GPUs : 7.801836 kWh. Total GPU Power : 378.6783087120109 W\n",
      "[codecarbon INFO @ 00:20:00] Energy consumed for all CPUs : 2.941464 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:00] 26.517747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:07] Energy consumed for RAM : 0.383619 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:07] Energy consumed for all GPUs : 0.192097 kWh. Total GPU Power : 374.80429948181643 W\n",
      "[codecarbon INFO @ 00:20:07] Energy consumed for all CPUs : 0.071510 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:07] 0.647226 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:15] Energy consumed for RAM : 15.738875 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:15] Energy consumed for RAM : 15.777577 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:15] Energy consumed for all GPUs : 7.798942 kWh. Total GPU Power : 373.45259963075085 W\n",
      "[codecarbon INFO @ 00:20:15] Energy consumed for all CPUs : 2.934672 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:15] 26.472489 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:15] Energy consumed for all GPUs : 7.803391 kWh. Total GPU Power : 373.4788394832635 W\n",
      "[codecarbon INFO @ 00:20:15] Energy consumed for all CPUs : 2.942047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:15] 26.523015 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:23] Energy consumed for RAM : 0.386981 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:23] Energy consumed for all GPUs : 0.193779 kWh. Total GPU Power : 375.7046139957788 W\n",
      "[codecarbon INFO @ 00:20:23] Energy consumed for all CPUs : 0.072137 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:23] 0.652897 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:30] Energy consumed for RAM : 15.742004 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:30] Energy consumed for RAM : 15.780706 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:30] Energy consumed for all GPUs : 7.800517 kWh. Total GPU Power : 378.18984348541505 W\n",
      "[codecarbon INFO @ 00:20:30] Energy consumed for all CPUs : 2.935255 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:30] 26.477777 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:30] Energy consumed for all GPUs : 7.804966 kWh. Total GPU Power : 378.22729548408216 W\n",
      "[codecarbon INFO @ 00:20:30] Energy consumed for all CPUs : 2.942630 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:30] 26.528302 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:38] Energy consumed for RAM : 0.390110 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:38] Energy consumed for all GPUs : 0.195363 kWh. Total GPU Power : 380.4474862892861 W\n",
      "[codecarbon INFO @ 00:20:38] Energy consumed for all CPUs : 0.072720 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:38] 0.658194 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:45] Energy consumed for RAM : 15.745134 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:45] Energy consumed for RAM : 15.783835 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:45] Energy consumed for all GPUs : 7.802085 kWh. Total GPU Power : 376.5762774825449 W\n",
      "[codecarbon INFO @ 00:20:45] Energy consumed for all CPUs : 2.935839 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:45] 26.483058 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:45] Energy consumed for all GPUs : 7.806535 kWh. Total GPU Power : 376.59688231075415 W\n",
      "[codecarbon INFO @ 00:20:45] Energy consumed for all CPUs : 2.943214 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:45] 26.533583 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:53] Energy consumed for RAM : 0.393238 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:20:53] Energy consumed for all GPUs : 0.196918 kWh. Total GPU Power : 373.43219470720373 W\n",
      "[codecarbon INFO @ 00:20:53] Energy consumed for all CPUs : 0.073303 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:20:53] 0.663460 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:01] Energy consumed for RAM : 15.748494 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:01] Energy consumed for RAM : 15.787195 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:01] Energy consumed for all GPUs : 7.803762 kWh. Total GPU Power : 374.7939744138441 W\n",
      "[codecarbon INFO @ 00:21:01] Energy consumed for all CPUs : 2.936465 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:01] 26.488722 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:01] Energy consumed for all GPUs : 7.808212 kWh. Total GPU Power : 374.84340968463886 W\n",
      "[codecarbon INFO @ 00:21:01] Energy consumed for all CPUs : 2.943841 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:01] 26.539247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:08] Energy consumed for RAM : 0.396368 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:08] Energy consumed for all GPUs : 0.198498 kWh. Total GPU Power : 379.2244397193241 W\n",
      "[codecarbon INFO @ 00:21:08] Energy consumed for all CPUs : 0.073887 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:08] 0.668752 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:16] Energy consumed for RAM : 15.751622 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:16] Energy consumed for RAM : 15.790321 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:16] Energy consumed for all GPUs : 7.805346 kWh. Total GPU Power : 380.5951883040788 W\n",
      "[codecarbon INFO @ 00:21:16] Energy consumed for all CPUs : 2.937048 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:16] 26.494016 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:16] Energy consumed for all GPUs : 7.809796 kWh. Total GPU Power : 380.83463559092513 W\n",
      "[codecarbon INFO @ 00:21:16] Energy consumed for all CPUs : 2.944424 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:16] 26.544540 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:23] Energy consumed for RAM : 0.399497 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:23] Energy consumed for all GPUs : 0.200064 kWh. Total GPU Power : 376.1537090217557 W\n",
      "[codecarbon INFO @ 00:21:23] Energy consumed for all CPUs : 0.074470 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:23] 0.674031 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:31] Energy consumed for RAM : 15.754751 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:31] Energy consumed for RAM : 15.793449 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:31] Energy consumed for all GPUs : 7.806911 kWh. Total GPU Power : 375.6779419194954 W\n",
      "[codecarbon INFO @ 00:21:31] Energy consumed for all CPUs : 2.937632 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:31] 26.499294 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:31] Energy consumed for all GPUs : 7.811360 kWh. Total GPU Power : 375.8091887136042 W\n",
      "[codecarbon INFO @ 00:21:31] Energy consumed for all CPUs : 2.945007 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:31] 26.549816 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:39] Energy consumed for RAM : 0.402831 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:39] Energy consumed for all GPUs : 0.201730 kWh. Total GPU Power : 375.4221398585536 W\n",
      "[codecarbon INFO @ 00:21:39] Energy consumed for all CPUs : 0.075091 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:39] 0.679652 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:46] Energy consumed for RAM : 15.757880 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:46] Energy consumed for RAM : 15.796578 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:46] Energy consumed for all GPUs : 7.808490 kWh. Total GPU Power : 379.2407892795675 W\n",
      "[codecarbon INFO @ 00:21:46] Energy consumed for all CPUs : 2.938215 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:46] 26.504586 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:46] Energy consumed for all GPUs : 7.812940 kWh. Total GPU Power : 379.2581213000935 W\n",
      "[codecarbon INFO @ 00:21:46] Energy consumed for all CPUs : 2.945590 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:46] 26.555108 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:54] Energy consumed for RAM : 0.405959 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:21:54] Energy consumed for all GPUs : 0.203301 kWh. Total GPU Power : 377.33221371179474 W\n",
      "[codecarbon INFO @ 00:21:54] Energy consumed for all CPUs : 0.075674 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:21:54] 0.684935 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:01] Energy consumed for RAM : 15.761009 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:01] Energy consumed for RAM : 15.799707 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:01] Energy consumed for all GPUs : 7.810044 kWh. Total GPU Power : 373.163733313971 W\n",
      "[codecarbon INFO @ 00:22:01] Energy consumed for all CPUs : 2.938798 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:01] 26.509852 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:01] Energy consumed for all GPUs : 7.814494 kWh. Total GPU Power : 373.1833832334371 W\n",
      "[codecarbon INFO @ 00:22:01] Energy consumed for all CPUs : 2.946173 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:01] 26.560374 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:09] Energy consumed for RAM : 0.409088 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:09] Energy consumed for all GPUs : 0.204863 kWh. Total GPU Power : 374.87250861440253 W\n",
      "[codecarbon INFO @ 00:22:09] Energy consumed for all CPUs : 0.076258 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:09] 0.690209 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:17] Energy consumed for RAM : 15.764337 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:17] Energy consumed for RAM : 15.803035 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:17] Energy consumed for all GPUs : 7.811706 kWh. Total GPU Power : 375.0616564412796 W\n",
      "[codecarbon INFO @ 00:22:17] Energy consumed for all CPUs : 2.939419 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:17] 26.515463 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:17] Energy consumed for all GPUs : 7.816156 kWh. Total GPU Power : 375.1204839374378 W\n",
      "[codecarbon INFO @ 00:22:17] Energy consumed for all CPUs : 2.946794 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:17] 26.565985 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:24] Energy consumed for RAM : 0.412218 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:24] Energy consumed for all GPUs : 0.206434 kWh. Total GPU Power : 377.3353605405675 W\n",
      "[codecarbon INFO @ 00:22:24] Energy consumed for all CPUs : 0.076841 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:24] 0.695493 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:32] Energy consumed for RAM : 15.767466 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:32] Energy consumed for RAM : 15.806162 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:32] Energy consumed for all GPUs : 7.813283 kWh. Total GPU Power : 378.59097332372176 W\n",
      "[codecarbon INFO @ 00:22:32] Energy consumed for all CPUs : 2.940002 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:32] 26.520751 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:32] Energy consumed for all GPUs : 7.817733 kWh. Total GPU Power : 378.6946389843015 W\n",
      "[codecarbon INFO @ 00:22:32] Energy consumed for all CPUs : 2.947377 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:32] 26.571272 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:39] Energy consumed for RAM : 0.415346 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:39] Energy consumed for all GPUs : 0.207991 kWh. Total GPU Power : 373.9640567706258 W\n",
      "[codecarbon INFO @ 00:22:39] Energy consumed for all CPUs : 0.077424 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:39] 0.700762 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:47] Energy consumed for RAM : 15.809290 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:47] Energy consumed for RAM : 15.770594 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:47] Energy consumed for all GPUs : 7.819280 kWh. Total GPU Power : 371.6384782625614 W\n",
      "[codecarbon INFO @ 00:22:47] Energy consumed for all CPUs : 2.947960 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:47] 26.576530 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:47] Energy consumed for all GPUs : 7.814830 kWh. Total GPU Power : 371.49829067661943 W\n",
      "[codecarbon INFO @ 00:22:47] Energy consumed for all CPUs : 2.940585 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:47] 26.526010 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:55] Energy consumed for RAM : 0.418684 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:22:55] Energy consumed for all GPUs : 0.209657 kWh. Total GPU Power : 374.9666828581425 W\n",
      "[codecarbon INFO @ 00:22:55] Energy consumed for all CPUs : 0.078046 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:22:55] 0.706388 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:02] Energy consumed for RAM : 15.812419 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:02] Energy consumed for RAM : 15.773723 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:02] Energy consumed for all GPUs : 7.820851 kWh. Total GPU Power : 377.1528301096916 W\n",
      "[codecarbon INFO @ 00:23:02] Energy consumed for all CPUs : 2.948544 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:02] 26.581813 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:02] Energy consumed for all GPUs : 7.816401 kWh. Total GPU Power : 377.2254678043319 W\n",
      "[codecarbon INFO @ 00:23:02] Energy consumed for all CPUs : 2.941169 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:02] 26.531293 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:10] Energy consumed for RAM : 0.421813 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:10] Energy consumed for all GPUs : 0.211240 kWh. Total GPU Power : 380.0396915377973 W\n",
      "[codecarbon INFO @ 00:23:10] Energy consumed for all CPUs : 0.078630 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:10] 0.711682 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:17] Energy consumed for RAM : 15.815547 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:17] Energy consumed for RAM : 15.776851 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:17] Energy consumed for all GPUs : 7.822425 kWh. Total GPU Power : 378.15462878399524 W\n",
      "[codecarbon INFO @ 00:23:17] Energy consumed for all CPUs : 2.949127 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:17] 26.587099 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:17] Energy consumed for all GPUs : 7.817976 kWh. Total GPU Power : 378.17071847570384 W\n",
      "[codecarbon INFO @ 00:23:17] Energy consumed for all CPUs : 2.941752 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:17] 26.536579 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:25] Energy consumed for RAM : 0.424942 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:25] Energy consumed for all GPUs : 0.212796 kWh. Total GPU Power : 373.67641656144815 W\n",
      "[codecarbon INFO @ 00:23:25] Energy consumed for all CPUs : 0.079213 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:25] 0.716951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:33] Energy consumed for RAM : 15.780193 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:33] Energy consumed for all GPUs : 7.819639 kWh. Total GPU Power : 373.9066769984695 W\n",
      "[codecarbon INFO @ 00:23:33] Energy consumed for RAM : 15.818889 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:33] Energy consumed for all CPUs : 2.942375 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:33] 26.542208 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:33] Energy consumed for all GPUs : 7.824089 kWh. Total GPU Power : 373.66747235987475 W\n",
      "[codecarbon INFO @ 00:23:33] Energy consumed for all CPUs : 2.949750 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:33] 26.592728 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:40] Energy consumed for RAM : 0.428072 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:40] Energy consumed for all GPUs : 0.214366 kWh. Total GPU Power : 376.9039778443119 W\n",
      "[codecarbon INFO @ 00:23:40] Energy consumed for all CPUs : 0.079796 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:40] 0.722234 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:48] Energy consumed for RAM : 15.822014 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:48] Energy consumed for RAM : 15.783321 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:48] Energy consumed for all GPUs : 7.825660 kWh. Total GPU Power : 377.83713837718244 W\n",
      "[codecarbon INFO @ 00:23:48] Energy consumed for all CPUs : 2.950333 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:48] 26.598007 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:48] Energy consumed for all GPUs : 7.821211 kWh. Total GPU Power : 377.46238402567207 W\n",
      "[codecarbon INFO @ 00:23:48] Energy consumed for all CPUs : 2.942958 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:48] 26.547490 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:55] Energy consumed for RAM : 0.431202 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:23:55] Energy consumed for all GPUs : 0.215934 kWh. Total GPU Power : 376.2717337471163 W\n",
      "[codecarbon INFO @ 00:23:55] Energy consumed for all CPUs : 0.080379 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:23:55] 0.727515 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:03] Energy consumed for RAM : 15.825142 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:03] Energy consumed for all GPUs : 7.827211 kWh. Total GPU Power : 372.50769911151303 W\n",
      "[codecarbon INFO @ 00:24:03] Energy consumed for all CPUs : 2.950916 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:03] 26.603269 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:03] Energy consumed for RAM : 15.786451 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:03] Energy consumed for all GPUs : 7.822773 kWh. Total GPU Power : 375.1460200304884 W\n",
      "[codecarbon INFO @ 00:24:03] Energy consumed for all CPUs : 2.943541 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:03] 26.552765 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:11] Energy consumed for RAM : 0.434534 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:11] Energy consumed for all GPUs : 0.217591 kWh. Total GPU Power : 373.6206924072155 W\n",
      "[codecarbon INFO @ 00:24:11] Energy consumed for all CPUs : 0.081001 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:11] 0.733126 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:18] Energy consumed for RAM : 15.828271 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:18] Energy consumed for all GPUs : 7.828794 kWh. Total GPU Power : 379.92433596110743 W\n",
      "[codecarbon INFO @ 00:24:18] Energy consumed for all CPUs : 2.951499 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:18] 26.608564 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:18] Energy consumed for RAM : 15.789580 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:18] Energy consumed for all GPUs : 7.824355 kWh. Total GPU Power : 379.8237459090198 W\n",
      "[codecarbon INFO @ 00:24:18] Energy consumed for all CPUs : 2.944125 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:18] 26.558059 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:26] Energy consumed for RAM : 0.437660 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:26] Energy consumed for all GPUs : 0.219185 kWh. Total GPU Power : 383.2732084408165 W\n",
      "[codecarbon INFO @ 00:24:26] Energy consumed for all CPUs : 0.081584 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:26] 0.738429 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:33] Energy consumed for RAM : 15.831400 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:33] Energy consumed for all GPUs : 7.830369 kWh. Total GPU Power : 378.35033187700157 W\n",
      "[codecarbon INFO @ 00:24:33] Energy consumed for all CPUs : 2.952083 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:33] 26.613852 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:33] Energy consumed for RAM : 15.792709 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:33] Energy consumed for all GPUs : 7.825920 kWh. Total GPU Power : 375.6591590390772 W\n",
      "[codecarbon INFO @ 00:24:33] Energy consumed for all CPUs : 2.944708 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:33] 26.563337 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:41] Energy consumed for RAM : 0.440789 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:41] Energy consumed for all GPUs : 0.220740 kWh. Total GPU Power : 373.2876490291662 W\n",
      "[codecarbon INFO @ 00:24:41] Energy consumed for all CPUs : 0.082167 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:41] 0.743696 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:49] Energy consumed for RAM : 15.834732 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:49] Energy consumed for RAM : 15.796039 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:49] Energy consumed for all GPUs : 7.832030 kWh. Total GPU Power : 374.57221631866014 W\n",
      "[codecarbon INFO @ 00:24:49] Energy consumed for all CPUs : 2.952704 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:49] 26.619466 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:49] Energy consumed for all GPUs : 7.827581 kWh. Total GPU Power : 374.7002300684815 W\n",
      "[codecarbon INFO @ 00:24:49] Energy consumed for all CPUs : 2.945329 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:49] 26.568949 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:56] Energy consumed for RAM : 0.443918 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:24:56] Energy consumed for all GPUs : 0.222304 kWh. Total GPU Power : 375.43097150482555 W\n",
      "[codecarbon INFO @ 00:24:56] Energy consumed for all CPUs : 0.082750 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:24:56] 0.748972 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:04] Energy consumed for RAM : 15.799180 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:04] Energy consumed for RAM : 15.837873 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:04] Energy consumed for all GPUs : 7.829162 kWh. Total GPU Power : 378.31734789597516 W\n",
      "[codecarbon INFO @ 00:25:04] Energy consumed for all CPUs : 2.945914 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:04] 26.574256 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:04] Energy consumed for all GPUs : 7.833612 kWh. Total GPU Power : 378.2046659114847 W\n",
      "[codecarbon INFO @ 00:25:04] Energy consumed for all CPUs : 2.953289 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:04] 26.624775 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:11] Energy consumed for RAM : 0.447047 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:11] Energy consumed for all GPUs : 0.223862 kWh. Total GPU Power : 374.21839162123706 W\n",
      "[codecarbon INFO @ 00:25:11] Energy consumed for all CPUs : 0.083334 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:11] 0.754243 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:19] Energy consumed for RAM : 15.802308 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:19] Energy consumed for RAM : 15.841002 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:19] Energy consumed for all GPUs : 7.830720 kWh. Total GPU Power : 374.0734598078323 W\n",
      "[codecarbon INFO @ 00:25:19] Energy consumed for all CPUs : 2.946498 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:19] 26.579526 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:19] Energy consumed for all GPUs : 7.835170 kWh. Total GPU Power : 374.08319172305715 W\n",
      "[codecarbon INFO @ 00:25:19] Energy consumed for all CPUs : 2.953873 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:19] 26.630045 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:27] Energy consumed for RAM : 0.450393 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:27] Energy consumed for all GPUs : 0.225543 kWh. Total GPU Power : 377.4621615950109 W\n",
      "[codecarbon INFO @ 00:25:27] Energy consumed for all CPUs : 0.083957 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:27] 0.759893 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:34] Energy consumed for RAM : 15.805438 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:34] Energy consumed for RAM : 15.844132 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:34] Energy consumed for all GPUs : 7.832299 kWh. Total GPU Power : 378.9739799401805 W\n",
      "[codecarbon INFO @ 00:25:34] Energy consumed for all CPUs : 2.947081 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:34] 26.584817 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:34] Energy consumed for all GPUs : 7.836748 kWh. Total GPU Power : 378.9506779630427 W\n",
      "[codecarbon INFO @ 00:25:34] Energy consumed for all CPUs : 2.954456 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:34] 26.635336 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:42] Energy consumed for RAM : 0.453522 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:42] Energy consumed for all GPUs : 0.227124 kWh. Total GPU Power : 379.6468293967531 W\n",
      "[codecarbon INFO @ 00:25:42] Energy consumed for all CPUs : 0.084540 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:42] 0.765186 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:49] Energy consumed for RAM : 15.808567 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:49] Energy consumed for RAM : 15.847261 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:49] Energy consumed for all GPUs : 7.833870 kWh. Total GPU Power : 377.27357201065655 W\n",
      "[codecarbon INFO @ 00:25:49] Energy consumed for all CPUs : 2.947664 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:49] 26.590101 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:49] Energy consumed for all GPUs : 7.838320 kWh. Total GPU Power : 377.29506086952114 W\n",
      "[codecarbon INFO @ 00:25:49] Energy consumed for all CPUs : 2.955039 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:49] 26.640620 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:57] Energy consumed for RAM : 0.456651 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:25:57] Energy consumed for all GPUs : 0.228681 kWh. Total GPU Power : 373.79686199236886 W\n",
      "[codecarbon INFO @ 00:25:57] Energy consumed for all CPUs : 0.085124 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:25:57] 0.770456 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:05] Energy consumed for RAM : 15.811920 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:05] Energy consumed for RAM : 15.850614 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:05] Energy consumed for all GPUs : 7.835536 kWh. Total GPU Power : 373.09419206345456 W\n",
      "[codecarbon INFO @ 00:26:05] Energy consumed for all CPUs : 2.948290 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:05] 26.595746 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:05] Energy consumed for all GPUs : 7.839985 kWh. Total GPU Power : 373.13660542600155 W\n",
      "[codecarbon INFO @ 00:26:05] Energy consumed for all CPUs : 2.955665 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:05] 26.646264 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:12] Energy consumed for RAM : 0.459780 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:12] Energy consumed for all GPUs : 0.230256 kWh. Total GPU Power : 378.20147114462986 W\n",
      "[codecarbon INFO @ 00:26:12] Energy consumed for all CPUs : 0.085707 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:12] 0.775743 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:20] Energy consumed for RAM : 15.815047 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:20] Energy consumed for RAM : 15.853737 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:20] Energy consumed for all GPUs : 7.837117 kWh. Total GPU Power : 379.98014844515814 W\n",
      "[codecarbon INFO @ 00:26:20] Energy consumed for all CPUs : 2.948873 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:20] 26.601036 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:20] Energy consumed for all GPUs : 7.841567 kWh. Total GPU Power : 380.35605562424666 W\n",
      "[codecarbon INFO @ 00:26:20] Energy consumed for all CPUs : 2.956248 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:20] 26.651552 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:27] Energy consumed for RAM : 0.462909 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:27] Energy consumed for all GPUs : 0.231822 kWh. Total GPU Power : 375.96463091059144 W\n",
      "[codecarbon INFO @ 00:26:27] Energy consumed for all CPUs : 0.086290 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:27] 0.781022 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:35] Energy consumed for RAM : 15.818176 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:35] Energy consumed for RAM : 15.856866 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:35] Energy consumed for all GPUs : 7.838676 kWh. Total GPU Power : 374.14032503226343 W\n",
      "[codecarbon INFO @ 00:26:35] Energy consumed for all CPUs : 2.949456 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:35] 26.606307 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:35] Energy consumed for all GPUs : 7.843125 kWh. Total GPU Power : 374.2573087295327 W\n",
      "[codecarbon INFO @ 00:26:35] Energy consumed for all CPUs : 2.956831 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:35] 26.656822 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:43] Energy consumed for RAM : 0.466253 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:43] Energy consumed for all GPUs : 0.233494 kWh. Total GPU Power : 375.67344557486854 W\n",
      "[codecarbon INFO @ 00:26:43] Energy consumed for all CPUs : 0.086914 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:43] 0.786661 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:50] Energy consumed for RAM : 15.821304 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:50] Energy consumed for RAM : 15.859995 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:50] Energy consumed for all GPUs : 7.840251 kWh. Total GPU Power : 378.3615765205322 W\n",
      "[codecarbon INFO @ 00:26:50] Energy consumed for all CPUs : 2.950039 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:50] 26.611595 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:50] Energy consumed for all GPUs : 7.844701 kWh. Total GPU Power : 378.3608303961601 W\n",
      "[codecarbon INFO @ 00:26:50] Energy consumed for all CPUs : 2.957414 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:50] 26.662109 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:58] Energy consumed for RAM : 0.469382 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:26:58] Energy consumed for all GPUs : 0.235076 kWh. Total GPU Power : 379.6944852213865 W\n",
      "[codecarbon INFO @ 00:26:58] Energy consumed for all CPUs : 0.087497 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:26:58] 0.791955 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:05] Energy consumed for RAM : 15.824434 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:05] Energy consumed for RAM : 15.863124 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:05] Energy consumed for all GPUs : 7.841819 kWh. Total GPU Power : 376.4949589857823 W\n",
      "[codecarbon INFO @ 00:27:05] Energy consumed for all CPUs : 2.950622 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:05] 26.616875 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:05] Energy consumed for all GPUs : 7.846269 kWh. Total GPU Power : 376.4712113404874 W\n",
      "[codecarbon INFO @ 00:27:05] Energy consumed for all CPUs : 2.957997 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:05] 26.667390 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:13] Energy consumed for RAM : 0.472512 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:13] Energy consumed for all GPUs : 0.236629 kWh. Total GPU Power : 372.85796973284636 W\n",
      "[codecarbon INFO @ 00:27:13] Energy consumed for all CPUs : 0.088081 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:13] 0.797221 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:21] Energy consumed for RAM : 15.866445 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:21] Energy consumed for RAM : 15.827759 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:21] Energy consumed for all GPUs : 7.847925 kWh. Total GPU Power : 374.2535330165175 W\n",
      "[codecarbon INFO @ 00:27:21] Energy consumed for all CPUs : 2.958617 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:21] 26.672986 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:21] Energy consumed for all GPUs : 7.843475 kWh. Total GPU Power : 374.04740920122043 W\n",
      "[codecarbon INFO @ 00:27:21] Energy consumed for all CPUs : 2.951242 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:21] 26.622477 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:28] Energy consumed for RAM : 0.475639 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:28] Energy consumed for all GPUs : 0.238196 kWh. Total GPU Power : 376.68141420680064 W\n",
      "[codecarbon INFO @ 00:27:28] Energy consumed for all CPUs : 0.088663 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:28] 0.802498 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:36] Energy consumed for RAM : 15.869571 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:36] Energy consumed for all GPUs : 7.849496 kWh. Total GPU Power : 377.77533190176604 W\n",
      "[codecarbon INFO @ 00:27:36] Energy consumed for all CPUs : 2.959200 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:36] 26.678267 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:36] Energy consumed for RAM : 15.830888 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:36] Energy consumed for all GPUs : 7.845047 kWh. Total GPU Power : 377.5008840652139 W\n",
      "[codecarbon INFO @ 00:27:36] Energy consumed for all CPUs : 2.951825 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:36] 26.627760 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:43] Energy consumed for RAM : 0.478768 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:43] Energy consumed for all GPUs : 0.239770 kWh. Total GPU Power : 377.72198121049774 W\n",
      "[codecarbon INFO @ 00:27:43] Energy consumed for all CPUs : 0.089247 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:43] 0.807785 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:51] Energy consumed for RAM : 15.872699 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:51] Energy consumed for all GPUs : 7.851062 kWh. Total GPU Power : 375.90365185411605 W\n",
      "[codecarbon INFO @ 00:27:51] Energy consumed for all CPUs : 2.959783 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:51] 26.683544 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:51] Energy consumed for RAM : 15.834017 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:51] Energy consumed for all GPUs : 7.846612 kWh. Total GPU Power : 375.8060206087997 W\n",
      "[codecarbon INFO @ 00:27:51] Energy consumed for all CPUs : 2.952409 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:51] 26.633038 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:59] Energy consumed for RAM : 0.482113 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:27:59] Energy consumed for all GPUs : 0.241438 kWh. Total GPU Power : 374.83650316681917 W\n",
      "[codecarbon INFO @ 00:27:59] Energy consumed for all CPUs : 0.089870 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:27:59] 0.813422 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:06] Energy consumed for RAM : 15.875828 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:06] Energy consumed for all GPUs : 7.852637 kWh. Total GPU Power : 378.18826087360264 W\n",
      "[codecarbon INFO @ 00:28:06] Energy consumed for all CPUs : 2.960366 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:06] 26.688831 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:06] Energy consumed for RAM : 15.837147 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:06] Energy consumed for all GPUs : 7.848187 kWh. Total GPU Power : 378.0897151650246 W\n",
      "[codecarbon INFO @ 00:28:06] Energy consumed for all CPUs : 2.952992 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:06] 26.638326 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:14] Energy consumed for RAM : 0.485242 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:14] Energy consumed for all GPUs : 0.243024 kWh. Total GPU Power : 380.8167423705191 W\n",
      "[codecarbon INFO @ 00:28:14] Energy consumed for all CPUs : 0.090453 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:14] 0.818720 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:21] Energy consumed for RAM : 15.878958 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:21] Energy consumed for all GPUs : 7.854198 kWh. Total GPU Power : 374.8067911728816 W\n",
      "[codecarbon INFO @ 00:28:21] Energy consumed for all CPUs : 2.960950 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:21] 26.694105 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:21] Energy consumed for RAM : 15.840277 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:21] Energy consumed for all GPUs : 7.849759 kWh. Total GPU Power : 377.4328001813063 W\n",
      "[codecarbon INFO @ 00:28:21] Energy consumed for all CPUs : 2.953575 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:21] 26.643611 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:29] Energy consumed for RAM : 0.488372 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:29] Energy consumed for all GPUs : 0.244578 kWh. Total GPU Power : 373.0628505286818 W\n",
      "[codecarbon INFO @ 00:28:29] Energy consumed for all CPUs : 0.091037 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:29] 0.823987 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:37] Energy consumed for RAM : 15.882310 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:37] Energy consumed for RAM : 15.843626 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:37] Energy consumed for all GPUs : 7.855874 kWh. Total GPU Power : 375.7256517753165 W\n",
      "[codecarbon INFO @ 00:28:37] Energy consumed for all CPUs : 2.961574 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:37] 26.699759 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:37] Energy consumed for all GPUs : 7.851425 kWh. Total GPU Power : 373.5605332061398 W\n",
      "[codecarbon INFO @ 00:28:37] Energy consumed for all CPUs : 2.954200 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:37] 26.649251 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:44] Energy consumed for RAM : 0.491501 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:44] Energy consumed for all GPUs : 0.246151 kWh. Total GPU Power : 377.6724279155649 W\n",
      "[codecarbon INFO @ 00:28:44] Energy consumed for all CPUs : 0.091620 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:44] 0.829272 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:52] Energy consumed for RAM : 15.885439 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:52] Energy consumed for RAM : 15.846755 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:52] Energy consumed for all GPUs : 7.857448 kWh. Total GPU Power : 377.86474094129 W\n",
      "[codecarbon INFO @ 00:28:52] Energy consumed for all CPUs : 2.962158 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:52] 26.705044 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:52] Energy consumed for all GPUs : 7.853009 kWh. Total GPU Power : 380.45649386793997 W\n",
      "[codecarbon INFO @ 00:28:52] Energy consumed for all CPUs : 2.954783 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:52] 26.654547 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:59] Energy consumed for RAM : 0.494630 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:28:59] Energy consumed for all GPUs : 0.247722 kWh. Total GPU Power : 377.1565429250493 W\n",
      "[codecarbon INFO @ 00:28:59] Energy consumed for all CPUs : 0.092203 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:28:59] 0.834556 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:07] Energy consumed for RAM : 15.888568 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:07] Energy consumed for RAM : 15.849884 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:07] Energy consumed for all GPUs : 7.859012 kWh. Total GPU Power : 375.5682412040877 W\n",
      "[codecarbon INFO @ 00:29:07] Energy consumed for all CPUs : 2.962741 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:07] 26.710321 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:07] Energy consumed for all GPUs : 7.854563 kWh. Total GPU Power : 373.0412005610715 W\n",
      "[codecarbon INFO @ 00:29:07] Energy consumed for all CPUs : 2.955366 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:07] 26.659813 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:15] Energy consumed for RAM : 0.498008 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:15] Energy consumed for all GPUs : 0.249395 kWh. Total GPU Power : 372.2013563831614 W\n",
      "[codecarbon INFO @ 00:29:15] Energy consumed for all CPUs : 0.092833 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:15] 0.840236 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:22] Energy consumed for RAM : 15.891697 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:22] Energy consumed for RAM : 15.853013 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:22] Energy consumed for all GPUs : 7.860577 kWh. Total GPU Power : 375.57944706102 W\n",
      "[codecarbon INFO @ 00:29:22] Energy consumed for all CPUs : 2.963324 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:22] 26.715598 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:22] Energy consumed for all GPUs : 7.856127 kWh. Total GPU Power : 375.59134997764136 W\n",
      "[codecarbon INFO @ 00:29:22] Energy consumed for all CPUs : 2.955949 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:22] 26.665090 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:30] Energy consumed for RAM : 0.501137 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:30] Energy consumed for all GPUs : 0.250975 kWh. Total GPU Power : 379.28002055827795 W\n",
      "[codecarbon INFO @ 00:29:30] Energy consumed for all CPUs : 0.093416 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:30] 0.845528 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:37] Energy consumed for RAM : 15.894827 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:37] Energy consumed for RAM : 15.856143 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:37] Energy consumed for all GPUs : 7.862148 kWh. Total GPU Power : 377.287184762204 W\n",
      "[codecarbon INFO @ 00:29:37] Energy consumed for all CPUs : 2.963908 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:37] 26.720882 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:37] Energy consumed for all GPUs : 7.857698 kWh. Total GPU Power : 377.2860149794234 W\n",
      "[codecarbon INFO @ 00:29:37] Energy consumed for all CPUs : 2.956533 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:37] 26.670374 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:45] Energy consumed for RAM : 0.504266 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:45] Energy consumed for all GPUs : 0.252527 kWh. Total GPU Power : 372.6915921968214 W\n",
      "[codecarbon INFO @ 00:29:45] Energy consumed for all CPUs : 0.093999 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:45] 0.850793 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:53] Energy consumed for RAM : 15.898195 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:53] Energy consumed for RAM : 15.859511 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:29:53] Energy consumed for all GPUs : 7.863824 kWh. Total GPU Power : 373.73474677767575 W\n",
      "[codecarbon INFO @ 00:29:53] Energy consumed for all CPUs : 2.964536 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:53] 26.726554 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:53] Energy consumed for all GPUs : 7.859374 kWh. Total GPU Power : 373.7874685892606 W\n",
      "[codecarbon INFO @ 00:29:53] Energy consumed for all CPUs : 2.957161 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:29:53] 26.676046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:00] Energy consumed for RAM : 0.507395 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:00] Energy consumed for all GPUs : 0.254107 kWh. Total GPU Power : 379.3562481576584 W\n",
      "[codecarbon INFO @ 00:30:00] Energy consumed for all CPUs : 0.094583 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:00] 0.856085 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:08] Energy consumed for RAM : 15.862638 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:08] Energy consumed for RAM : 15.901323 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:08] Energy consumed for all GPUs : 7.865394 kWh. Total GPU Power : 377.18021972973276 W\n",
      "[codecarbon INFO @ 00:30:08] Energy consumed for all CPUs : 2.965119 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:08] 26.731837 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:08] Energy consumed for all GPUs : 7.860945 kWh. Total GPU Power : 377.258749082627 W\n",
      "[codecarbon INFO @ 00:30:08] Energy consumed for all CPUs : 2.957744 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:08] 26.681327 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:15] Energy consumed for RAM : 0.510524 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:15] Energy consumed for all GPUs : 0.255666 kWh. Total GPU Power : 374.38034342350556 W\n",
      "[codecarbon INFO @ 00:30:15] Energy consumed for all CPUs : 0.095166 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:15] 0.861356 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:23] Energy consumed for RAM : 15.865765 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:23] Energy consumed for RAM : 15.904452 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:23] Energy consumed for all GPUs : 7.866959 kWh. Total GPU Power : 375.66259120528053 W\n",
      "[codecarbon INFO @ 00:30:23] Energy consumed for all CPUs : 2.965702 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:23] 26.737112 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:23] Energy consumed for all GPUs : 7.862509 kWh. Total GPU Power : 375.7542469161602 W\n",
      "[codecarbon INFO @ 00:30:23] Energy consumed for all CPUs : 2.958328 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:23] 26.686602 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:31] Energy consumed for RAM : 0.513877 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:31] Energy consumed for all GPUs : 0.257341 kWh. Total GPU Power : 375.36008208533906 W\n",
      "[codecarbon INFO @ 00:30:31] Energy consumed for all CPUs : 0.095791 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:31] 0.867009 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:38] Energy consumed for RAM : 15.868893 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:38] Energy consumed for RAM : 15.907582 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:38] Energy consumed for all GPUs : 7.864078 kWh. Total GPU Power : 376.8460387781222 W\n",
      "[codecarbon INFO @ 00:30:38] Energy consumed for all CPUs : 2.958911 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:38] 26.691881 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:38] Energy consumed for all GPUs : 7.868527 kWh. Total GPU Power : 376.5787357390155 W\n",
      "[codecarbon INFO @ 00:30:38] Energy consumed for all CPUs : 2.966286 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:38] 26.742395 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:46] Energy consumed for RAM : 0.517006 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:46] Energy consumed for all GPUs : 0.258913 kWh. Total GPU Power : 377.3546457820877 W\n",
      "[codecarbon INFO @ 00:30:46] Energy consumed for all CPUs : 0.096374 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:46] 0.872293 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:53] Energy consumed for RAM : 15.872021 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:53] Energy consumed for RAM : 15.910710 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:30:53] Energy consumed for all GPUs : 7.865639 kWh. Total GPU Power : 375.0029671050787 W\n",
      "[codecarbon INFO @ 00:30:53] Energy consumed for all CPUs : 2.959494 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:53] 26.697154 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:53] Energy consumed for all GPUs : 7.870089 kWh. Total GPU Power : 375.0759051680535 W\n",
      "[codecarbon INFO @ 00:30:53] Energy consumed for all CPUs : 2.966869 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:30:53] 26.747667 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:01] Energy consumed for RAM : 0.520135 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:01] Energy consumed for all GPUs : 0.260472 kWh. Total GPU Power : 374.33602215311254 W\n",
      "[codecarbon INFO @ 00:31:01] Energy consumed for all CPUs : 0.096958 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:01] 0.877565 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:09] Energy consumed for RAM : 15.914058 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:09] Energy consumed for RAM : 15.875371 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:09] Energy consumed for all GPUs : 7.871768 kWh. Total GPU Power : 376.71655931607535 W\n",
      "[codecarbon INFO @ 00:31:09] Energy consumed for all CPUs : 2.967493 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:09] 26.753320 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:09] Energy consumed for all GPUs : 7.867319 kWh. Total GPU Power : 376.63850109682005 W\n",
      "[codecarbon INFO @ 00:31:09] Energy consumed for all CPUs : 2.960119 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:09] 26.702808 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:16] Energy consumed for RAM : 0.523265 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:16] Energy consumed for all GPUs : 0.262058 kWh. Total GPU Power : 380.76009999375833 W\n",
      "[codecarbon INFO @ 00:31:16] Energy consumed for all CPUs : 0.097541 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:16] 0.882863 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:24] Energy consumed for RAM : 15.917186 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:24] Energy consumed for RAM : 15.878495 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:24] Energy consumed for all GPUs : 7.873347 kWh. Total GPU Power : 379.2279736086287 W\n",
      "[codecarbon INFO @ 00:31:24] Energy consumed for all CPUs : 2.968076 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:24] 26.758610 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:24] Energy consumed for all GPUs : 7.868898 kWh. Total GPU Power : 379.6688293316893 W\n",
      "[codecarbon INFO @ 00:31:24] Energy consumed for all CPUs : 2.960701 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:24] 26.708094 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:31] Energy consumed for RAM : 0.526394 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:31] Energy consumed for all GPUs : 0.263622 kWh. Total GPU Power : 375.565073891645 W\n",
      "[codecarbon INFO @ 00:31:31] Energy consumed for all CPUs : 0.098124 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:31] 0.888140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:39] Energy consumed for RAM : 15.920316 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:39] Energy consumed for RAM : 15.881624 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:39] Energy consumed for all GPUs : 7.874920 kWh. Total GPU Power : 377.56378070454844 W\n",
      "[codecarbon INFO @ 00:31:39] Energy consumed for all CPUs : 2.968659 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:39] 26.763895 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:39] Energy consumed for all GPUs : 7.870470 kWh. Total GPU Power : 377.6134252415816 W\n",
      "[codecarbon INFO @ 00:31:39] Energy consumed for all CPUs : 2.961285 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:39] 26.713379 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:47] Energy consumed for RAM : 0.529719 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:47] Energy consumed for all GPUs : 0.265281 kWh. Total GPU Power : 374.77097230205675 W\n",
      "[codecarbon INFO @ 00:31:47] Energy consumed for all CPUs : 0.098744 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:47] 0.893744 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:54] Energy consumed for RAM : 15.923445 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:54] Energy consumed for RAM : 15.884754 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:31:54] Energy consumed for all GPUs : 7.876485 kWh. Total GPU Power : 375.744905830471 W\n",
      "[codecarbon INFO @ 00:31:54] Energy consumed for all CPUs : 2.969243 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:54] 26.769172 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:54] Energy consumed for all GPUs : 7.872035 kWh. Total GPU Power : 375.74913007652316 W\n",
      "[codecarbon INFO @ 00:31:54] Energy consumed for all CPUs : 2.961868 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:31:54] 26.718657 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:02] Energy consumed for RAM : 0.532848 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:02] Energy consumed for all GPUs : 0.266859 kWh. Total GPU Power : 379.0374966716468 W\n",
      "[codecarbon INFO @ 00:32:02] Energy consumed for all CPUs : 0.099327 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:02] 0.899035 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:09] Energy consumed for RAM : 15.926574 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:09] Energy consumed for RAM : 15.887883 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:09] Energy consumed for all GPUs : 7.878045 kWh. Total GPU Power : 374.5457138511409 W\n",
      "[codecarbon INFO @ 00:32:09] Energy consumed for all CPUs : 2.969826 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:09] 26.774445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:09] Energy consumed for all GPUs : 7.873595 kWh. Total GPU Power : 374.58585359001063 W\n",
      "[codecarbon INFO @ 00:32:09] Energy consumed for all CPUs : 2.962451 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:09] 26.723930 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:17] Energy consumed for RAM : 0.535978 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:17] Energy consumed for all GPUs : 0.268416 kWh. Total GPU Power : 373.74772265605895 W\n",
      "[codecarbon INFO @ 00:32:17] Energy consumed for all CPUs : 0.099911 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:17] 0.904304 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:25] Energy consumed for RAM : 15.891214 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:25] Energy consumed for RAM : 15.929906 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:25] Energy consumed for all GPUs : 7.875260 kWh. Total GPU Power : 375.62045609999717 W\n",
      "[codecarbon INFO @ 00:32:25] Energy consumed for all CPUs : 2.963072 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:25] 26.729546 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:25] Energy consumed for all GPUs : 7.879710 kWh. Total GPU Power : 375.50035639023287 W\n",
      "[codecarbon INFO @ 00:32:25] Energy consumed for all CPUs : 2.970447 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:25] 26.780063 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:32] Energy consumed for RAM : 0.539108 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:32] Energy consumed for all GPUs : 0.269993 kWh. Total GPU Power : 378.58897171786214 W\n",
      "[codecarbon INFO @ 00:32:32] Energy consumed for all CPUs : 0.100494 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:32] 0.909595 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:40] Energy consumed for RAM : 15.894342 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:40] Energy consumed for RAM : 15.933034 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:40] Energy consumed for all GPUs : 7.876830 kWh. Total GPU Power : 376.9047407748084 W\n",
      "[codecarbon INFO @ 00:32:40] Energy consumed for all CPUs : 2.963655 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:40] 26.734828 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:40] Energy consumed for all GPUs : 7.881280 kWh. Total GPU Power : 376.9411992045305 W\n",
      "[codecarbon INFO @ 00:32:40] Energy consumed for all CPUs : 2.971030 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:40] 26.785344 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:47] Energy consumed for RAM : 0.542237 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:47] Energy consumed for all GPUs : 0.271553 kWh. Total GPU Power : 374.4446123938661 W\n",
      "[codecarbon INFO @ 00:32:47] Energy consumed for all CPUs : 0.101077 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:47] 0.914867 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:55] Energy consumed for RAM : 15.897472 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:55] Energy consumed for RAM : 15.936163 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:32:55] Energy consumed for all GPUs : 7.878397 kWh. Total GPU Power : 376.12584585633317 W\n",
      "[codecarbon INFO @ 00:32:55] Energy consumed for all CPUs : 2.964239 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:55] 26.740107 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:55] Energy consumed for all GPUs : 7.882846 kWh. Total GPU Power : 376.15483613279673 W\n",
      "[codecarbon INFO @ 00:32:55] Energy consumed for all CPUs : 2.971614 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:32:55] 26.790623 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:03] Energy consumed for RAM : 0.545581 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:03] Energy consumed for all GPUs : 0.273220 kWh. Total GPU Power : 374.5967992028241 W\n",
      "[codecarbon INFO @ 00:33:03] Energy consumed for all CPUs : 0.101700 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:03] 0.920501 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:10] Energy consumed for RAM : 15.900601 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:10] Energy consumed for RAM : 15.939292 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:10] Energy consumed for all GPUs : 7.879971 kWh. Total GPU Power : 377.97284898848943 W\n",
      "[codecarbon INFO @ 00:33:10] Energy consumed for all CPUs : 2.964822 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:10] 26.745393 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:10] Energy consumed for all GPUs : 7.884420 kWh. Total GPU Power : 377.97099767740707 W\n",
      "[codecarbon INFO @ 00:33:10] Energy consumed for all CPUs : 2.972197 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:10] 26.795909 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:18] Energy consumed for RAM : 0.548709 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:18] Energy consumed for all GPUs : 0.274798 kWh. Total GPU Power : 378.946625329906 W\n",
      "[codecarbon INFO @ 00:33:18] Energy consumed for all CPUs : 0.102284 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:18] 0.925791 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:25] Energy consumed for RAM : 15.903730 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:25] Energy consumed for RAM : 15.942421 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:25] Energy consumed for all GPUs : 7.881532 kWh. Total GPU Power : 374.9130952166919 W\n",
      "[codecarbon INFO @ 00:33:25] Energy consumed for all CPUs : 2.965405 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:25] 26.750668 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:25] Energy consumed for all GPUs : 7.885982 kWh. Total GPU Power : 374.9645627570959 W\n",
      "[codecarbon INFO @ 00:33:25] Energy consumed for all CPUs : 2.972780 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:25] 26.801183 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:33] Energy consumed for RAM : 0.551839 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:33] Energy consumed for all GPUs : 0.276366 kWh. Total GPU Power : 376.4455618008936 W\n",
      "[codecarbon INFO @ 00:33:33] Energy consumed for all CPUs : 0.102867 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:33] 0.931072 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:41] Energy consumed for RAM : 15.945776 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:41] Energy consumed for RAM : 15.907086 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:41] Energy consumed for all GPUs : 7.887661 kWh. Total GPU Power : 375.87637386750237 W\n",
      "[codecarbon INFO @ 00:33:41] Energy consumed for all CPUs : 2.973406 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:41] 26.806842 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:41] Energy consumed for all GPUs : 7.883211 kWh. Total GPU Power : 375.7776629511058 W\n",
      "[codecarbon INFO @ 00:33:41] Energy consumed for all CPUs : 2.966031 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:41] 26.756328 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:48] Energy consumed for RAM : 0.554968 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:48] Energy consumed for all GPUs : 0.277937 kWh. Total GPU Power : 377.1332002420091 W\n",
      "[codecarbon INFO @ 00:33:48] Energy consumed for all CPUs : 0.103450 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:48] 0.936355 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:56] Energy consumed for RAM : 15.910213 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:56] Energy consumed for RAM : 15.948905 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:33:56] Energy consumed for all GPUs : 7.884799 kWh. Total GPU Power : 381.5701122741894 W\n",
      "[codecarbon INFO @ 00:33:56] Energy consumed for all CPUs : 2.966614 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:56] 26.761627 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:56] Energy consumed for all GPUs : 7.889249 kWh. Total GPU Power : 381.4271288391403 W\n",
      "[codecarbon INFO @ 00:33:56] Energy consumed for all CPUs : 2.973989 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:33:56] 26.812142 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:03] Energy consumed for RAM : 0.558098 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:03] Energy consumed for all GPUs : 0.279513 kWh. Total GPU Power : 378.4984077524306 W\n",
      "[codecarbon INFO @ 00:34:03] Energy consumed for all CPUs : 0.104034 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:03] 0.941645 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:11] Energy consumed for RAM : 15.913342 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:11] Energy consumed for RAM : 15.952034 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:11] Energy consumed for all GPUs : 7.886355 kWh. Total GPU Power : 373.50916085511807 W\n",
      "[codecarbon INFO @ 00:34:11] Energy consumed for all CPUs : 2.967197 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:11] 26.766894 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:11] Energy consumed for all GPUs : 7.890804 kWh. Total GPU Power : 373.51728584724964 W\n",
      "[codecarbon INFO @ 00:34:11] Energy consumed for all CPUs : 2.974572 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:11] 26.817411 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:19] Energy consumed for RAM : 0.561425 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:19] Energy consumed for all GPUs : 0.281170 kWh. Total GPU Power : 374.0057441772165 W\n",
      "[codecarbon INFO @ 00:34:19] Energy consumed for all CPUs : 0.104654 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:19] 0.947249 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:26] Energy consumed for RAM : 15.916471 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:26] Energy consumed for RAM : 15.955163 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:26] Energy consumed for all GPUs : 7.887933 kWh. Total GPU Power : 378.96719172822714 W\n",
      "[codecarbon INFO @ 00:34:26] Energy consumed for all CPUs : 2.967780 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:26] 26.772185 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:26] Energy consumed for all GPUs : 7.892383 kWh. Total GPU Power : 378.87486052396827 W\n",
      "[codecarbon INFO @ 00:34:26] Energy consumed for all CPUs : 2.975156 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:26] 26.822701 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:34] Energy consumed for RAM : 0.564553 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:34] Energy consumed for all GPUs : 0.282750 kWh. Total GPU Power : 379.49852930990335 W\n",
      "[codecarbon INFO @ 00:34:34] Energy consumed for all CPUs : 0.105237 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:34] 0.952540 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:41] Energy consumed for RAM : 15.919601 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:41] Energy consumed for RAM : 15.958292 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:41] Energy consumed for all GPUs : 7.889495 kWh. Total GPU Power : 375.04596891579735 W\n",
      "[codecarbon INFO @ 00:34:41] Energy consumed for all CPUs : 2.968364 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:41] 26.777460 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:41] Energy consumed for all GPUs : 7.893945 kWh. Total GPU Power : 375.17462178240993 W\n",
      "[codecarbon INFO @ 00:34:41] Energy consumed for all CPUs : 2.975739 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:41] 26.827976 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:49] Energy consumed for RAM : 0.567682 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:49] Energy consumed for all GPUs : 0.284314 kWh. Total GPU Power : 375.74981158917853 W\n",
      "[codecarbon INFO @ 00:34:49] Energy consumed for all CPUs : 0.105820 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:49] 0.957817 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:57] Energy consumed for RAM : 15.922927 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:57] Energy consumed for RAM : 15.961617 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:34:57] Energy consumed for all GPUs : 7.891158 kWh. Total GPU Power : 375.4582387924319 W\n",
      "[codecarbon INFO @ 00:34:57] Energy consumed for all CPUs : 2.968984 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:57] 26.783069 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:57] Energy consumed for all GPUs : 7.895607 kWh. Total GPU Power : 375.53165444894097 W\n",
      "[codecarbon INFO @ 00:34:57] Energy consumed for all CPUs : 2.976359 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:34:57] 26.833584 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:04] Energy consumed for RAM : 0.570812 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:04] Energy consumed for all GPUs : 0.285878 kWh. Total GPU Power : 375.28569071913245 W\n",
      "[codecarbon INFO @ 00:35:04] Energy consumed for all CPUs : 0.106404 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:04] 0.963093 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:12] Energy consumed for RAM : 15.964745 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:12] Energy consumed for RAM : 15.926055 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:12] Energy consumed for all GPUs : 7.897177 kWh. Total GPU Power : 376.9908186046169 W\n",
      "[codecarbon INFO @ 00:35:12] Energy consumed for all CPUs : 2.976942 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:12] 26.838864 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:12] Energy consumed for all GPUs : 7.892727 kWh. Total GPU Power : 376.88927510925214 W\n",
      "[codecarbon INFO @ 00:35:12] Energy consumed for all CPUs : 2.969567 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:12] 26.788349 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:19] Energy consumed for RAM : 0.573941 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:19] Energy consumed for all GPUs : 0.287438 kWh. Total GPU Power : 374.7910004651094 W\n",
      "[codecarbon INFO @ 00:35:19] Energy consumed for all CPUs : 0.106987 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:19] 0.968366 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:27] Energy consumed for RAM : 15.929184 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:27] Energy consumed for RAM : 15.967875 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:27] Energy consumed for all GPUs : 7.894282 kWh. Total GPU Power : 373.33221219316783 W\n",
      "[codecarbon INFO @ 00:35:27] Energy consumed for all CPUs : 2.970150 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:27] 26.793616 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:27] Energy consumed for all GPUs : 7.898732 kWh. Total GPU Power : 373.2395317564535 W\n",
      "[codecarbon INFO @ 00:35:27] Energy consumed for all CPUs : 2.977525 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:27] 26.844132 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:35] Energy consumed for RAM : 0.577294 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:35] Energy consumed for all GPUs : 0.289117 kWh. Total GPU Power : 376.1575909326544 W\n",
      "[codecarbon INFO @ 00:35:35] Energy consumed for all CPUs : 0.107612 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:35] 0.974024 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:42] Energy consumed for RAM : 15.932313 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:42] Energy consumed for RAM : 15.971004 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:42] Energy consumed for all GPUs : 7.900319 kWh. Total GPU Power : 381.15665065584136 W\n",
      "[codecarbon INFO @ 00:35:42] Energy consumed for all CPUs : 2.978109 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:42] 26.849431 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:42] Energy consumed for all GPUs : 7.895869 kWh. Total GPU Power : 381.0849217690252 W\n",
      "[codecarbon INFO @ 00:35:42] Energy consumed for all CPUs : 2.970734 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:42] 26.798917 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:50] Energy consumed for RAM : 0.580423 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:50] Energy consumed for all GPUs : 0.290708 kWh. Total GPU Power : 381.90447662228706 W\n",
      "[codecarbon INFO @ 00:35:50] Energy consumed for all CPUs : 0.108196 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:50] 0.979326 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:57] Energy consumed for RAM : 15.974133 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:57] Energy consumed for RAM : 15.935442 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:35:57] Energy consumed for all GPUs : 7.901880 kWh. Total GPU Power : 374.885935179137 W\n",
      "[codecarbon INFO @ 00:35:57] Energy consumed for all CPUs : 2.978692 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:57] 26.854705 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:57] Energy consumed for all GPUs : 7.897431 kWh. Total GPU Power : 374.9398669142875 W\n",
      "[codecarbon INFO @ 00:35:57] Energy consumed for all CPUs : 2.971317 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:35:57] 26.804191 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:05] Energy consumed for RAM : 0.583552 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:05] Energy consumed for all GPUs : 0.292261 kWh. Total GPU Power : 372.9926277194564 W\n",
      "[codecarbon INFO @ 00:36:05] Energy consumed for all CPUs : 0.108779 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:05] 0.984592 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:13] Energy consumed for RAM : 15.938786 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:13] Energy consumed for RAM : 15.977478 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:13] Energy consumed for all GPUs : 7.899102 kWh. Total GPU Power : 375.4290467728596 W\n",
      "[codecarbon INFO @ 00:36:13] Energy consumed for all CPUs : 2.971941 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:13] 26.809829 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:13] Energy consumed for all GPUs : 7.903551 kWh. Total GPU Power : 375.2623285026311 W\n",
      "[codecarbon INFO @ 00:36:13] Energy consumed for all CPUs : 2.979316 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:13] 26.860346 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:20] Energy consumed for RAM : 0.586681 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:20] Energy consumed for all GPUs : 0.293830 kWh. Total GPU Power : 376.76890459351364 W\n",
      "[codecarbon INFO @ 00:36:20] Energy consumed for all CPUs : 0.109362 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:20] 0.989874 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:28] Energy consumed for RAM : 15.941914 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:28] Energy consumed for RAM : 15.980604 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:28] Energy consumed for all GPUs : 7.900675 kWh. Total GPU Power : 377.877048171456 W\n",
      "[codecarbon INFO @ 00:36:28] Energy consumed for all CPUs : 2.972524 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:28] 26.815113 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:28] Energy consumed for all GPUs : 7.905125 kWh. Total GPU Power : 378.1088671650407 W\n",
      "[codecarbon INFO @ 00:36:28] Energy consumed for all CPUs : 2.979899 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:28] 26.865628 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:35] Energy consumed for RAM : 0.589811 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:35] Energy consumed for all GPUs : 0.295389 kWh. Total GPU Power : 374.134262578587 W\n",
      "[codecarbon INFO @ 00:36:35] Energy consumed for all CPUs : 0.109946 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:35] 0.995145 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:43] Energy consumed for RAM : 15.945043 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:43] Energy consumed for RAM : 15.983733 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:43] Energy consumed for all GPUs : 7.902236 kWh. Total GPU Power : 374.63654416808646 W\n",
      "[codecarbon INFO @ 00:36:43] Energy consumed for all CPUs : 2.973107 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:43] 26.820386 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:43] Energy consumed for all GPUs : 7.906685 kWh. Total GPU Power : 374.6925451698497 W\n",
      "[codecarbon INFO @ 00:36:43] Energy consumed for all CPUs : 2.980482 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:43] 26.870900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:51] Energy consumed for RAM : 0.593138 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:51] Energy consumed for all GPUs : 0.297057 kWh. Total GPU Power : 376.7719489849208 W\n",
      "[codecarbon INFO @ 00:36:51] Energy consumed for all CPUs : 0.110566 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:51] 1.000761 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:58] Energy consumed for RAM : 15.948172 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:58] Energy consumed for RAM : 15.986861 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:36:58] Energy consumed for all GPUs : 7.903812 kWh. Total GPU Power : 378.3571625189017 W\n",
      "[codecarbon INFO @ 00:36:58] Energy consumed for all CPUs : 2.973691 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:58] 26.825674 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:58] Energy consumed for all GPUs : 7.908261 kWh. Total GPU Power : 378.4843174512511 W\n",
      "[codecarbon INFO @ 00:36:58] Energy consumed for all CPUs : 2.981066 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:36:58] 26.876187 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:06] Energy consumed for RAM : 0.596266 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:06] Energy consumed for all GPUs : 0.298640 kWh. Total GPU Power : 380.00539947529546 W\n",
      "[codecarbon INFO @ 00:37:06] Energy consumed for all CPUs : 0.111149 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:06] 1.006055 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:13] Energy consumed for RAM : 15.951301 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:13] Energy consumed for RAM : 15.989989 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:13] Energy consumed for all GPUs : 7.905385 kWh. Total GPU Power : 377.7436272583534 W\n",
      "[codecarbon INFO @ 00:37:13] Energy consumed for all CPUs : 2.974274 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:13] 26.830960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:13] Energy consumed for all GPUs : 7.909835 kWh. Total GPU Power : 377.79515968111235 W\n",
      "[codecarbon INFO @ 00:37:13] Energy consumed for all CPUs : 2.981649 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:13] 26.881473 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:21] Energy consumed for RAM : 0.599396 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:21] Energy consumed for all GPUs : 0.300206 kWh. Total GPU Power : 376.09657079943304 W\n",
      "[codecarbon INFO @ 00:37:21] Energy consumed for all CPUs : 0.111732 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:21] 1.011334 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:29] Energy consumed for RAM : 15.954638 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:29] Energy consumed for RAM : 15.993326 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:29] Energy consumed for all GPUs : 7.907047 kWh. Total GPU Power : 373.9869617593089 W\n",
      "[codecarbon INFO @ 00:37:29] Energy consumed for all CPUs : 2.974896 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:29] 26.836581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:29] Energy consumed for all GPUs : 7.911496 kWh. Total GPU Power : 373.55223446136574 W\n",
      "[codecarbon INFO @ 00:37:29] Energy consumed for all CPUs : 2.982272 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:29] 26.887093 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:36] Energy consumed for RAM : 0.602525 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:36] Energy consumed for all GPUs : 0.301767 kWh. Total GPU Power : 374.69071195674474 W\n",
      "[codecarbon INFO @ 00:37:36] Energy consumed for all CPUs : 0.112316 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:36] 1.016607 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:44] Energy consumed for RAM : 15.996450 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:44] Energy consumed for RAM : 15.957764 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:44] Energy consumed for all GPUs : 7.913066 kWh. Total GPU Power : 377.52116500098606 W\n",
      "[codecarbon INFO @ 00:37:44] Energy consumed for all CPUs : 2.982854 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:44] 26.892370 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:44] Energy consumed for all GPUs : 7.908617 kWh. Total GPU Power : 377.34424280366187 W\n",
      "[codecarbon INFO @ 00:37:44] Energy consumed for all CPUs : 2.975479 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:44] 26.841859 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:51] Energy consumed for RAM : 0.605655 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:51] Energy consumed for all GPUs : 0.303330 kWh. Total GPU Power : 375.29351166695704 W\n",
      "[codecarbon INFO @ 00:37:51] Energy consumed for all CPUs : 0.112899 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:51] 1.021884 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:59] Energy consumed for RAM : 15.999579 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:59] Energy consumed for RAM : 15.960892 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:37:59] Energy consumed for all GPUs : 7.914623 kWh. Total GPU Power : 373.85201892573986 W\n",
      "[codecarbon INFO @ 00:37:59] Energy consumed for all CPUs : 2.983437 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:59] 26.897640 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:37:59] Energy consumed for all GPUs : 7.910174 kWh. Total GPU Power : 373.92799036194816 W\n",
      "[codecarbon INFO @ 00:37:59] Energy consumed for all CPUs : 2.976063 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:37:59] 26.847128 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:07] Energy consumed for RAM : 0.609012 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:07] Energy consumed for all GPUs : 0.305010 kWh. Total GPU Power : 375.93533426378485 W\n",
      "[codecarbon INFO @ 00:38:07] Energy consumed for all CPUs : 0.113525 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:07] 1.027546 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:14] Energy consumed for RAM : 16.002708 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:14] Energy consumed for RAM : 15.964019 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:14] Energy consumed for all GPUs : 7.916207 kWh. Total GPU Power : 380.20415421954834 W\n",
      "[codecarbon INFO @ 00:38:14] Energy consumed for all CPUs : 2.984021 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:14] 26.902936 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:14] Energy consumed for all GPUs : 7.911757 kWh. Total GPU Power : 380.40889373242925 W\n",
      "[codecarbon INFO @ 00:38:14] Energy consumed for all CPUs : 2.976646 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:14] 26.852423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:22] Energy consumed for RAM : 0.612140 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:22] Energy consumed for all GPUs : 0.306596 kWh. Total GPU Power : 380.93611137652243 W\n",
      "[codecarbon INFO @ 00:38:22] Energy consumed for all CPUs : 0.114108 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:22] 1.032844 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:29] Energy consumed for RAM : 15.967148 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:29] Energy consumed for RAM : 16.005837 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:29] Energy consumed for all GPUs : 7.913325 kWh. Total GPU Power : 376.5110385238646 W\n",
      "[codecarbon INFO @ 00:38:29] Energy consumed for all CPUs : 2.977229 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:29] 26.857702 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:29] Energy consumed for all GPUs : 7.917775 kWh. Total GPU Power : 376.41830865400055 W\n",
      "[codecarbon INFO @ 00:38:29] Energy consumed for all CPUs : 2.984604 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:29] 26.908216 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:37] Energy consumed for RAM : 0.615269 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:37] Energy consumed for all GPUs : 0.308153 kWh. Total GPU Power : 373.8825744595066 W\n",
      "[codecarbon INFO @ 00:38:37] Energy consumed for all CPUs : 0.114691 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:37] 1.038114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:45] Energy consumed for RAM : 15.970486 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:45] Energy consumed for RAM : 16.009175 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:45] Energy consumed for all GPUs : 7.914987 kWh. Total GPU Power : 373.8628736783599 W\n",
      "[codecarbon INFO @ 00:38:45] Energy consumed for all CPUs : 2.977851 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:45] 26.863324 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:45] Energy consumed for all GPUs : 7.919436 kWh. Total GPU Power : 373.90231072385257 W\n",
      "[codecarbon INFO @ 00:38:45] Energy consumed for all CPUs : 2.985227 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:45] 26.913838 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:38:52] Energy consumed for RAM : 0.618399 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:38:52] Energy consumed for all GPUs : 0.309718 kWh. Total GPU Power : 375.7122826119228 W\n",
      "[codecarbon INFO @ 00:38:52] Energy consumed for all CPUs : 0.115275 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:38:52] 1.043391 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:00] Energy consumed for RAM : 15.973614 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:00] Energy consumed for RAM : 16.012303 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:00] Energy consumed for all GPUs : 7.916556 kWh. Total GPU Power : 376.82952843134206 W\n",
      "[codecarbon INFO @ 00:39:00] Energy consumed for all CPUs : 2.978434 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:00] 26.868605 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:00] Energy consumed for all GPUs : 7.921005 kWh. Total GPU Power : 376.8627003523854 W\n",
      "[codecarbon INFO @ 00:39:00] Energy consumed for all CPUs : 2.985810 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:00] 26.919119 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:07] Energy consumed for RAM : 0.621528 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:07] Energy consumed for all GPUs : 0.311272 kWh. Total GPU Power : 373.2629476311757 W\n",
      "[codecarbon INFO @ 00:39:07] Energy consumed for all CPUs : 0.115858 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:07] 1.048658 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:15] Energy consumed for RAM : 15.976743 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:15] Energy consumed for RAM : 16.015432 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:15] Energy consumed for all GPUs : 7.918114 kWh. Total GPU Power : 374.03125244326554 W\n",
      "[codecarbon INFO @ 00:39:15] Energy consumed for all CPUs : 2.979018 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:15] 26.873875 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:15] Energy consumed for all GPUs : 7.922563 kWh. Total GPU Power : 374.0344166877698 W\n",
      "[codecarbon INFO @ 00:39:15] Energy consumed for all CPUs : 2.986393 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:15] 26.924389 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:23] Energy consumed for RAM : 0.624857 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:23] Energy consumed for all GPUs : 0.312937 kWh. Total GPU Power : 375.7633551347865 W\n",
      "[codecarbon INFO @ 00:39:23] Energy consumed for all CPUs : 0.116478 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:23] 1.054272 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:30] Energy consumed for RAM : 15.979872 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:30] Energy consumed for RAM : 16.018561 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:30] Energy consumed for all GPUs : 7.919686 kWh. Total GPU Power : 377.44906575069666 W\n",
      "[codecarbon INFO @ 00:39:30] Energy consumed for all CPUs : 2.979601 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:30] 26.879159 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:30] Energy consumed for all GPUs : 7.924135 kWh. Total GPU Power : 377.5840093014522 W\n",
      "[codecarbon INFO @ 00:39:30] Energy consumed for all CPUs : 2.986976 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:30] 26.929672 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:38] Energy consumed for RAM : 0.627985 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:38] Energy consumed for all GPUs : 0.314518 kWh. Total GPU Power : 379.6320277222372 W\n",
      "[codecarbon INFO @ 00:39:38] Energy consumed for all CPUs : 0.117062 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:38] 1.059565 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:45] Energy consumed for RAM : 15.983002 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:45] Energy consumed for RAM : 16.021690 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:45] Energy consumed for all GPUs : 7.921237 kWh. Total GPU Power : 372.6228639526184 W\n",
      "[codecarbon INFO @ 00:39:45] Energy consumed for all CPUs : 2.980184 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:45] 26.884423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:45] Energy consumed for all GPUs : 7.925687 kWh. Total GPU Power : 372.6240727969118 W\n",
      "[codecarbon INFO @ 00:39:45] Energy consumed for all CPUs : 2.987560 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:45] 26.934936 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:39:53] Energy consumed for RAM : 0.631113 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:39:53] Energy consumed for all GPUs : 0.316070 kWh. Total GPU Power : 372.72014511321356 W\n",
      "[codecarbon INFO @ 00:39:53] Energy consumed for all CPUs : 0.117645 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:39:53] 1.064828 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:01] Energy consumed for RAM : 15.986335 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:01] Energy consumed for RAM : 16.025022 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:01] Energy consumed for all GPUs : 7.922906 kWh. Total GPU Power : 375.9451828281844 W\n",
      "[codecarbon INFO @ 00:40:01] Energy consumed for all CPUs : 2.980806 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:01] 26.890046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:01] Energy consumed for all GPUs : 7.927355 kWh. Total GPU Power : 376.01383335470024 W\n",
      "[codecarbon INFO @ 00:40:01] Energy consumed for all CPUs : 2.988182 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:01] 26.940559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:08] Energy consumed for RAM : 0.634243 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:08] Energy consumed for all GPUs : 0.317636 kWh. Total GPU Power : 376.1216455850213 W\n",
      "[codecarbon INFO @ 00:40:08] Energy consumed for all CPUs : 0.118229 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:08] 1.070108 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:16] Energy consumed for RAM : 15.989463 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:16] Energy consumed for RAM : 16.028148 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:16] Energy consumed for all GPUs : 7.924486 kWh. Total GPU Power : 379.4733321124718 W\n",
      "[codecarbon INFO @ 00:40:16] Energy consumed for all CPUs : 2.981389 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:16] 26.895337 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:16] Energy consumed for all GPUs : 7.928935 kWh. Total GPU Power : 379.74420649180473 W\n",
      "[codecarbon INFO @ 00:40:16] Energy consumed for all CPUs : 2.988764 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:16] 26.945847 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:23] Energy consumed for RAM : 0.637369 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:23] Energy consumed for all GPUs : 0.319196 kWh. Total GPU Power : 374.69648940520113 W\n",
      "[codecarbon INFO @ 00:40:23] Energy consumed for all CPUs : 0.118811 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:23] 1.075376 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:31] Energy consumed for RAM : 15.992592 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:31] Energy consumed for RAM : 16.031275 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:31] Energy consumed for all GPUs : 7.926042 kWh. Total GPU Power : 373.64947378643285 W\n",
      "[codecarbon INFO @ 00:40:31] Energy consumed for all CPUs : 2.981972 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:31] 26.900605 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:31] Energy consumed for all GPUs : 7.930491 kWh. Total GPU Power : 373.79477748823774 W\n",
      "[codecarbon INFO @ 00:40:31] Energy consumed for all CPUs : 2.989347 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:31] 26.951114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:39] Energy consumed for RAM : 0.640717 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:39] Energy consumed for all GPUs : 0.320873 kWh. Total GPU Power : 376.3747785099612 W\n",
      "[codecarbon INFO @ 00:40:39] Energy consumed for all CPUs : 0.119435 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:39] 1.081025 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:46] Energy consumed for RAM : 15.995721 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:46] Energy consumed for RAM : 16.034404 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:46] Energy consumed for all GPUs : 7.927609 kWh. Total GPU Power : 376.20475663703104 W\n",
      "[codecarbon INFO @ 00:40:46] Energy consumed for all CPUs : 2.982556 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:46] 26.905886 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:46] Energy consumed for all GPUs : 7.932058 kWh. Total GPU Power : 376.2612810680332 W\n",
      "[codecarbon INFO @ 00:40:46] Energy consumed for all CPUs : 2.989931 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:46] 26.956394 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:40:54] Energy consumed for RAM : 0.643846 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:40:54] Energy consumed for all GPUs : 0.322443 kWh. Total GPU Power : 377.1271903199193 W\n",
      "[codecarbon INFO @ 00:40:54] Energy consumed for all CPUs : 0.120019 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:40:54] 1.086308 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:01] Energy consumed for RAM : 15.998849 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:01] Energy consumed for RAM : 16.037532 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:01] Energy consumed for all GPUs : 7.929182 kWh. Total GPU Power : 377.8271887472994 W\n",
      "[codecarbon INFO @ 00:41:01] Energy consumed for all CPUs : 2.983139 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:01] 26.911170 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:01] Energy consumed for all GPUs : 7.933632 kWh. Total GPU Power : 377.90589282354716 W\n",
      "[codecarbon INFO @ 00:41:01] Energy consumed for all CPUs : 2.990514 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:01] 26.961678 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:09] Energy consumed for RAM : 0.646975 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:09] Energy consumed for all GPUs : 0.323990 kWh. Total GPU Power : 371.24867895077386 W\n",
      "[codecarbon INFO @ 00:41:09] Energy consumed for all CPUs : 0.120602 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:09] 1.091567 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:17] Energy consumed for RAM : 16.040911 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:17] Energy consumed for RAM : 16.002230 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:18] Energy consumed for all GPUs : 7.935301 kWh. Total GPU Power : 370.9130545842064 W\n",
      "[codecarbon INFO @ 00:41:18] Energy consumed for all CPUs : 2.991145 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:18] 26.967357 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:18] Energy consumed for all GPUs : 7.930851 kWh. Total GPU Power : 370.4696808107076 W\n",
      "[codecarbon INFO @ 00:41:18] Energy consumed for all CPUs : 2.983771 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:18] 26.916852 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:24] Energy consumed for RAM : 0.650105 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:24] Energy consumed for all GPUs : 0.325575 kWh. Total GPU Power : 380.58035586035703 W\n",
      "[codecarbon INFO @ 00:41:24] Energy consumed for all CPUs : 0.121185 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:24] 1.096865 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:32] Energy consumed for RAM : 16.044031 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:32] Energy consumed for RAM : 16.005349 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:32] Energy consumed for all GPUs : 7.936888 kWh. Total GPU Power : 382.15931827738797 W\n",
      "[codecarbon INFO @ 00:41:32] Energy consumed for all CPUs : 2.991727 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:32] 26.972646 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:32] Energy consumed for all GPUs : 7.932439 kWh. Total GPU Power : 382.36344071855774 W\n",
      "[codecarbon INFO @ 00:41:33] Energy consumed for all CPUs : 2.984352 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:33] 26.922140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:39] Energy consumed for RAM : 0.653234 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:39] Energy consumed for all GPUs : 0.327140 kWh. Total GPU Power : 375.84512962781037 W\n",
      "[codecarbon INFO @ 00:41:39] Energy consumed for all CPUs : 0.121769 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:39] 1.102143 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:47] Energy consumed for RAM : 16.047160 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:47] Energy consumed for RAM : 16.008478 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:47] Energy consumed for all GPUs : 7.938442 kWh. Total GPU Power : 373.1295831135274 W\n",
      "[codecarbon INFO @ 00:41:47] Energy consumed for all CPUs : 2.992310 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:47] 26.977913 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:47] Energy consumed for all GPUs : 7.933993 kWh. Total GPU Power : 373.16764686161747 W\n",
      "[codecarbon INFO @ 00:41:48] Energy consumed for all CPUs : 2.984935 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:48] 26.927407 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:41:55] Energy consumed for RAM : 0.656592 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:41:55] Energy consumed for all GPUs : 0.328818 kWh. Total GPU Power : 375.22780474619356 W\n",
      "[codecarbon INFO @ 00:41:55] Energy consumed for all CPUs : 0.122395 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:41:55] 1.107805 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:02] Energy consumed for RAM : 16.050289 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:02] Energy consumed for RAM : 16.011607 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:02] Energy consumed for all GPUs : 7.940026 kWh. Total GPU Power : 380.3231710326627 W\n",
      "[codecarbon INFO @ 00:42:02] Energy consumed for all CPUs : 2.992894 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:02] 26.983209 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:02] Energy consumed for all GPUs : 7.935577 kWh. Total GPU Power : 380.27907359638436 W\n",
      "[codecarbon INFO @ 00:42:02] Energy consumed for all CPUs : 2.985519 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:03] 26.932703 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:10] Energy consumed for RAM : 0.659721 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:10] Energy consumed for all GPUs : 0.330405 kWh. Total GPU Power : 381.12044252270596 W\n",
      "[codecarbon INFO @ 00:42:10] Energy consumed for all CPUs : 0.122978 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:10] 1.113104 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:17] Energy consumed for RAM : 16.053419 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:17] Energy consumed for RAM : 16.014737 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:17] Energy consumed for all GPUs : 7.941590 kWh. Total GPU Power : 375.3736855555274 W\n",
      "[codecarbon INFO @ 00:42:17] Energy consumed for all CPUs : 2.993477 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:17] 26.988485 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:17] Energy consumed for all GPUs : 7.937140 kWh. Total GPU Power : 375.40805613428125 W\n",
      "[codecarbon INFO @ 00:42:18] Energy consumed for all CPUs : 2.986102 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:18] 26.937979 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:25] Energy consumed for RAM : 0.662850 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:25] Energy consumed for all GPUs : 0.331962 kWh. Total GPU Power : 373.7878503012005 W\n",
      "[codecarbon INFO @ 00:42:25] Energy consumed for all CPUs : 0.123561 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:25] 1.118373 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:33] Energy consumed for RAM : 16.056752 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:33] Energy consumed for RAM : 16.018070 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:33] Energy consumed for all GPUs : 7.938808 kWh. Total GPU Power : 375.83043334368614 W\n",
      "[codecarbon INFO @ 00:42:33] Energy consumed for all CPUs : 2.986723 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:33] 26.943601 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:33] Energy consumed for all GPUs : 7.943257 kWh. Total GPU Power : 375.7745744081492 W\n",
      "[codecarbon INFO @ 00:42:33] Energy consumed for all CPUs : 2.994098 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:33] 26.994108 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:40] Energy consumed for RAM : 0.665980 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:40] Energy consumed for all GPUs : 0.333531 kWh. Total GPU Power : 376.8808854329519 W\n",
      "[codecarbon INFO @ 00:42:40] Energy consumed for all CPUs : 0.124144 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:40] 1.123655 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:48] Energy consumed for RAM : 16.059879 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:48] Energy consumed for RAM : 16.021198 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:48] Energy consumed for all GPUs : 7.944828 kWh. Total GPU Power : 377.2960486672961 W\n",
      "[codecarbon INFO @ 00:42:48] Energy consumed for all CPUs : 2.994681 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:48] 26.999389 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:48] Energy consumed for all GPUs : 7.940378 kWh. Total GPU Power : 377.1596401035056 W\n",
      "[codecarbon INFO @ 00:42:48] Energy consumed for all CPUs : 2.987307 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:48] 26.948884 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:42:55] Energy consumed for RAM : 0.669109 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:42:55] Energy consumed for all GPUs : 0.335104 kWh. Total GPU Power : 377.56022657691256 W\n",
      "[codecarbon INFO @ 00:42:55] Energy consumed for all CPUs : 0.124728 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:42:55] 1.128940 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:03] Energy consumed for RAM : 16.063009 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:03] Energy consumed for RAM : 16.024327 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:03] Energy consumed for all GPUs : 7.946388 kWh. Total GPU Power : 374.5998057640379 W\n",
      "[codecarbon INFO @ 00:43:03] Energy consumed for all CPUs : 2.995265 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:03] 27.004662 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:03] Energy consumed for all GPUs : 7.941939 kWh. Total GPU Power : 374.66590606602693 W\n",
      "[codecarbon INFO @ 00:43:03] Energy consumed for all CPUs : 2.987890 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:03] 26.954156 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:12] Energy consumed for RAM : 0.672465 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:12] Energy consumed for all GPUs : 0.336779 kWh. Total GPU Power : 374.96159784405035 W\n",
      "[codecarbon INFO @ 00:43:12] Energy consumed for all CPUs : 0.125354 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:12] 1.134598 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:18] Energy consumed for RAM : 16.066138 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:18] Energy consumed for RAM : 16.027457 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:18] Energy consumed for all GPUs : 7.947973 kWh. Total GPU Power : 380.53709500814176 W\n",
      "[codecarbon INFO @ 00:43:18] Energy consumed for all CPUs : 2.995848 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:18] 27.009958 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:18] Energy consumed for all GPUs : 7.943523 kWh. Total GPU Power : 380.4951545023561 W\n",
      "[codecarbon INFO @ 00:43:18] Energy consumed for all CPUs : 2.988473 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:18] 26.959453 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:27] Energy consumed for RAM : 0.675588 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:27] Energy consumed for all GPUs : 0.338360 kWh. Total GPU Power : 380.48834638286974 W\n",
      "[codecarbon INFO @ 00:43:27] Energy consumed for all CPUs : 0.125936 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:27] 1.139885 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:33] Energy consumed for RAM : 16.069267 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:33] Energy consumed for RAM : 16.030586 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:33] Energy consumed for all GPUs : 7.949536 kWh. Total GPU Power : 375.30180199831295 W\n",
      "[codecarbon INFO @ 00:43:33] Energy consumed for all CPUs : 2.996431 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:33] 27.015234 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:33] Energy consumed for all GPUs : 7.945087 kWh. Total GPU Power : 375.3031149111852 W\n",
      "[codecarbon INFO @ 00:43:33] Energy consumed for all CPUs : 2.989056 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:33] 26.964729 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:42] Energy consumed for RAM : 0.678717 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:42] Energy consumed for all GPUs : 0.339918 kWh. Total GPU Power : 374.0400767830874 W\n",
      "[codecarbon INFO @ 00:43:42] Energy consumed for all CPUs : 0.126520 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:42] 1.145155 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:50] Energy consumed for RAM : 16.072620 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:50] Energy consumed for RAM : 16.033938 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:50] Energy consumed for all GPUs : 7.951213 kWh. Total GPU Power : 375.6737927449009 W\n",
      "[codecarbon INFO @ 00:43:50] Energy consumed for all CPUs : 2.997056 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:50] 27.020888 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:50] Energy consumed for all GPUs : 7.946763 kWh. Total GPU Power : 375.7051637041037 W\n",
      "[codecarbon INFO @ 00:43:50] Energy consumed for all CPUs : 2.989682 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:50] 26.970383 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:43:57] Energy consumed for RAM : 0.681846 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:43:57] Energy consumed for all GPUs : 0.341490 kWh. Total GPU Power : 377.27202564863455 W\n",
      "[codecarbon INFO @ 00:43:57] Energy consumed for all CPUs : 0.127103 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:43:57] 1.150439 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:05] Energy consumed for RAM : 16.075748 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:05] Energy consumed for RAM : 16.037066 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:05] Energy consumed for all GPUs : 7.952783 kWh. Total GPU Power : 376.9379660428547 W\n",
      "[codecarbon INFO @ 00:44:05] Energy consumed for all CPUs : 2.997640 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:05] 27.026170 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:05] Energy consumed for all GPUs : 7.948333 kWh. Total GPU Power : 377.01283864755226 W\n",
      "[codecarbon INFO @ 00:44:05] Energy consumed for all CPUs : 2.990265 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:05] 26.975664 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:12] Energy consumed for RAM : 0.684975 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:12] Energy consumed for all GPUs : 0.343053 kWh. Total GPU Power : 375.3542000522636 W\n",
      "[codecarbon INFO @ 00:44:12] Energy consumed for all CPUs : 0.127686 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:12] 1.155714 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:20] Energy consumed for RAM : 16.078877 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:20] Energy consumed for RAM : 16.040195 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:20] Energy consumed for all GPUs : 7.949886 kWh. Total GPU Power : 373.00121120373865 W\n",
      "[codecarbon INFO @ 00:44:20] Energy consumed for all CPUs : 2.990848 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:20] 26.980929 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:20] Energy consumed for all GPUs : 7.954336 kWh. Total GPU Power : 372.92240083158066 W\n",
      "[codecarbon INFO @ 00:44:20] Energy consumed for all CPUs : 2.998223 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:20] 27.031435 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:28] Energy consumed for RAM : 0.688301 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:28] Energy consumed for all GPUs : 0.344706 kWh. Total GPU Power : 373.32475164011015 W\n",
      "[codecarbon INFO @ 00:44:28] Energy consumed for all CPUs : 0.128307 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:28] 1.161313 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:35] Energy consumed for RAM : 16.082005 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:35] Energy consumed for RAM : 16.043324 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:35] Energy consumed for all GPUs : 7.955917 kWh. Total GPU Power : 379.8426624926552 W\n",
      "[codecarbon INFO @ 00:44:35] Energy consumed for all CPUs : 2.998806 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:35] 27.036728 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:35] Energy consumed for all GPUs : 7.951468 kWh. Total GPU Power : 379.72815892012704 W\n",
      "[codecarbon INFO @ 00:44:35] Energy consumed for all CPUs : 2.991431 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:35] 26.986223 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:43] Energy consumed for RAM : 0.691428 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:43] Energy consumed for all GPUs : 0.346282 kWh. Total GPU Power : 378.76253305857114 W\n",
      "[codecarbon INFO @ 00:44:43] Energy consumed for all CPUs : 0.128889 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:43] 1.166600 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:50] Energy consumed for RAM : 16.085134 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:50] Energy consumed for RAM : 16.046453 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:50] Energy consumed for all GPUs : 7.957477 kWh. Total GPU Power : 374.46814848125337 W\n",
      "[codecarbon INFO @ 00:44:50] Energy consumed for all CPUs : 2.999389 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:50] 27.042000 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:50] Energy consumed for all GPUs : 7.953027 kWh. Total GPU Power : 374.5032606791549 W\n",
      "[codecarbon INFO @ 00:44:50] Energy consumed for all CPUs : 2.992014 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:50] 26.991495 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:44:58] Energy consumed for RAM : 0.694558 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:44:58] Energy consumed for all GPUs : 0.347854 kWh. Total GPU Power : 377.33423402356215 W\n",
      "[codecarbon INFO @ 00:44:58] Energy consumed for all CPUs : 0.129473 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:44:58] 1.171884 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:05] Energy consumed for RAM : 16.049772 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:05] Energy consumed for RAM : 16.088454 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:05] Energy consumed for all GPUs : 7.954688 kWh. Total GPU Power : 375.7949152915773 W\n",
      "[codecarbon INFO @ 00:45:05] Energy consumed for all CPUs : 2.992634 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:05] 26.997093 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:05] Energy consumed for all GPUs : 7.959137 kWh. Total GPU Power : 375.6483728282341 W\n",
      "[codecarbon INFO @ 00:45:05] Energy consumed for all CPUs : 3.000009 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:05] 27.047600 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:13] Energy consumed for RAM : 0.697687 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:13] Energy consumed for all GPUs : 0.349420 kWh. Total GPU Power : 376.00791966966017 W\n",
      "[codecarbon INFO @ 00:45:13] Energy consumed for all CPUs : 0.130056 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:13] 1.177163 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:20] Energy consumed for RAM : 16.091579 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:20] Energy consumed for RAM : 16.052899 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:20] Energy consumed for all GPUs : 7.960707 kWh. Total GPU Power : 377.3859020114508 W\n",
      "[codecarbon INFO @ 00:45:20] Energy consumed for all CPUs : 3.000591 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:20] 27.052878 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:20] Energy consumed for all GPUs : 7.956267 kWh. Total GPU Power : 379.4750111101629 W\n",
      "[codecarbon INFO @ 00:45:20] Energy consumed for all CPUs : 2.993217 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:20] 27.002383 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:28] Energy consumed for RAM : 0.700816 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:28] Energy consumed for all GPUs : 0.350982 kWh. Total GPU Power : 375.06055654077574 W\n",
      "[codecarbon INFO @ 00:45:28] Energy consumed for all CPUs : 0.130639 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:28] 1.182437 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:35] Energy consumed for RAM : 16.094708 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:35] Energy consumed for RAM : 16.056027 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:35] Energy consumed for all GPUs : 7.962278 kWh. Total GPU Power : 377.03585057635826 W\n",
      "[codecarbon INFO @ 00:45:35] Energy consumed for all CPUs : 3.001175 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:35] 27.058161 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:35] Energy consumed for all GPUs : 7.957828 kWh. Total GPU Power : 374.8467048149955 W\n",
      "[codecarbon INFO @ 00:45:35] Energy consumed for all CPUs : 2.993800 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:35] 27.007655 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:43] Energy consumed for RAM : 0.704147 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:43] Energy consumed for all GPUs : 0.352646 kWh. Total GPU Power : 375.48259915236184 W\n",
      "[codecarbon INFO @ 00:45:43] Energy consumed for all CPUs : 0.131260 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:43] 1.188054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:50] Energy consumed for RAM : 16.097838 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:50] Energy consumed for RAM : 16.059155 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:50] Energy consumed for all GPUs : 7.963847 kWh. Total GPU Power : 376.85266466687887 W\n",
      "[codecarbon INFO @ 00:45:50] Energy consumed for all CPUs : 3.001758 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:50] 27.063443 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:50] Energy consumed for all GPUs : 7.959398 kWh. Total GPU Power : 376.91497777207724 W\n",
      "[codecarbon INFO @ 00:45:50] Energy consumed for all CPUs : 2.994383 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:50] 27.012937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:45:58] Energy consumed for RAM : 0.707276 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:45:58] Energy consumed for all GPUs : 0.354224 kWh. Total GPU Power : 378.8241087472313 W\n",
      "[codecarbon INFO @ 00:45:58] Energy consumed for all CPUs : 0.131843 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:45:58] 1.193344 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:05] Energy consumed for RAM : 16.100966 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:05] Energy consumed for RAM : 16.062282 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:05] Energy consumed for all GPUs : 7.960957 kWh. Total GPU Power : 374.54157483828624 W\n",
      "[codecarbon INFO @ 00:46:05] Energy consumed for all CPUs : 2.994966 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:05] 27.018206 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:05] Energy consumed for all GPUs : 7.965407 kWh. Total GPU Power : 374.436032502294 W\n",
      "[codecarbon INFO @ 00:46:05] Energy consumed for all CPUs : 3.002342 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:05] 27.068714 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:13] Energy consumed for RAM : 0.710405 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:13] Energy consumed for all GPUs : 0.355788 kWh. Total GPU Power : 375.3355127544472 W\n",
      "[codecarbon INFO @ 00:46:14] Energy consumed for all CPUs : 0.132427 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:14] 1.198620 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:21] Energy consumed for RAM : 16.065626 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:21] Energy consumed for RAM : 16.104306 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:22] Energy consumed for all GPUs : 7.962626 kWh. Total GPU Power : 374.8175899390583 W\n",
      "[codecarbon INFO @ 00:46:22] Energy consumed for all CPUs : 2.995591 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:22] 27.023842 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:22] Energy consumed for all GPUs : 7.967075 kWh. Total GPU Power : 374.78344973441745 W\n",
      "[codecarbon INFO @ 00:46:22] Energy consumed for all CPUs : 3.002966 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:22] 27.074347 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:28] Energy consumed for RAM : 0.713534 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:28] Energy consumed for all GPUs : 0.357350 kWh. Total GPU Power : 375.1767955244949 W\n",
      "[codecarbon INFO @ 00:46:28] Energy consumed for all CPUs : 0.133010 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:28] 1.203894 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:36] Energy consumed for RAM : 16.107427 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:36] Energy consumed for all GPUs : 7.968650 kWh. Total GPU Power : 379.0492526396425 W\n",
      "[codecarbon INFO @ 00:46:36] Energy consumed for all CPUs : 3.003547 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:36] 27.079624 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:36] Energy consumed for RAM : 16.068750 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:36] Energy consumed for all GPUs : 7.964200 kWh. Total GPU Power : 378.7289439941604 W\n",
      "[codecarbon INFO @ 00:46:36] Energy consumed for all CPUs : 2.996173 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:36] 27.029123 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:43] Energy consumed for RAM : 0.716664 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:44] Energy consumed for all GPUs : 0.358915 kWh. Total GPU Power : 375.6440882740643 W\n",
      "[codecarbon INFO @ 00:46:44] Energy consumed for all CPUs : 0.133593 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:44] 1.209172 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:51] Energy consumed for RAM : 16.110557 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:51] Energy consumed for all GPUs : 7.970208 kWh. Total GPU Power : 374.0711306807403 W\n",
      "[codecarbon INFO @ 00:46:51] Energy consumed for all CPUs : 3.004131 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:51] 27.084895 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:51] Energy consumed for RAM : 16.071880 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:51] Energy consumed for all GPUs : 7.965758 kWh. Total GPU Power : 374.0130899997148 W\n",
      "[codecarbon INFO @ 00:46:51] Energy consumed for all CPUs : 2.996756 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:51] 27.034394 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:46:59] Energy consumed for RAM : 0.719992 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:46:59] Energy consumed for all GPUs : 0.360582 kWh. Total GPU Power : 376.4054126813925 W\n",
      "[codecarbon INFO @ 00:46:59] Energy consumed for all CPUs : 0.134214 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:46:59] 1.214788 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:06] Energy consumed for RAM : 16.113686 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:06] Energy consumed for all GPUs : 7.971794 kWh. Total GPU Power : 380.8104273443347 W\n",
      "[codecarbon INFO @ 00:47:06] Energy consumed for all CPUs : 3.004714 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:06] 27.090194 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:06] Energy consumed for RAM : 16.075009 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:06] Energy consumed for all GPUs : 7.967344 kWh. Total GPU Power : 380.8013267150017 W\n",
      "[codecarbon INFO @ 00:47:06] Energy consumed for all CPUs : 2.997339 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:06] 27.039693 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:14] Energy consumed for RAM : 0.723121 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:14] Energy consumed for all GPUs : 0.362158 kWh. Total GPU Power : 378.4695985203514 W\n",
      "[codecarbon INFO @ 00:47:14] Energy consumed for all CPUs : 0.134797 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:14] 1.220077 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:21] Energy consumed for RAM : 16.116815 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:21] Energy consumed for all GPUs : 7.973358 kWh. Total GPU Power : 375.52185669198343 W\n",
      "[codecarbon INFO @ 00:47:21] Energy consumed for all CPUs : 3.005297 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:21] 27.095470 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:21] Energy consumed for RAM : 16.078139 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:22] Energy consumed for all GPUs : 7.968908 kWh. Total GPU Power : 375.4397881042333 W\n",
      "[codecarbon INFO @ 00:47:22] Energy consumed for all CPUs : 2.997923 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:22] 27.044970 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:29] Energy consumed for RAM : 0.726250 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:29] Energy consumed for all GPUs : 0.363725 kWh. Total GPU Power : 376.0595830501373 W\n",
      "[codecarbon INFO @ 00:47:29] Energy consumed for all CPUs : 0.135380 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:29] 1.225355 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:37] Energy consumed for RAM : 16.081468 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:37] Energy consumed for RAM : 16.120147 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:37] Energy consumed for all GPUs : 7.975017 kWh. Total GPU Power : 374.0509185488796 W\n",
      "[codecarbon INFO @ 00:47:37] Energy consumed for all CPUs : 3.005918 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:37] 27.101082 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:37] Energy consumed for all GPUs : 7.970567 kWh. Total GPU Power : 374.3894596174712 W\n",
      "[codecarbon INFO @ 00:47:37] Energy consumed for all CPUs : 2.998544 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:37] 27.050579 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:44] Energy consumed for RAM : 0.729379 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:44] Energy consumed for all GPUs : 0.365292 kWh. Total GPU Power : 376.42548676460973 W\n",
      "[codecarbon INFO @ 00:47:44] Energy consumed for all CPUs : 0.135964 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:44] 1.230635 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:52] Energy consumed for RAM : 16.084596 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:52] Energy consumed for RAM : 16.123276 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:52] Energy consumed for all GPUs : 7.972146 kWh. Total GPU Power : 379.1477289589786 W\n",
      "[codecarbon INFO @ 00:47:52] Energy consumed for all CPUs : 2.999127 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:52] 27.055868 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:52] Energy consumed for all GPUs : 7.976595 kWh. Total GPU Power : 378.9986103274554 W\n",
      "[codecarbon INFO @ 00:47:52] Energy consumed for all CPUs : 3.006502 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:52] 27.106373 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:47:59] Energy consumed for RAM : 0.732509 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:47:59] Energy consumed for all GPUs : 0.366857 kWh. Total GPU Power : 375.67080107798364 W\n",
      "[codecarbon INFO @ 00:47:59] Energy consumed for all CPUs : 0.136547 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:47:59] 1.235913 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:07] Energy consumed for RAM : 16.087725 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:07] Energy consumed for RAM : 16.126404 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:07] Energy consumed for all GPUs : 7.973699 kWh. Total GPU Power : 372.86226872611803 W\n",
      "[codecarbon INFO @ 00:48:07] Energy consumed for all CPUs : 2.999710 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:07] 27.061134 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:07] Energy consumed for all GPUs : 7.978148 kWh. Total GPU Power : 372.9135530273088 W\n",
      "[codecarbon INFO @ 00:48:07] Energy consumed for all CPUs : 3.007085 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:07] 27.111638 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:16] Energy consumed for RAM : 0.735871 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:16] Energy consumed for all GPUs : 0.368538 kWh. Total GPU Power : 375.64571078111067 W\n",
      "[codecarbon INFO @ 00:48:16] Energy consumed for all CPUs : 0.137174 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:16] 1.241583 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:22] Energy consumed for RAM : 16.090853 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:22] Energy consumed for RAM : 16.129533 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:22] Energy consumed for all GPUs : 7.975283 kWh. Total GPU Power : 380.49701304758224 W\n",
      "[codecarbon INFO @ 00:48:22] Energy consumed for all CPUs : 3.000293 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:22] 27.066430 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:22] Energy consumed for all GPUs : 7.979733 kWh. Total GPU Power : 380.49481628603456 W\n",
      "[codecarbon INFO @ 00:48:22] Energy consumed for all CPUs : 3.007668 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:22] 27.116934 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:31] Energy consumed for RAM : 0.739000 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:31] Energy consumed for all GPUs : 0.370119 kWh. Total GPU Power : 379.6760152772696 W\n",
      "[codecarbon INFO @ 00:48:31] Energy consumed for all CPUs : 0.137757 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:31] 1.246876 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:37] Energy consumed for RAM : 16.093982 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:37] Energy consumed for RAM : 16.132661 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:37] Energy consumed for all GPUs : 7.976843 kWh. Total GPU Power : 374.48106807589306 W\n",
      "[codecarbon INFO @ 00:48:37] Energy consumed for all CPUs : 3.000877 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:37] 27.071702 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:37] Energy consumed for all GPUs : 7.981292 kWh. Total GPU Power : 374.5100542630194 W\n",
      "[codecarbon INFO @ 00:48:37] Energy consumed for all CPUs : 3.008251 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:37] 27.122205 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:46] Energy consumed for RAM : 0.742129 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:46] Energy consumed for all GPUs : 0.371672 kWh. Total GPU Power : 372.77621391284634 W\n",
      "[codecarbon INFO @ 00:48:46] Energy consumed for all CPUs : 0.138340 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:46] 1.252141 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:54] Energy consumed for RAM : 16.097344 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:54] Energy consumed for RAM : 16.136023 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:48:54] Energy consumed for all GPUs : 7.978521 kWh. Total GPU Power : 375.0543001925734 W\n",
      "[codecarbon INFO @ 00:48:54] Energy consumed for all CPUs : 3.001503 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:54] 27.077369 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:48:54] Energy consumed for all GPUs : 7.982971 kWh. Total GPU Power : 375.1105323512515 W\n",
      "[codecarbon INFO @ 00:48:54] Energy consumed for all CPUs : 3.008878 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:48:54] 27.127872 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:01] Energy consumed for RAM : 0.745258 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:01] Energy consumed for all GPUs : 0.373248 kWh. Total GPU Power : 378.4019297605063 W\n",
      "[codecarbon INFO @ 00:49:01] Energy consumed for all CPUs : 0.138923 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:01] 1.257429 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:09] Energy consumed for RAM : 16.100473 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:09] Energy consumed for RAM : 16.139151 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:09] Energy consumed for all GPUs : 7.980098 kWh. Total GPU Power : 378.63094649712747 W\n",
      "[codecarbon INFO @ 00:49:09] Energy consumed for all CPUs : 3.002087 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:09] 27.082657 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:09] Energy consumed for all GPUs : 7.984548 kWh. Total GPU Power : 378.65717955931865 W\n",
      "[codecarbon INFO @ 00:49:09] Energy consumed for all CPUs : 3.009461 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:09] 27.133160 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:16] Energy consumed for RAM : 0.748388 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:16] Energy consumed for all GPUs : 0.374821 kWh. Total GPU Power : 377.77753980462705 W\n",
      "[codecarbon INFO @ 00:49:16] Energy consumed for all CPUs : 0.139507 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:16] 1.262716 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:24] Energy consumed for RAM : 16.103602 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:24] Energy consumed for RAM : 16.142280 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:24] Energy consumed for all GPUs : 7.981665 kWh. Total GPU Power : 376.2009721197399 W\n",
      "[codecarbon INFO @ 00:49:24] Energy consumed for all CPUs : 3.002670 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:24] 27.087937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:24] Energy consumed for all GPUs : 7.986115 kWh. Total GPU Power : 376.2308369579915 W\n",
      "[codecarbon INFO @ 00:49:24] Energy consumed for all CPUs : 3.010045 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:24] 27.138439 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:32] Energy consumed for RAM : 0.751715 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:32] Energy consumed for all GPUs : 0.376484 kWh. Total GPU Power : 375.26917896594995 W\n",
      "[codecarbon INFO @ 00:49:32] Energy consumed for all CPUs : 0.140127 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:32] 1.268326 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:39] Energy consumed for RAM : 16.106731 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:39] Energy consumed for RAM : 16.145409 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:39] Energy consumed for all GPUs : 7.983240 kWh. Total GPU Power : 378.0861641399245 W\n",
      "[codecarbon INFO @ 00:49:39] Energy consumed for all CPUs : 3.003253 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:39] 27.093224 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:39] Energy consumed for all GPUs : 7.987689 kWh. Total GPU Power : 378.12779014473904 W\n",
      "[codecarbon INFO @ 00:49:39] Energy consumed for all CPUs : 3.010628 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:39] 27.143727 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:47] Energy consumed for RAM : 0.754844 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:47] Energy consumed for all GPUs : 0.378066 kWh. Total GPU Power : 380.07066731153117 W\n",
      "[codecarbon INFO @ 00:49:47] Energy consumed for all CPUs : 0.140710 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:47] 1.273621 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:54] Energy consumed for RAM : 16.109860 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:54] Energy consumed for RAM : 16.148538 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:49:54] Energy consumed for all GPUs : 7.984807 kWh. Total GPU Power : 376.1840298505977 W\n",
      "[codecarbon INFO @ 00:49:54] Energy consumed for all CPUs : 3.003836 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:54] 27.098503 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:49:54] Energy consumed for all GPUs : 7.989256 kWh. Total GPU Power : 376.1870329397765 W\n",
      "[codecarbon INFO @ 00:49:54] Energy consumed for all CPUs : 3.011211 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:49:54] 27.149006 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:02] Energy consumed for RAM : 0.757974 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:02] Energy consumed for all GPUs : 0.379633 kWh. Total GPU Power : 376.029737848571 W\n",
      "[codecarbon INFO @ 00:50:02] Energy consumed for all CPUs : 0.141293 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:02] 1.278900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:09] Energy consumed for RAM : 16.151855 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:09] Energy consumed for RAM : 16.113178 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:09] Energy consumed for all GPUs : 7.990923 kWh. Total GPU Power : 377.49994728466925 W\n",
      "[codecarbon INFO @ 00:50:09] Energy consumed for all CPUs : 3.011830 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:09] 27.154608 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:09] Energy consumed for all GPUs : 7.986473 kWh. Total GPU Power : 377.3986440777889 W\n",
      "[codecarbon INFO @ 00:50:09] Energy consumed for all CPUs : 3.004455 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:09] 27.104107 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:17] Energy consumed for RAM : 0.761103 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:17] Energy consumed for all GPUs : 0.381198 kWh. Total GPU Power : 375.86344385106963 W\n",
      "[codecarbon INFO @ 00:50:17] Energy consumed for all CPUs : 0.141877 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:17] 1.284178 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:24] Energy consumed for RAM : 16.154984 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:24] Energy consumed for RAM : 16.116306 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:24] Energy consumed for all GPUs : 7.992499 kWh. Total GPU Power : 378.5047850154438 W\n",
      "[codecarbon INFO @ 00:50:24] Energy consumed for all CPUs : 3.012413 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:24] 27.159896 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:24] Energy consumed for all GPUs : 7.988050 kWh. Total GPU Power : 378.60941236261505 W\n",
      "[codecarbon INFO @ 00:50:24] Energy consumed for all CPUs : 3.005038 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:24] 27.109394 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:32] Energy consumed for RAM : 0.764233 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:32] Energy consumed for all GPUs : 0.382772 kWh. Total GPU Power : 377.86168853462254 W\n",
      "[codecarbon INFO @ 00:50:32] Energy consumed for all CPUs : 0.142460 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:32] 1.289465 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:39] Energy consumed for RAM : 16.158113 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:39] Energy consumed for RAM : 16.119435 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:39] Energy consumed for all GPUs : 7.994057 kWh. Total GPU Power : 373.9648926740152 W\n",
      "[codecarbon INFO @ 00:50:39] Energy consumed for all CPUs : 3.012996 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:39] 27.165166 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:39] Energy consumed for all GPUs : 7.989608 kWh. Total GPU Power : 374.0409788826124 W\n",
      "[codecarbon INFO @ 00:50:39] Energy consumed for all CPUs : 3.005621 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:39] 27.114664 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:48] Energy consumed for RAM : 0.767569 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:48] Energy consumed for all GPUs : 0.384439 kWh. Total GPU Power : 375.2591630999043 W\n",
      "[codecarbon INFO @ 00:50:48] Energy consumed for all CPUs : 0.143082 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:48] 1.295090 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:54] Energy consumed for RAM : 16.161242 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:54] Energy consumed for RAM : 16.122564 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:50:54] Energy consumed for all GPUs : 7.995642 kWh. Total GPU Power : 380.4933769052603 W\n",
      "[codecarbon INFO @ 00:50:54] Energy consumed for all CPUs : 3.013580 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:54] 27.170464 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:50:54] Energy consumed for all GPUs : 7.991192 kWh. Total GPU Power : 380.5251132660333 W\n",
      "[codecarbon INFO @ 00:50:54] Energy consumed for all CPUs : 3.006205 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:50:54] 27.119961 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:03] Energy consumed for RAM : 0.770697 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:03] Energy consumed for all GPUs : 0.386017 kWh. Total GPU Power : 379.2974584379406 W\n",
      "[codecarbon INFO @ 00:51:03] Energy consumed for all CPUs : 0.143665 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:03] 1.300380 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:09] Energy consumed for RAM : 16.164371 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:09] Energy consumed for RAM : 16.125692 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:09] Energy consumed for all GPUs : 7.992752 kWh. Total GPU Power : 374.65873805689495 W\n",
      "[codecarbon INFO @ 00:51:09] Energy consumed for all CPUs : 3.006788 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:09] 27.125232 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:09] Energy consumed for all GPUs : 7.997202 kWh. Total GPU Power : 374.5734024979828 W\n",
      "[codecarbon INFO @ 00:51:09] Energy consumed for all CPUs : 3.014163 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:09] 27.175735 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:18] Energy consumed for RAM : 0.773826 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:18] Energy consumed for all GPUs : 0.387574 kWh. Total GPU Power : 373.67588038782895 W\n",
      "[codecarbon INFO @ 00:51:18] Energy consumed for all CPUs : 0.144249 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:18] 1.305648 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:25] Energy consumed for RAM : 16.167699 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:25] Energy consumed for RAM : 16.129021 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:25] Energy consumed for all GPUs : 7.998863 kWh. Total GPU Power : 374.9872576748628 W\n",
      "[codecarbon INFO @ 00:51:25] Energy consumed for all CPUs : 3.014783 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:25] 27.181346 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:25] Energy consumed for all GPUs : 7.994417 kWh. Total GPU Power : 375.64242797303837 W\n",
      "[codecarbon INFO @ 00:51:25] Energy consumed for all CPUs : 3.007409 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:25] 27.130846 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:33] Energy consumed for RAM : 0.776955 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:33] Energy consumed for all GPUs : 0.389160 kWh. Total GPU Power : 380.8925389600013 W\n",
      "[codecarbon INFO @ 00:51:33] Energy consumed for all CPUs : 0.144832 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:33] 1.310947 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:40] Energy consumed for RAM : 16.170828 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:40] Energy consumed for RAM : 16.132149 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:40] Energy consumed for all GPUs : 8.000447 kWh. Total GPU Power : 380.4417584860177 W\n",
      "[codecarbon INFO @ 00:51:40] Energy consumed for all CPUs : 3.015367 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:40] 27.186642 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:40] Energy consumed for all GPUs : 7.995998 kWh. Total GPU Power : 379.7207556518499 W\n",
      "[codecarbon INFO @ 00:51:40] Energy consumed for all CPUs : 3.007992 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:40] 27.136139 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:48] Energy consumed for RAM : 0.780084 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:48] Energy consumed for all GPUs : 0.390718 kWh. Total GPU Power : 373.956670342387 W\n",
      "[codecarbon INFO @ 00:51:48] Energy consumed for all CPUs : 0.145415 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:48] 1.316217 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:55] Energy consumed for RAM : 16.173956 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:55] Energy consumed for RAM : 16.135277 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:51:55] Energy consumed for all GPUs : 8.002002 kWh. Total GPU Power : 373.27742483356485 W\n",
      "[codecarbon INFO @ 00:51:55] Energy consumed for all CPUs : 3.015950 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:55] 27.191908 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:51:55] Energy consumed for all GPUs : 7.997552 kWh. Total GPU Power : 373.31230132290386 W\n",
      "[codecarbon INFO @ 00:51:55] Energy consumed for all CPUs : 3.008575 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:51:55] 27.141404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:52:03] Energy consumed for RAM : 0.783402 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:52:03] Energy consumed for all GPUs : 0.392378 kWh. Total GPU Power : 375.94470722208877 W\n",
      "[codecarbon INFO @ 00:52:03] Energy consumed for all CPUs : 0.146034 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:52:03] 1.321814 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:52:10] Energy consumed for RAM : 16.177085 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:52:10] Energy consumed for RAM : 16.138405 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:52:10] Energy consumed for all GPUs : 8.003582 kWh. Total GPU Power : 379.30612065257935 W\n",
      "[codecarbon INFO @ 00:52:10] Energy consumed for all CPUs : 3.016533 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:52:10] 27.197200 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:52:10] Energy consumed for all GPUs : 7.999132 kWh. Total GPU Power : 379.3432608846744 W\n",
      "[codecarbon INFO @ 00:52:10] Energy consumed for all CPUs : 3.009158 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:52:10] 27.146695 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:52:18] Energy consumed for RAM : 0.786529 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:52:18] Energy consumed for all GPUs : 0.393959 kWh. Total GPU Power : 379.9502995384881 W\n",
      "[codecarbon INFO @ 00:52:18] Energy consumed for all CPUs : 0.146617 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:52:18] 1.327105 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:52:25] Energy consumed for RAM : 16.180214 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:52:25] Energy consumed for RAM : 16.141534 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:52:25] Energy consumed for all GPUs : 8.005159 kWh. Total GPU Power : 378.8170267830688 W\n",
      "[codecarbon INFO @ 00:52:25] Energy consumed for all CPUs : 3.017117 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:52:25] 27.202490 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:52:25] Energy consumed for all GPUs : 8.000710 kWh. Total GPU Power : 378.8818565562215 W\n",
      "[codecarbon INFO @ 00:52:25] Energy consumed for all CPUs : 3.009742 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:52:25] 27.151986 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:52:33] Energy consumed for RAM : 0.789657 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:52:33] Energy consumed for all GPUs : 0.395532 kWh. Total GPU Power : 377.7742140065743 W\n",
      "[codecarbon INFO @ 00:52:33] Energy consumed for all CPUs : 0.147200 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:52:33] 1.332389 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:52:41] Energy consumed for RAM : 16.183559 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:52:41] Energy consumed for RAM : 16.144878 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:52:41] Energy consumed for all GPUs : 8.006821 kWh. Total GPU Power : 373.20076192710076 W\n",
      "[codecarbon INFO @ 00:52:41] Energy consumed for all CPUs : 3.017740 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:52:41] 27.208120 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:52:41] Energy consumed for all GPUs : 8.002372 kWh. Total GPU Power : 373.271194691735 W\n",
      "[codecarbon INFO @ 00:52:41] Energy consumed for all CPUs : 3.010366 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:52:41] 27.157615 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:123\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:434\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/trainer.py:2721\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2719\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2724\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/trainer.py:3565\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3562\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   3563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 3565\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   3567\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/trainer.py:933\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;124;03mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;124;03m        by the `model.forward()` method are automatically removed. It must implement `__len__`.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;66;03m# If we have persistent workers, don't do a fork bomb especially as eval datasets\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# don't change during training\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_dataloader\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_persistent_workers:\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1622, training_loss=0.5641915246605432, metrics={'train_runtime': 70425.8905, 'train_samples_per_second': 0.737, 'train_steps_per_second': 0.023, 'total_flos': 1.0669921359359902e+19, 'train_loss': 0.5641915246605432, 'epoch': 1.0})"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 05:02:19] Energy consumed for RAM : 3.915959 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:02:19] Energy consumed for all GPUs : 0.673799 kWh. Total GPU Power : 66.50461677138593 W\n",
      "[codecarbon INFO @ 05:02:19] Energy consumed for all CPUs : 0.729900 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:02:19] 5.319658 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:02:27] Energy consumed for RAM : 19.271216 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:02:27] Energy consumed for all GPUs : 8.279957 kWh. Total GPU Power : 66.45437356308804 W\n",
      "[codecarbon INFO @ 05:02:27] Energy consumed for all CPUs : 3.593066 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:02:27] 31.144239 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:02:27] Energy consumed for RAM : 19.309969 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:02:27] Energy consumed for all GPUs : 8.284409 kWh. Total GPU Power : 66.89956247884466 W\n",
      "[codecarbon INFO @ 05:02:27] Energy consumed for all CPUs : 3.600449 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:02:27] 31.194826 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:02:34] Energy consumed for RAM : 3.919089 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:02:34] Energy consumed for all GPUs : 0.674076 kWh. Total GPU Power : 66.40386858432889 W\n",
      "[codecarbon INFO @ 05:02:34] Energy consumed for all CPUs : 0.730483 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:02:34] 5.323648 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:02:42] Energy consumed for RAM : 19.274345 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:02:42] Energy consumed for all GPUs : 8.280234 kWh. Total GPU Power : 66.42604258912105 W\n",
      "[codecarbon INFO @ 05:02:42] Energy consumed for all CPUs : 3.593649 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:02:42] 31.148229 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:02:42] Energy consumed for RAM : 19.313098 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:02:42] Energy consumed for all GPUs : 8.284685 kWh. Total GPU Power : 66.42151160516465 W\n",
      "[codecarbon INFO @ 05:02:42] Energy consumed for all CPUs : 3.601032 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:02:42] 31.198815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:02:49] Energy consumed for RAM : 3.922218 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:02:49] Energy consumed for all GPUs : 0.674354 kWh. Total GPU Power : 66.87395860609169 W\n",
      "[codecarbon INFO @ 05:02:49] Energy consumed for all CPUs : 0.731066 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:02:49] 5.327639 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:02:57] Energy consumed for RAM : 19.277475 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:02:57] Energy consumed for all GPUs : 8.280511 kWh. Total GPU Power : 66.41575744753355 W\n",
      "[codecarbon INFO @ 05:02:57] Energy consumed for all CPUs : 3.594233 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:02:57] 31.152218 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:02:57] Energy consumed for RAM : 19.316228 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:02:57] Energy consumed for all GPUs : 8.284962 kWh. Total GPU Power : 66.42168482971036 W\n",
      "[codecarbon INFO @ 05:02:57] Energy consumed for all CPUs : 3.601615 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:02:57] 31.202805 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:03:04] Energy consumed for RAM : 3.925347 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:03:04] Energy consumed for all GPUs : 0.674631 kWh. Total GPU Power : 66.41905246024547 W\n",
      "[codecarbon INFO @ 05:03:04] Energy consumed for all CPUs : 0.731650 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:03:04] 5.331628 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 26 21:12:40 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              66W / 400W |  49989MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:46:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              67W / 400W |  47111MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:4C:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              65W / 400W |  47111MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              68W / 400W |  46967MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Feb__7_19:32:13_PST_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.66\n",
      "Build cuda_12.1.r12.1/compiler.32415258_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:12:44] Energy consumed for RAM : 0.031360 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:12:44] Energy consumed for all GPUs : 0.011417 kWh. Total GPU Power : 274.8827443950337 W\n",
      "[codecarbon INFO @ 21:12:44] Energy consumed for all CPUs : 0.005904 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:12:44] 0.048681 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:12:59] Energy consumed for RAM : 0.034488 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:12:59] Energy consumed for all GPUs : 0.012530 kWh. Total GPU Power : 267.4108124760764 W\n",
      "[codecarbon INFO @ 21:12:59] Energy consumed for all CPUs : 0.006487 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:12:59] 0.053505 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:13:14] Energy consumed for RAM : 0.037616 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:13:14] Energy consumed for all GPUs : 0.013648 kWh. Total GPU Power : 268.69069465643133 W\n",
      "[codecarbon INFO @ 21:13:14] Energy consumed for all CPUs : 0.007071 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:13:14] 0.058335 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 4\n",
      "Current CUDA Device: 0\n",
      "CUDA Device Name: NVIDIA A100-SXM4-80GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:13:29] Energy consumed for RAM : 0.040744 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:13:29] Energy consumed for all GPUs : 0.014761 kWh. Total GPU Power : 267.2918160880798 W\n",
      "[codecarbon INFO @ 21:13:29] Energy consumed for all CPUs : 0.007654 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:13:29] 0.063159 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current CUDA Device:\", torch.cuda.current_device())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'timestamp', 'url', 'source'],\n",
       "    num_rows: 12436596\n",
       "})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:08:01] Energy consumed for RAM : 14.837151 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:08:01] Energy consumed for all GPUs : 7.550383 kWh. Total GPU Power : 66.46975767643141 W\n",
      "[codecarbon INFO @ 23:08:01] Energy consumed for all CPUs : 2.766349 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:08:01] 25.153884 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:08:01] Energy consumed for RAM : 14.875846 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:08:01] Energy consumed for all GPUs : 7.554833 kWh. Total GPU Power : 66.44680723875162 W\n",
      "[codecarbon INFO @ 23:08:01] Energy consumed for all CPUs : 2.773577 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:08:01] 25.204256 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:08:16] Energy consumed for RAM : 14.840281 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:08:16] Energy consumed for all GPUs : 7.550660 kWh. Total GPU Power : 66.4084366852242 W\n",
      "[codecarbon INFO @ 23:08:16] Energy consumed for all CPUs : 2.766933 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:08:16] 25.157873 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:08:16] Energy consumed for RAM : 14.878975 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:08:16] Energy consumed for all GPUs : 7.555111 kWh. Total GPU Power : 66.85134796831777 W\n",
      "[codecarbon INFO @ 23:08:16] Energy consumed for all CPUs : 2.774160 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:08:16] 25.208247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:08:31] Energy consumed for RAM : 14.843410 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:08:31] Energy consumed for all GPUs : 7.550938 kWh. Total GPU Power : 66.83708585019306 W\n",
      "[codecarbon INFO @ 23:08:31] Energy consumed for all CPUs : 2.767516 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:08:31] 25.161865 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:08:31] Energy consumed for RAM : 14.882105 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:08:31] Energy consumed for all GPUs : 7.555388 kWh. Total GPU Power : 66.38998551138368 W\n",
      "[codecarbon INFO @ 23:08:31] Energy consumed for all CPUs : 2.774743 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:08:31] 25.212236 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:08:46] Energy consumed for RAM : 14.846540 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:08:46] Energy consumed for all GPUs : 7.551215 kWh. Total GPU Power : 66.38035372935856 W\n",
      "[codecarbon INFO @ 23:08:46] Energy consumed for all CPUs : 2.768099 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:08:46] 25.165854 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:08:46] Energy consumed for RAM : 14.885234 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:08:46] Energy consumed for all GPUs : 7.555664 kWh. Total GPU Power : 66.38888590266701 W\n",
      "[codecarbon INFO @ 23:08:46] Energy consumed for all CPUs : 2.775327 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:08:46] 25.216225 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139972791104176"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139973424920752"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(merged_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /workspace/bangla-llama/data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_gguf='/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_dir='/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Base'\n",
    "target_dir='/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls $target_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_dir='/workspace/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Base/snapshots/defb6a155755ad3547c62908c0dc6cda7aa7d018/GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/tokenizer_config.json',\n",
       " '/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/special_tokens_map.json',\n",
       " '/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repo_id = 'BanglaLLM/BanglaLLama-3-8b-BnWiki-Base-GGUF'\n",
    "repo_id = 'BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct-GGUF')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(repo_id=repo_id, repo_type=\"model\", private=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def push_to_hub(target_model_path, repo_id, hf_token):\n",
    "    print(\"Pushing model to hub...\")\n",
    "    if os.path.exists(f\"{target_model_path}/training_params.json\"):\n",
    "        training_params = json.load(\n",
    "            open(f\"{target_model_path}/training_params.json\")\n",
    "        )\n",
    "        # Optionally, remove sensitive info if needed\n",
    "        # training_params.pop(\"token\")\n",
    "        json.dump(\n",
    "            training_params, open(f\"{target_model_path}/training_params.json\", \"w\")\n",
    "        )\n",
    "\n",
    "    api = HfApi(token=hf_token)\n",
    "    api.create_repo(repo_id=repo_id, repo_type=\"model\", private=True, exist_ok=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=target_model_path, repo_id=repo_id, repo_type=\"model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing model to hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 0.00/16.1G [00:00<?, ?B/\n",
      "Upload 2 LFS files:   0%|                                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   0%| | 0.00/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 16.4k/16.1G [00:00<49:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 246k/16.1G [00:00<4:16:4\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 2.05M/16.1G [00:00<36:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 10.1M/16.1G [00:00<12:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   0%| | 6.96M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 16.1M/16.1G [00:01<16:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 23.5M/16.1G [00:01<10:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 30.9M/16.1G [00:01<10:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   1%| | 27.0M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 44.2M/16.1G [00:02<11:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 47.4M/16.1G [00:02<10:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 56.0M/16.1G [00:02<12:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 59.8M/16.1G [00:02<11:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 66.2M/16.1G [00:03<16:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 76.0M/16.1G [00:03<09:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 80.4M/16.1G [00:03<11:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 88.2M/16.1G [00:04<08:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 95.0M/16.1G [00:04<08:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 104M/16.1G [00:04<10:25,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 108M/16.1G [00:05<12:43,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 111M/16.1G [00:05<12:05,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   2%| | 79.0M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 114M/16.1G [00:05<17:50,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 128M/16.1G [00:05<09:07,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   2%| | 96.0M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 132M/16.1G [00:06<14:54,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 140M/16.1G [00:06<10:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 145M/16.1G [00:06<13:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 156M/16.1G [00:06<08:28,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   3%| | 140M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 161M/16.1G [00:07<10:42,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 188M/16.1G [00:07<06:47,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   3%| | 161M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 205M/16.1G [00:08<07:36,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 210M/16.1G [00:08<08:40,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 221M/16.1G [00:08<06:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 227M/16.1G [00:09<08:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 237M/16.1G [00:09<06:14,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   4%| | 210M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 253M/16.1G [00:09<06:32,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   5%| | 226M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 265M/16.1G [00:10<07:28,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   5%| | 242M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 270M/16.1G [00:10<09:07,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 274M/16.1G [00:10<14:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 280M/16.1G [00:10<11:10,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 286M/16.1G [00:11<10:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   6%| | 284M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   6%| | 290M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 301M/16.1G [00:11<09:56,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   6%| | 306M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 317M/16.1G [00:12<08:14,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   7%| | 322M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 332M/16.1G [00:12<07:08,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   7%| | 338M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 338M/16.1G [00:13<09:52,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 349M/16.1G [00:13<07:03,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 355M/16.1G [00:13<09:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 365M/16.1G [00:13<06:52,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   8%| | 380M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 371M/16.1G [00:14<08:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 387M/16.1G [00:14<07:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 397M/16.1G [00:14<05:51,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 404M/16.1G [00:14<08:28,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 413M/16.1G [00:15<06:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 419M/16.1G [00:15<07:45,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 430M/16.1G [00:15<05:59,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 462M/16.1G [00:16<06:54,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 468M/16.1G [00:16<08:01,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 478M/16.1G [00:17<06:13,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   9%| | 464M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 469M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 480M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 485M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 489M/16.1G [00:18<12:09,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 502M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 509M/16.1G [00:18<08:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  11%| | 518M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  11%| | 528M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  11%| | 534M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 524M/16.1G [00:19<09:16,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 530M/16.1G [00:19<10:11,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 542M/16.1G [00:19<07:09,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 548M/16.1G [00:20<08:48,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 558M/16.1G [00:20<06:45,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 564M/16.1G [00:20<08:03,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 574M/16.1G [00:20<06:15,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  12%| | 598M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 590M/16.1G [00:21<06:11,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  12%| | 614M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|| 624M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|| 630M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 602M/16.1G [00:21<10:46,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|| 646M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 621M/16.1G [00:22<07:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|| 662M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 638M/16.1G [00:22<06:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  14%|| 678M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 654M/16.1G [00:23<05:49,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  14%|| 694M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 671M/16.1G [00:23<05:46,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  14%|| 710M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 686M/16.1G [00:24<06:31,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  15%|| 726M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 703M/16.1G [00:24<06:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 709M/16.1G [00:24<06:43,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 735M/16.1G [00:25<05:43,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  15%|| 757M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 751M/16.1G [00:25<05:37,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  16%|| 771M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 767M/16.1G [00:26<05:50,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  16%|| 785M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 782M/16.1G [00:26<06:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 788M/16.1G [00:26<07:37,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 799M/16.1G [00:26<05:41,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  17%|| 817M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 815M/16.1G [00:27<06:03,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 821M/16.1G [00:27<07:10,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 831M/16.1G [00:27<05:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 837M/16.1G [00:28<07:16,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 863M/16.1G [00:28<05:47,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  18%|| 866M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 876M/16.1G [00:29<06:59,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  18%|| 880M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 880M/16.1G [00:29<09:42,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  18%|| 898M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 895M/16.1G [00:30<09:55,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  19%|| 914M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 912M/16.1G [00:30<08:01,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  19%|| 930M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 928M/16.1G [00:31<07:12,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  19%|| 946M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 944M/16.1G [00:31<06:33,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  20%|| 962M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 960M/16.1G [00:31<06:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  20%|| 978M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 974M/16.1G [00:32<08:15,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  20%|| 992M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 992M/16.1G [00:33<06:52,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  21%|| 1.01G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.01G/16.1G [00:33<06:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.01G/16.1G [00:33<07:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.02G/16.1G [00:33<06:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.03G/16.1G [00:34<07:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.04G/16.1G [00:34<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.05G/16.1G [00:34<07:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.07G/16.1G [00:35<05:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  22%|| 1.07G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.09G/16.1G [00:35<05:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  22%|| 1.09G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.09G/16.1G [00:35<07:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.10G/16.1G [00:35<06:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  23%|| 1.12G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.11G/16.1G [00:36<07:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.12G/16.1G [00:36<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.13G/16.1G [00:36<07:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.14G/16.1G [00:36<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.14G/16.1G [00:37<07:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.15G/16.1G [00:37<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  24%|| 1.17G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.17G/16.1G [00:37<06:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  24%|| 1.19G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.18G/16.1G [00:38<06:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  24%|| 1.20G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.20G/16.1G [00:38<06:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  25%|| 1.22G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.22G/16.1G [00:39<06:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  25%|| 1.23G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.23G/16.1G [00:39<05:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  25%|| 1.25G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.25G/16.1G [00:39<05:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  26%|| 1.27G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.26G/16.1G [00:40<10:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.26G/16.1G [00:40<07:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.27G/16.1G [00:41<09:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.28G/16.1G [00:41<06:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.29G/16.1G [00:41<08:09\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.30G/16.1G [00:41<06:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|| 1.32G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|| 1.33G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|| 1.34G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|| 1.35G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|| 1.36G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|| 1.36G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|| 1.37G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|| 1.38G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|| 1.39G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|| 1.39G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|| 1.40G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|| 1.41G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|| 1.42G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|| 1.43G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|| 1.44G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|| 1.44G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.30G/16.1G [00:45<42:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  30%|| 1.46G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.31G/16.1G [00:45<28:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  30%|| 1.47G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.33G/16.1G [00:46<15:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  30%|| 1.49G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.35G/16.1G [00:46<09:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  31%|| 1.51G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.36G/16.1G [00:47<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  31%|| 1.52G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.38G/16.1G [00:47<06:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  31%|| 1.54G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.38G/16.1G [00:48<09:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.39G/16.1G [00:48<07:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.41G/16.1G [00:48<06:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  32%|| 1.57G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.42G/16.1G [00:49<09:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.43G/16.1G [00:49<07:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.43G/16.1G [00:49<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.44G/16.1G [00:49<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.45G/16.1G [00:49<07:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.46G/16.1G [00:49<05:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.46G/16.1G [00:50<07:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.47G/16.1G [00:50<05:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.48G/16.1G [00:50<07:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.49G/16.1G [00:50<05:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  34%|| 1.66G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.50G/16.1G [00:51<07:32\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.51G/16.1G [00:51<05:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.51G/16.1G [00:51<08:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.52G/16.1G [00:51<06:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  35%|| 1.70G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.54G/16.1G [00:52<06:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  35%|| 1.71G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.55G/16.1G [00:52<06:09\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.56G/16.1G [00:53<08:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.58G/16.1G [00:53<08:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.59G/16.1G [00:53<06:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.59G/16.1G [00:53<07:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.60G/16.1G [00:54<05:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.62G/16.1G [00:54<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  36%|| 1.78G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.63G/16.1G [00:55<06:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  36%|| 1.79G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.64G/16.1G [00:55<07:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.65G/16.1G [00:55<06:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  37%|| 1.82G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.66G/16.1G [00:55<08:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.67G/16.1G [00:56<06:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  37%|| 1.84G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.68G/16.1G [00:56<06:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  38%|| 1.86G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.70G/16.1G [00:57<07:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.72G/16.1G [00:57<05:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  38%|| 1.88G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.72G/16.1G [00:57<07:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.74G/16.1G [00:58<07:32\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.75G/16.1G [00:58<05:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  39%|| 1.92G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.75G/16.1G [00:58<07:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.76G/16.1G [00:58<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  39%|| 1.94G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.78G/16.1G [00:59<07:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  40%|| 1.95G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.78G/16.1G [00:59<09:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  40%|| 1.97G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.79G/16.1G [01:00<14:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.80G/16.1G [01:00<09:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.80G/16.1G [01:00<11:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.81G/16.1G [01:00<07:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.82G/16.1G [01:01<08:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.83G/16.1G [01:01<06:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.85G/16.1G [01:02<07:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.86G/16.1G [01:02<05:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.88G/16.1G [01:02<04:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  42%|| 2.05G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.88G/16.1G [01:02<08:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.89G/16.1G [01:03<06:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  42%|| 2.08G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.90G/16.1G [01:03<07:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.91G/16.1G [01:03<06:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.91G/16.1G [01:03<07:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.92G/16.1G [01:03<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.93G/16.1G [01:04<07:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.94G/16.1G [01:04<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.95G/16.1G [01:04<07:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.96G/16.1G [01:04<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.96G/16.1G [01:05<06:54\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.97G/16.1G [01:05<05:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.98G/16.1G [01:05<06:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.99G/16.1G [01:05<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.99G/16.1G [01:05<06:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 2.00G/16.1G [01:06<05:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.01G/16.1G [01:06<06:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.02G/16.1G [01:06<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  45%|| 2.21G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.03G/16.1G [01:07<07:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.04G/16.1G [01:07<08:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.05G/16.1G [01:07<05:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.06G/16.1G [01:07<07:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.07G/16.1G [01:07<05:34\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.07G/16.1G [01:08<06:54\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.08G/16.1G [01:08<05:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  46%|| 2.27G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.10G/16.1G [01:08<05:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  47%|| 2.29G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.12G/16.1G [01:09<05:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  47%|| 2.31G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.13G/16.1G [01:09<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  47%|| 2.32G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.14G/16.1G [01:10<07:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.15G/16.1G [01:10<05:48\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  48%|| 2.35G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.15G/16.1G [01:10<07:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|| 2.16G/16.1G [01:10<05:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  48%|| 2.37G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.18G/16.1G [01:11<06:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  48%|| 2.39G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.20G/16.1G [01:11<05:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  49%|| 2.40G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.20G/16.1G [01:11<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.21G/16.1G [01:12<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.22G/16.1G [01:12<07:34\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.23G/16.1G [01:12<05:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  50%|| 2.44G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.24G/16.1G [01:12<07:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.25G/16.1G [01:13<06:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.26G/16.1G [01:13<05:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  50%|| 2.48G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.27G/16.1G [01:13<07:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.28G/16.1G [01:13<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.28G/16.1G [01:14<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.29G/16.1G [01:14<05:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  51%|| 2.51G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.31G/16.1G [01:14<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  51%|| 2.53G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  52%|| 2.54G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  52%|| 2.55G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  52%|| 2.56G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.32G/16.1G [01:15<12:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|| 2.32G/16.1G [01:15<10:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.33G/16.1G [01:16<10:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.34G/16.1G [01:16<07:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.35G/16.1G [01:16<08:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.36G/16.1G [01:16<06:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.36G/16.1G [01:16<07:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.38G/16.1G [01:17<06:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.38G/16.1G [01:17<06:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.39G/16.1G [01:17<07:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  54%|| 2.64G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.41G/16.1G [01:18<07:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  54%|| 2.66G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.42G/16.1G [01:18<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  54%|| 2.67G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|| 2.68G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|| 2.69G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|| 2.70G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|| 2.71G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.44G/16.1G [01:19<08:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|| 2.72G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.46G/16.1G [01:20<06:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  56%|| 2.74G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.47G/16.1G [01:20<05:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.48G/16.1G [01:21<07:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|| 2.49G/16.1G [01:21<05:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  56%|| 2.77G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  56%|| 2.78G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  57%|| 2.79G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.50G/16.1G [01:22<13:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.51G/16.1G [01:22<09:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.53G/16.1G [01:23<08:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.54G/16.1G [01:23<06:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.55G/16.1G [01:23<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  58%|| 2.83G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  58%|| 2.84G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.56G/16.1G [01:24<07:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.57G/16.1G [01:24<05:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  58%|| 2.86G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.59G/16.1G [01:24<05:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  59%|| 2.88G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.59G/16.1G [01:25<08:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  59%|| 2.90G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.60G/16.1G [01:25<10:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  59%|| 2.91G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.60G/16.1G [01:25<13:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  60%|| 2.93G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.60G/16.1G [01:26<17:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.61G/16.1G [01:26<10:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.62G/16.1G [01:26<10:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.63G/16.1G [01:26<06:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  60%|| 2.97G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.64G/16.1G [01:27<08:09\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|| 2.65G/16.1G [01:27<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.65G/16.1G [01:27<07:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.68G/16.1G [01:28<05:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  61%|| 3.01G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.70G/16.1G [01:28<05:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.70G/16.1G [01:28<05:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.71G/16.1G [01:28<04:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.72G/16.1G [01:29<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.73G/16.1G [01:29<04:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.73G/16.1G [01:29<05:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.75G/16.1G [01:29<05:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.76G/16.1G [01:30<04:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  63%|| 3.08G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.77G/16.1G [01:30<07:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.78G/16.1G [01:30<05:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.78G/16.1G [01:30<06:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.79G/16.1G [01:30<06:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.79G/16.1G [01:31<08:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  64%|| 3.13G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  64%|| 3.14G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.80G/16.1G [01:31<14:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|| 2.81G/16.1G [01:32<08:32\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  64%|| 3.16G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.81G/16.1G [01:32<10:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.82G/16.1G [01:32<08:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.82G/16.1G [01:32<10:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.83G/16.1G [01:33<10:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  65%|| 3.20G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.84G/16.1G [01:33<11:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  65%|| 3.22G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.86G/16.1G [01:34<06:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.86G/16.1G [01:34<07:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.87G/16.1G [01:34<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  66%|| 3.25G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.89G/16.1G [01:34<05:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  66%|| 3.27G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.90G/16.1G [01:35<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.91G/16.1G [01:35<07:01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.92G/16.1G [01:35<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.93G/16.1G [01:36<06:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.94G/16.1G [01:36<04:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.94G/16.1G [01:36<06:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.95G/16.1G [01:36<05:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.96G/16.1G [01:37<06:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|| 2.97G/16.1G [01:37<04:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  68%|| 3.35G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 2.98G/16.1G [01:37<05:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  68%|| 3.36G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.01G/16.1G [01:38<06:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.02G/16.1G [01:38<05:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  69%|| 3.39G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.02G/16.1G [01:38<06:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.04G/16.1G [01:39<06:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.05G/16.1G [01:39<05:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  69%|| 3.42G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.05G/16.1G [01:39<08:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.08G/16.1G [01:40<05:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  70%|| 3.44G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.10G/16.1G [01:40<04:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  70%|| 3.46G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.11G/16.1G [01:41<05:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  71%|| 3.47G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|| 3.13G/16.1G [01:41<05:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  71%|| 3.49G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.15G/16.1G [01:42<05:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  71%|| 3.51G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.15G/16.1G [01:42<07:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.16G/16.1G [01:42<05:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.17G/16.1G [01:43<06:54\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.18G/16.1G [01:43<05:16\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  72%|| 3.55G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.18G/16.1G [01:43<08:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.19G/16.1G [01:43<06:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  73%|| 3.57G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.21G/16.1G [01:44<06:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  73%|| 3.59G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.23G/16.1G [01:44<05:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  73%|| 3.60G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.24G/16.1G [01:45<05:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.25G/16.1G [01:45<06:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.26G/16.1G [01:45<05:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  74%|| 3.63G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.27G/16.1G [01:46<05:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  74%|| 3.65G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|| 3.29G/16.1G [01:46<05:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.30G/16.1G [01:46<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.31G/16.1G [01:46<05:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.31G/16.1G [01:47<06:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.32G/16.1G [01:47<04:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  75%|| 3.70G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.34G/16.1G [01:47<05:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  75%|| 3.71G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.35G/16.1G [01:48<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.36G/16.1G [01:48<06:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.37G/16.1G [01:48<05:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.38G/16.1G [01:49<06:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.39G/16.1G [01:49<06:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.40G/16.1G [01:49<04:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.41G/16.1G [01:49<06:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.42G/16.1G [01:50<05:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  77%|| 3.79G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.42G/16.1G [01:50<07:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.43G/16.1G [01:50<05:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  77%|| 3.81G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|| 3.45G/16.1G [01:51<06:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  78%|| 3.83G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.47G/16.1G [01:51<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  78%|| 3.84G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.47G/16.1G [01:52<07:29\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.48G/16.1G [01:52<05:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.49G/16.1G [01:52<07:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  79%|| 3.87G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.50G/16.1G [01:52<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.50G/16.1G [01:53<08:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.52G/16.1G [01:53<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.53G/16.1G [01:53<05:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  80%|| 3.92G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.54G/16.1G [01:54<06:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.55G/16.1G [01:54<05:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.55G/16.1G [01:54<06:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.56G/16.1G [01:54<05:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.57G/16.1G [01:54<06:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.58G/16.1G [01:54<04:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  81%|| 3.97G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.60G/16.1G [01:55<05:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  81%|| 3.99G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.60G/16.1G [01:55<07:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|| 3.61G/16.1G [01:56<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.62G/16.1G [01:56<07:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  82%|| 4.02G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.63G/16.1G [01:56<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.63G/16.1G [01:56<06:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.64G/16.1G [01:56<05:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  82%|| 4.05G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.66G/16.1G [01:57<05:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|| 4.07G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.68G/16.1G [01:57<05:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|| 4.08G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.69G/16.1G [01:58<05:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|| 4.10G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|| 4.11G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  84%|| 4.11G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.70G/16.1G [01:59<08:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  84%|| 4.13G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.71G/16.1G [01:59<08:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.72G/16.1G [01:59<06:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  84%|| 4.16G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.73G/16.1G [02:00<07:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.74G/16.1G [02:00<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  85%|| 4.18G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.75G/16.1G [02:01<07:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.76G/16.1G [02:01<08:16\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|| 3.77G/16.1G [02:01<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  86%|| 4.21G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.79G/16.1G [02:01<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.79G/16.1G [02:02<06:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.81G/16.1G [02:02<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.82G/16.1G [02:02<04:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.83G/16.1G [02:03<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.84G/16.1G [02:03<04:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  87%|| 4.27G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.84G/16.1G [02:03<06:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.85G/16.1G [02:03<05:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  87%|| 4.29G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.87G/16.1G [02:04<05:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  88%|| 4.31G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.88G/16.1G [02:04<04:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  88%|| 4.32G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  88%|| 4.33G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.89G/16.1G [02:05<08:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.90G/16.1G [02:05<06:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.90G/16.1G [02:05<07:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.92G/16.1G [02:05<05:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|| 3.92G/16.1G [02:06<06:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 3.94G/16.1G [02:06<06:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  89%|| 4.39G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 3.95G/16.1G [02:06<05:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 3.95G/16.1G [02:07<07:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 3.96G/16.1G [02:07<05:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 3.97G/16.1G [02:07<07:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 3.98G/16.1G [02:07<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  90%|| 4.43G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  90%|| 4.44G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  90%|| 4.45G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 3.99G/16.1G [02:08<09:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  91%|| 4.47G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.00G/16.1G [02:09<09:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.01G/16.1G [02:09<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.02G/16.1G [02:09<07:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.03G/16.1G [02:09<05:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  92%|| 4.51G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.03G/16.1G [02:10<07:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.05G/16.1G [02:10<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.05G/16.1G [02:10<06:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.06G/16.1G [02:10<05:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.07G/16.1G [02:10<06:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.08G/16.1G [02:11<04:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|| 4.08G/16.1G [02:11<06:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.10G/16.1G [02:11<06:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.11G/16.1G [02:11<04:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  93%|| 4.59G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.12G/16.1G [02:12<06:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.13G/16.1G [02:12<05:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.14G/16.1G [02:12<04:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  94%|| 4.62G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.15G/16.1G [02:13<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.16G/16.1G [02:13<04:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.16G/16.1G [02:13<06:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.17G/16.1G [02:13<04:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.18G/16.1G [02:14<06:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.19G/16.1G [02:14<04:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.20G/16.1G [02:14<06:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.21G/16.1G [02:14<04:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.21G/16.1G [02:14<05:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.23G/16.1G [02:15<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.24G/16.1G [02:15<04:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  96%|| 4.72G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  96%|| 4.72G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|| 4.25G/16.1G [02:16<06:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  96%|| 4.74G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.26G/16.1G [02:16<07:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.27G/16.1G [02:16<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  97%|| 4.76G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  97%|| 4.77G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.29G/16.1G [02:17<05:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  97%|| 4.79G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.29G/16.1G [02:17<08:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.30G/16.1G [02:17<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.31G/16.1G [02:18<07:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.32G/16.1G [02:18<05:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.32G/16.1G [02:18<06:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.33G/16.1G [02:18<04:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  98%|| 4.84G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.34G/16.1G [02:19<06:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.35G/16.1G [02:19<04:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.36G/16.1G [02:19<06:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.37G/16.1G [02:19<04:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.37G/16.1G [02:19<06:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.38G/16.1G [02:20<04:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.39G/16.1G [02:20<06:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|| 4.40G/16.1G [02:20<04:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf: 100%|| 4.92G/4.92G [02\u001b[A\u001b[A\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf: 100%|| 16.1G/16.1G [08:16<00:00\n",
      "\n",
      "Upload 2 LFS files: 100%|| 2/2 [08:17<00:00, 248.91s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "push_to_hub(target_dir, repo_id, hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_gguf='/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Base/GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p $target_dir_gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/bangla-llama/llama.cpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/anaconda3/envs/autotrain/lib/python310.zip',\n",
       " '/workspace/anaconda3/envs/autotrain/lib/python3.10',\n",
       " '/workspace/anaconda3/envs/autotrain/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages',\n",
       " '/tmp/tmpj3yslgo3',\n",
       " '/workspace/bangla-llama/llama.cpp']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'BanglaLLM/BanglaLLama-3-8b-BnWiki-Base-GGUF'\n",
    "#model.save_pretrained_gguf(target_dir_gguf, tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9dRBJZulavZ"
   },
   "source": [
    "### Instruction Finetuning\n",
    "\n",
    "We now use the [Alpaca in GPT4 Dataset](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-korean) but translated in Korean!\n",
    "\n",
    "Go to [vicgalle/alpaca-gpt4](https://huggingface.co/datasets/vicgalle/alpaca-gpt4) for the original GPT4 dataset for Alpaca or [MultilingualSIFT project](https://github.com/FreedomIntelligence/MultilingualSIFT) for other translations of the Alpaca dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196,
     "referenced_widgets": [
      "5ad923a4f66c48f293727c51202e3d8b",
      "ef246bb6829e4816a009b3f6875e20b8",
      "aadc85ded2694638a4faa0bb6dc9caa5",
      "c713a841e9cd402bb4f1acfccf9c4a7e",
      "34e31cad78ea462bb50193a9ee4c1372",
      "772ed124915740aeb2f5a614e8cfe2e1",
      "bf79d1ed09134e50b29aa3b6e28ecde5",
      "f59b229ad1ad4ab088682c83bb6c5ce4",
      "93d68696e1d345daaa4e819a47166a85",
      "a3bb01312bcd45649373c3790e327c18",
      "49666651fca242b2b777b1705dd4fa8a",
      "66744f20d34f4cd5b75570d269261b94",
      "03137bba9d7d4ef3ae7e073064b5438d",
      "cbece197ea7f41f9855fa94b6e049254",
      "cc5764233d5d4af79c1221d626b21322",
      "add58b43a7cd4047adb11778c26812f7",
      "b12e798c92f848bc9e1d430d504ee3c7",
      "ebb68cccf04641ed8996e9c9b62c67be",
      "7626531fcae84ddbb8f482b8ac2f7439",
      "119f3c027e3f4beda1c026fd3ebb3f7c",
      "97f0c9d5be3245fb8b1320cc17818ae2",
      "be8fed98ffb3473ba6667d2c17b3929b",
      "c3e586fbe5d34ecdbcc05a2ecbc17fc0",
      "ba1fd50e24904c6988967eab2ab64225",
      "f611865e89774fe4ba6dc047f97eaa61",
      "5fd4be6bc35d459eb335da817e346724",
      "e91b73923c044243b5ed8efcfa252f2f",
      "ee3c23c50c5b435bbada1bcec39c23b4",
      "6d83eb47f7b34c7c930e63649da55097",
      "7621ead96516491db1ebb4fba0055702",
      "790cc80cfe574ee58da93f8e54c8517f",
      "c0e9aae192fa4c62af7379d157c0893f",
      "5e0e10a542654766bf78a9e90c1daa8a"
     ]
    },
    "executionInfo": {
     "elapsed": 4839,
     "status": "ok",
     "timestamp": 1717663490088,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "1oNjUwxOyG8C",
    "outputId": "800603ed-2857-4687-a072-7c5589b5b5e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|| 124/124 [00:00<00:00, 470kB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|| 51.6M/51.6M [00:01<00:00, 32.6MB/s]\n",
      "Generating train split: 100%|| 49969/49969 [00:00<00:00, 59775.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#alpaca_dataset_korean = load_dataset(\"FreedomIntelligence/alpaca-gpt4-korean\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd8fea0b485431bb3137f3ad32904a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b640c033ffa404daa88d5fef368af0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/158M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb6aeb8c5ba4de79c62115fa34a6c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/144M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768c6ad7e7da443d8be58bb0ccb66d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/172026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "alpaca_dataset = load_dataset(\"BanglaLLM/bangla-alpaca-orca\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text', 'system_prompt'],\n",
       "    num_rows: 172026\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmFG41nFytgi"
   },
   "source": [
    "We print 1 example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1717663490089,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "QvTfIKaUQxQ5",
    "outputId": "83e1b097-e215-4b2e-c6bc-9b7f639914ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ,        ,                     ### Instruction:      ### Input:  ### Response:           ,         ,             ,                      ,              \n",
      "\n",
      "                     ,         ,              \n",
      "\n",
      "    ,        -,             \n",
      "\n",
      ",                \n"
     ]
    }
   ],
   "source": [
    "print(alpaca_dataset[111]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO8UY34ql2vJ"
   },
   "source": [
    "We again use https://translate.google.com/ to translate the Alpaca format into Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69,
     "referenced_widgets": [
      "a6babbf2f467476f8a650a0b3cf4b225",
      "4d1479b7392549beb8459113a42e2610",
      "a1b291badc6041beae070c2e262ae005",
      "a424608a868c46b2be65767c093c3dd1",
      "4ad139d347eb449ab7b4bd6df4f64516",
      "bdc71b983e5f48b1800ebdc457f22f3e",
      "3bc92134440649aba5cf0ff5f4393c55",
      "e6ae977baad248078458445fecc03963",
      "55b47d1ed657480d9a860b46eaa1d678",
      "8b93d48735a347c1bc7a5b72692387e7",
      "960c9eba4fec494989f66a6c0dd5fce5"
     ]
    },
    "executionInfo": {
     "elapsed": 1949,
     "status": "ok",
     "timestamp": 1717663492023,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "B2tv8AEhPTu6",
    "outputId": "acd18935-ee12-4fbd-fc55-b813dc389d13"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# becomes:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m alpaca_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m    ,        ,                     \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m### Instruction: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m### Input:\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m EOS_TOKEN \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39meos_token \u001b[38;5;66;03m# Must add EOS_TOKEN\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformatting_prompts_func\u001b[39m(conversations):\n\u001b[1;32m     17\u001b[0m     texts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Wikipedia provides a title and an article text.\n",
    "# Use https://translate.google.com!\n",
    "alpaca_prompt = \"\"\"    ,        ,                     \n",
    "### Instruction: {}\n",
    "\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "# becomes:\n",
    "alpaca_prompt = \"\"\"    ,        ,                     \n",
    "### Instruction: {}\n",
    "\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(convo[0][\"value\"], convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "#alpaca_dataset = alpaca_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpaca_dataset = alpaca_dataset.train_test_split(train_size = 0.01)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text', 'system_prompt'],\n",
       "    num_rows: 172026\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxWWh8xsl9XT"
   },
   "source": [
    "We again employ `UnslothTrainer` and do instruction finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "f09ab65302ea49f38ad65c7285439b66",
      "89727d06874849f5b681f3705bfe0978",
      "3b939980516140c588220612194de161",
      "fed11544e08c4fb3bf9fe247f7c37cd1",
      "1531e137a0b84e3db00599d6311fbc04",
      "aa120e115285432da310d295df2bb739",
      "7c319ad4809c47189345d0d158dcd922",
      "9918c010fe1f418c831d5fffd0a94180",
      "90655a74b97944f58cad529fa8adf1fa",
      "e57c379628ce49da904b447a74b0e1b1",
      "d6217028b7744abdb85c305237cb52a0"
     ]
    },
    "executionInfo": {
     "elapsed": 115741,
     "status": "ok",
     "timestamp": 1717663607754,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "1Zul21NSRRLP",
    "outputId": "d3bab1d0-6399-4c14-9e26-b2d3ea1d27d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[codecarbon INFO @ 18:07:37] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:07:37] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:07:37] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 18:07:37] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:07:37] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:07:39] CPU Model on constant consumption mode: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 18:07:39] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:07:39]   Platform system: Linux-5.4.0-169-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 18:07:39]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 18:07:39]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 18:07:39]   Available RAM : 1007.784 GB\n",
      "[codecarbon INFO @ 18:07:39]   CPU count: 128\n",
      "[codecarbon INFO @ 18:07:39]   CPU model: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 18:07:39]   GPU count: 1\n",
      "[codecarbon INFO @ 18:07:39]   GPU model: 1 x NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = alpaca_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        # Use num_train_epochs and warmup_ratio for longer runs!\n",
    "        #max_steps = 120,\n",
    "        warmup_steps = 10,\n",
    "        warmup_ratio = 0.05,\n",
    "        num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3624846,
     "status": "ok",
     "timestamp": 1717667232586,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "DIO7c1FoRe-X",
    "outputId": "68e3537d-b30c-4f35-af67-c73bd589a37e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 172,026 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 128 | Total steps = 1,344\n",
      " \"-____-\"     Number of trainable parameters = 1,386,217,472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n",
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:08:11] Energy consumed for RAM : 0.001635 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:11] Energy consumed for all GPUs : 0.001025 kWh. Total GPU Power : 236.9192245871228 W\n",
      "[codecarbon INFO @ 18:08:11] Energy consumed for all CPUs : 0.000487 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:11] 0.003148 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:26] Energy consumed for RAM : 0.003211 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:26] Energy consumed for all GPUs : 0.002222 kWh. Total GPU Power : 286.9437672889438 W\n",
      "[codecarbon INFO @ 18:08:26] Energy consumed for all CPUs : 0.000957 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:26] 0.006391 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:41] Energy consumed for RAM : 0.004784 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:41] Energy consumed for all GPUs : 0.003419 kWh. Total GPU Power : 287.4171218110107 W\n",
      "[codecarbon INFO @ 18:08:41] Energy consumed for all CPUs : 0.001425 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:41] 0.009628 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:56] Energy consumed for RAM : 0.006357 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:56] Energy consumed for all GPUs : 0.004618 kWh. Total GPU Power : 287.9541976131156 W\n",
      "[codecarbon INFO @ 18:08:56] Energy consumed for all CPUs : 0.001894 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:56] 0.012869 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:11] Energy consumed for RAM : 0.007931 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:11] Energy consumed for all GPUs : 0.005827 kWh. Total GPU Power : 290.31942944392415 W\n",
      "[codecarbon INFO @ 18:09:11] Energy consumed for all CPUs : 0.002363 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:11] 0.016121 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='1344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/1344 02:25 < 27:06:44, 0.01 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.606300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:09:26] Energy consumed for RAM : 0.009505 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:26] Energy consumed for all GPUs : 0.006993 kWh. Total GPU Power : 279.876770470884 W\n",
      "[codecarbon INFO @ 18:09:26] Energy consumed for all CPUs : 0.002831 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:26] 0.019329 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:42] Energy consumed for RAM : 0.011183 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:42] Energy consumed for all GPUs : 0.008284 kWh. Total GPU Power : 290.7094274699353 W\n",
      "[codecarbon INFO @ 18:09:42] Energy consumed for all CPUs : 0.003331 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:42] 0.022799 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:57] Energy consumed for RAM : 0.012757 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:57] Energy consumed for all GPUs : 0.009497 kWh. Total GPU Power : 291.1135479397693 W\n",
      "[codecarbon INFO @ 18:09:57] Energy consumed for all CPUs : 0.003800 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:57] 0.026054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:12] Energy consumed for RAM : 0.014331 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:12] Energy consumed for all GPUs : 0.010686 kWh. Total GPU Power : 285.60071279964137 W\n",
      "[codecarbon INFO @ 18:10:12] Energy consumed for all CPUs : 0.004269 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:12] 0.029286 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:27] Energy consumed for RAM : 0.015922 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:27] Energy consumed for all GPUs : 0.011910 kWh. Total GPU Power : 290.57925000400616 W\n",
      "[codecarbon INFO @ 18:10:27] Energy consumed for all CPUs : 0.004743 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:27] 0.032576 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:42] Energy consumed for RAM : 0.017495 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:42] Energy consumed for all GPUs : 0.013114 kWh. Total GPU Power : 289.0136369761482 W\n",
      "[codecarbon INFO @ 18:10:42] Energy consumed for all CPUs : 0.005212 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:42] 0.035820 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:57] Energy consumed for RAM : 0.019069 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:57] Energy consumed for all GPUs : 0.014335 kWh. Total GPU Power : 293.2924827348806 W\n",
      "[codecarbon INFO @ 18:10:57] Energy consumed for all CPUs : 0.005680 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:57] 0.039084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:13] Energy consumed for RAM : 0.020734 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:13] Energy consumed for all GPUs : 0.015614 kWh. Total GPU Power : 290.08496905337216 W\n",
      "[codecarbon INFO @ 18:11:13] Energy consumed for all CPUs : 0.006176 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:13] 0.042525 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:28] Energy consumed for RAM : 0.022308 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:28] Energy consumed for all GPUs : 0.016829 kWh. Total GPU Power : 291.682251376309 W\n",
      "[codecarbon INFO @ 18:11:28] Energy consumed for all CPUs : 0.006645 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:28] 0.045782 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:43] Energy consumed for RAM : 0.023880 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:43] Energy consumed for all GPUs : 0.018023 kWh. Total GPU Power : 286.96374393688865 W\n",
      "[codecarbon INFO @ 18:11:43] Energy consumed for all CPUs : 0.007114 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:43] 0.049018 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for RAM : 0.025543 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for all GPUs : 0.019302 kWh. Total GPU Power : 290.56786476108346 W\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for all CPUs : 0.007609 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:59] 0.052455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for RAM : 0.027117 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for all GPUs : 0.020517 kWh. Total GPU Power : 291.6241488975196 W\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for all CPUs : 0.008078 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:12:14] 0.055711 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1717667232587,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "198797f5-4842-4a8d-e15c-3a60643816c0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_gpu_memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Show final memory and time stats\u001b[39;00m\n\u001b[1;32m      2\u001b[0m used_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m used_memory_for_lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory \u001b[38;5;241m-\u001b[39m \u001b[43mstart_gpu_memory\u001b[49m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m used_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory         \u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m lora_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory_for_lora\u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_gpu_memory' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=1344, training_loss=0.3289879886433482, metrics={'train_runtime': 98065.4069, 'train_samples_per_second': 1.754, 'train_steps_per_second': 0.014, 'total_flos': 1.7581221468435382e+19, 'train_loss': 0.3289879886433482, 'epoch': 1.0})\n"
     ]
    }
   ],
   "source": [
    "print(trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done finetuning\n"
     ]
    }
   ],
   "source": [
    "print(\"Done finetuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n",
    "Remember to use https://translate.google.com/!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export HF_HOME=/workspace/.cache/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/.cache/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834994c4de95497aaf2022e4eca710f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea025efb0854eb9a15bfb83691da8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4df056df8034a5a8f107b3c0d123e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c495fb8645f24f97a664465550cadbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00007.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49248d3708284f349bc3b88beda92286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27623d53b37b410ba3c28d46cfb6c3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda77fcf49c34bd79b6a101e1377ddae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c38e642a0ac4bf79c1a5dd0f5d20097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080cbe74acbf4487942b80af88b8b8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b310701c38f24c9da78fa3457f50eab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00007.safetensors:   0%|          | 0.00/2.57G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6022ba64bf41a19adcb4614f393b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ff6592998e4694acead151d81cd420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12457a03ec6b446ab52b5bd895ff7424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7447b4f039be44b3b3ba37da575d26a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00341891ddad41c388165482b244bb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "#max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name = \"BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    #max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unsloth: Please import Unsloth before bitsandbytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m \u001b[38;5;66;03m# Choose any! We auto support RoPE Scaling internally!\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m MODULES_TO_CHECK:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Please import Unsloth before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Unsloth: Please import Unsloth before bitsandbytes."
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 7/7 [02:05<00:00, 17.89s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "#model_name2 = \"BanglaLLM/bangla-llama-7b-instruct-v0.1\"\n",
    "model_name2 = \"BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct\"\n",
    "model2, tokenizer2 = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name2, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 3/3 [00:06<00:00,  2.16s/it]\n",
      "Some weights of the model checkpoint at BanglaLLM/bangla-llama-13b-instruct-v0.1 were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.base_layer.weight', 'model.layers.0.mlp.down_proj.lora_A.default.weight', 'model.layers.0.mlp.down_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight', 'model.layers.0.mlp.up_proj.base_layer.weight', 'model.layers.0.mlp.up_proj.lora_A.default.weight', 'model.layers.0.mlp.up_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.mlp.down_proj.base_layer.weight', 'model.layers.1.mlp.down_proj.lora_A.default.weight', 'model.layers.1.mlp.down_proj.lora_B.default.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight', 'model.layers.1.mlp.gate_proj.lora_A.default.weight', 'model.layers.1.mlp.gate_proj.lora_B.default.weight', 'model.layers.1.mlp.up_proj.base_layer.weight', 'model.layers.1.mlp.up_proj.lora_A.default.weight', 'model.layers.1.mlp.up_proj.lora_B.default.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.mlp.down_proj.base_layer.weight', 'model.layers.10.mlp.down_proj.lora_A.default.weight', 'model.layers.10.mlp.down_proj.lora_B.default.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight', 'model.layers.10.mlp.gate_proj.lora_A.default.weight', 'model.layers.10.mlp.gate_proj.lora_B.default.weight', 'model.layers.10.mlp.up_proj.base_layer.weight', 'model.layers.10.mlp.up_proj.lora_A.default.weight', 'model.layers.10.mlp.up_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.mlp.down_proj.base_layer.weight', 'model.layers.11.mlp.down_proj.lora_A.default.weight', 'model.layers.11.mlp.down_proj.lora_B.default.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight', 'model.layers.11.mlp.gate_proj.lora_A.default.weight', 'model.layers.11.mlp.gate_proj.lora_B.default.weight', 'model.layers.11.mlp.up_proj.base_layer.weight', 'model.layers.11.mlp.up_proj.lora_A.default.weight', 'model.layers.11.mlp.up_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.mlp.down_proj.base_layer.weight', 'model.layers.12.mlp.down_proj.lora_A.default.weight', 'model.layers.12.mlp.down_proj.lora_B.default.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight', 'model.layers.12.mlp.gate_proj.lora_A.default.weight', 'model.layers.12.mlp.gate_proj.lora_B.default.weight', 'model.layers.12.mlp.up_proj.base_layer.weight', 'model.layers.12.mlp.up_proj.lora_A.default.weight', 'model.layers.12.mlp.up_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.mlp.down_proj.base_layer.weight', 'model.layers.13.mlp.down_proj.lora_A.default.weight', 'model.layers.13.mlp.down_proj.lora_B.default.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight', 'model.layers.13.mlp.gate_proj.lora_A.default.weight', 'model.layers.13.mlp.gate_proj.lora_B.default.weight', 'model.layers.13.mlp.up_proj.base_layer.weight', 'model.layers.13.mlp.up_proj.lora_A.default.weight', 'model.layers.13.mlp.up_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.mlp.down_proj.base_layer.weight', 'model.layers.14.mlp.down_proj.lora_A.default.weight', 'model.layers.14.mlp.down_proj.lora_B.default.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight', 'model.layers.14.mlp.gate_proj.lora_A.default.weight', 'model.layers.14.mlp.gate_proj.lora_B.default.weight', 'model.layers.14.mlp.up_proj.base_layer.weight', 'model.layers.14.mlp.up_proj.lora_A.default.weight', 'model.layers.14.mlp.up_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.mlp.down_proj.base_layer.weight', 'model.layers.15.mlp.down_proj.lora_A.default.weight', 'model.layers.15.mlp.down_proj.lora_B.default.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight', 'model.layers.15.mlp.gate_proj.lora_A.default.weight', 'model.layers.15.mlp.gate_proj.lora_B.default.weight', 'model.layers.15.mlp.up_proj.base_layer.weight', 'model.layers.15.mlp.up_proj.lora_A.default.weight', 'model.layers.15.mlp.up_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.mlp.down_proj.base_layer.weight', 'model.layers.16.mlp.down_proj.lora_A.default.weight', 'model.layers.16.mlp.down_proj.lora_B.default.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight', 'model.layers.16.mlp.gate_proj.lora_A.default.weight', 'model.layers.16.mlp.gate_proj.lora_B.default.weight', 'model.layers.16.mlp.up_proj.base_layer.weight', 'model.layers.16.mlp.up_proj.lora_A.default.weight', 'model.layers.16.mlp.up_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.mlp.down_proj.base_layer.weight', 'model.layers.17.mlp.down_proj.lora_A.default.weight', 'model.layers.17.mlp.down_proj.lora_B.default.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight', 'model.layers.17.mlp.gate_proj.lora_A.default.weight', 'model.layers.17.mlp.gate_proj.lora_B.default.weight', 'model.layers.17.mlp.up_proj.base_layer.weight', 'model.layers.17.mlp.up_proj.lora_A.default.weight', 'model.layers.17.mlp.up_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.mlp.down_proj.base_layer.weight', 'model.layers.18.mlp.down_proj.lora_A.default.weight', 'model.layers.18.mlp.down_proj.lora_B.default.weight', 'model.layers.18.mlp.gate_proj.base_layer.weight', 'model.layers.18.mlp.gate_proj.lora_A.default.weight', 'model.layers.18.mlp.gate_proj.lora_B.default.weight', 'model.layers.18.mlp.up_proj.base_layer.weight', 'model.layers.18.mlp.up_proj.lora_A.default.weight', 'model.layers.18.mlp.up_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.base_layer.weight', 'model.layers.18.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight', 'model.layers.18.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.o_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.mlp.down_proj.base_layer.weight', 'model.layers.19.mlp.down_proj.lora_A.default.weight', 'model.layers.19.mlp.down_proj.lora_B.default.weight', 'model.layers.19.mlp.gate_proj.base_layer.weight', 'model.layers.19.mlp.gate_proj.lora_A.default.weight', 'model.layers.19.mlp.gate_proj.lora_B.default.weight', 'model.layers.19.mlp.up_proj.base_layer.weight', 'model.layers.19.mlp.up_proj.lora_A.default.weight', 'model.layers.19.mlp.up_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.base_layer.weight', 'model.layers.19.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight', 'model.layers.19.self_attn.o_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.mlp.down_proj.base_layer.weight', 'model.layers.2.mlp.down_proj.lora_A.default.weight', 'model.layers.2.mlp.down_proj.lora_B.default.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight', 'model.layers.2.mlp.gate_proj.lora_A.default.weight', 'model.layers.2.mlp.gate_proj.lora_B.default.weight', 'model.layers.2.mlp.up_proj.base_layer.weight', 'model.layers.2.mlp.up_proj.lora_A.default.weight', 'model.layers.2.mlp.up_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.mlp.down_proj.base_layer.weight', 'model.layers.20.mlp.down_proj.lora_A.default.weight', 'model.layers.20.mlp.down_proj.lora_B.default.weight', 'model.layers.20.mlp.gate_proj.base_layer.weight', 'model.layers.20.mlp.gate_proj.lora_A.default.weight', 'model.layers.20.mlp.gate_proj.lora_B.default.weight', 'model.layers.20.mlp.up_proj.base_layer.weight', 'model.layers.20.mlp.up_proj.lora_A.default.weight', 'model.layers.20.mlp.up_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.k_proj.lora_A.default.weight', 'model.layers.20.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight', 'model.layers.20.self_attn.o_proj.lora_A.default.weight', 'model.layers.20.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.mlp.down_proj.base_layer.weight', 'model.layers.21.mlp.down_proj.lora_A.default.weight', 'model.layers.21.mlp.down_proj.lora_B.default.weight', 'model.layers.21.mlp.gate_proj.base_layer.weight', 'model.layers.21.mlp.gate_proj.lora_A.default.weight', 'model.layers.21.mlp.gate_proj.lora_B.default.weight', 'model.layers.21.mlp.up_proj.base_layer.weight', 'model.layers.21.mlp.up_proj.lora_A.default.weight', 'model.layers.21.mlp.up_proj.lora_B.default.weight', 'model.layers.21.self_attn.k_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.lora_A.default.weight', 'model.layers.21.self_attn.k_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.o_proj.lora_A.default.weight', 'model.layers.21.self_attn.o_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.mlp.down_proj.base_layer.weight', 'model.layers.22.mlp.down_proj.lora_A.default.weight', 'model.layers.22.mlp.down_proj.lora_B.default.weight', 'model.layers.22.mlp.gate_proj.base_layer.weight', 'model.layers.22.mlp.gate_proj.lora_A.default.weight', 'model.layers.22.mlp.gate_proj.lora_B.default.weight', 'model.layers.22.mlp.up_proj.base_layer.weight', 'model.layers.22.mlp.up_proj.lora_A.default.weight', 'model.layers.22.mlp.up_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.base_layer.weight', 'model.layers.22.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.k_proj.lora_B.default.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.lora_A.default.weight', 'model.layers.22.self_attn.o_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.mlp.down_proj.base_layer.weight', 'model.layers.23.mlp.down_proj.lora_A.default.weight', 'model.layers.23.mlp.down_proj.lora_B.default.weight', 'model.layers.23.mlp.gate_proj.base_layer.weight', 'model.layers.23.mlp.gate_proj.lora_A.default.weight', 'model.layers.23.mlp.gate_proj.lora_B.default.weight', 'model.layers.23.mlp.up_proj.base_layer.weight', 'model.layers.23.mlp.up_proj.lora_A.default.weight', 'model.layers.23.mlp.up_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.base_layer.weight', 'model.layers.23.self_attn.k_proj.lora_A.default.weight', 'model.layers.23.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight', 'model.layers.23.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.o_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.mlp.down_proj.base_layer.weight', 'model.layers.24.mlp.down_proj.lora_A.default.weight', 'model.layers.24.mlp.down_proj.lora_B.default.weight', 'model.layers.24.mlp.gate_proj.base_layer.weight', 'model.layers.24.mlp.gate_proj.lora_A.default.weight', 'model.layers.24.mlp.gate_proj.lora_B.default.weight', 'model.layers.24.mlp.up_proj.base_layer.weight', 'model.layers.24.mlp.up_proj.lora_A.default.weight', 'model.layers.24.mlp.up_proj.lora_B.default.weight', 'model.layers.24.self_attn.k_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.lora_A.default.weight', 'model.layers.24.self_attn.k_proj.lora_B.default.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.o_proj.lora_A.default.weight', 'model.layers.24.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.mlp.down_proj.base_layer.weight', 'model.layers.25.mlp.down_proj.lora_A.default.weight', 'model.layers.25.mlp.down_proj.lora_B.default.weight', 'model.layers.25.mlp.gate_proj.base_layer.weight', 'model.layers.25.mlp.gate_proj.lora_A.default.weight', 'model.layers.25.mlp.gate_proj.lora_B.default.weight', 'model.layers.25.mlp.up_proj.base_layer.weight', 'model.layers.25.mlp.up_proj.lora_A.default.weight', 'model.layers.25.mlp.up_proj.lora_B.default.weight', 'model.layers.25.self_attn.k_proj.base_layer.weight', 'model.layers.25.self_attn.k_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_B.default.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.lora_A.default.weight', 'model.layers.25.self_attn.o_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.mlp.down_proj.base_layer.weight', 'model.layers.26.mlp.down_proj.lora_A.default.weight', 'model.layers.26.mlp.down_proj.lora_B.default.weight', 'model.layers.26.mlp.gate_proj.base_layer.weight', 'model.layers.26.mlp.gate_proj.lora_A.default.weight', 'model.layers.26.mlp.gate_proj.lora_B.default.weight', 'model.layers.26.mlp.up_proj.base_layer.weight', 'model.layers.26.mlp.up_proj.lora_A.default.weight', 'model.layers.26.mlp.up_proj.lora_B.default.weight', 'model.layers.26.self_attn.k_proj.base_layer.weight', 'model.layers.26.self_attn.k_proj.lora_A.default.weight', 'model.layers.26.self_attn.k_proj.lora_B.default.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.lora_A.default.weight', 'model.layers.26.self_attn.o_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.mlp.down_proj.base_layer.weight', 'model.layers.27.mlp.down_proj.lora_A.default.weight', 'model.layers.27.mlp.down_proj.lora_B.default.weight', 'model.layers.27.mlp.gate_proj.base_layer.weight', 'model.layers.27.mlp.gate_proj.lora_A.default.weight', 'model.layers.27.mlp.gate_proj.lora_B.default.weight', 'model.layers.27.mlp.up_proj.base_layer.weight', 'model.layers.27.mlp.up_proj.lora_A.default.weight', 'model.layers.27.mlp.up_proj.lora_B.default.weight', 'model.layers.27.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.lora_A.default.weight', 'model.layers.27.self_attn.k_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight', 'model.layers.27.self_attn.o_proj.lora_A.default.weight', 'model.layers.27.self_attn.o_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.mlp.down_proj.base_layer.weight', 'model.layers.28.mlp.down_proj.lora_A.default.weight', 'model.layers.28.mlp.down_proj.lora_B.default.weight', 'model.layers.28.mlp.gate_proj.base_layer.weight', 'model.layers.28.mlp.gate_proj.lora_A.default.weight', 'model.layers.28.mlp.gate_proj.lora_B.default.weight', 'model.layers.28.mlp.up_proj.base_layer.weight', 'model.layers.28.mlp.up_proj.lora_A.default.weight', 'model.layers.28.mlp.up_proj.lora_B.default.weight', 'model.layers.28.self_attn.k_proj.base_layer.weight', 'model.layers.28.self_attn.k_proj.lora_A.default.weight', 'model.layers.28.self_attn.k_proj.lora_B.default.weight', 'model.layers.28.self_attn.o_proj.base_layer.weight', 'model.layers.28.self_attn.o_proj.lora_A.default.weight', 'model.layers.28.self_attn.o_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.mlp.down_proj.base_layer.weight', 'model.layers.29.mlp.down_proj.lora_A.default.weight', 'model.layers.29.mlp.down_proj.lora_B.default.weight', 'model.layers.29.mlp.gate_proj.base_layer.weight', 'model.layers.29.mlp.gate_proj.lora_A.default.weight', 'model.layers.29.mlp.gate_proj.lora_B.default.weight', 'model.layers.29.mlp.up_proj.base_layer.weight', 'model.layers.29.mlp.up_proj.lora_A.default.weight', 'model.layers.29.mlp.up_proj.lora_B.default.weight', 'model.layers.29.self_attn.k_proj.base_layer.weight', 'model.layers.29.self_attn.k_proj.lora_A.default.weight', 'model.layers.29.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.o_proj.base_layer.weight', 'model.layers.29.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.o_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.mlp.down_proj.base_layer.weight', 'model.layers.3.mlp.down_proj.lora_A.default.weight', 'model.layers.3.mlp.down_proj.lora_B.default.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight', 'model.layers.3.mlp.gate_proj.lora_A.default.weight', 'model.layers.3.mlp.gate_proj.lora_B.default.weight', 'model.layers.3.mlp.up_proj.base_layer.weight', 'model.layers.3.mlp.up_proj.lora_A.default.weight', 'model.layers.3.mlp.up_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.mlp.down_proj.base_layer.weight', 'model.layers.30.mlp.down_proj.lora_A.default.weight', 'model.layers.30.mlp.down_proj.lora_B.default.weight', 'model.layers.30.mlp.gate_proj.base_layer.weight', 'model.layers.30.mlp.gate_proj.lora_A.default.weight', 'model.layers.30.mlp.gate_proj.lora_B.default.weight', 'model.layers.30.mlp.up_proj.base_layer.weight', 'model.layers.30.mlp.up_proj.lora_A.default.weight', 'model.layers.30.mlp.up_proj.lora_B.default.weight', 'model.layers.30.self_attn.k_proj.base_layer.weight', 'model.layers.30.self_attn.k_proj.lora_A.default.weight', 'model.layers.30.self_attn.k_proj.lora_B.default.weight', 'model.layers.30.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.o_proj.lora_A.default.weight', 'model.layers.30.self_attn.o_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.mlp.down_proj.base_layer.weight', 'model.layers.31.mlp.down_proj.lora_A.default.weight', 'model.layers.31.mlp.down_proj.lora_B.default.weight', 'model.layers.31.mlp.gate_proj.base_layer.weight', 'model.layers.31.mlp.gate_proj.lora_A.default.weight', 'model.layers.31.mlp.gate_proj.lora_B.default.weight', 'model.layers.31.mlp.up_proj.base_layer.weight', 'model.layers.31.mlp.up_proj.lora_A.default.weight', 'model.layers.31.mlp.up_proj.lora_B.default.weight', 'model.layers.31.self_attn.k_proj.base_layer.weight', 'model.layers.31.self_attn.k_proj.lora_A.default.weight', 'model.layers.31.self_attn.k_proj.lora_B.default.weight', 'model.layers.31.self_attn.o_proj.base_layer.weight', 'model.layers.31.self_attn.o_proj.lora_A.default.weight', 'model.layers.31.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.32.mlp.down_proj.base_layer.weight', 'model.layers.32.mlp.down_proj.lora_A.default.weight', 'model.layers.32.mlp.down_proj.lora_B.default.weight', 'model.layers.32.mlp.gate_proj.base_layer.weight', 'model.layers.32.mlp.gate_proj.lora_A.default.weight', 'model.layers.32.mlp.gate_proj.lora_B.default.weight', 'model.layers.32.mlp.up_proj.base_layer.weight', 'model.layers.32.mlp.up_proj.lora_A.default.weight', 'model.layers.32.mlp.up_proj.lora_B.default.weight', 'model.layers.32.self_attn.k_proj.base_layer.weight', 'model.layers.32.self_attn.k_proj.lora_A.default.weight', 'model.layers.32.self_attn.k_proj.lora_B.default.weight', 'model.layers.32.self_attn.o_proj.base_layer.weight', 'model.layers.32.self_attn.o_proj.lora_A.default.weight', 'model.layers.32.self_attn.o_proj.lora_B.default.weight', 'model.layers.32.self_attn.q_proj.base_layer.weight', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.v_proj.base_layer.weight', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.mlp.down_proj.base_layer.weight', 'model.layers.33.mlp.down_proj.lora_A.default.weight', 'model.layers.33.mlp.down_proj.lora_B.default.weight', 'model.layers.33.mlp.gate_proj.base_layer.weight', 'model.layers.33.mlp.gate_proj.lora_A.default.weight', 'model.layers.33.mlp.gate_proj.lora_B.default.weight', 'model.layers.33.mlp.up_proj.base_layer.weight', 'model.layers.33.mlp.up_proj.lora_A.default.weight', 'model.layers.33.mlp.up_proj.lora_B.default.weight', 'model.layers.33.self_attn.k_proj.base_layer.weight', 'model.layers.33.self_attn.k_proj.lora_A.default.weight', 'model.layers.33.self_attn.k_proj.lora_B.default.weight', 'model.layers.33.self_attn.o_proj.base_layer.weight', 'model.layers.33.self_attn.o_proj.lora_A.default.weight', 'model.layers.33.self_attn.o_proj.lora_B.default.weight', 'model.layers.33.self_attn.q_proj.base_layer.weight', 'model.layers.33.self_attn.q_proj.lora_A.default.weight', 'model.layers.33.self_attn.q_proj.lora_B.default.weight', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.v_proj.base_layer.weight', 'model.layers.33.self_attn.v_proj.lora_A.default.weight', 'model.layers.33.self_attn.v_proj.lora_B.default.weight', 'model.layers.34.mlp.down_proj.base_layer.weight', 'model.layers.34.mlp.down_proj.lora_A.default.weight', 'model.layers.34.mlp.down_proj.lora_B.default.weight', 'model.layers.34.mlp.gate_proj.base_layer.weight', 'model.layers.34.mlp.gate_proj.lora_A.default.weight', 'model.layers.34.mlp.gate_proj.lora_B.default.weight', 'model.layers.34.mlp.up_proj.base_layer.weight', 'model.layers.34.mlp.up_proj.lora_A.default.weight', 'model.layers.34.mlp.up_proj.lora_B.default.weight', 'model.layers.34.self_attn.k_proj.base_layer.weight', 'model.layers.34.self_attn.k_proj.lora_A.default.weight', 'model.layers.34.self_attn.k_proj.lora_B.default.weight', 'model.layers.34.self_attn.o_proj.base_layer.weight', 'model.layers.34.self_attn.o_proj.lora_A.default.weight', 'model.layers.34.self_attn.o_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.base_layer.weight', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.v_proj.base_layer.weight', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.mlp.down_proj.base_layer.weight', 'model.layers.35.mlp.down_proj.lora_A.default.weight', 'model.layers.35.mlp.down_proj.lora_B.default.weight', 'model.layers.35.mlp.gate_proj.base_layer.weight', 'model.layers.35.mlp.gate_proj.lora_A.default.weight', 'model.layers.35.mlp.gate_proj.lora_B.default.weight', 'model.layers.35.mlp.up_proj.base_layer.weight', 'model.layers.35.mlp.up_proj.lora_A.default.weight', 'model.layers.35.mlp.up_proj.lora_B.default.weight', 'model.layers.35.self_attn.k_proj.base_layer.weight', 'model.layers.35.self_attn.k_proj.lora_A.default.weight', 'model.layers.35.self_attn.k_proj.lora_B.default.weight', 'model.layers.35.self_attn.o_proj.base_layer.weight', 'model.layers.35.self_attn.o_proj.lora_A.default.weight', 'model.layers.35.self_attn.o_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.base_layer.weight', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.v_proj.base_layer.weight', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.36.mlp.down_proj.base_layer.weight', 'model.layers.36.mlp.down_proj.lora_A.default.weight', 'model.layers.36.mlp.down_proj.lora_B.default.weight', 'model.layers.36.mlp.gate_proj.base_layer.weight', 'model.layers.36.mlp.gate_proj.lora_A.default.weight', 'model.layers.36.mlp.gate_proj.lora_B.default.weight', 'model.layers.36.mlp.up_proj.base_layer.weight', 'model.layers.36.mlp.up_proj.lora_A.default.weight', 'model.layers.36.mlp.up_proj.lora_B.default.weight', 'model.layers.36.self_attn.k_proj.base_layer.weight', 'model.layers.36.self_attn.k_proj.lora_A.default.weight', 'model.layers.36.self_attn.k_proj.lora_B.default.weight', 'model.layers.36.self_attn.o_proj.base_layer.weight', 'model.layers.36.self_attn.o_proj.lora_A.default.weight', 'model.layers.36.self_attn.o_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.base_layer.weight', 'model.layers.36.self_attn.q_proj.lora_A.default.weight', 'model.layers.36.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.v_proj.base_layer.weight', 'model.layers.36.self_attn.v_proj.lora_A.default.weight', 'model.layers.36.self_attn.v_proj.lora_B.default.weight', 'model.layers.37.mlp.down_proj.base_layer.weight', 'model.layers.37.mlp.down_proj.lora_A.default.weight', 'model.layers.37.mlp.down_proj.lora_B.default.weight', 'model.layers.37.mlp.gate_proj.base_layer.weight', 'model.layers.37.mlp.gate_proj.lora_A.default.weight', 'model.layers.37.mlp.gate_proj.lora_B.default.weight', 'model.layers.37.mlp.up_proj.base_layer.weight', 'model.layers.37.mlp.up_proj.lora_A.default.weight', 'model.layers.37.mlp.up_proj.lora_B.default.weight', 'model.layers.37.self_attn.k_proj.base_layer.weight', 'model.layers.37.self_attn.k_proj.lora_A.default.weight', 'model.layers.37.self_attn.k_proj.lora_B.default.weight', 'model.layers.37.self_attn.o_proj.base_layer.weight', 'model.layers.37.self_attn.o_proj.lora_A.default.weight', 'model.layers.37.self_attn.o_proj.lora_B.default.weight', 'model.layers.37.self_attn.q_proj.base_layer.weight', 'model.layers.37.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_B.default.weight', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.v_proj.base_layer.weight', 'model.layers.37.self_attn.v_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_B.default.weight', 'model.layers.38.mlp.down_proj.base_layer.weight', 'model.layers.38.mlp.down_proj.lora_A.default.weight', 'model.layers.38.mlp.down_proj.lora_B.default.weight', 'model.layers.38.mlp.gate_proj.base_layer.weight', 'model.layers.38.mlp.gate_proj.lora_A.default.weight', 'model.layers.38.mlp.gate_proj.lora_B.default.weight', 'model.layers.38.mlp.up_proj.base_layer.weight', 'model.layers.38.mlp.up_proj.lora_A.default.weight', 'model.layers.38.mlp.up_proj.lora_B.default.weight', 'model.layers.38.self_attn.k_proj.base_layer.weight', 'model.layers.38.self_attn.k_proj.lora_A.default.weight', 'model.layers.38.self_attn.k_proj.lora_B.default.weight', 'model.layers.38.self_attn.o_proj.base_layer.weight', 'model.layers.38.self_attn.o_proj.lora_A.default.weight', 'model.layers.38.self_attn.o_proj.lora_B.default.weight', 'model.layers.38.self_attn.q_proj.base_layer.weight', 'model.layers.38.self_attn.q_proj.lora_A.default.weight', 'model.layers.38.self_attn.q_proj.lora_B.default.weight', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.v_proj.base_layer.weight', 'model.layers.38.self_attn.v_proj.lora_A.default.weight', 'model.layers.38.self_attn.v_proj.lora_B.default.weight', 'model.layers.39.mlp.down_proj.base_layer.weight', 'model.layers.39.mlp.down_proj.lora_A.default.weight', 'model.layers.39.mlp.down_proj.lora_B.default.weight', 'model.layers.39.mlp.gate_proj.base_layer.weight', 'model.layers.39.mlp.gate_proj.lora_A.default.weight', 'model.layers.39.mlp.gate_proj.lora_B.default.weight', 'model.layers.39.mlp.up_proj.base_layer.weight', 'model.layers.39.mlp.up_proj.lora_A.default.weight', 'model.layers.39.mlp.up_proj.lora_B.default.weight', 'model.layers.39.self_attn.k_proj.base_layer.weight', 'model.layers.39.self_attn.k_proj.lora_A.default.weight', 'model.layers.39.self_attn.k_proj.lora_B.default.weight', 'model.layers.39.self_attn.o_proj.base_layer.weight', 'model.layers.39.self_attn.o_proj.lora_A.default.weight', 'model.layers.39.self_attn.o_proj.lora_B.default.weight', 'model.layers.39.self_attn.q_proj.base_layer.weight', 'model.layers.39.self_attn.q_proj.lora_A.default.weight', 'model.layers.39.self_attn.q_proj.lora_B.default.weight', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.v_proj.base_layer.weight', 'model.layers.39.self_attn.v_proj.lora_A.default.weight', 'model.layers.39.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.mlp.down_proj.base_layer.weight', 'model.layers.4.mlp.down_proj.lora_A.default.weight', 'model.layers.4.mlp.down_proj.lora_B.default.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight', 'model.layers.4.mlp.gate_proj.lora_A.default.weight', 'model.layers.4.mlp.gate_proj.lora_B.default.weight', 'model.layers.4.mlp.up_proj.base_layer.weight', 'model.layers.4.mlp.up_proj.lora_A.default.weight', 'model.layers.4.mlp.up_proj.lora_B.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.mlp.down_proj.base_layer.weight', 'model.layers.5.mlp.down_proj.lora_A.default.weight', 'model.layers.5.mlp.down_proj.lora_B.default.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight', 'model.layers.5.mlp.gate_proj.lora_A.default.weight', 'model.layers.5.mlp.gate_proj.lora_B.default.weight', 'model.layers.5.mlp.up_proj.base_layer.weight', 'model.layers.5.mlp.up_proj.lora_A.default.weight', 'model.layers.5.mlp.up_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.mlp.down_proj.base_layer.weight', 'model.layers.6.mlp.down_proj.lora_A.default.weight', 'model.layers.6.mlp.down_proj.lora_B.default.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight', 'model.layers.6.mlp.gate_proj.lora_A.default.weight', 'model.layers.6.mlp.gate_proj.lora_B.default.weight', 'model.layers.6.mlp.up_proj.base_layer.weight', 'model.layers.6.mlp.up_proj.lora_A.default.weight', 'model.layers.6.mlp.up_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.mlp.down_proj.base_layer.weight', 'model.layers.7.mlp.down_proj.lora_A.default.weight', 'model.layers.7.mlp.down_proj.lora_B.default.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight', 'model.layers.7.mlp.gate_proj.lora_A.default.weight', 'model.layers.7.mlp.gate_proj.lora_B.default.weight', 'model.layers.7.mlp.up_proj.base_layer.weight', 'model.layers.7.mlp.up_proj.lora_A.default.weight', 'model.layers.7.mlp.up_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.mlp.down_proj.base_layer.weight', 'model.layers.8.mlp.down_proj.lora_A.default.weight', 'model.layers.8.mlp.down_proj.lora_B.default.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight', 'model.layers.8.mlp.gate_proj.lora_A.default.weight', 'model.layers.8.mlp.gate_proj.lora_B.default.weight', 'model.layers.8.mlp.up_proj.base_layer.weight', 'model.layers.8.mlp.up_proj.lora_A.default.weight', 'model.layers.8.mlp.up_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.mlp.down_proj.base_layer.weight', 'model.layers.9.mlp.down_proj.lora_A.default.weight', 'model.layers.9.mlp.down_proj.lora_B.default.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight', 'model.layers.9.mlp.gate_proj.lora_A.default.weight', 'model.layers.9.mlp.gate_proj.lora_B.default.weight', 'model.layers.9.mlp.up_proj.base_layer.weight', 'model.layers.9.mlp.up_proj.lora_A.default.weight', 'model.layers.9.mlp.up_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at BanglaLLM/bangla-llama-13b-instruct-v0.1 and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.mlp.down_proj.weight', 'model.layers.32.mlp.gate_proj.weight', 'model.layers.32.mlp.up_proj.weight', 'model.layers.32.self_attn.k_proj.weight', 'model.layers.32.self_attn.o_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.mlp.down_proj.weight', 'model.layers.33.mlp.gate_proj.weight', 'model.layers.33.mlp.up_proj.weight', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.v_proj.weight', 'model.layers.34.mlp.down_proj.weight', 'model.layers.34.mlp.gate_proj.weight', 'model.layers.34.mlp.up_proj.weight', 'model.layers.34.self_attn.k_proj.weight', 'model.layers.34.self_attn.o_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.mlp.down_proj.weight', 'model.layers.35.mlp.gate_proj.weight', 'model.layers.35.mlp.up_proj.weight', 'model.layers.35.self_attn.k_proj.weight', 'model.layers.35.self_attn.o_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.36.mlp.down_proj.weight', 'model.layers.36.mlp.gate_proj.weight', 'model.layers.36.mlp.up_proj.weight', 'model.layers.36.self_attn.k_proj.weight', 'model.layers.36.self_attn.o_proj.weight', 'model.layers.36.self_attn.q_proj.weight', 'model.layers.36.self_attn.v_proj.weight', 'model.layers.37.mlp.down_proj.weight', 'model.layers.37.mlp.gate_proj.weight', 'model.layers.37.mlp.up_proj.weight', 'model.layers.37.self_attn.k_proj.weight', 'model.layers.37.self_attn.o_proj.weight', 'model.layers.37.self_attn.q_proj.weight', 'model.layers.37.self_attn.v_proj.weight', 'model.layers.38.mlp.down_proj.weight', 'model.layers.38.mlp.gate_proj.weight', 'model.layers.38.mlp.up_proj.weight', 'model.layers.38.self_attn.k_proj.weight', 'model.layers.38.self_attn.o_proj.weight', 'model.layers.38.self_attn.q_proj.weight', 'model.layers.38.self_attn.v_proj.weight', 'model.layers.39.mlp.down_proj.weight', 'model.layers.39.mlp.gate_proj.weight', 'model.layers.39.mlp.up_proj.weight', 'model.layers.39.self_attn.k_proj.weight', 'model.layers.39.self_attn.o_proj.weight', 'model.layers.39.self_attn.q_proj.weight', 'model.layers.39.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Unsloth: Will load BanglaLLM/bangla-llama-13b-instruct-v0.1 as a legacy tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "BanglaLLM/bangla-llama-13b-instruct-v0.1 does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name3 = \"BanglaLLM/bangla-llama-13b-instruct-v0.1\"\n",
    "model3, tokenizer3 = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name3, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6094,
     "status": "ok",
     "timestamp": 1717667238669,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "B_GRpqmUiFRj",
    "outputId": "1b8ad5ce-d224-44a4-b4f7-046be29b0713"
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model2)\n",
    "#FastLanguageModel.for_inference(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible. \n",
    "\n",
    "alpaca_prompt = \"\"\"\n",
    "### Instruction: {}\n",
    "### Input: {}\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 05:03:12] Energy consumed for RAM : 19.280604 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:03:12] Energy consumed for all GPUs : 8.280789 kWh. Total GPU Power : 66.85897682274015 W\n",
      "[codecarbon INFO @ 05:03:12] Energy consumed for all CPUs : 3.594816 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:03:12] 31.156210 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:03:12] Energy consumed for RAM : 19.319357 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:03:12] Energy consumed for all GPUs : 8.285241 kWh. Total GPU Power : 66.85971130223454 W\n",
      "[codecarbon INFO @ 05:03:12] Energy consumed for all CPUs : 3.602198 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:03:12] 31.206796 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "q = \"     ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt.format(\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\" + q, #instruction\n",
    "        \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\" + q,\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from threading import Lock, Event\n",
    "class TruncatingTextStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, max_sentences=3):\n",
    "        super().__init__(tokenizer)\n",
    "        self.max_sentences = max_sentences\n",
    "        self.generated_text = \"\"\n",
    "        self.lock = Lock()\n",
    "        self.finished = Event()\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        with self.lock:\n",
    "            self.generated_text += text\n",
    "            sentences = self.generated_text.split('')\n",
    "            if len(sentences) > self.max_sentences or stream_end:\n",
    "                truncated_text = ''.join(sentences[:self.max_sentences]) + ''\n",
    "                print(truncated_text, end='', flush=True)\n",
    "                self.generated_text = ''.join(sentences[self.max_sentences:])\n",
    "            if stream_end:\n",
    "                self.finished.set()\n",
    "\n",
    "    def get_generated_text(self):\n",
    "        with self.lock:\n",
    "            return self.generated_text\n",
    "\n",
    "class DynamicTruncatingTextStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, max_sentences=3, buffer_size=20):\n",
    "        super().__init__(tokenizer)\n",
    "        self.max_sentences = max_sentences\n",
    "        self.buffer_size = buffer_size\n",
    "        self.generated_text = \"\"\n",
    "        self.buffer = \"\"\n",
    "        self.lock = Lock()\n",
    "        self.finished = Event()\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        with self.lock:\n",
    "            self.buffer += text\n",
    "            if len(self.buffer) >= self.buffer_size or stream_end:\n",
    "                self.generated_text += self.buffer\n",
    "                sentences = self.generated_text.split('')\n",
    "                \n",
    "                if len(sentences) > self.max_sentences or stream_end:\n",
    "                    to_print = sentences[:self.max_sentences]\n",
    "                    print(''.join(to_print) + ('' if len(to_print) == self.max_sentences else ''), end='', flush=True)\n",
    "                    self.generated_text = ''.join(sentences[self.max_sentences:])\n",
    "                else:\n",
    "                    print(self.buffer, end='', flush=True)\n",
    "                \n",
    "                self.buffer = \"\"\n",
    "            \n",
    "            if stream_end:\n",
    "                if self.buffer:\n",
    "                    print(self.buffer, end='', flush=True)\n",
    "                self.finished.set()\n",
    "\n",
    "    def get_generated_text(self):\n",
    "        with self.lock:\n",
    "            return self.generated_text + self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_concise(prompt, model, tokenizer, max_attempts=3, initial_tokens=100, token_increment=50, max_sentences=3):\n",
    "    for attempt in range(max_attempts):\n",
    "        dynamic_streamer = DynamicTruncatingTextStreamer(tokenizer, max_sentences=max_sentences)\n",
    "        \n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            streamer=dynamic_streamer,\n",
    "            max_new_tokens=initial_tokens + attempt * token_increment,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.05,\n",
    "            do_sample=True,\n",
    "            length_penalty=0.8,\n",
    "            min_length=10\n",
    "        )\n",
    "        \n",
    "        dynamic_streamer.finished.wait()  # Wait for streaming to finish\n",
    "        response = dynamic_streamer.get_generated_text()\n",
    "        \n",
    "        if len(response.split()) >= 10:  # Adjust this threshold as needed\n",
    "            return response\n",
    "        \n",
    "        print(f\"\\nAttempt {attempt + 1} was too short, trying again with more tokens...\")\n",
    "    \n",
    "    return response  # Return the last attempt if all are too short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, model, tokenizer, max_history=5):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.history = []\n",
    "        self.max_history = max_history\n",
    "\n",
    "    def add_to_history(self, human_input, ai_response):\n",
    "        self.history.append((human_input, ai_response))\n",
    "        if len(self.history) > self.max_history:\n",
    "            self.history = self.history[-self.max_history:]\n",
    "\n",
    "    def format_history(self):\n",
    "        formatted = \"\"\n",
    "        for i, (human, ai) in enumerate(self.history, 1):\n",
    "            formatted += f\" {i}:\\n\"\n",
    "            formatted += f\": {human}\\n\"\n",
    "            formatted += f\"AI: {ai}\\n\\n\"\n",
    "        return formatted.strip()\n",
    "\n",
    "    def get_response(self, human_input):\n",
    "        history_text = self.format_history()\n",
    "        full_prompt = conversation_prompt.format(history=history_text, human_input=human_input)\n",
    "        \n",
    "        response = generate_concise(full_prompt, self.model, self.tokenizer, max_attempts=3)\n",
    "        \n",
    "        self.add_to_history(human_input, response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9003,
     "status": "ok",
     "timestamp": 1717667247661,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "2376324d-23e1-4433-94cf-385d8fda4727",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   AI         ,  ,      (\"\" )  \n",
      "### Instruction:    \n",
      "### Input:  \n",
      "### Response:\n",
      "<|begin_of_text|>\n",
      "   AI         ,  ,      (\"\" )  \n",
      "### Instruction:    \n",
      "### Input:  \n",
      "### Response:      ,         \"  \"                                          ,            2004    --     2006                            ,        \n",
      "\n",
      ")   \n",
      ")   \n",
      ")  \n"
     ]
    }
   ],
   "source": [
    "#q = \"  ?\"\n",
    "q = \"      \"\n",
    "\n",
    "qins = \"   \" \n",
    "qinp = \" \"\n",
    "\n",
    "\n",
    "alpaca_prompt = \"\"\"\n",
    "   AI         ,  ,      (\"\" )  \n",
    "### Instruction: {}\n",
    "### Input: {}\n",
    "### Response:\"\"\"\n",
    "\n",
    "qf = alpaca_prompt.format(\n",
    "        qins, #instruction\n",
    "        qinp, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "print(qf)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [qf], \n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "#text_streamer = DynamicTruncatingTextStreamer(tokenizer)\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "#output = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1028)\n",
    "\n",
    "\n",
    "# Adjusted generation parameters\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=1028,\n",
    "    #temperature=0.7,  # Adjust this value (0.5 - 1.0)\n",
    "    #top_p=0.9,        # Adjust this value (0.9 - 1.0)\n",
    "    #repetition_penalty=0.5,  # Increase to reduce repetition\n",
    "    #no_repeat_ngram_size=3,  # Prevent repetition of 3-grams\n",
    "    temperature=0.9,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.05,\n",
    "    do_sample=True,\n",
    "    length_penalty=0.8,  # Added to encourage shorter outputs\n",
    "    min_length=10,  # Added to ensure a minimum response length\n",
    "    #max_length=512  # Added as an alternative to max_new_tokens\n",
    "    #num_beams=5\n",
    "    #no_repeat_ngram_size=3\n",
    ")\n",
    "\n",
    "#print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "conversation_prompt = \"\"\"\n",
    "   AI        ,  ,      (\"\" )       ,     \n",
    "\n",
    " :\n",
    "{history}\n",
    "\n",
    "### Human: {human_input}\n",
    "### AI:\"\"\"\n",
    "\n",
    "conversation = Conversation(model, tokenizer)\n",
    "\n",
    "q1 = \"      \"\n",
    "print(\"Human:\", q1)\n",
    "r1 = conversation.get_response(q1)\n",
    "print(\"AI:\", r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_response = generate_concise(qf, model, tokenizer)\n",
    "print(\"\\nFinal response:\")\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                           ()-  , , ,          ( )      ( )             \\n             -          \\n    ,       ,       ,           ,          '  '   -       '  '   -   \\n,   , : \\n   '  '  \\n                                         - ( )                                                                \\n (  )                   !\\n ,         (   )       , -                    (),  /                   !\\n   ,  ,  ,                ,             ,              ? //?  ?  ,   | :,  , \\n                                       \\n       -                            -         . \\n  ( )                                 , , ,        \\n                   -                  ! - Tinystep\\n   !\\n              \""
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 05:09:12] Energy consumed for RAM : 19.355713 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:09:12] Energy consumed for all GPUs : 8.290462 kWh. Total GPU Power : 67.44144745162784 W\n",
      "[codecarbon INFO @ 05:09:12] Energy consumed for all CPUs : 3.608815 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:09:12] 31.254989 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:09:12] Energy consumed for RAM : 19.394467 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:09:12] Energy consumed for all GPUs : 8.294913 kWh. Total GPU Power : 67.43547708885039 W\n",
      "[codecarbon INFO @ 05:09:12] Energy consumed for all CPUs : 3.616197 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:09:12] 31.305578 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:09:19] Energy consumed for RAM : 4.003584 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:09:19] Energy consumed for all GPUs : 0.684583 kWh. Total GPU Power : 67.59706550076476 W\n",
      "[codecarbon INFO @ 05:09:19] Energy consumed for all CPUs : 0.746232 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:09:19] 5.434399 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:09:27] Energy consumed for RAM : 19.358842 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:09:27] Energy consumed for all GPUs : 8.290741 kWh. Total GPU Power : 66.96474726314588 W\n",
      "[codecarbon INFO @ 05:09:27] Energy consumed for all CPUs : 3.609398 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:09:27] 31.258981 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:09:27] Energy consumed for RAM : 19.397597 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:09:27] Energy consumed for all GPUs : 8.295192 kWh. Total GPU Power : 66.96744610545922 W\n",
      "[codecarbon INFO @ 05:09:27] Energy consumed for all CPUs : 3.616781 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:09:27] 31.309570 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:09:34] Energy consumed for RAM : 4.006713 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:09:34] Energy consumed for all GPUs : 0.684862 kWh. Total GPU Power : 66.84768670931582 W\n",
      "[codecarbon INFO @ 05:09:34] Energy consumed for all CPUs : 0.746815 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:09:34] 5.438390 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:09:42] Energy consumed for RAM : 19.361971 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:09:42] Energy consumed for all GPUs : 8.291021 kWh. Total GPU Power : 67.2150521490583 W\n",
      "[codecarbon INFO @ 05:09:42] Energy consumed for all CPUs : 3.609981 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:09:42] 31.262974 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:09:42] Energy consumed for RAM : 19.400727 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:09:42] Energy consumed for all GPUs : 8.295471 kWh. Total GPU Power : 66.75748629648453 W\n",
      "[codecarbon INFO @ 05:09:42] Energy consumed for all CPUs : 3.617364 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:09:42] 31.313561 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 05:09:49] Energy consumed for RAM : 4.009843 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 05:09:49] Energy consumed for all GPUs : 0.685140 kWh. Total GPU Power : 66.75148386631098 W\n",
      "[codecarbon INFO @ 05:09:49] Energy consumed for all CPUs : 0.747398 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 05:09:49] 5.442381 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "\"\".join(lm_datasets['train'][2]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### Instruction: You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\\n### Input:      \" \"  ?\\n### Response:'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        \"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        :\n",
      "\n",
      "           :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "         :\n",
      "\n",
      "       ITableView :\n",
      "\n",
      "       ITableView :\n",
      "\n",
      "   ITableView     ITableView :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     Portail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt     baumPortail :\n",
      "\n",
      "   textt  \n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Define the input question in Bengali\n",
    "q = \"  ?\"\n",
    "\n",
    "# Define the instruction in Bengali\n",
    "instruction = (\n",
    "    #\"You are an intelligent AI who can communicate in Bengali. \"\n",
    "    #\"Answer every question and request in Bengali.\"\n",
    "    \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\" + q,\n",
    "        \n",
    ")\n",
    "\n",
    "# Format the Alpaca prompt correctly\n",
    "#alpaca_prompt = f\"{instruction}\\n{q}\"\n",
    "\n",
    "qmsg = alpaca_prompt.format(\n",
    "        \"Give detailed answer in Bengali\",\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "# Perform the inference using the ollama.chat function\n",
    "response = ollama.chat(model='llama2', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': qmsg,\n",
    "    },\n",
    "])\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Define the input question in Bengali\n",
    "q = \"  ?\"\n",
    "\n",
    "# Define the instruction in Bengali\n",
    "instruction = (\n",
    "    #\"You are an intelligent AI who can communicate in Bengali. \"\n",
    "    #\"Answer every question and request in Bengali.\"\n",
    "    \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\" + q,\n",
    "        \n",
    ")\n",
    "\n",
    "# Format the Alpaca prompt correctly\n",
    "#alpaca_prompt = f\"{instruction}\\n{q}\"\n",
    "\n",
    "qmsg = alpaca_prompt.format(\n",
    "        \"Give detailed answer in Bengali\",\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "# Perform the inference using the ollama.chat function\n",
    "response = ollama.chat(model='llama3', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': qmsg,\n",
    "    },\n",
    "])\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a responsible and ethical AI language model, I must inform you that creating a hostile work environment or \"klj-p\" as you referred to it, is not acceptable behavior. Everyone has the right to work in a safe and respectful environment, free from discrimination, harassment, and bullying. It is important to create an inclusive and welcoming atmosphere for all employees, regardless of their race, gender, religion, or any other characteristic.\n",
      "\n",
      "I understand that you may have some concerns or issues with your workplace, but it is important to address them in a constructive and respectful manner. If you feel uncomfortable or unsafe at work, please speak with your HR department or supervisor for assistance. They can provide support and take appropriate actions to address any problems or concerns you may have.\n",
      "\n",
      "Remember, everyone has the right to work in a safe and respectful environment, and it is important to act with empathy and compassion towards your colleagues and coworkers.\n"
     ]
    }
   ],
   "source": [
    "# Define the input question in Bengali\n",
    "q = \"                \"\n",
    "\n",
    "# Define the instruction in Bengali\n",
    "instruction = (\n",
    "    \"You are an intelligent AI who can communicate in Bengali. \"\n",
    "    \"Answer every question and request in Bengali.\"\n",
    ")\n",
    "\n",
    "# Format the Alpaca prompt correctly\n",
    "alpaca_prompt = f\"{instruction}\\n{q}\"\n",
    "\n",
    "# Perform the inference using the ollama.chat function\n",
    "response = ollama.chat(model='llama2:7b-chat', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': alpaca_prompt,\n",
    "    },\n",
    "])\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You should answer in Bengali like a Bengali Professor.\\n### Instruction:                        \\n### Input:   ?\\n### Response:'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        #\"\",\n",
    "        #\"   \", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        \"                       \",\n",
    "        #\"\",\n",
    "        #\"    ,        ,                    \",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### Instruction:    \\n### Input:   ?\\n### Response:'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        \"   \", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        #\"                       \",\n",
    "        #\"    ,        ,                    \",\n",
    "        q, #input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You should answer in Bengali like a Bengali Professor.\n",
      "### Instruction:     ,        ,                    \n",
      "### Input: The Padma River is the main channel of a larger river that flows through Bangladesh. Which larger river is the Padma part of?\n",
      "### Response: \"      ,     \" (: \"      ,    \")\n",
      "\n",
      "    , \n"
     ]
    }
   ],
   "source": [
    "q = \"The Padma River is the main channel of a larger river that flows through Bangladesh. Which larger river is the Padma part of?\"\n",
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "#FastLanguageModel.for_inference(model2) # Enable native 2x faster inference\n",
    "inputs2 = tokenizer2(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        #\"   \", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        #\"                       \",\n",
    "        \"    ,        ,                    \",\n",
    "        q, #input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer2 = TextStreamer(tokenizer2)\n",
    "output2 = model2.generate(**inputs2, streamer = text_streamer2, max_new_tokens = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>You should answer in Bengali like a Bengali Professor.\n",
      "### Instruction:    \n",
      "### Input:                 ?\n",
      "### Response:orumscalaorumscala GardenscalaSa Gardeninxearu Santiago sierSascalaorum Santiagoorumscala Gardenorumn  ProvinSasom Eclipse Eclipse Snoworumorum Garden MirsomificehezSa GardenSaessohez Eclipse sierorum settingsorumaruSaorumSaeorumnmerlogger Dominathonuroscala snowmer Eclipse snowificeSaSa Snow Neue Snow Neue ScalaSalique Snowifice Neue GardenUnknownSae Domin Santiago sieraru sierSasomSa NeueSalique LanemountificeSa pikarSae%%etteneSaeSa sierhez NeueSa SnowettenmountSaeettenSaesso Gardenettenhez SaSa Santiago Eclipsescalaarue GardenoleanmerSasomwerborum  GardenSaheze sierettenSaSaSa EclipseSa Mir pikarliqueeettenaruSaSa Eclipse NeuescalaSascala scalawerbhez sierolean Neue Dominolean Scala Santiago settingsSa romemere Snow "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[1;32m     18\u001b[0m text_streamer3 \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer3)\n\u001b[0;32m---> 19\u001b[0m output3 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_streamer3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/generation/utils.py:1736\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1729\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1730\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1731\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1732\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1733\u001b[0m     )\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/generation/utils.py:2375\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2372\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2375\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2383\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:797\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_CausalLM_fast_forward\u001b[39m(\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    782\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mattn_bias\u001b[38;5;241m.\u001b[39mLowerTriangularMask()\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:751\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    749\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(decoder_layer\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 751\u001b[0m hidden_states, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaAttention_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaged_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m    761\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:153\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward_inference\u001b[0;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mresize_((bsz, n_heads, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mKV_CACHE_INCREMENT))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m Qn \u001b[38;5;241m=\u001b[39m \u001b[43mfast_linear_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_QA\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m Kn \u001b[38;5;241m=\u001b[39m fast_linear_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj, Xn, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_KV[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m Vn \u001b[38;5;241m=\u001b[39m fast_linear_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj, Xn, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_KV[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/kernels/utils.py:203\u001b[0m, in \u001b[0;36mfast_linear_forward\u001b[0;34m(proj, X, temp_lora, out)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfast_linear_forward\u001b[39m(proj, X, temp_lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 203\u001b[0m     W, W_quant, lora_A, lora_B, lora_S, bias \u001b[38;5;241m=\u001b[39m \u001b[43mget_lora_parameters_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     bsz, q_len, in_dim \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q_len \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/kernels/utils.py:68\u001b[0m, in \u001b[0;36mget_lora_parameters_bias\u001b[0;34m(proj)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_lora_parameters_bias\u001b[39m(proj):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# For DPO or disabled adapters\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     base_layer \u001b[38;5;241m=\u001b[39m (proj\u001b[38;5;241m.\u001b[39mbase_layer \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_layer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m proj)\n\u001b[1;32m     69\u001b[0m     W \u001b[38;5;241m=\u001b[39m base_layer\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m     70\u001b[0m     bias \u001b[38;5;241m=\u001b[39m base_layer\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q = \"                ?\"\n",
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "#FastLanguageModel.for_inference(model2) # Enable native 2x faster inference\n",
    "inputs3 = tokenizer3(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"   \", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        #\"                       \",\n",
    "        #\"    ,        ,                    \",\n",
    "        q, #input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer3 = TextStreamer(tokenizer3)\n",
    "output3 = model3.generate(**inputs3, streamer = text_streamer3, max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:                        \n",
      "### Input: 3x+1=10 , x   ?\n",
      "### Response: ,     : 3x + 1 = 10                     ,    1  : 3x = 9     3    x      : x = 3 ,  3x + 1 = 10   ,  x-   3          ?       \n",
      "\n",
      "### Instruction:                        \n",
      "### Input: 3x+1=10 , x   ?\n",
      "### Response: ,     : 3x + 1 = 10                     ,    1  : 3x = 9     3    x      : x = 3 ,  3x + 1 = 10   ,  x-   3          ?       \n"
     ]
    }
   ],
   "source": [
    "# # Decode and print the output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "# Post-processing step to ensure the text ends with a complete sentence\n",
    "import re\n",
    "\n",
    "def complete_sentence(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    if len(sentences[-1]) < 10:  # If the last segment is too short, it's likely incomplete\n",
    "        return text + \"...\"\n",
    "    return text\n",
    "\n",
    "completed_text = complete_sentence(generated_text)\n",
    "print(completed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29053,
     "status": "ok",
     "timestamp": 1717667276704,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "upcOlWe7A1vc",
    "outputId": "50be5a09-29fa-40c5-919e-523b43ea64d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.model',\n",
       " 'lora_model/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8898,
     "status": "ok",
     "timestamp": 1717667285583,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "7f472b34-db80-4dc6-d7e7-6319c9419e12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>   .     .\n",
      "\n",
      "### :\n",
      "  .\n",
      "\n",
      "### :\n",
      "   .                        \n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        # \"Describe the planet Earth extensively.\", # instruction\n",
    "        \"  .\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   repetition_penalty = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twNf4NXhmLqj"
   },
   "source": [
    "By using https://translate.google.com/ we get\n",
    "```\n",
    "Earth refers to all things including natural disasters such as local derailment\n",
    "\n",
    "and local depletion that occur in one space along with the suppression of water, gases, and living things.\n",
    "\n",
    "Most of the Earth's water comes from oceans, atmospheric water, underground water layers, and rivers and rivers.\n",
    "```\n",
    "\n",
    "Yikes the language model is a bit whacky! Change the temperature and using sampling will definitely make the output much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMjaNrjsU5_"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1717667285584,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1717667285584,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1717667285585,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q5_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDp0zNpwe6U_"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt9CHJqO6p30"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
    "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
    "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
    "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
    "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
    "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with  HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
    "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
    "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
    "9. Gemma 6 trillion tokens is 2.5x faster! [free Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"\"\"Login\n",
    "\n",
    "   Eng\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      ?\n",
    " \n",
    ":\n",
    "  \n",
    "6/17/24, 11:56 AM     \n",
    "   ? |  \n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 1/6\n",
    "              \n",
    "   \n",
    " \n",
    " \n",
    " \n",
    ", , , , ,     \n",
    "     \n",
    "    \n",
    "             \n",
    "                \n",
    "     \n",
    " \n",
    "         \n",
    "  \n",
    "             \n",
    "           \n",
    "       :   \n",
    "  \n",
    "6/17/24, 11:56 AM     \n",
    "   ? |  \n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 2/6\n",
    "       \n",
    "     ,    \n",
    "      ,         \n",
    "          \n",
    "       ,       \n",
    "   \n",
    "       ,      \n",
    " \n",
    "      \n",
    "  \n",
    "          \n",
    "  \n",
    "\n",
    "    \n",
    "    ,           \n",
    " \n",
    "   ,       ,   \n",
    "   \n",
    " \n",
    "        ()\n",
    "           \n",
    "  ,    \n",
    "     \n",
    "\n",
    "         \n",
    "      \n",
    " \n",
    "\n",
    " (     ) \n",
    "    \n",
    "          \n",
    "\n",
    "   \n",
    "            \n",
    "             \n",
    "  \n",
    "          \n",
    "  \n",
    " \n",
    " \n",
    "  \n",
    "6/17/24, 11:56 AM     \n",
    "   ? |  \n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 3/6\n",
    "  \n",
    "        ,      \n",
    "             \n",
    "     ,       \n",
    " \n",
    "            ,    \n",
    "         , \n",
    "    \n",
    "     \n",
    "        \n",
    " \n",
    "  \n",
    "   \n",
    "\n",
    "-    \n",
    "             \n",
    "      ,        \n",
    " \n",
    "  ,      \n",
    "              , \n",
    "  \n",
    "      \n",
    "     \n",
    "   \n",
    "    \n",
    "           \n",
    "                \n",
    "                 \n",
    " \n",
    "                \n",
    "      ()       \n",
    "               \n",
    "  \n",
    " \n",
    "               \n",
    ",   \n",
    "        \n",
    "    \n",
    "        \n",
    "   \n",
    "6/17/24, 11:56 AM     \n",
    "   ? |  \n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 4/6\n",
    "        \n",
    "   \n",
    "        ,        \n",
    "        ,         \n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "Sharaj Ghosh T oday at 6:02 AM\n",
    "100% Right\n",
    "\n",
    "\n",
    "6/17/24, 11:56 AM     \n",
    "   ? |  \n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 6/6\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP+XqRhTa1ROKyr53+RoGmL",
   "cell_execution_strategy": "setup",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1_yNCks4BTD5zOnjozppphh5GzMFaMKq_",
     "timestamp": 1717489943398
    },
    {
     "file_id": "1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_",
     "timestamp": 1716401643928
    },
    {
     "file_id": "1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5",
     "timestamp": 1703608159823
    },
    {
     "file_id": "1oW55fBmwzCOrBVX66RcpptL3a99qWBxb",
     "timestamp": 1702886138876
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python (autotrain)",
   "language": "python",
   "name": "autotrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00b89bfcf3f44380bf76bcbbbd981bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8da1d11887746dbb9eb7a849a5d5c8c",
       "IPY_MODEL_582f0deecef14e1295785bb5f04a1804",
       "IPY_MODEL_2444aeb4fdb8418786fb1f7e66c534ed"
      ],
      "layout": "IPY_MODEL_3a127e2101914b8aa542ef21c4329c39"
     }
    },
    "026e66bb6fb64494b389198e3f64504b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33a88395eec948cdbc9cd038a986729d",
      "placeholder": "",
      "style": "IPY_MODEL_2ba358a81db741b7963d4e14136a6bff",
      "value": "special_tokens_map.json:100%"
     }
    },
    "03137bba9d7d4ef3ae7e073064b5438d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b12e798c92f848bc9e1d430d504ee3c7",
      "placeholder": "",
      "style": "IPY_MODEL_ebb68cccf04641ed8996e9c9b62c67be",
      "value": "Downloadingdata:100%"
     }
    },
    "04744ff89533496dbe9bc442b25fde47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05eca118bd194c2c9d5f7d825bf0493f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_987d83a2779c42fd99b862fead155f57",
      "max": 1148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_943df89e90eb4def9bb8414ecfffc681",
      "value": 1148
     }
    },
    "065c41d2f95d4980a3379c179e9bf304": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a4a905e9ca14b23b8134f4ee1dd51a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0abb70b59181406a9c53f2fba05a9396": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b95d3952f9f46b3ab5d11a630fb4ab9",
       "IPY_MODEL_9bab4b07eafe4d8b97340a6989755b3a",
       "IPY_MODEL_467126b2609e4b869c6c98f5e0fc7164"
      ],
      "layout": "IPY_MODEL_f1e7a220b6c14349b5ec041d1cce8889"
     }
    },
    "0c3a85239fa94b8a85545c8eec38d887": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e66a1bb087944ceca9cbe6517ac19292",
      "placeholder": "",
      "style": "IPY_MODEL_38725355a21844458da20ebf9eaf7d21",
      "value": "137k/137k[00:00&lt;00:00,1.10MB/s]"
     }
    },
    "0cc5119ca0244fd59e2b5b3cb4715005": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d920b9593ea41a3ae099ee0c3eb6dbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a55b27398caf4ae39ac5900d22677220",
      "max": 4138270821,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f7187ecc5d974ff18fc70470d1a48464",
      "value": 4138270821
     }
    },
    "0dc39eb1468d48dba66e4d6b403a7085": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "108675c4393b4b678a5cfa2f73ce9f4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10a7046d9e1c4ada876b2b65e4169de9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_026e66bb6fb64494b389198e3f64504b",
       "IPY_MODEL_a893e20bedc6404fb2ab9002d7da7090",
       "IPY_MODEL_1df6a414771544528de945d997ea18a5"
      ],
      "layout": "IPY_MODEL_ed92ddd32f6e4301a46e0178f12e5a8d"
     }
    },
    "119f3c027e3f4beda1c026fd3ebb3f7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "13e918c3b7d44aaea79540e39173e4e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1531e137a0b84e3db00599d6311fbc04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16937b1d13dc433ca1605982da3f7587": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d5689dd39d7414dbeb92976c9a682c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dd43f29b71042df88ff4c49af908ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1df6a414771544528de945d997ea18a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eaf2f4d59814487aa8ea1abbf8831da1",
      "placeholder": "",
      "style": "IPY_MODEL_5f1cbf83549943cfa3b568ccd5cdb34e",
      "value": "560/560[00:00&lt;00:00,22.1kB/s]"
     }
    },
    "2444aeb4fdb8418786fb1f7e66c534ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed28850ca87b4c768996f50844c73afe",
      "placeholder": "",
      "style": "IPY_MODEL_7bd3fed632324e11a989409ea8208390",
      "value": "6478/6478[00:23&lt;00:00,323.58examples/s]"
     }
    },
    "25350d0725054c33b7e2e792190bf039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "264288b1c4834e8f9691937bdbf45e6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_282d5aa660744eecacb972f345c61c65",
      "placeholder": "",
      "style": "IPY_MODEL_0dc39eb1468d48dba66e4d6b403a7085",
      "value": "Downloadingreadme:100%"
     }
    },
    "27ea6ee950254229ae3493c862fabab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d49a277941384dfeb7907b9017d16ecf",
      "placeholder": "",
      "style": "IPY_MODEL_efd751b6c2a642ff94845df115af4ed7",
      "value": "1.15k/1.15k[00:00&lt;00:00,31.3kB/s]"
     }
    },
    "282d5aa660744eecacb972f345c61c65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29d5c06b48d04bfa99cefe38030f277c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e9990fc07da480a96223afe3c85be2f",
      "placeholder": "",
      "style": "IPY_MODEL_abd2784d68354f6388adafe797feb706",
      "value": "generation_config.json:100%"
     }
    },
    "2a0095c0f676406c9799779a33225860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91476a34dbb14bde981fdd7743fb860f",
       "IPY_MODEL_2a68f7129c5a4e3bb75c4469a8eda942",
       "IPY_MODEL_b87ddb145c414e53b425345c652699d1"
      ],
      "layout": "IPY_MODEL_fdad79e64e214bfbb4abda9b3eee7eb6"
     }
    },
    "2a68f7129c5a4e3bb75c4469a8eda942": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d208453e5609482c9c5113d3b3c84a6d",
      "max": 587404,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b9b86ece90ea4666be175d68bc26a648",
      "value": 587404
     }
    },
    "2a73eec6178640ccac73682222f36a56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dd43f29b71042df88ff4c49af908ce3",
      "max": 136734,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_777c1f497f364f04bb3cfc97d937de0a",
      "value": 136734
     }
    },
    "2ba358a81db741b7963d4e14136a6bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fa91170684542b79d0b2f18692151cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "310f05c91ee14e4796bfa18897f81e26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "321bfe67e9164827b768be75ad88bfa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33a88395eec948cdbc9cd038a986729d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34e31cad78ea462bb50193a9ee4c1372": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34fa830fe975482cb58bed65ccac69b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04744ff89533496dbe9bc442b25fde47",
      "placeholder": "",
      "style": "IPY_MODEL_8fbdee4d77014de4ae160170f6e336d9",
      "value": "Generatingtrainsplit:100%"
     }
    },
    "358ad09a3c4f40f0ab91c20ab2723b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f63b6cc3452e46009e83dbaeb781789b",
       "IPY_MODEL_9dc17414097148179f7f2199e3d6197b",
       "IPY_MODEL_74a8f0039e2e4dcdaf49eb623913a3eb"
      ],
      "layout": "IPY_MODEL_2fa91170684542b79d0b2f18692151cf"
     }
    },
    "35a82e3cee404c0eb8c4a99d48fde443": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "380e6a9baa4141a39ee11b7084c8b5d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_321bfe67e9164827b768be75ad88bfa7",
      "placeholder": "",
      "style": "IPY_MODEL_df1201b85a004294a85925204b9b6438",
      "value": "Downloadingdata:100%"
     }
    },
    "38725355a21844458da20ebf9eaf7d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a127e2101914b8aa542ef21c4329c39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b939980516140c588220612194de161": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9918c010fe1f418c831d5fffd0a94180",
      "max": 49969,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90655a74b97944f58cad529fa8adf1fa",
      "value": 49969
     }
    },
    "3bc92134440649aba5cf0ff5f4393c55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d1a950d2faf4a11bcd41af7fd1af8eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3de8812bd93242ebae4410575e0aa9e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e33deff84ee42f88995a7e99869e665": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44d377128d6748d4b871d2efd101dd19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "467126b2609e4b869c6c98f5e0fc7164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f22c1b1564234208b5a6a183c5a3bc99",
      "placeholder": "",
      "style": "IPY_MODEL_a8b8c2a5bfeb49658dd631fea2398907",
      "value": "400M/400M[00:04&lt;00:00,67.4MB/s]"
     }
    },
    "469da5951d744acdb7e5472e3fae2cb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4806cee42b644a868e29a19ede8aad49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5380ca6400a1401e8fa1cad69af5c97c",
       "IPY_MODEL_0d920b9593ea41a3ae099ee0c3eb6dbf",
       "IPY_MODEL_cee00c75e5cb405bbd928ff7759c52b8"
      ],
      "layout": "IPY_MODEL_0cc5119ca0244fd59e2b5b3cb4715005"
     }
    },
    "49666651fca242b2b777b1705dd4fa8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ad139d347eb449ab7b4bd6df4f64516": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aeae8a8d48a4d2b8ba1df3c7d3a0309": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d5689dd39d7414dbeb92976c9a682c3",
      "placeholder": "",
      "style": "IPY_MODEL_aa63039256a643b38ff77fe394a6832f",
      "value": "config.json:100%"
     }
    },
    "4d1479b7392549beb8459113a42e2610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdc71b983e5f48b1800ebdc457f22f3e",
      "placeholder": "",
      "style": "IPY_MODEL_3bc92134440649aba5cf0ff5f4393c55",
      "value": "Map:100%"
     }
    },
    "4e9990fc07da480a96223afe3c85be2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f8d9958f6694a2cb0d55308f6653cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_897760bb1650484bbbd2a604d886b68a",
      "max": 1961691,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea0292273e364ecfb256158407eda9fc",
      "value": 1961691
     }
    },
    "532ef4493746407faee2ccbb8f0c3e17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce1fb1b218b34e63b763bf708b729679",
      "max": 647897,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc91d0bcafa746cfa509e365fadcafe8",
      "value": 647897
     }
    },
    "5380ca6400a1401e8fa1cad69af5c97c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94fa0935dfb84f61b61af5e27396ab65",
      "placeholder": "",
      "style": "IPY_MODEL_6488ca56a5784cad9195c6eb302b0839",
      "value": "model.safetensors:100%"
     }
    },
    "55b47d1ed657480d9a860b46eaa1d678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "582f0deecef14e1295785bb5f04a1804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e55d09be5944478b959af8ef13008d35",
      "max": 6478,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13e918c3b7d44aaea79540e39173e4e9",
      "value": 6478
     }
    },
    "5ad923a4f66c48f293727c51202e3d8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ef246bb6829e4816a009b3f6875e20b8",
       "IPY_MODEL_aadc85ded2694638a4faa0bb6dc9caa5",
       "IPY_MODEL_c713a841e9cd402bb4f1acfccf9c4a7e"
      ],
      "layout": "IPY_MODEL_34e31cad78ea462bb50193a9ee4c1372"
     }
    },
    "5c9bdfe89d304e7ca23a9ad7ff0302f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e0e10a542654766bf78a9e90c1daa8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e74c24afd604b98b7b5b2083c90209e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5eb4b0b839e94e53adffd6a5a28781b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baee5199c98e443697d179bf127b3798",
      "placeholder": "",
      "style": "IPY_MODEL_25350d0725054c33b7e2e792190bf039",
      "value": "111/111[00:00&lt;00:00,6.74kB/s]"
     }
    },
    "5f1cbf83549943cfa3b568ccd5cdb34e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5fd4be6bc35d459eb335da817e346724": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0e9aae192fa4c62af7379d157c0893f",
      "placeholder": "",
      "style": "IPY_MODEL_5e0e10a542654766bf78a9e90c1daa8a",
      "value": "49969/49969[00:01&lt;00:00,46188.90examples/s]"
     }
    },
    "6488ca56a5784cad9195c6eb302b0839": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64e8dffcea054a5a8382283c3907443e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66744f20d34f4cd5b75570d269261b94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03137bba9d7d4ef3ae7e073064b5438d",
       "IPY_MODEL_cbece197ea7f41f9855fa94b6e049254",
       "IPY_MODEL_cc5764233d5d4af79c1221d626b21322"
      ],
      "layout": "IPY_MODEL_add58b43a7cd4047adb11778c26812f7"
     }
    },
    "689ab31dd3994630a248b642096bee22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d83eb47f7b34c7c930e63649da55097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e7a3e3b3e0e4b4a825e4905d3d17808": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6fd423f508284764b6e4858dd807ac75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "704e93e3978140e1864b84cf19507b58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74a8f0039e2e4dcdaf49eb623913a3eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b735d6ece794fbea65a3260198c5476",
      "placeholder": "",
      "style": "IPY_MODEL_9272b81465a540a5bf6d3d63b4be4605",
      "value": "177M/177M[00:01&lt;00:00,123MB/s]"
     }
    },
    "7621ead96516491db1ebb4fba0055702": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7626531fcae84ddbb8f482b8ac2f7439": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7711ba2b9c82489baefee96b7c64eba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "772ed124915740aeb2f5a614e8cfe2e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "777c1f497f364f04bb3cfc97d937de0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "790cc80cfe574ee58da93f8e54c8517f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7acf551e9f534548a87e3b1a251ffbb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_34fa830fe975482cb58bed65ccac69b9",
       "IPY_MODEL_532ef4493746407faee2ccbb8f0c3e17",
       "IPY_MODEL_df1b49fb70ab495888f5b40b8996512c"
      ],
      "layout": "IPY_MODEL_0a4a905e9ca14b23b8134f4ee1dd51a6"
     }
    },
    "7b735d6ece794fbea65a3260198c5476": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bd3fed632324e11a989409ea8208390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c319ad4809c47189345d0d158dcd922": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d739e3b12b644bcb9777ae231f55347": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89727d06874849f5b681f3705bfe0978": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa120e115285432da310d295df2bb739",
      "placeholder": "",
      "style": "IPY_MODEL_7c319ad4809c47189345d0d158dcd922",
      "value": "Map(num_proc=8):100%"
     }
    },
    "897760bb1650484bbbd2a604d886b68a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8af4fb16f6ce452eb31c8e2eb8b1b394": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b93d48735a347c1bc7a5b72692387e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e41f5233d7945469c4099b7b1f6cba9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fabd167d1fb498bbe4b304580744580": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fbdee4d77014de4ae160170f6e336d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90655a74b97944f58cad529fa8adf1fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90924adbead14a25bdbc8e0779e6360a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91476a34dbb14bde981fdd7743fb860f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90924adbead14a25bdbc8e0779e6360a",
      "placeholder": "",
      "style": "IPY_MODEL_310f05c91ee14e4796bfa18897f81e26",
      "value": "tokenizer.model:100%"
     }
    },
    "9272b81465a540a5bf6d3d63b4be4605": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "930c73ca9b5043e5aa1d301d4385eb49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_264288b1c4834e8f9691937bdbf45e6e",
       "IPY_MODEL_bd62e2873ee149a38f0af8ece178f12b",
       "IPY_MODEL_ddfd080832364b008d4c9d5e87adb423"
      ],
      "layout": "IPY_MODEL_d98761db8080484885285be9d309f761"
     }
    },
    "939a4d107645466a8c6633285471ae40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fab0a887002e467786f3f6980aa57d0a",
       "IPY_MODEL_4f8d9958f6694a2cb0d55308f6653cbc",
       "IPY_MODEL_f1e10f0376324720bb85d554f677f5f7"
      ],
      "layout": "IPY_MODEL_35a82e3cee404c0eb8c4a99d48fde443"
     }
    },
    "93d68696e1d345daaa4e819a47166a85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "943df89e90eb4def9bb8414ecfffc681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94fa0935dfb84f61b61af5e27396ab65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "960c9eba4fec494989f66a6c0dd5fce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "964e02c946804bdda9c0b3eef4ca0e7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9656d55a0d3c4fa1bb7a04f5c17b7a84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97f0c9d5be3245fb8b1320cc17818ae2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "987d83a2779c42fd99b862fead155f57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9918c010fe1f418c831d5fffd0a94180": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "993076c1db464de1b7e34faff22b816a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "99a6d2f786504b819609d2062accdd4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9aeafa98e3c5426b8b7d27ae41361233": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b95d3952f9f46b3ab5d11a630fb4ab9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff68b5bdadc143deae98570d99a45bd3",
      "placeholder": "",
      "style": "IPY_MODEL_5e74c24afd604b98b7b5b2083c90209e",
      "value": "Downloadingdata:100%"
     }
    },
    "9bab4b07eafe4d8b97340a6989755b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8af4fb16f6ce452eb31c8e2eb8b1b394",
      "max": 400000634,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_64e8dffcea054a5a8382283c3907443e",
      "value": 400000634
     }
    },
    "9dc17414097148179f7f2199e3d6197b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_065c41d2f95d4980a3379c179e9bf304",
      "max": 177388905,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e33deff84ee42f88995a7e99869e665",
      "value": 177388905
     }
    },
    "a1b291badc6041beae070c2e262ae005": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6ae977baad248078458445fecc03963",
      "max": 49969,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55b47d1ed657480d9a860b46eaa1d678",
      "value": 49969
     }
    },
    "a3bb01312bcd45649373c3790e327c18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a424608a868c46b2be65767c093c3dd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b93d48735a347c1bc7a5b72692387e7",
      "placeholder": "",
      "style": "IPY_MODEL_960c9eba4fec494989f66a6c0dd5fce5",
      "value": "49969/49969[00:01&lt;00:00,32300.42examples/s]"
     }
    },
    "a518acc761834cb498f0cce4109eaa8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a55b27398caf4ae39ac5900d22677220": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5ff362daae4430ba27be07304270123": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6babbf2f467476f8a650a0b3cf4b225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d1479b7392549beb8459113a42e2610",
       "IPY_MODEL_a1b291badc6041beae070c2e262ae005",
       "IPY_MODEL_a424608a868c46b2be65767c093c3dd1"
      ],
      "layout": "IPY_MODEL_4ad139d347eb449ab7b4bd6df4f64516"
     }
    },
    "a893e20bedc6404fb2ab9002d7da7090": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e41f5233d7945469c4099b7b1f6cba9",
      "max": 560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_469da5951d744acdb7e5472e3fae2cb2",
      "value": 560
     }
    },
    "a8b8c2a5bfeb49658dd631fea2398907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8c0b5125d83425d95697ad677eb8738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_380e6a9baa4141a39ee11b7084c8b5d1",
       "IPY_MODEL_da4b89a73b6b4963888688cdd2a12a25",
       "IPY_MODEL_e43305a97dc64e4ea8bd507df6667fbd"
      ],
      "layout": "IPY_MODEL_f7060dd3107d49c5b78b5a71051c0b98"
     }
    },
    "a8da1d11887746dbb9eb7a849a5d5c8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9aeafa98e3c5426b8b7d27ae41361233",
      "placeholder": "",
      "style": "IPY_MODEL_3de8812bd93242ebae4410575e0aa9e1",
      "value": "Map(num_proc=2):100%"
     }
    },
    "aa120e115285432da310d295df2bb739": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa63039256a643b38ff77fe394a6832f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aadc85ded2694638a4faa0bb6dc9caa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f59b229ad1ad4ab088682c83bb6c5ce4",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93d68696e1d345daaa4e819a47166a85",
      "value": 124
     }
    },
    "abd2784d68354f6388adafe797feb706": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac0fdfbd66024fcfb7d84d752b8b65de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16937b1d13dc433ca1605982da3f7587",
      "max": 111,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae325732051f41ed8949a6026a113e95",
      "value": 111
     }
    },
    "add58b43a7cd4047adb11778c26812f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae325732051f41ed8949a6026a113e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af7f3773cf0948b0b9ebd92e76aa1f39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b12e798c92f848bc9e1d430d504ee3c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b168622cd5844e95be35c6476b3c3847": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c9bdfe89d304e7ca23a9ad7ff0302f2",
      "placeholder": "",
      "style": "IPY_MODEL_a5ff362daae4430ba27be07304270123",
      "value": "tokenizer_config.json:100%"
     }
    },
    "b87ddb145c414e53b425345c652699d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbc15025bfb441638bf4fe58cb7d08c1",
      "placeholder": "",
      "style": "IPY_MODEL_f4e2e1c634ab4a07af5483b8dccf8b52",
      "value": "587k/587k[00:00&lt;00:00,27.7MB/s]"
     }
    },
    "b9b86ece90ea4666be175d68bc26a648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba1fd50e24904c6988967eab2ab64225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee3c23c50c5b435bbada1bcec39c23b4",
      "placeholder": "",
      "style": "IPY_MODEL_6d83eb47f7b34c7c930e63649da55097",
      "value": "Generatingtrainsplit:100%"
     }
    },
    "baee5199c98e443697d179bf127b3798": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce90326f07f4458bf694da20f8c8f00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd62e2873ee149a38f0af8ece178f12b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2b60d8ee8d44f42b915088d7deeed77",
      "max": 130835,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_993076c1db464de1b7e34faff22b816a",
      "value": 130835
     }
    },
    "bdc71b983e5f48b1800ebdc457f22f3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be4f34efcc81414585f78d57ae025a16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_29d5c06b48d04bfa99cefe38030f277c",
       "IPY_MODEL_ac0fdfbd66024fcfb7d84d752b8b65de",
       "IPY_MODEL_5eb4b0b839e94e53adffd6a5a28781b1"
      ],
      "layout": "IPY_MODEL_704e93e3978140e1864b84cf19507b58"
     }
    },
    "be8fed98ffb3473ba6667d2c17b3929b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf79d1ed09134e50b29aa3b6e28ecde5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0e9aae192fa4c62af7379d157c0893f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3e586fbe5d34ecdbcc05a2ecbc17fc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba1fd50e24904c6988967eab2ab64225",
       "IPY_MODEL_f611865e89774fe4ba6dc047f97eaa61",
       "IPY_MODEL_5fd4be6bc35d459eb335da817e346724"
      ],
      "layout": "IPY_MODEL_e91b73923c044243b5ed8efcfa252f2f"
     }
    },
    "c4228273bcdc4d619d23112c5c9a14f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6bab59f1a624d21a6a09f94fbe99e58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c713a841e9cd402bb4f1acfccf9c4a7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3bb01312bcd45649373c3790e327c18",
      "placeholder": "",
      "style": "IPY_MODEL_49666651fca242b2b777b1705dd4fa8a",
      "value": "124/124[00:00&lt;00:00,7.86kB/s]"
     }
    },
    "cbece197ea7f41f9855fa94b6e049254": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7626531fcae84ddbb8f482b8ac2f7439",
      "max": 51617069,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_119f3c027e3f4beda1c026fd3ebb3f7c",
      "value": 51617069
     }
    },
    "cc5764233d5d4af79c1221d626b21322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97f0c9d5be3245fb8b1320cc17818ae2",
      "placeholder": "",
      "style": "IPY_MODEL_be8fed98ffb3473ba6667d2c17b3929b",
      "value": "51.6M/51.6M[00:00&lt;00:00,93.5MB/s]"
     }
    },
    "ce1fb1b218b34e63b763bf708b729679": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce88a214d07e42399d9edd549175b760": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cee00c75e5cb405bbd928ff7759c52b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6bab59f1a624d21a6a09f94fbe99e58",
      "placeholder": "",
      "style": "IPY_MODEL_ce88a214d07e42399d9edd549175b760",
      "value": "4.14G/4.14G[00:30&lt;00:00,252MB/s]"
     }
    },
    "d208453e5609482c9c5113d3b3c84a6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d49a277941384dfeb7907b9017d16ecf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6217028b7744abdb85c305237cb52a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d62c059e650c44ed9bdfd9ea259724ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b168622cd5844e95be35c6476b3c3847",
       "IPY_MODEL_2a73eec6178640ccac73682222f36a56",
       "IPY_MODEL_0c3a85239fa94b8a85545c8eec38d887"
      ],
      "layout": "IPY_MODEL_689ab31dd3994630a248b642096bee22"
     }
    },
    "d98761db8080484885285be9d309f761": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da4b89a73b6b4963888688cdd2a12a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99a6d2f786504b819609d2062accdd4d",
      "max": 205287522,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e7a3e3b3e0e4b4a825e4905d3d17808",
      "value": 205287522
     }
    },
    "ddfd080832364b008d4c9d5e87adb423": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fabd167d1fb498bbe4b304580744580",
      "placeholder": "",
      "style": "IPY_MODEL_964e02c946804bdda9c0b3eef4ca0e7a",
      "value": "131k/131k[00:00&lt;00:00,837kB/s]"
     }
    },
    "df1201b85a004294a85925204b9b6438": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df1b49fb70ab495888f5b40b8996512c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a518acc761834cb498f0cce4109eaa8f",
      "placeholder": "",
      "style": "IPY_MODEL_7711ba2b9c82489baefee96b7c64eba6",
      "value": "647897/647897[00:15&lt;00:00,99743.43examples/s]"
     }
    },
    "e185802b926d44cfaeadf604733d4a5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4aeae8a8d48a4d2b8ba1df3c7d3a0309",
       "IPY_MODEL_05eca118bd194c2c9d5f7d825bf0493f",
       "IPY_MODEL_27ea6ee950254229ae3493c862fabab7"
      ],
      "layout": "IPY_MODEL_3d1a950d2faf4a11bcd41af7fd1af8eb"
     }
    },
    "e2b60d8ee8d44f42b915088d7deeed77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e43305a97dc64e4ea8bd507df6667fbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fd423f508284764b6e4858dd807ac75",
      "placeholder": "",
      "style": "IPY_MODEL_af7f3773cf0948b0b9ebd92e76aa1f39",
      "value": "205M/205M[00:02&lt;00:00,93.8MB/s]"
     }
    },
    "e55d09be5944478b959af8ef13008d35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e57c379628ce49da904b447a74b0e1b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e66a1bb087944ceca9cbe6517ac19292": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6ae977baad248078458445fecc03963": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e91b73923c044243b5ed8efcfa252f2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea0292273e364ecfb256158407eda9fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eaf2f4d59814487aa8ea1abbf8831da1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebb68cccf04641ed8996e9c9b62c67be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed28850ca87b4c768996f50844c73afe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed92ddd32f6e4301a46e0178f12e5a8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee3c23c50c5b435bbada1bcec39c23b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef246bb6829e4816a009b3f6875e20b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_772ed124915740aeb2f5a614e8cfe2e1",
      "placeholder": "",
      "style": "IPY_MODEL_bf79d1ed09134e50b29aa3b6e28ecde5",
      "value": "Downloadingreadme:100%"
     }
    },
    "efd751b6c2a642ff94845df115af4ed7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f09ab65302ea49f38ad65c7285439b66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89727d06874849f5b681f3705bfe0978",
       "IPY_MODEL_3b939980516140c588220612194de161",
       "IPY_MODEL_fed11544e08c4fb3bf9fe247f7c37cd1"
      ],
      "layout": "IPY_MODEL_1531e137a0b84e3db00599d6311fbc04"
     }
    },
    "f1e10f0376324720bb85d554f677f5f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9656d55a0d3c4fa1bb7a04f5c17b7a84",
      "placeholder": "",
      "style": "IPY_MODEL_108675c4393b4b678a5cfa2f73ce9f4f",
      "value": "1.96M/1.96M[00:00&lt;00:00,10.5MB/s]"
     }
    },
    "f1e7a220b6c14349b5ec041d1cce8889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f22c1b1564234208b5a6a183c5a3bc99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4e2e1c634ab4a07af5483b8dccf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f59b229ad1ad4ab088682c83bb6c5ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f611865e89774fe4ba6dc047f97eaa61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7621ead96516491db1ebb4fba0055702",
      "max": 49969,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_790cc80cfe574ee58da93f8e54c8517f",
      "value": 49969
     }
    },
    "f63b6cc3452e46009e83dbaeb781789b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bce90326f07f4458bf694da20f8c8f00",
      "placeholder": "",
      "style": "IPY_MODEL_7d739e3b12b644bcb9777ae231f55347",
      "value": "Downloadingdata:100%"
     }
    },
    "f7060dd3107d49c5b78b5a71051c0b98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7187ecc5d974ff18fc70470d1a48464": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fab0a887002e467786f3f6980aa57d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44d377128d6748d4b871d2efd101dd19",
      "placeholder": "",
      "style": "IPY_MODEL_c4228273bcdc4d619d23112c5c9a14f0",
      "value": "tokenizer.json:100%"
     }
    },
    "fbc15025bfb441638bf4fe58cb7d08c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc91d0bcafa746cfa509e365fadcafe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fdad79e64e214bfbb4abda9b3eee7eb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fed11544e08c4fb3bf9fe247f7c37cd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e57c379628ce49da904b447a74b0e1b1",
      "placeholder": "",
      "style": "IPY_MODEL_d6217028b7744abdb85c305237cb52a0",
      "value": "49969/49969[01:55&lt;00:00,1256.37examples/s]"
     }
    },
    "ff68b5bdadc143deae98570d99a45bd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
