{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49031,
     "status": "ok",
     "timestamp": 1717658104173,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "2eSvM9zX_2d3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" #\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Home Directory: ['/workspace/.cache/huggingface/']\n",
      "Updated PATH: ['/workspace/anaconda3/envs/autotrain/bin:/workspace/anaconda3/envs/autotrain/bin:/workspace/anaconda3/bin:/workspace/anaconda3/bin:/workspace/anaconda3/condabin:/workspace/anaconda3/bin:/usr/local/cuda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin']\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface_hub in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: filelock in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.6.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0musage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: --version\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /workspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the PIP_CACHE_DIR environment variable\n",
    "os.environ['PIP_CACHE_DIR'] = '/workspace/.pip/'\n",
    "# Set the HF_HOME environment variable to the desired directory\n",
    "os.environ['HF_HOME'] = '/workspace/.cache/huggingface/'\n",
    "\n",
    "# Add /workspace/anaconda3/envs/autotrain/bin/ to the PATH environment variable\n",
    "os.environ['PATH'] = '/workspace/anaconda3/envs/autotrain/bin:' + os.environ['PATH']\n",
    "\n",
    "# Verify the changes\n",
    "hf_home_dir = !echo $HF_HOME\n",
    "path_dirs = !echo $PATH\n",
    "print(\"Hugging Face Home Directory:\", hf_home_dir)\n",
    "print(\"Updated PATH:\", path_dirs)\n",
    "\n",
    "# Install huggingface-cli\n",
    "!pip install huggingface_hub\n",
    "\n",
    "# Verify the installation\n",
    "!huggingface-cli --version\n",
    "\n",
    "# Test the Hugging Face CLI\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  2 20:11:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              60W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Delete model and other training-related objects\n",
    "try:\n",
    "    #del model\n",
    "    del trainer\n",
    "    #del tokenizer\n",
    "    #del train_dataset\n",
    "except NameError:\n",
    "    pass  # Ignore if they are not defined\n",
    "\n",
    "# Invoke garbage collector multiple times\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Reset the CUDA runtime\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Optionally, you can use nvidia-smi command to kill all processes using GPU\n",
    "import os\n",
    "os.system('nvidia-smi')\n",
    "\n",
    "# Optionally, restart the notebook kernel (uncomment if necessary)\n",
    "# import IPython\n",
    "# IPython.display.clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    PeftModelForCausalLM,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, get_last_checkpoint\n",
    "from transformers.utils import send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /workspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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 --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/.cache/huggingface/\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367,
     "referenced_widgets": [
      "e185802b926d44cfaeadf604733d4a5d",
      "4aeae8a8d48a4d2b8ba1df3c7d3a0309",
      "05eca118bd194c2c9d5f7d825bf0493f",
      "27ea6ee950254229ae3493c862fabab7",
      "3d1a950d2faf4a11bcd41af7fd1af8eb",
      "1d5689dd39d7414dbeb92976c9a682c3",
      "aa63039256a643b38ff77fe394a6832f",
      "987d83a2779c42fd99b862fead155f57",
      "943df89e90eb4def9bb8414ecfffc681",
      "d49a277941384dfeb7907b9017d16ecf",
      "efd751b6c2a642ff94845df115af4ed7",
      "4806cee42b644a868e29a19ede8aad49",
      "5380ca6400a1401e8fa1cad69af5c97c",
      "0d920b9593ea41a3ae099ee0c3eb6dbf",
      "cee00c75e5cb405bbd928ff7759c52b8",
      "0cc5119ca0244fd59e2b5b3cb4715005",
      "94fa0935dfb84f61b61af5e27396ab65",
      "6488ca56a5784cad9195c6eb302b0839",
      "a55b27398caf4ae39ac5900d22677220",
      "f7187ecc5d974ff18fc70470d1a48464",
      "c6bab59f1a624d21a6a09f94fbe99e58",
      "ce88a214d07e42399d9edd549175b760",
      "be4f34efcc81414585f78d57ae025a16",
      "29d5c06b48d04bfa99cefe38030f277c",
      "ac0fdfbd66024fcfb7d84d752b8b65de",
      "5eb4b0b839e94e53adffd6a5a28781b1",
      "704e93e3978140e1864b84cf19507b58",
      "4e9990fc07da480a96223afe3c85be2f",
      "abd2784d68354f6388adafe797feb706",
      "16937b1d13dc433ca1605982da3f7587",
      "ae325732051f41ed8949a6026a113e95",
      "baee5199c98e443697d179bf127b3798",
      "25350d0725054c33b7e2e792190bf039",
      "d62c059e650c44ed9bdfd9ea259724ef",
      "b168622cd5844e95be35c6476b3c3847",
      "2a73eec6178640ccac73682222f36a56",
      "0c3a85239fa94b8a85545c8eec38d887",
      "689ab31dd3994630a248b642096bee22",
      "5c9bdfe89d304e7ca23a9ad7ff0302f2",
      "a5ff362daae4430ba27be07304270123",
      "1dd43f29b71042df88ff4c49af908ce3",
      "777c1f497f364f04bb3cfc97d937de0a",
      "e66a1bb087944ceca9cbe6517ac19292",
      "38725355a21844458da20ebf9eaf7d21",
      "2a0095c0f676406c9799779a33225860",
      "91476a34dbb14bde981fdd7743fb860f",
      "2a68f7129c5a4e3bb75c4469a8eda942",
      "b87ddb145c414e53b425345c652699d1",
      "fdad79e64e214bfbb4abda9b3eee7eb6",
      "90924adbead14a25bdbc8e0779e6360a",
      "310f05c91ee14e4796bfa18897f81e26",
      "d208453e5609482c9c5113d3b3c84a6d",
      "b9b86ece90ea4666be175d68bc26a648",
      "fbc15025bfb441638bf4fe58cb7d08c1",
      "f4e2e1c634ab4a07af5483b8dccf8b52",
      "10a7046d9e1c4ada876b2b65e4169de9",
      "026e66bb6fb64494b389198e3f64504b",
      "a893e20bedc6404fb2ab9002d7da7090",
      "1df6a414771544528de945d997ea18a5",
      "ed92ddd32f6e4301a46e0178f12e5a8d",
      "33a88395eec948cdbc9cd038a986729d",
      "2ba358a81db741b7963d4e14136a6bff",
      "8e41f5233d7945469c4099b7b1f6cba9",
      "469da5951d744acdb7e5472e3fae2cb2",
      "eaf2f4d59814487aa8ea1abbf8831da1",
      "5f1cbf83549943cfa3b568ccd5cdb34e",
      "939a4d107645466a8c6633285471ae40",
      "fab0a887002e467786f3f6980aa57d0a",
      "4f8d9958f6694a2cb0d55308f6653cbc",
      "f1e10f0376324720bb85d554f677f5f7",
      "35a82e3cee404c0eb8c4a99d48fde443",
      "44d377128d6748d4b871d2efd101dd19",
      "c4228273bcdc4d619d23112c5c9a14f0",
      "897760bb1650484bbbd2a604d886b68a",
      "ea0292273e364ecfb256158407eda9fc",
      "9656d55a0d3c4fa1bb7a04f5c17b7a84",
      "108675c4393b4b678a5cfa2f73ce9f4f"
     ]
    },
    "executionInfo": {
     "elapsed": 72427,
     "status": "ok",
     "timestamp": 1717658176589,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "4807874d-7225-42ac-e0d3-fe7e3f2de8cd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4959b276eb949a2bece9d72029523e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "meta-llama/Meta-Llama-3-8B does not have a padding token! Will use pad_token = <|reserved_special_token_250|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    #model_name = \"BanglaLLM/BanglaLLama-3-8b-BnWiki-Base\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"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\" # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/logs/*\n"
     ]
    }
   ],
   "source": [
    "log_file_base = \"/workspace/logs/\"\n",
    "existing_log_files = log_file_base + \"*\"\n",
    "print(existing_log_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file should be at: /workspace/logs/BanglaLLM_Llama8B_Uonlp_CulturaX_2024-07-03_21:31:35.962624.log\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback, TrainingArguments, logging as hf_logging\n",
    "from unsloth import is_bfloat16_supported, UnslothTrainer, UnslothTrainingArguments\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Setup the log file name\n",
    "log_file_base = \"/workspace/logs/BanglaLLM_Llama8B_Uonlp_CulturaX_\"\n",
    "log_file = log_file_base + \"_\".join(str(datetime.datetime.now()).split()) + \".log\"\n",
    "\n",
    "existing_log_files = log_file_base + \"*\"\n",
    "#!rm -rf $existing_log_files\n",
    "\n",
    "# Create a dedicated logger\n",
    "logger = logging.getLogger('BanglaLLMLogger')\n",
    "logger.setLevel(logging.DEBUG)  # Set to DEBUG to capture all levels of logs\n",
    "\n",
    "# Create handlers\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters and add it to the handlers\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Redirect Hugging Face Transformers logging to our logger\n",
    "transformers_logger = hf_logging.get_logger()\n",
    "transformers_logger.setLevel(logging.DEBUG)\n",
    "for handler in logger.handlers:\n",
    "    transformers_logger.addHandler(handler)\n",
    "\n",
    "print(f\"Log file should be at: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "\n",
    "We also add `embed_tokens` and `lm_head` to allow the model to learn out of distribution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15122,
     "status": "ok",
     "timestamp": 1717658191701,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "1f0fc2ba-b073-45e5-eba1-7fc51ff64c8f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): ModulesToSaveWrapper(\n",
       "          (original_module): Embedding(128256, 4096)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Embedding(128256, 4096)\n",
       "          )\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a015bbfe004a4216b89004f27f614eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/30.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70aad84bbaa46388798b1109f764b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ee0ec407144e58b80be94a0035b308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/18 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'uonlp/CulturaX' #french novels\n",
    "dataset = load_dataset(dataset_name, \"bn\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'timestamp', 'url', 'source'],\n",
       "        num_rows: 12436596\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/'\n",
    "data_cache_dir = '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00001-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00003-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00000-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00004-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00002-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00005-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00007-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00006-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00065-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00008-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00009-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00015-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00011-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00010-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00018-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00021-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00012-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00022-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00014-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00013-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00017-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00016-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00019-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00020-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00023-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00032-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00033-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00038-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00025-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00035-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00027-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00026-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00024-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00029-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00034-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00047-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00031-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00040-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00030-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00028-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00037-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00046-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00049-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00051-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00039-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00036-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00048-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00042-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00041-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00054-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00050-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00044-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00053-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00052-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00060-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00043-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00057-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00059-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00045-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00055-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00058-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00056-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00062-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00070-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00068-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00067-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00063-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00069-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00061-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00066-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00064-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00072-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00078-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00081-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00073-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00074-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00080-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00071-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00075-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00076-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00086-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00077-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00079-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00083-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00082-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00085-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00084-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00088-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00087-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00089-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00090-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00093-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00092-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00091-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00100-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00094-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00108-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00104-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00096-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00095-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00098-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00107-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00097-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00110-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00101-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00099-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00109-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00105-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00106-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00114-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00113-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00115-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00102-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00117-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00111-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00116-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00112-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00122-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00103-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00120-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00118-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00121-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00125-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00123-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00124-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00127-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00130-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00131-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00132-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00119-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00126-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00129-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00128-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00133-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00134-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00135-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00136-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00138-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00140-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00142-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00144-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00137-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00145-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00139-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00141-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00147-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00149-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00146-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00143-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00148-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00150-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00152-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00156-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00157-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00153-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00154-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00158-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00151-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00159-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00160-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00155-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00161-of-00163.parquet',\n",
       " '/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/cultura_x-train-00162-of-00163.parquet']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(dataset_dir)\n",
    "parquet_files = [str(path) + \"/\" + file.name for file in path.glob(\"*.parquet\")]\n",
    "parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb575e289f0482da5efebb7e5b1a4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bb41f6e23a457e96fc8f5e3745c2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('parquet', data_files=parquet_files, split='train', keep_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'timestamp', 'url', 'source'],\n",
       "    num_rows: 12436596\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset_dir = \"/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/parquet_files/\"\n",
    "\n",
    "# # Convert the Hugging Face dataset to a pandas DataFrame\n",
    "# df = pd.DataFrame(dataset)\n",
    "\n",
    "# # Set the number of chunks equal to the number of files\n",
    "# num_chunks = len(text_files)\n",
    "\n",
    "# # Calculate the chunk size\n",
    "# chunk_size = len(df) // num_chunks\n",
    "\n",
    "# # Save the DataFrame as multiple Parquet files\n",
    "# output_dir = dataset_dir + \"parquet_files\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# for i in range(num_chunks):\n",
    "#     print(f\"processing text_files {text_files[0]}\")\n",
    "#     start_idx = i * chunk_size\n",
    "#     end_idx = (i + 1) * chunk_size if i < num_chunks - 1 else len(df)\n",
    "#     chunk_df = df[start_idx:end_idx]\n",
    "#     output_parquet_path = os.path.join(output_dir, f\"output_dataset_part_{i + 1}.parquet\")\n",
    "#     chunk_df.to_parquet(output_parquet_path, index=False)\n",
    "#     print(f\"Saved chunk {i + 1} as {output_parquet_path}\")\n",
    "\n",
    "# print(\"Dataset saved as multiple Parquet files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = \"/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/\"\n",
    "dataset_dir = dataset_base_dir + \"parquet_files/\"\n",
    "data_cache_dir = dataset_dir + \"uonlp_cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: /workspace/data/Bangla2B+/shards/parquet_files/cache//*\n"
     ]
    }
   ],
   "source": [
    "#!rm -rf $data_cache_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "if block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        logger.warning(\n",
    "            \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n",
    "            \" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n",
    "            \" override this default with `--block_size xxx`.\"\n",
    "        )\n",
    "        block_size = 1024\n",
    "else:\n",
    "    if block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(block_size, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "# since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "tok_logger = transformers.utils.logging.get_logger(\n",
    "    \"transformers.tokenization_utils_base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[\"text\"])\n",
    "    # clm input could be much much longer than block_size\n",
    "    # if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "    #     tok_logger.warning(\n",
    "    #         \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "    #         \" before being passed to the model.\"\n",
    "    #     )\n",
    "    return output\n",
    "\n",
    "# # Example tokenizer function that does not truncate\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True, max_length=block_size)\n",
    "\n",
    "def tokenize_and_check(examples):\n",
    "    result = tokenize_function(examples)\n",
    "    print(f\"Average tokens per text: {sum(len(ids) for ids in result['input_ids']) / len(result['input_ids'])}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_check(examples):\n",
    "    result = tokenize_function(examples)\n",
    "    print(f\"Average tokens per text: {sum(len(ids) for ids in result['input_ids']) / len(result['input_ids'])}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def combine_short_sequences(examples, max_length=256, absolute_max_length=4096):\n",
    "    combined = {k: [] for k in examples.keys()}\n",
    "    current_sequence = {k: [] for k in examples.keys()}\n",
    "    current_length = 0\n",
    "    newline_token_ids = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "    \n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        sequence_length = len(examples['input_ids'][i])\n",
    "        if current_length + sequence_length > max_length: # or current_length + sequence_length > absolute_max_length:\n",
    "            # Add the current combined sequence and start a new one\n",
    "            for k in combined.keys():\n",
    "                combined[k].append(list(chain(*current_sequence[k])))\n",
    "                current_sequence[k] = []\n",
    "            current_length = 0\n",
    "        \n",
    "        # Add the current sequence\n",
    "        for k in examples.keys():\n",
    "            if k == 'input_ids':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append(newline_token_ids)\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            elif k == 'attention_mask':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append([1] * len(newline_token_ids))\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            else:\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "        \n",
    "        current_length += sequence_length\n",
    "        if current_sequence['input_ids']:\n",
    "            current_length += len(newline_token_ids)\n",
    "    \n",
    "    # Add the last combined sequence\n",
    "    for k in combined.keys():\n",
    "        if current_sequence[k]:\n",
    "            combined[k].append(list(chain(*current_sequence[k])))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def group_texts_dynamic(examples):\n",
    "    # Use this before group_texts\n",
    "    examples = combine_short_sequences(examples)\n",
    "\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    block_size = 512\n",
    "    # More granular block size selection\n",
    "    if total_length < 512:\n",
    "        block_size = 512\n",
    "    if total_length < 1024:\n",
    "        block_size = 1024\n",
    "    elif total_length < 2048:\n",
    "        block_size = 2048\n",
    "    elif total_length < 4096:\n",
    "        block_size = 4096\n",
    "    else:\n",
    "        block_size = 8192  # Increase maximum block size, ensure your model can handle this\n",
    "    \n",
    "    # Adaptive padding\n",
    "    padding_unit = 128  # or another suitable value\n",
    "    if total_length < block_size:\n",
    "        block_size = ((total_length + padding_unit - 1) // padding_unit) * padding_unit\n",
    "    \n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "    result = {k: [] for k in concatenated_examples.keys()}\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        block_start = i * block_size\n",
    "        block_end = min((i + 1) * block_size, total_length)\n",
    "        for k, t in concatenated_examples.items():\n",
    "            block = t[block_start:block_end]\n",
    "            if len(block) < block_size:\n",
    "                padding_length = block_size - len(block)\n",
    "                if k == 'input_ids':\n",
    "                    block = block + [tokenizer.pad_token_id] * padding_length\n",
    "                elif k == 'attention_mask':\n",
    "                    block = block + [0] * padding_length\n",
    "            result[k].append(block)\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # Calculate how many blocks we'll create\n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "\n",
    "    # Split by chunks of block_size, allowing the last chunk to be smaller\n",
    "    result = {\n",
    "        k: [t[i * block_size : (i + 1) * block_size] for i in range(num_blocks)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # Create labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'timestamp', 'url', 'source'],\n",
       "    num_rows: 12436596\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00054-of-00163_text_512\n",
      "cultura_x-train-00055-of-00163_512\n",
      "cultura_x-train-00055-of-00163_text_512\n",
      "cultura_x-train-00056-of-00163_512\n",
      "cultura_x-train-00056-of-00163_text_512\n",
      "cultura_x-train-00057-of-00163_512\n",
      "cultura_x-train-00057-of-00163_text_512\n",
      "cultura_x-train-00058-of-00163_512\n",
      "cultura_x-train-00058-of-00163_text_512\n",
      "cultura_x-train-00059-of-00163_512\n",
      "cultura_x-train-00059-of-00163_text_512\n",
      "cultura_x-train-00060-of-00163_512\n",
      "cultura_x-train-00060-of-00163_text_512\n",
      "cultura_x-train-00061-of-00163_512\n",
      "cultura_x-train-00061-of-00163_text_512\n",
      "cultura_x-train-00062-of-00163_512\n",
      "cultura_x-train-00062-of-00163_text_512\n",
      "cultura_x-train-00063-of-00163_512\n",
      "cultura_x-train-00063-of-00163_text_512\n",
      "cultura_x-train-00064-of-00163_512\n",
      "cultura_x-train-00064-of-00163_text_512\n",
      "cultura_x-train-00065-of-00163_512\n",
      "cultura_x-train-00065-of-00163_text_512\n",
      "cultura_x-train-00066-of-00163_512\n",
      "cultura_x-train-00066-of-00163_text_512\n",
      "cultura_x-train-00067-of-00163_512\n",
      "cultura_x-train-00067-of-00163_text_512\n",
      "cultura_x-train-00068-of-00163_512\n",
      "cultura_x-train-00068-of-00163_text_512\n",
      "cultura_x-train-00069-of-00163_512\n",
      "cultura_x-train-00069-of-00163_text_512\n",
      "cultura_x-train-00070-of-00163_512\n",
      "cultura_x-train-00070-of-00163_text_512\n",
      "cultura_x-train-00071-of-00163_512\n",
      "cultura_x-train-00071-of-00163_text_512\n",
      "cultura_x-train-00072-of-00163_512\n",
      "cultura_x-train-00072-of-00163_text_512\n",
      "cultura_x-train-00073-of-00163_512\n",
      "cultura_x-train-00073-of-00163_text_512\n",
      "cultura_x-train-00074-of-00163_512\n",
      "cultura_x-train-00074-of-00163_text_512\n",
      "cultura_x-train-00075-of-00163_512\n",
      "cultura_x-train-00075-of-00163_text_512\n",
      "cultura_x-train-00076-of-00163_512\n",
      "cultura_x-train-00076-of-00163_text_512\n",
      "cultura_x-train-00077-of-00163_512\n",
      "cultura_x-train-00077-of-00163_text_512\n",
      "cultura_x-train-00078-of-00163_512\n",
      "cultura_x-train-00078-of-00163_text_512\n",
      "cultura_x-train-00079-of-00163_512\n",
      "cultura_x-train-00079-of-00163_text_512\n",
      "cultura_x-train-00080-of-00163_512\n",
      "cultura_x-train-00080-of-00163_text_512\n",
      "cultura_x-train-00081-of-00163_512\n",
      "cultura_x-train-00081-of-00163_text_512\n",
      "cultura_x-train-00082-of-00163_512\n",
      "cultura_x-train-00082-of-00163_text_512\n",
      "cultura_x-train-00083-of-00163_512\n",
      "cultura_x-train-00083-of-00163_text_512\n",
      "cultura_x-train-00084-of-00163_512\n",
      "cultura_x-train-00084-of-00163_text_512\n",
      "cultura_x-train-00085-of-00163_512\n",
      "cultura_x-train-00085-of-00163_text_512\n",
      "cultura_x-train-00086-of-00163_512\n",
      "cultura_x-train-00086-of-00163_text_512\n",
      "cultura_x-train-00087-of-00163_512\n",
      "cultura_x-train-00087-of-00163_text_512\n",
      "cultura_x-train-00088-of-00163_512\n",
      "cultura_x-train-00088-of-00163_text_512\n",
      "cultura_x-train-00089-of-00163_512\n",
      "cultura_x-train-00089-of-00163_text_512\n",
      "cultura_x-train-00090-of-00163_512\n",
      "cultura_x-train-00090-of-00163_text_512\n",
      "cultura_x-train-00091-of-00163_512\n",
      "cultura_x-train-00091-of-00163_text_512\n",
      "cultura_x-train-00092-of-00163_512\n",
      "cultura_x-train-00092-of-00163_text_512\n",
      "cultura_x-train-00093-of-00163_512\n",
      "cultura_x-train-00093-of-00163_text_512\n",
      "cultura_x-train-00094-of-00163_512\n",
      "cultura_x-train-00094-of-00163_text_512\n",
      "cultura_x-train-00095-of-00163_512\n",
      "cultura_x-train-00095-of-00163_text_512\n",
      "cultura_x-train-00096-of-00163_512\n",
      "cultura_x-train-00096-of-00163_text_512\n",
      "cultura_x-train-00097-of-00163_512\n",
      "cultura_x-train-00097-of-00163_text_512\n",
      "cultura_x-train-00098-of-00163_512\n",
      "cultura_x-train-00098-of-00163_text_512\n",
      "cultura_x-train-00099-of-00163_512\n",
      "cultura_x-train-00099-of-00163_text_512\n",
      "cultura_x-train-00100-of-00163_512\n",
      "cultura_x-train-00100-of-00163_text_512\n",
      "cultura_x-train-00101-of-00163_512\n",
      "cultura_x-train-00101-of-00163_text_512\n",
      "cultura_x-train-00102-of-00163_512\n",
      "cultura_x-train-00102-of-00163_text_512\n",
      "cultura_x-train-00103-of-00163_512\n",
      "cultura_x-train-00103-of-00163_text_512\n",
      "cultura_x-train-00104-of-00163_512\n",
      "cultura_x-train-00104-of-00163_text_512\n",
      "cultura_x-train-00105-of-00163_512\n",
      "cultura_x-train-00105-of-00163_text_512\n",
      "cultura_x-train-00106-of-00163_512\n",
      "cultura_x-train-00106-of-00163_text_512\n",
      "cultura_x-train-00107-of-00163_512\n",
      "cultura_x-train-00107-of-00163_text_512\n",
      "cultura_x-train-00108-of-00163_512\n",
      "cultura_x-train-00108-of-00163_text_512\n",
      "cultura_x-train-00109-of-00163_512\n",
      "cultura_x-train-00109-of-00163_text_512\n",
      "cultura_x-train-00110-of-00163_512\n",
      "cultura_x-train-00110-of-00163_text_512\n",
      "cultura_x-train-00111-of-00163_512\n",
      "cultura_x-train-00111-of-00163_text_512\n",
      "cultura_x-train-00112-of-00163_512\n",
      "cultura_x-train-00112-of-00163_text_512\n",
      "cultura_x-train-00113-of-00163_512\n",
      "cultura_x-train-00113-of-00163_text_512\n",
      "cultura_x-train-00114-of-00163_512\n",
      "cultura_x-train-00114-of-00163_text_512\n",
      "cultura_x-train-00115-of-00163_512\n",
      "cultura_x-train-00115-of-00163_text_512\n",
      "cultura_x-train-00116-of-00163_512\n",
      "cultura_x-train-00116-of-00163_text_512\n",
      "cultura_x-train-00117-of-00163_512\n",
      "cultura_x-train-00117-of-00163_text_512\n",
      "cultura_x-train-00118-of-00163_512\n",
      "cultura_x-train-00118-of-00163_text_512\n",
      "cultura_x-train-00119-of-00163_512\n",
      "cultura_x-train-00119-of-00163_text_512\n",
      "cultura_x-train-00120-of-00163_512\n",
      "cultura_x-train-00120-of-00163_text_512\n",
      "cultura_x-train-00121-of-00163_512\n",
      "cultura_x-train-00121-of-00163_text_512\n",
      "cultura_x-train-00122-of-00163_512\n",
      "cultura_x-train-00122-of-00163_text_512\n",
      "cultura_x-train-00123-of-00163_512\n",
      "cultura_x-train-00123-of-00163_text_512\n",
      "cultura_x-train-00124-of-00163_512\n",
      "cultura_x-train-00124-of-00163_text_512\n",
      "cultura_x-train-00125-of-00163_512\n",
      "cultura_x-train-00125-of-00163_text_512\n",
      "cultura_x-train-00126-of-00163_512\n",
      "cultura_x-train-00126-of-00163_text_512\n",
      "cultura_x-train-00127-of-00163_512\n",
      "cultura_x-train-00127-of-00163_text_512\n",
      "cultura_x-train-00128-of-00163_512\n",
      "cultura_x-train-00128-of-00163_text_512\n",
      "cultura_x-train-00129-of-00163_512\n",
      "cultura_x-train-00129-of-00163_text_512\n",
      "cultura_x-train-00130-of-00163_512\n",
      "cultura_x-train-00130-of-00163_text_512\n",
      "cultura_x-train-00131-of-00163_512\n",
      "cultura_x-train-00131-of-00163_text_512\n",
      "cultura_x-train-00132-of-00163_512\n",
      "cultura_x-train-00132-of-00163_text_512\n",
      "cultura_x-train-00133-of-00163_512\n",
      "cultura_x-train-00133-of-00163_text_512\n",
      "cultura_x-train-00134-of-00163_512\n",
      "cultura_x-train-00134-of-00163_text_512\n",
      "cultura_x-train-00135-of-00163_512\n",
      "cultura_x-train-00135-of-00163_text_512\n",
      "cultura_x-train-00136-of-00163_512\n",
      "cultura_x-train-00136-of-00163_text_512\n",
      "cultura_x-train-00137-of-00163_512\n",
      "cultura_x-train-00137-of-00163_text_512\n",
      "cultura_x-train-00138-of-00163_512\n",
      "cultura_x-train-00138-of-00163_text_512\n",
      "cultura_x-train-00140-of-00163_512\n",
      "cultura_x-train-00140-of-00163_parquet_512\n",
      "cultura_x-train-00140-of-00163_text_512\n",
      "cultura_x-train-00142-of-00163_512\n",
      "cultura_x-train-00142-of-00163_text_512\n",
      "cultura_x-train-00144-of-00163_512\n",
      "cultura_x-train-00144-of-00163_text_512\n",
      "tmp_cache\n"
     ]
    }
   ],
   "source": [
    "!ls $data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: /workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache//*\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $data_cache_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"à¦¯à§‡ à¦›à¦¯à¦¼ à¦•à¦¾à¦°à¦£à§‡ à¦˜à§à¦°à§‡ à¦¦à¦¾à¦à¦¡à¦¼à¦¿à¦¯à¦¼à§‡à¦›à§‡ à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦¬à¦¾à¦œà¦¾à¦° â€“ SSTV\\nHome / à¦¸à¦¾à¦°à¦¾à¦¦à§‡à¦¶ / à¦¯à§‡ à¦›à¦¯à¦¼ à¦•à¦¾à¦°à¦£à§‡ à¦˜à§à¦°à§‡ à¦¦à¦¾à¦à¦¡à¦¼à¦¿à¦¯à¦¼à§‡à¦›à§‡ à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦¬à¦¾à¦œà¦¾à¦°\\nà¦¸à§à¦¬à¦¾à¦­à¦¾à¦¬à¦¿à¦• à¦¸à¦®à¦¯à¦¼à§‡ à¦…à¦¸à§à¦¥à¦¿à¦°à¦¤à¦¾ à¦¥à¦¾à¦•à¦²à§‡à¦“ à¦•à¦°à§‹à¦¨à¦¾à¦•à¦¾à¦²à§‡ à¦¸à§à¦¬à¦¾à¦¤à¦¾à¦¸ à¦¬à¦‡à¦›à§‡ à¦¦à§‡à¦¶à§‡à¦° à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦¬à¦¾à¦œà¦¾à¦°à§‡à¥¤ à¦Ÿà¦¾à¦¨à¦¾ à¦—à¦¤ à§¯ à¦¸à¦ªà§à¦¤à¦¾à¦¹ à¦§à¦°à§‡ à¦¢à¦¾à¦•à¦¾ à¦¸à§à¦Ÿà¦• à¦à¦•à§à¦¸à¦šà§‡à¦žà§à¦œà§‡à¦° (à¦¡à¦¿à¦à¦¸à¦‡) à¦ªà§à¦°à¦§à¦¾à¦¨ à¦®à§‚à¦²à§à¦¯ à¦¸à§‚à¦šà¦• à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡à¥¤ à¦à¦° à¦¸à¦™à§à¦—à§‡ à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à¦²à§‡à¦¨à¦¦à§‡à¦¨à¦“à¥¤ à¦¶à§à¦§à§à¦¤à¦¾à¦‡ à¦¨à¦¯à¦¼, à¦¸à§‚à¦šà¦• à¦“ à¦²à§‡à¦¨à¦¦à§‡à¦¨ à¦¬à¦¾à¦¡à¦¼à¦¾à¦° à¦ªà§à¦°à¦­à¦¾à¦¬à§‡ à¦¡à¦¿à¦à¦¸à¦‡'à¦° à¦¬à¦¾à¦œà¦¾à¦° à¦®à§‚à¦²à¦§à¦¨ à¦…à¦¬à§à¦¯à¦¾à¦¹à¦¤à¦­à¦¾à¦¬à§‡ à¦¬à§‡à¦¡à¦¼à§‡à¦‡ à¦šà¦²à§‡à¦›à§‡à¥¤ à¦…à¦°à§à¦¥à¦¨à§€à¦¤à¦¿à¦° à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ à¦à¦‡ à¦¸à§‡à¦•à§à¦Ÿà¦° à¦¨à¦¿à¦¯à¦¼à§‡ à¦¸à¦°à¦•à¦¾à¦°à¦¿-à¦¬à§‡à¦¸à¦°à¦•à¦¾à¦°à¦¿ à¦¸à¦¬ à¦®à¦¹à¦²à§‡à¦‡ à¦à¦–à¦¨ à¦†à¦—à§à¦°à¦¹ à¦¬à¦¾à¦¡à¦¼à¦›à§‡à¥¤ à¦ªà§à¦°à¦¾à¦¯à¦¼ à§§à§¦ à¦¬à¦›à¦° à¦ªà¦° à¦…à¦¨à§‡à¦•à§‡à¦‡ à¦¸à¦®à§à¦­à¦¾à¦¬à¦¨à¦¾ à¦¦à§‡à¦–à¦›à§‡à¦¨ à¦à¦‡ à¦¬à¦¾à¦œà¦¾à¦° à¦¨à¦¿à¦¯à¦¼à§‡à¥¤ à¦à¦°à¦‡ à¦®à¦§à§à¦¯à§‡ à¦¬à¦¾à¦œà¦¾à¦°à§‡ à¦¨à¦¤à§à¦¨ à¦¨à¦¤à§à¦¨ à¦¬à¦¿à¦¨à¦¿à¦¯à¦¼à§‹à¦—à¦“ à¦†à¦¸à¦¤à§‡ à¦¶à§à¦°à§ à¦•à¦°à§‡à¦›à§‡à¥¤\\nà¦œà¦¾à¦¨à¦¾ à¦—à§‡à¦›à§‡, à¦¤à¦¾à¦²à¦¿à¦­à§à¦•à§à¦¤ à¦•à§‹à¦®à§à¦ªà¦¾à¦¨à¦¿à¦—à§à¦²à§‹à¦° à¦®à¦§à§à¦¯à§‡ à¦¸à§à¦¶à¦¾à¦¸à¦¨ à¦¨à¦¿à¦¶à§à¦šà¦¿à¦¤ à¦•à¦°à¦¤à§‡ à¦šà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦•à¦®à¦¿à¦¶à¦¨à¥¤ à¦à¦°à¦‡ à¦®à¦§à§à¦¯à§‡ à¦¬à§‡à¦¶ à¦•à¦¿à¦›à§ à¦ªà¦¦à¦•à§à¦·à§‡à¦ªà¦“ à¦¨à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡à¥¤ à¦¯à¦¾à¦° à¦ªà§à¦°à¦­à¦¾à¦¬ à¦ªà¦¡à¦¼à¦›à§‡ à¦¬à¦¾à¦œà¦¾à¦°à§‡à¥¤ à¦¡à¦¿à¦à¦¸à¦‡ à¦¬à§à¦°à§‹à¦•à¦¾à¦°à§‡à¦œ à¦…à§à¦¯à¦¾à¦¸à§‹à¦¸à¦¿à¦¯à¦¼à§‡à¦¶à¦¨ à¦…à¦¬ à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§‡à¦° (à¦¡à¦¿à¦¬à¦¿à¦) à¦¸à¦¾à¦¬à§‡à¦• à¦¸à¦­à¦¾à¦ªà¦¤à¦¿ à¦†à¦¹à¦®à§‡à¦¦ à¦°à¦¶à¦¿à¦¦ à¦²à¦¾à¦²à§€ à¦¬à¦¾à¦œà¦¾à¦° à¦˜à§à¦°à§‡ à¦¦à¦¾à¦à¦¡à¦¼à¦¾à¦¨à§‹à¦° à¦›à¦¯à¦¼à¦Ÿà¦¿ à¦•à¦¾à¦°à¦£ à¦¤à§à¦²à§‡ à¦§à¦°à§‡à¦›à§‡à¦¨à¥¤ à¦à¦—à§à¦²à§‹ à¦¹à¦²à§‹:\\nà¦†à¦¹à¦®à§‡à¦¦ à¦°à¦¶à¦¿à¦¦ à¦²à¦¾à¦²à§€ à¦œà¦¾à¦¨à¦¾à¦¨, à¦—à¦¤ à§§à§¦ à¦¬à¦›à¦°à§‡ à¦…à¦¨à§à¦¤à¦¤ à§¨à§­ à¦¬à¦¾à¦° à¦¬à¦¾à¦œà¦¾à¦° à¦‰à¦ à§‡ à¦¦à¦¾à¦à¦¡à¦¼à¦¾à¦¬à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§‡à¦›à§‡à¥¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦¸à¦°à¦•à¦¾à¦°à¦¿ à¦¸à¦‚à¦¸à§à¦¥à¦¾à¦—à§à¦²à§‹à¦° à¦‡à¦¨à§à¦Ÿà¦¾à¦°à¦­à§‡à¦¨à¦¶à¦¨ à¦“ à¦‡à¦¨à¦¡à§‡à¦•à§à¦¸ à¦•à¦¨à¦Ÿà§à¦°à§‹à¦² à¦•à¦°à¦¾à¦° à¦•à¦¾à¦°à¦£à§‡ à¦¸à§‡à¦Ÿà¦¾ à¦ªà¦¾à¦°à§‡à¦¨à¦¿à¥¤ à¦•à¦¾à¦°à¦£, à¦¬à¦¾à¦œà¦¾à¦° à¦¯à¦–à¦¨à¦‡ à¦‰à¦ à§‡ à¦¦à¦¾à¦à¦¡à¦¼à¦¾à¦¨à§‹à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§‡à¦›à§‡ à¦¤à¦–à¦¨à¦‡ à¦šà¦¾à¦ª à¦ªà§à¦°à¦¯à¦¼à§‹à¦— à¦•à¦°à§‡ à¦¬à¦¾à¦œà¦¾à¦° à¦¨à¦¿à¦¯à¦¼à¦¨à§à¦¤à§à¦°à¦£ à¦•à¦°à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡à¥¤ à¦¤à¦¿à¦¨à¦¿ à¦®à¦¨à§‡ à¦•à¦°à§‡à¦¨, à¦à¦‡ à¦¬à¦¾à¦œà¦¾à¦°à¦•à§‡ à¦à¦–à¦¨ à¦¨à¦¿à¦œà¦¸à§à¦¬ à¦¶à¦•à§à¦¤à¦¿à¦¤à§‡ à¦šà¦²à¦¤à§‡ à¦¦à¦¿à¦¤à§‡ à¦¹à¦¬à§‡à¥¤ à¦¬à¦¾à¦œà¦¾à¦°à§‡ à¦•à§‹à¦¨à¦“à¦­à¦¾à¦¬à§‡à¦‡ à¦‡à¦¨à§à¦Ÿà¦¾à¦°à¦­à§‡à¦¨à¦¶à¦¨ à¦“ à¦‡à¦¨à¦¡à§‡à¦•à§à¦¸ à¦•à¦¨à¦Ÿà§à¦°à§‹à¦² à¦•à¦°à¦¾ à¦ à¦¿à¦• à¦¹à¦¬à§‡ à¦¨à¦¾à¥¤ à¦à¦›à¦¾à¦¡à¦¼à¦¾ à¦¸à¦¿à¦¨à§à¦¡à¦¿à¦•à§‡à¦Ÿà§‡à¦¡ à¦°à§‡à¦Ÿ (à¦•à¦¯à¦¼à§‡à¦•à¦Ÿà¦¿ à¦¬à§à¦°à§‹à¦•à¦¾à¦°à§‡à¦œ à¦¹à¦¾à¦‰à¦œ à¦¸à¦¿à¦¨à§à¦¡à¦¿à¦•à§‡à¦Ÿ à¦•à¦°à§‡ à¦•à§‹à¦¨à¦“ à¦•à§‹à¦®à§à¦ªà¦¾à¦¨à¦¿à¦° à¦¦à¦¾à¦® à¦¨à¦¿à¦œà§‡à¦°à¦¾ à¦¬à¦¾à¦¡à¦¼à¦¿à¦¯à¦¼à§‡ à¦“ à¦•à¦®à¦¿à¦¯à¦¼à§‡ à¦¬à§‡à¦šà¦¾-à¦•à§‡à¦¨à¦¾ à¦•à¦°à§‡) à¦¨à¦¿à¦¯à¦¼à¦¨à§à¦¤à§à¦°à¦£ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡à¥¤ à¦†à¦° à¦¬à¦¾à¦œà¦¾à¦°à§‡ à¦•à§‹à¦¨à¦“ à¦ªà¦•à§à¦·à¦‡ à¦¯à¦¾à¦¤à§‡ à¦¬à§à¦¯à¦¾à¦¡ à¦ªà§à¦²à§‡ à¦•à¦°à¦¤à§‡ à¦¨à¦¾ à¦ªà¦¾à¦°à§‡ à¦¸à§‡ à¦¦à¦¿à¦•à§‡à¦“ à¦¨à¦œà¦° à¦°à¦¾à¦–à¦¾à¦° à¦ªà¦°à¦¾à¦®à¦°à§à¦¶ à¦¦à§‡à¦¨ à¦¤à¦¿à¦¨à¦¿à¥¤\\nà¦¤à¦¿à¦¨à¦¿ à¦¬à¦²à§‡à¦¨, 'à¦¬à¦¾à¦œà¦¾à¦° à¦¬à¦¿à¦¨à¦¿à¦¯à¦¼à§‹à¦—à¦•à¦¾à¦°à§€ à¦¬à¦¾à¦¨à§à¦§à¦¬ à¦¹à¦¬à§‡ à¦¤à¦–à¦¨à¦‡, à¦¯à¦–à¦¨ à¦¸à¦¬ à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¾à¦¨à¦—à§à¦²à§‹à¦° à¦®à¦§à§à¦¯à§‡ à¦¸à§à¦¶à¦¾à¦¸à¦¨ à¦¥à¦¾à¦•à¦¬à§‡à¥¤'\\nà¦à¦•à¦®à¦¤ à¦ªà§‹à¦·à¦£ à¦•à¦°à§‡ à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ à¦‰à¦¨à§à¦¨à¦¯à¦¼à¦¨ à¦—à¦¬à§‡à¦·à¦£à¦¾ à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¾à¦¨à§‡à¦° (à¦¬à¦¿à¦†à¦‡à¦¡à¦¿à¦à¦¸) à¦—à¦¬à§‡à¦·à¦• à¦“ à¦…à¦—à§à¦°à¦£à§€ à¦¬à§à¦¯à¦¾à¦‚à¦•à§‡à¦° à¦šà§‡à¦¯à¦¼à¦¾à¦°à¦®à§à¦¯à¦¾à¦¨ à¦¡. à¦œà¦¾à¦¯à¦¼à§‡à¦¦ à¦¬à¦–à¦¤ à¦¬à¦²à§‡à¦¨, 'à¦à¦•à¦¦à¦¿à¦•à§‡ à¦¬à¦¾à¦œà¦¾à¦°à§‡ à¦¤à¦¾à¦°à¦²à§à¦¯à§‡à¦° à¦¸à¦®à¦¸à§à¦¯à¦¾à¦° à¦¸à¦®à¦¾à¦§à¦¾à¦¨ à¦¹à¦¯à¦¼à§‡à¦›à§‡à¥¤ à¦…à¦¨à§à¦¯à¦¦à¦¿à¦•à§‡ à¦«à§à¦²à§‹à¦° à¦ªà§à¦°à¦¾à¦‡à¦¸ à¦¥à¦¾à¦•à¦¾à¦° à¦•à¦¾à¦°à¦£à§‡ à¦•à§à¦·à§à¦¦à§à¦° à¦¬à¦¿à¦¨à¦¿à¦¯à¦¼à§‹à¦—à¦•à¦¾à¦°à§€à¦°à¦¾ à¦‡à¦¤à¦¿à¦¬à¦¾à¦šà¦•à¦­à¦¾à¦¬à§‡ à¦¦à§‡à¦–à¦›à§‡à¦¨à¥¤ à¦à¦›à¦¾à¦¡à¦¼à¦¾ à¦¨à¦¿à¦¯à¦¼à¦¨à§à¦¤à§à¦°à¦• à¦¸à¦‚à¦¸à§à¦¥à¦¾à¦° à¦•à¦¿à¦›à§ à¦­à§‚à¦®à¦¿à¦•à¦¾à¦° à¦•à¦¾à¦°à¦£à§‡à¦“ à¦¬à¦¾à¦œà¦¾à¦°à§‡ à¦‡à¦¤à¦¿à¦¬à¦¾à¦šà¦• à¦ªà§à¦°à¦­à¦¾à¦¬ à¦ªà¦¡à¦¼à¦›à§‡à¥¤'\\nà¦à¦¦à¦¿à¦•à§‡ à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹ à¦œà§à¦¡à¦¼à§‡ à¦¡à¦¿à¦à¦¸à¦‡'à¦° à¦ªà§à¦°à¦§à¦¾à¦¨ à¦®à§‚à¦²à§à¦¯ à¦¸à§‚à¦šà¦• à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à¦ªà§à¦°à¦¾à¦¯à¦¼ à¦¦à§à¦‡ à¦¶à¦¤à¦¾à¦‚à¦¶à¥¤ à¦à¦° à¦¸à¦™à§à¦—à§‡ à¦²à§‡à¦¨à¦¦à§‡à¦¨ à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à¦ªà§à¦°à¦¾à¦¯à¦¼ à¦ªà¦¾à¦à¦š à¦¶à¦¤à¦¾à¦‚à¦¶à¥¤ à¦¸à§‚à¦šà¦• à¦“ à¦²à§‡à¦¨à¦¦à§‡à¦¨à§‡à¦° à¦¬à¦¡à¦¼ à¦‰à¦¤à§à¦¥à¦¾à¦¨à§‡à¦° à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡ à¦¡à¦¿à¦à¦¸à¦‡à¦° à¦¬à¦¾à¦œà¦¾à¦° à¦®à§‚à¦²à¦§à¦¨ à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à§ª à¦¹à¦¾à¦œà¦¾à¦° à§­à§¦à§¨ à¦•à§‹à¦Ÿà¦¿ à¦Ÿà¦¾à¦•à¦¾à¥¤\\nà¦¸à§‚à¦šà¦• à¦“ à¦¬à¦¾à¦œà¦¾à¦° à¦®à§‚à¦²à¦§à¦¨ à¦¬à¦¾à¦¡à¦¼à¦¾à¦° à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡ à¦¡à¦¿à¦à¦¸à¦‡à¦¤à§‡ à¦²à§‡à¦¨à¦¦à§‡à¦¨à§‡à¦° à¦—à¦¤à¦¿à¦“ à¦¯à¦¥à§‡à¦·à§à¦Ÿ à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡à¥¤ à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡à¦° à¦ªà§à¦°à¦¤à¦¿ à¦•à¦¾à¦°à§à¦¯à¦¦à¦¿à¦¬à¦¸à§‡ à¦¡à¦¿à¦à¦¸à¦‡à¦¤à§‡ à¦—à¦¡à¦¼à§‡ à¦²à§‡à¦¨à¦¦à§‡à¦¨ à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦à¦• à¦¹à¦¾à¦œà¦¾à¦° à§§à§­à§¯ à¦•à§‹à¦Ÿà¦¿ à§¬à§¦ à¦²à¦¾à¦– à¦Ÿà¦¾à¦•à¦¾à¥¤ à¦†à¦—à§‡à¦° à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡ à¦ªà§à¦°à¦¤à¦¿à¦¦à¦¿à¦¨ à¦—à¦¡à¦¼à§‡ à¦²à§‡à¦¨à¦¦à§‡à¦¨ à¦¹à¦¯à¦¼ à¦à¦• à¦¹à¦¾à¦œà¦¾à¦° à§§à§¨à§¬ à¦•à§‹à¦Ÿà¦¿ à§¨à§¨ à¦²à¦¾à¦– à¦Ÿà¦¾à¦•à¦¾à¥¤ à¦…à¦°à§à¦¥à¦¾à§Ž à¦ªà§à¦°à¦¤à¦¿ à¦•à¦¾à¦°à§à¦¯à¦¦à¦¿à¦¬à¦¸à§‡ à¦—à¦¡à¦¼ à¦²à§‡à¦¨à¦¦à§‡à¦¨ à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à§«à§© à¦•à§‹à¦Ÿà¦¿ à§©à§® à¦²à¦¾à¦– à¦Ÿà¦¾à¦•à¦¾ à¦¬à¦¾ à§ª à¦¦à¦¶à¦®à¦¿à¦• à§­à§ª à¦¶à¦¤à¦¾à¦‚à¦¶à¥¤\\nà¦†à¦° à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à¦œà§à¦¡à¦¼à§‡ à¦¡à¦¿à¦à¦¸à¦‡à¦¤à§‡ à¦®à§‹à¦Ÿ à¦²à§‡à¦¨à¦¦à§‡à¦¨ à¦¹à¦¯à¦¼à§‡à¦›à§‡ à§« à¦¹à¦¾à¦œà¦¾à¦° à§®à§¯à§® à¦•à§‹à¦Ÿà¦¿ à¦Ÿà¦¾à¦•à¦¾à¥¤ à¦†à¦—à§‡à¦° à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡ à¦²à§‡à¦¨à¦¦à§‡à¦¨ à¦¹à¦¯à¦¼à§‡à¦›à§‡ à§ª à¦¹à¦¾à¦œà¦¾à¦° à§«à§¦à§ª à¦•à§‹à¦Ÿà¦¿ à§®à§¯ à¦²à¦¾à¦– à¦Ÿà¦¾à¦•à¦¾à¥¤ à¦¸à§‡ à¦¹à¦¿à¦¸à¦¾à¦¬à§‡ à¦®à§‹à¦Ÿ à¦²à§‡à¦¨à¦¦à§‡à¦¨ à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à¦à¦• à¦¹à¦¾à¦œà¦¾à¦° à§©à§¯à§© à¦•à§‹à¦Ÿà¦¿ à§§à§§ à¦²à¦¾à¦– à¦Ÿà¦¾à¦•à¦¾à¥¤\\nà¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à¦œà§à¦¡à¦¼à§‡ à¦¡à¦¿à¦à¦¸à¦‡à¦° à¦ªà§à¦°à¦§à¦¾à¦¨ à¦¸à§‚à¦šà¦• à¦¡à¦¿à¦à¦¸à¦‡à¦à¦•à§à¦¸ à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à§¯à§¦ à¦¦à¦¶à¦®à¦¿à¦• à§­à§« à¦ªà¦¯à¦¼à§‡à¦¨à§à¦Ÿ à¦¬à¦¾ à§§ à¦¦à¦¶à¦®à¦¿à¦• à§¯à§© à¦¶à¦¤à¦¾à¦‚à¦¶à¥¤ à¦Ÿà¦¾à¦¨à¦¾ à§¯ à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡à¦° à¦‰à¦¤à§à¦¥à¦¾à¦¨à§‡ à¦¡à¦¿à¦à¦¸à¦‡à¦° à¦ªà§à¦°à¦§à¦¾à¦¨ à¦®à§‚à¦²à§à¦¯ à¦¸à§‚à¦šà¦• à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à§®à§©à§§ à¦ªà¦¯à¦¼à§‡à¦¨à§à¦Ÿà¥¤ à¦ªà§à¦°à¦§à¦¾à¦¨ à¦®à§‚à¦²à§à¦¯ à¦¸à§‚à¦šà¦•à§‡à¦° à¦ªà¦¾à¦¶à¦¾à¦ªà¦¾à¦¶à¦¿ à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡ à¦‰à¦¤à§à¦¥à¦¾à¦¨ à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦¡à¦¿à¦à¦¸à¦‡à¦° à¦¶à¦°à¦¿à¦¯à¦¼à¦¾à¦¹ à¦¸à§‚à¦šà¦•à§‡à¦°à¦“à¥¤ à¦¶à¦°à¦¿à¦¯à¦¼à¦¾à¦¹ à¦­à¦¿à¦¤à§à¦¤à¦¿à¦¤à§‡ à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¿à¦¤ à¦•à§‹à¦®à§à¦ªà¦¾à¦¨à¦¿ à¦¨à¦¿à¦¯à¦¼à§‡ à¦—à¦ à¦¿à¦¤ à¦à¦‡ à¦¸à§‚à¦šà¦•à¦Ÿà¦¿ à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡ à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à§§à§§ à¦¦à¦¶à¦®à¦¿à¦• à§®à§® à¦ªà¦¯à¦¼à§‡à¦¨à§à¦Ÿ à¦¬à¦¾ à§§ à¦¦à¦¶à¦®à¦¿à¦• à¦¶à§‚à¦¨à§à¦¯ à§¯ à¦¶à¦¤à¦¾à¦‚à¦¶à¥¤ à¦à¦‡ à¦¸à§‚à¦šà¦•à¦Ÿà¦¿à¦“ à¦Ÿà¦¾à¦¨à¦¾ à§¯ à¦¸à¦ªà§à¦¤à¦¾à¦¹ à¦¬à¦¾à¦¡à¦¼à¦²à¥¤\\nà¦¬à¦¾à¦›à¦¾à¦‡ à¦•à¦°à¦¾ à¦­à¦¾à¦²à§‹ à¦•à§‹à¦®à§à¦ªà¦¾à¦¨à¦¿ à¦¨à¦¿à¦¯à¦¼à§‡ à¦—à¦ à¦¿à¦¤ à¦¡à¦¿à¦à¦¸à¦‡à¦° à¦†à¦°à§‡à¦• à¦¸à§‚à¦šà¦• à¦¡à¦¿à¦à¦¸à¦‡-à§©à§¦à¥¤ à¦à¦‡ à¦¸à§‚à¦šà¦•à¦Ÿà¦¿ à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡ à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡ à§«à§¨ à¦¦à¦¶à¦®à¦¿à¦• à§­à§¬ à¦ªà¦¯à¦¼à§‡à¦¨à§à¦Ÿ à¦¬à¦¾ à§© à¦¦à¦¶à¦®à¦¿à¦• à§©à§§ à¦¶à¦¤à¦¾à¦‚à¦¶à¥¤\\nà¦à¦¦à¦¿à¦•à§‡ à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à¦œà§à¦¡à¦¼à§‡ à¦¡à¦¿à¦à¦¸à¦‡à¦¤à§‡ à¦²à§‡à¦¨à¦¦à§‡à¦¨à§‡ à¦…à¦‚à¦¶ à¦¨à§‡à¦“à¦¯à¦¼à¦¾ à§§à§®à§¨à¦Ÿà¦¿ à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¾à¦¨à§‡à¦° à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦“ à¦‡à¦‰à¦¨à¦¿à¦Ÿà§‡à¦° à¦¦à¦¾à¦® à¦¬à§‡à¦¡à¦¼à§‡à¦›à§‡à¥¤ à¦¬à¦¿à¦ªà¦°à¦¿à¦¤à§‡ à¦¦à¦¾à¦® à¦•à¦®à§‡à¦›à§‡ à§§à§¬à§¨à¦Ÿà¦¿à¦°à¥¤ à¦†à¦° à§§à§«à¦Ÿà¦¿à¦° à¦¦à¦¾à¦® à¦…à¦ªà¦°à¦¿à¦¬à¦°à§à¦¤à¦¿à¦¤ à¦°à¦¯à¦¼à§‡à¦›à§‡à¥¤\\nà¦à¦¦à¦¿à¦•à§‡ à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à¦œà§à¦¡à¦¼à§‡ à¦¡à¦¿à¦à¦¸à¦‡'à¦° à¦¬à§à¦²à¦• à¦®à¦¾à¦°à§à¦•à§‡à¦Ÿà§‡à¦° à¦²à§‡à¦¨à¦¦à§‡à¦¨à§‡ à§®à§¯à¦Ÿà¦¿ à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¾à¦¨ à¦…à¦‚à¦¶ à¦¨à§‡à¦¯à¦¼à¥¤ à¦ à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¾à¦¨à¦—à§à¦²à§‹à¦° à§¨à§§à§¬ à¦•à§‹à¦Ÿà¦¿ à§ªà§¨ à¦²à¦¾à¦– à§©à§­ à¦¹à¦¾à¦œà¦¾à¦° à¦Ÿà¦¾à¦•à¦¾à¦° à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦“ à¦‡à¦‰à¦¨à¦¿à¦Ÿ à¦²à§‡à¦¨à¦¦à§‡à¦¨ à¦¹à¦¯à¦¼à§‡à¦›à§‡à¥¤\\nà¦¬à§‡à¦¶à¦¿à¦°à¦­à¦¾à¦— à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¾à¦¨à§‡à¦° à¦¦à¦¾à¦® à¦¬à¦¾à¦¡à¦¼à¦¾à¦¯à¦¼ à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡à¦° à¦¶à§‡à¦· à¦•à¦¾à¦°à§à¦¯à¦¦à¦¿à¦¬à¦¸ à¦¶à§‡à¦·à§‡ à¦¡à¦¿à¦à¦¸à¦‡à¦° à¦¬à¦¾à¦œà¦¾à¦° à¦®à§‚à¦²à¦§à¦¨ à¦¦à¦¾à¦à¦¡à¦¼à¦¿à¦¯à¦¼à§‡à¦›à§‡ à§© à¦²à¦¾à¦– à§«à§¯ à¦¹à¦¾à¦œà¦¾à¦° à§­à§®à§¯ à¦•à§‹à¦Ÿà¦¿ à¦Ÿà¦¾à¦•à¦¾à¥¤ à¦¯à¦¾ à¦†à¦—à§‡à¦° à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡à¦° à¦¶à§‡à¦· à¦•à¦¾à¦°à§à¦¯à¦¦à¦¿à¦¬à¦¸à§‡ à¦›à¦¿à¦² à§© à¦²à¦¾à¦– à§«à§« à¦¹à¦¾à¦œà¦¾à¦° à§®à§­ à¦•à§‹à¦Ÿà¦¿ à¦Ÿà¦¾à¦•à¦¾à¥¤\\nPrevious à¦¹à§‡à¦²à¦¿à¦•à¦ªà§à¦Ÿà¦¾à¦° à¦¥à§‡à¦•à§‡ à¦¬à§€à¦œ à¦›à¦¿à¦Ÿà¦¾à¦¨à§‹ à¦¶à§à¦°à§ à¦•à¦°à§‡à¦›à§‡ à¦¬à¦¿à¦®à¦¾à¦¨ à¦¬à¦¾à¦¹à¦¿à¦¨à§€\\nNext à¦…à¦¬à¦¶à§‡à¦·à§‡ à¦ªà§‡à¦à¦¯à¦¼à¦¾à¦œà§‡ à¦†à¦®à¦¦à¦¾à¦¨à¦¿ à¦¶à§à¦²à§à¦• à¦ªà§à¦°à¦¤à§à¦¯à¦¾à¦¹à¦¾à¦°\\nà¦¨à¦¾à¦°à¦¾à¦¯à¦¼à¦£à¦—à¦žà§à¦œà§‡à¦° à¦°à§‚à¦ªà¦—à¦žà§à¦œà§‡ à¦°à§‚à¦ªà¦¸à§€ à¦à¦²à¦¾à¦•à¦¾à¦¯à¦¼ à¦‡à¦‰à¦¨à¦¾à¦‡à¦Ÿà§‡à¦¡ à¦²à§‡à¦¦à¦¾à¦° à¦•à¦¾à¦°à¦–à¦¾à¦¨à¦¾à¦¯à¦¼ à¦†à¦—à§à¦¨ à¦²à§‡à¦—à§‡à¦›à§‡à¥¤ à¦¨à¦¿à¦¯à¦¼à¦¨à§à¦¤à§à¦°à¦£à§‡ à¦•à¦¾à¦œ à¦•à¦°à¦›à§‡ à¦«à¦¾à¦¯à¦¼à¦¾à¦° à¦¸à¦¾à¦°à§à¦­à¦¿à¦¸à§‡à¦° à§§à§©à¦Ÿà¦¿ â€¦\",\n",
       " 'timestamp': '2021/08/05 06:44:57',\n",
       " 'url': 'https://www.sstvbd.net/%E0%A6%AF%E0%A7%87-%E0%A6%9B%E0%A7%9F-%E0%A6%95%E0%A6%BE%E0%A6%B0%E0%A6%A3%E0%A7%87-%E0%A6%98%E0%A7%81%E0%A6%B0%E0%A7%87-%E0%A6%A6%E0%A6%BE%E0%A6%81%E0%A7%9C%E0%A6%BF%E0%A7%9F%E0%A7%87%E0%A6%9B/',\n",
       " 'source': 'mC4'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00001-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00001-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f534a366c80e43d88353a5d90aa0e26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca445e84110440aab5285823b226467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10382 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10348 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10904 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14530 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16483 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13772 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11607 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20566 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8250 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93872dc0d4ac474598e87ba8fd62b931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f432b7d4bf45b8b921b80860f001d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00003-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00003-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0625008a7e7a4d0db4fd0a573d9943b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d83a9bae4e4c038dd53ca6a1bcbb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9661 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8247 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9028 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9727 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8443 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11633 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14168 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15866 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10758 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11823 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10737 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9053 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15988 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12065 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892ed87527a4420687232d008877f903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e630a82b5f4e7c9c0464e5862d4039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00000-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00000-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72d8c3f1b9d477798a35c1121819dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6492be4cdda45dca3e4ad93dc5a5f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8464 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9185 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11758 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13701 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (105300 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64225 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13725 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022c8f99bc864cb3aa051fdd3891e31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c0856812b344e5851a3c793e607c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00004-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00004-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a18170a58f4d818c9662b587c55fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21806f4a5904cf692d18f0a755fbb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13882 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8892 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23059 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8665 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9034 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17577 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9199 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9440 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31047 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31758 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18112 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11448 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12517 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23507 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b60be3d3c4444d48051bce78303d9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde7df20ca7e47deb94a0cd690313da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00002-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00002-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa342118de64a3eb709da4b6832d221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f911a661fa94fbeb86309f92867f328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17533 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18747 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9217 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15130 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12279 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14116 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19635 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10914 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13990 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10790 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17783 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e68584dbf074bce86c269d4ba57b6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cb5d1ab3ca4fd9a1a9f1eaacbaccf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00005-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00005-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671a8d67d4ce42ec8db2bd796bf5a009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b63eb7ab944834b9883b461797ec56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10109 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10074 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (94334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9350 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14450 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13511 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10726 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16153 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23183 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9651 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177cd3725de74ba8a1918c57a59aee13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d6d918fe8947c89367f0858f86f82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00007-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00007-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a646d5271d5d44bd8cde4b05398361e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271bd6c13d4449449c42d85cfade3165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15015 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20854 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25985 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8601 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10184 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13087 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9015 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12746 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (51196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9736 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10702 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9306 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775187de1d1841aca40317266db47781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d4e4fdd9284d0193e158c6ac0bb8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00006-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00006-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb6ec84019f4450b61dabad3564ac0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98722eab0484f078d48ff55d0d7ab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8988 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8465 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9897 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9747 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9719 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24078 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13944 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12582 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (50634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9254 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8447 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117c6fa64cc647688954bb2574638a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005a023ab03b42be9bf53a065aeb3034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00065-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00065-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd7bb6f33814c2fb6274dfb67924dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2718b51fa6ec40d5bc90e515f7850142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8358 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13425 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8562 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9430 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8883 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17950 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33075 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11773 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10119 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc641e7f8294881a10c87189a98ff46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde8dbd860824023b62bf4dcad0bd083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00008-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00008-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a50b11ac6e84130b8b107cf1589c071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a093170a734ecf95e91819a13f24f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13305 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9222 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11006 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8679 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9168 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9040 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8991 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11453 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14617 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16302 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10541 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11235 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14359 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1541d43974b4ca3a91b5ad407413773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a416b35beb14dd283757d4166032a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00009-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00009-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69a8f84c95d499492cf1d3f12f3548e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1943a635744444069f1a6b7978d3ff2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8727 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12983 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9630 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14938 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20748 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11804 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18448 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18eb68816d8c448f960a4f9f9c4754a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0fe9b077074f72b570f275074169ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00015-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00015-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42a806cdc0145a4b2ea027621a647ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f200e75fd94d49dbb54ee1129c23ca84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8581 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9743 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8655 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20097 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9828 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30656 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10591 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14481 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17809 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8635d737e94b09a5c1f506149e38ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8233329bd7d2418896dc635351027763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00011-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00011-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a19e78d858a43ec8e2d09c43c1e3503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1bbeed4ad441a29eb732b3b74fd559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8341 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38202 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32220 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15012 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48299 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41474 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8944 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10292 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14793 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8738 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcda3ba776214bc1ae4e931ae17b8c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b518d2d2f447e89c3b1b4592e5a205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00010-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00010-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63482e084ae741219161b0bef1b03f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac139d1b29847c28b0ff8885ae87434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38017 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8822 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9981 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11588 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33085 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12904 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31461 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9897 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9363 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10906 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12708 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14450 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12550 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d153dfdb44f4c89ab0152457ffe6deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61230173ce80465393d961295a904e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00018-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00018-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6c71c302a047ac8a45a5693066a1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70c9b1a81e9459380cc464f792f56a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8220 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12537 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10784 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41743 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13691 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11517 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10703 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806d1b7aebf14887a6b2d43baebe556a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d1c02d088f484f9dab728e49f47b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00021-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00021-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51714a9fa3d042888c82848294838e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e479533b7441ff8056878762981edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12046 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8835 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13547 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9322 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9984 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22341 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12248 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10137 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17073 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (68324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24515 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081cd36ea1414ef19d05fd4f25a44b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935a2cfa2b2c4434ae815bb3fa850bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00012-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00012-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6711aa8ab7f04b13bc5b6d9b8b9dc5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3377acd63cf64aa2b682420a2b7cd848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8346 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8596 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (50544 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8759 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11266 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11880 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8895 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9505 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8212 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65c07df39f44ef5bdbd2668e4413af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be70e05609b43b39f1b7c0890ccf30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00022-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00022-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35af61a3690a4baea8e4c25bcf3f1195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690b3bd319464d64a902828336ccb8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10403 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14466 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8370 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10101 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18641 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15485 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11599 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24167 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34338 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28996 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1b18998335483f963cdddc1953811b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0039ec50b0a34dc2a3a668898d841dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00014-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00014-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce542398b928414f869cc8b3e920c206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa01bf71e2bd4e2f81aad807b98a67bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10211 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9577 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11928 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8894 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8562 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22492 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9659 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10320 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17667 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12553 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14897 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8542 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10319 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa49fc2adee64943b4ad219574e5b6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352e5f5bc3764c8790820d1f25fbb07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00013-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00013-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152d9b03231844de999ff4f6bec62daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd22b1ed1c804da3a2e212810324394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (99908 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13708 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13968 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13921 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8594 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10214 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11566 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11107 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64053 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21922 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12338 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4c295dd1d645258cbff23454e91238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0d27d297d04e14827b4bab431c7af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00017-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00017-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffad6a95d4342cba1308a7756850a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4498a56db5ab4425ab1686b50a4a9094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8491 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8426 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16018 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20027 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16097 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14142 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26498 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23095 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39238 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9812 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5ace6539c045d58541bdfdc71b2579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42598a5d676a489382da76cf0e29fe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00016-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00016-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc940afbcaa4b9e89511bf065fdb710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d3523fc38946ad98e7fa13e56ba6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9263 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11621 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8842 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16851 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15461 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15550 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (149750 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b02fa54e7744c7817a33bc170bfc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b24281980149188afee00b070de35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00019-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00019-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630b3272c8054d56a258b3cd037884e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c369fff605d4970b6794416877d76b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13443 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9193 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9319 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12808 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27004 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9553 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16937 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8426 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8932 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14428 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13649 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57f53e69efd43f9828bf9067dfb7388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6292baaa1a943408351b4361f4bf408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00020-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00020-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51ef81c4387423992bb0ebcb7382ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02db96e722454ad18277c4e1bb6a6d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18061 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13895 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16349 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8605 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9485 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13488 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (75690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9846 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18004 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8597 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10127 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11906 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11023 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12735 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91466596bc5f428d9c36fcfbfdd32d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04593a99527241e0858f51a6e244394d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00023-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00023-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f984609287e449cbad191c0b86df2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6e04c06fdb473fb7b8b1a7dd4bce9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10413 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29904 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8856 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11849 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19878 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13320 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12363 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35594 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99499677ba2349db9146784e9c8795d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afe6ae1130f470cba55a03feb56d0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00032-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00032-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf0049b26fe47099ab946cf4146a7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc3e3cf1fab46d79f5478da66057a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9673 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12937 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9859 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8211 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (85142 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9142 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20674 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10186 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30258 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6b0709360f4647a6328c88cf67c40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a272c7b6b658498d94528e0610a891a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00033-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00033-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c73660f6811496a986ce7d4f2549157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fda097033a493695ce1802069b92f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13805 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10915 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11890 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22854 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26748 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43108 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11718 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7492cb15938541da8b34f114fd0adf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e96a5aee2db44c79e74e4877a4b3e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00038-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00038-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f360fdae21a450980b74cd809aea16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b5e54fabca4b44b7153841f5ec14bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10071 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12546 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14087 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21532 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19495 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9039 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30416 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14601 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12981 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17160 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12572 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8419 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12927 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fc48232bbf43e9b0efc07e3482e8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fbd832fef34064b527cdbd39aadee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00025-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00025-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b803d8b4c917477faef9f134fe3acae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30dbc0f0e544cf99d9504154820135d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13227 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11263 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10702 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (130718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23817 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9078 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9895 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20605 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b664d052c9423ab281db6197115bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a031b77b2f429a99cfc03adcd678c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00035-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00035-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262b0cea66c24b0fb8ab35b6d5db55ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04b810fc51143598e7ae9a95afe1a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9549 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8601 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11880 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9233 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8950 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9127 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23608 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8482 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217a05c161974859aee7a482815ca93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1a1fd711c44ff0ae5c65bc1b3f8cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00027-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00027-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2de7e7a7b7647588fc1462710474150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dc7ca09f764f278bea6e6554d92255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10979 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10017 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17866 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9191 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10158 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10110 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16378 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9719 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11673 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6562ebc645432eb36db42b758b5f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c87290bfd24572940af7839aeaac80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00026-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00026-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3740e105c6cf4c27b9adb3315670c31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919ff466c8e44d7fafda2e94277a78c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9715 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9137 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12623 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12621 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9700 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10015 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13303 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24388 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8594 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8375 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7724d954219546208eb102c6d2b7e5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2726cc9352ee400db2279d71d4eb7f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00024-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00024-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6747e91e39134b188d5443bc43cc4636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfc55feb7454114832b9801a67a1f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14251 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9134 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8932 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9413 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17012 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18542 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16909 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11683 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18944 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9977 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9924 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d74e98ebede452698a614b623364cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76c5b46d7c34d589f5e37df1a206994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00029-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00029-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff9415f5cb741788f59770c0d1335d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca81e0d2b8634b4cbc05f9d591659500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9562 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8454 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9507 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11762 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12047 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37857 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10150 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8337 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9370 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9963 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20279 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11920 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10533 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abd7b8a6e7d4aea8d90cd3a5c1bdbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0226b8de33c8445d9f1985adfc7ad944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00034-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00034-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c295851aca44afb9fe55806a124e49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abfb92382e94ea4abf74233a938914d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26295 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16020 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8974 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10477 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15480 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10796 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10158 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8439 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (154951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11647 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35584 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c56528d8a294e58b23608b969ad9542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a8560716e74a93b9461cd951de0f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00047-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00047-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf29052c7d94b07b3361abce8c869c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0708d221b58e4551b73f19953def5c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16193 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32023 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8617 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10015 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8440 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11247 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19922 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9053 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8898 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8979 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15318 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa87b128d3f34314993348be6e17f4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448960de088c4df78e9b4cbf15caebc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00031-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00031-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b03ba1805b4f2a9d5a60e15d023b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4844da7b0645bba40ba06b61af044b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8198 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17535 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8521 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11456 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36397 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13027 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8320 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10630 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11435 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276ef49e40e04c59a23a1ec30d118216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cfd10265734556a00eec847a341b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00040-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00040-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7493fd279234bff95da688dd901a20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97fc6c927ed42d79a0c192e29cb951f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12078 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8330 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16633 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10112 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8405 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11076 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13834 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13766 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10203 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14529 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (90239 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c388f6ff4b4544ae07b50884084796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803b416dfc7442598df181764ba14fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00030-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00030-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50fc4d0302e459f965f4d689f550caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12c643f5d32481dbbe89028d2d19600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16036 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17450 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28153 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39648 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8588 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (155996 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10081 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24790 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab06f2435add4740b1d888c3457ca6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de5484551cd45149711493f8d468468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00028-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00028-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18364345d3c04190abfdc05a9ec23531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fec23d837740eaaa0e491cd2e452ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8303 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8464 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11421 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10829 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10836 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9911 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10097 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a54558fdc043ca85fb5723e20bc9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e305e6eb0049478c9adf128c23e89cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00037-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00037-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4050f4b772264163902a4e6cd83e1edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3efa96902f4426a63e379ea08d9bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15511 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11498 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15097 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13743 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11440 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24388 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (150295 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20360 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2542585df91749e1bd6d8512f7cd02e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde9309018fd40b5afdeb64d13250b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00046-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00046-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba68f8254a1478c819d77342f7444fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f297f0abe0f4dcaa8e9a2fee32d44b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18269 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8906 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10233 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8726 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9537 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9254 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28115 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11998 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13549 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10159 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd95c9fdf554db3a974b2a637f4f7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6e45ddff9a408ab4d5d716304daaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00049-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00049-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7687824887074c178e63a46140eac018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20865b4d3bd549d9aa619ebe50319be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15006 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8708 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11488 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14154 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9030 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8842 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12101 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10171 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15298 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8605 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49648 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9984420935425c8ec57faef0b5d825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256f92d6240e4c538460d4f22011839c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00051-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00051-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b719a65eb74905b3c6920deddcaabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b505a5ccc59043e4bef87e233598569d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34382 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8461 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11678 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15076 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12337 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12008 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17785 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12673 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14271 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9877 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851322325f1b433baca8605e6c9c00a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599ba48e408042c497f5b4a1307c6ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00039-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00039-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e258fd8126da46adbddc1444fe49ddc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7d8c402332449d9e4387c2fc6fb33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12562 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10168 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13605 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12891 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (124505 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9631 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10928 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47032 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bcb3d3fbf44f81abdf399458cc1b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c622afb992545798cc993a16c6636ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00036-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00036-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68bfebc3b20406d89847124a1f7f64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b243134d4c4d9e982a42d851c38018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9456 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12578 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13348 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9339 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23505 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8274 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8574 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9091 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (86611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18274 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13554 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9434 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64734ce291204dfaa0ec84c7a2a6a790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d2c0caf50542fd8568693dec6e4afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00048-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00048-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8fcfddb75748db9f9a94984885cb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011e468865d34d59a8482665c22490c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8498 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13805 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (63243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (65720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11575 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14792 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (176826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24646 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a239d1f697224f6f8eaf5bc3144db768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab40dd4297a4282863aef4f1f6027e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00042-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00042-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d067c987df47ed983715abca7fa9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014d5956105c44c0aa11cace7609d966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8898 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9921 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8503 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10089 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10240 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8798 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9816 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10963 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8993 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9527 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32020 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11518 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8218 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affa69123811435f942c099e41e7fd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86628595946c475fabb0fabba6cb5dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00041-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00041-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664aa08616654e439cc0a8d576d60c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bbf7ddfa0d48d4bec007e892bd7c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14679 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10426 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9918 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11691 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12535 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8400 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12382 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (60741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8439 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22698 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (61189 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe4d30f2bef4b8e9233b865391fdbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3335788bb5554e7cab9cc680b724d39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00054-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00054-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8299f5c823934c0dafb192aa096228e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d849fad85f554365a05f23f9f1883a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23683 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31199 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10578 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8728 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9908 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9497 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15570 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56636 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9437 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8724 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15685 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8962 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19541 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8623 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e05d41b1f14962aeec50f2f6482b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94305040c6d04c7592e30d09872dceb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00050-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00050-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af19bebb989468cb4f0ade9b8aaf007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4166ecc3abfe40b49cb140125d41aebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15321 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9866 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10423 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10914 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25517 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10784 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23273 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13477 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14341 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (165172 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c52274b9502421c9520553c67165277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcdb3a437fd424f94bed97f895d5ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00044-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00044-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e94ede3196a4c448ea1ea7318f7b813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a67b61d7ea740159877bdf1acbde768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9189 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9003 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9123 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10938 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17547 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14259 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10804 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10944 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58de074852704188bc09fbbd6eb46071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a8dea3990f42789c82bce81acae6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00053-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00053-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5732b8bd39543db8cd465cca8894d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc98dde4ab0e4d2c958f44ec9c4e6f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12026 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21453 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10403 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11086 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8377 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11807 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12181 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9886 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18941 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8420 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8854 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15016 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36005 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9319 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9b724fbdf341ed960dd9882bc22bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48de4e1563c14ae486a90619cd7cb840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00052-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00052-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6cf3d0e3304f9ca1601c5365c2a734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351a72ea96c148d89c444e1ec1db42a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12497 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13914 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8937 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10802 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14189 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19005 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11736 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10116 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11816 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8995 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9085 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0037b2e52a44219b926fc30313a7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cda31bc4cb4d7dacfd6e4e84af36e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00060-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00060-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aa963da01847c09ada450590909b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ce51da722a44718670d69db7d9a023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11423 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12572 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24216 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22453 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8253 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18191 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10038 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (98468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19221 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10804 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10895 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149ba9bd39ce4e828728ca0ba00f1652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81149bb56da140ce845bd60432b50fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00043-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00043-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261485f1006c4244b06dd99b0d673dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a02fbc2be884d398d107e415f2e4551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42448 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11998 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9554 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8578 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9989 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10154 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16881 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11436 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e5f28c9bb14cf1b81f6b0afe41eaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7059e898b07f416f8d6b0d4aeceddf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00057-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00057-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a00b67d9b214b3f97d32b6ad125e750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d54a3677e5420bba686af920efe3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14123 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14108 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11403 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12542 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10722 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8891 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8346 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10268 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d3e41e0ba34ac9b9e5fee18a71703e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83b402070c546ec8eb18b5dfc24a4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00059-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00059-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d32e8d668446f2997c0760aa2ffdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff3308a844e42be88885ddbf0b58e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23946 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11903 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9647 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (96037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16123 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14122 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14804 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8937 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11558 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10605 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8829 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10211 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556138fc64194b4c8a5c74b46efdb2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0058f407ed409a9abc27b414c1477d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00045-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00045-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70aeaaa80e214aae846c252aaf567236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e3285aa9744830b96532adde9d26ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20497 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18483 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10016 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11535 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9271 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15475 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16160 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12722 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffb040d31de4904a52488a9474759ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4369545c50244c6398d143d67f811a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00055-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00055-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca262afe6e644ad7b01ab2d7e2d0b191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b6652fd05b4e469336ea31fa02c131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9792 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (57199 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21238 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9808 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8909 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16952 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9370 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13984 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10795 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15303 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9423 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17298 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10662 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28b9ebf54e84d99a8cc1af1345a9b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5394915707464db278de2450df2a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00058-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00058-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383a197b7b094fcc9eec53ec7e5d8b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef831e3c2e6148638ea1459e81ef9a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9824 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10150 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11232 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8467 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12097 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24655 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14491 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18071 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8768 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44502 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21485 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8382 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10264 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38701 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9942 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981c0805385244f2a108b403c5b4b9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566a4515e59c4aada8cd47e973690f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00056-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00056-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a402407e379644e5aa8fc50edbac5555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d1a918d66b433088ea80c24e685d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14366 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14197 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8542 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19135 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10750 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12977 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9583 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (103213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8626 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10490 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (71514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (112838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8651 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55788e5dd02a4d02aba8c4c3952b2ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393ee73f3ea14b9186993ad8b2720292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00062-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00062-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e281990dd91412ebbb430d10a472438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca1c3a1af1b4a3496ef918f19113cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9599 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9729 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8748 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11939 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12529 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8665 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16107 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15828 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20083 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13932 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8527 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10026 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8970 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10692 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b88a4e421943e494c3f91870464cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb42482156b4427beaa1244af23f171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00070-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00070-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe676fa725441ab9ea169a0640f666f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190c63a3e62e4ecb9c69b1ee90064269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8303 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12871 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14251 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62700 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18659 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25292 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29225 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24183 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11030 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1eb96b25334fb9adb1762cb8c013b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba147bc60a84fa39f6072a631c81ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00068-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00068-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3218739fbe8e46239644b14edd8e898a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acff8d21e03456ab230944d1d050966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12413 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9540 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9321 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9282 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8762 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9808 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10259 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10626 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (59722 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9974 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9022 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d0792fe29f438bb5b86de7492585a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36820002a81a4ff4a33b0b20ccc65645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00067-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00067-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20424cb9df424fe597655280babc4e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc63251acc54f8483980e4d5bfc5f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14122 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15023 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9144 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11344 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (61613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10130 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12005 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10967 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11467 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (186663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (82837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33454 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4bd695d2f04869af4597fddaabe62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bfd50b363b44dfadca1aec635771a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00063-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00063-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8a9e0aa0c6414c89bc768addcbf0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4c1b6528d342c3a8073daf6013c83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12903 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10858 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9397 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10623 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14337 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16558 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8322 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21536 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8580 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14005 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9107 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20537 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9939 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8859 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8233 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e79b92f7234927b82069197b30a50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/75922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348b3c66b0de4288bd4fdaad2b248e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00069-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00069-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08acbae950914038b7d78ffb4be180ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050d1b842a674d2da513941e5d83cef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12143 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12050 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14393 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9167 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12269 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10828 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18834 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d44f7c6e3234f7f9be16c25854781b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b4b8363bda49e38bfd0839a62391ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00061-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00061-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d81b490b364e28921ef6880252ced4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65887cc52c714dd5b6674a04ccfd091e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9626 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11746 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10993 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22989 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (98754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9443 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10951 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6db8c77a6149d6883007578881286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7a420e5661436ba2f80dae5a954a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00066-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00066-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18ce7260f834b86b30f099867afc14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11dc1a7d7444cda88616387f88ddea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9182 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19882 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16582 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9998 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8580 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16490 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8859 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19030 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8580 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17071 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3634cb6f35f40ecb51e62f748b46262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c22fd59de9b4b64adbab93765acc6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00064-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00064-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143adcccfee149bbabc833382038072a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f898281bffd142c3b1d1923b4fca8cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9227 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9292 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9473 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9532 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8902 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (140834 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9312 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bdcacc4155424da681acc19d5f530f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacc0ac6fb5c4da8baba86547c2ae845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00072-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00072-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3a5630c34e40209d5b3b06d0c575bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effa5a1614e74c50850fa893297bc2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9144 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14181 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8983 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11106 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11475 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18075 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11295 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8656 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3db0639b104a21988e3ab4daf770c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b8c80d273f4090a3cb2b4ad6a7faa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00078-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00078-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8534a5ff9e8544fba2837284a15f092a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e7245b3009412392167ca2929833db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10765 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9187 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13968 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9667 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8914 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23889 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (68951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10419 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9202 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9254 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8492 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10489 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2405755e395d4bab8c6943dc5320547e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e7354425242dea00b9db17816b97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00081-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00081-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b5d2983d454779bf103d9b7b746050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73af9755a9074e1fa3da9f2490f13033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8858 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10851 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9059 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35228 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10674 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8977 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11260 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11202 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8881 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10818 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce56449985704b7eaf204f0b8b42ad4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4763410ec2aa4addbc6a872c6248bead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00073-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00073-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9ca085f32a423fac22ecd8b7fe3a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c513b4a75df74f649e9f9cbab71c4791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18289 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14924 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16926 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10240 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9597 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16798 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ca6d295c4347bea52c34513b44cb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ad2d1da7604c6faeccd0cf80f84960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00074-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00074-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c67ee569b64f149b6302f61744647e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb9c6ef5f82479aa57b5a8731e21bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13068 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9388 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13507 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12655 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13632 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16459 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23517 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36679 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9483 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15101 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10909 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10960 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12317 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09102218ae0646c8a02027b29a3bb6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffb4afbd7f745108b4a33f2c137ddf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00080-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00080-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a5d6b435ff42dcbca03517fac37661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32de6f78e179495997c8f05906d45306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17583 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19257 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27647 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9701 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11465 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8672 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10179 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9305 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9255ae37f21d4275a16163415be43ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3471333094408c9a391a92c18eb394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00071-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00071-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1912ede644464b893694e11cac0fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445d5c13c72e4776834c24969d79bfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12763 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8470 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39677 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8392 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12012 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9891 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9932 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12529 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9308 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249d41161e9f4d0daf0f5f6b12f4cb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096768035f5f40c7bddaf2f0b80da5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00075-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00075-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed33f16452e42b49100d016d8d783b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61408217ed6b4bd196dcdcc368cc5ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11004 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10530 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9047 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13515 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16001 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13594 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13006 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9491 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9677 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10979 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bde70ac54c94b5f83e8dc38d42bd7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb818bbb9d4408daba7724bd174c049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00076-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00076-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642321fd2ebb4e3da3aa94a5501ee94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133fb0ebdf2f47d4b3d7309a9c101663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33746 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12469 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14168 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14686 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10007 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10090 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16607 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9542 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27bad5c3d804710a3d9011339dda7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e9a404f6ca4b8f9e075399264f05d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00086-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00086-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58aa745281ee44fabb4f3a7a4e9766a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d3369cabe6401092f96744a306de27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9154 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9294 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9730 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12101 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15071 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16473 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11118 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea359c7138145f58f15f902a6426854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86695ce3efc24c20a5fe36a368a4e011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00077-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00077-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c8251635544f87915055c9045d098c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00a1203d636469eb14b58673cfe8e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10480 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17938 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8256 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10835 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9400 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9397 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8228 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41589 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8880 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13039 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9072 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32163 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37048 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75fb229394c4aaa8dc5f1f0f60955ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdddfef842046eba355970b258d1a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00079-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00079-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad3ed7044e5418aae0d9b668e799643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32472bc52d654b69907701ce302c3dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8237 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8547 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (138923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18127 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10137 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13869 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16137 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11266 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25553 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14993 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11195 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b3a1f816d34accbf4dbb46097bb03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ea55facaab41e8aa2005353ea2e32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00083-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00083-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa36ff0e08034a70bebc8da51c449c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a804d72cbcf5457d933490a813247ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11928 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9282 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10122 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11071 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10247 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9836 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14974 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20829 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8319 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9872 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10171 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16128 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93560c2b1b54b66a0470315a8969d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d527fd9e7ff4820a6aa48d0409cb007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00082-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00082-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94f8d1534734716b3edf5c8b2c54270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e31bec575da45928b1d0d44a6623eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11991 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8927 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8915 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20153 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10472 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15214 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9154 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34157 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21698 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9470 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13140 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10768 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9326 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf782ef7108438aae74a0af8bd860da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ea98ba11304c379fb143f6e1e47d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00085-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00085-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751a2e1408fe43f4abb0b9a01c8e36b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad29beb59a214b4ab737a5a69a33d7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9857 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36085 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8282 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9456 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10047 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13520 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23298 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (54490 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9348 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7c49f364d2467da2ca9908cab8acdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1683394cde54697bcbc6ea4df803756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00084-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00084-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb827545b4b40caa2a6e200566fa217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781144f2903b4d28960e9c977b130692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11414 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8661 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12962 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13020 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (67625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9530 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f58e639e77414c89f4b852b7046dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9126f1c237bd43178ac0ce175a475002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00088-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00088-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1d0e1cff2d4fdbbac6fcd657a47b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338292271fc2473f8f4854e0b29756fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8349 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17778 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9255 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8221 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14894 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10636 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29423 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9934 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12113 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f9c6048b4d476b8fe37546c8ca0099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d564ce28f0c842ef90bd60db235918d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00087-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00087-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26832f24239c4014bd10bc15b363d21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007bcc86330d475aa61b6b2199b754e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9507 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11456 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23338 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9996 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19618 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10567 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8898 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12724 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8762 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8373 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3850de523e9a4e8fb4cd95d508e27785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ac5fb02112453981515f979a6f747b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00089-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00089-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ffc71ac34f42aa83a06b81368bc17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15206cd38c66467f99ca4403c9020dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10339 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9896 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10016 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14282 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12846 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16121 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12544 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13185 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13742 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12120 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8470 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8706 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18906 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c03df50ac6b42ce93608af2bc39555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76d5f9a0389428da4e96773647ee72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00090-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00090-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388cfe5e92884e3da151e351c35a3a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b6534b5c06498c9ad68311a21986d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9129 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8988 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9273 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17293 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11721 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10151 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18798 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11091 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9049 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10728 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19781 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18453 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16637 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5202311684a4a10a71f334eb796c113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae01d17bb6074049a4d77dc077658919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00093-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00093-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966dc09a3dbe4f44828109e543c404d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a474b2590b8a45929e35b5d2710bea89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16225 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9163 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11728 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14882 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8214 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (90751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12772 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26170 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8493 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4232225c0ac2494b81159d22e27d2092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63912ff3af0d44f8bf504c79383fa3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00092-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00092-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069f82f499fc4698bf11b9982237b738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d448c31482734e5d878c4c2713264bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (47619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13321 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12750 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13210 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20272 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9591 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10787 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10465 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10050 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16452 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd9ad6e2d43472288a67ba9609dfb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4504a9413634628826713534dd82cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00091-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00091-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37a8016cb4e4a9289282c41e8b35541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d257b8020744b24b772d9506db65e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17835 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8822 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9213 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8858 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40784 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8484 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41186 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15260 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9975 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13703 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaa9730e80245b1be6d5faf1cf166d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f89dcf6b244da08fd98136de269280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00100-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00100-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e4c66f9df041f3a24c270fef6da96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1038a57fd094fba839a1adf52698f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39481 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25527 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12845 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13477 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10889 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40679 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16211 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8816 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10834 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9144 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841c1b52396a4b5f9e7a6fab131b1781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c8b5b1d19f4bb6bed815a57820419e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00094-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00094-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46512c8a44d5472b93f228eef5bb5956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56dea201bae543b4b6834180f02afe72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10722 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9040 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12838 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13171 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16392 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16207 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17921 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8594 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27095 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8515 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35217 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33393 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12587 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26946 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11be80372e8c47e2a51814fcf78d418a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc68c3a52d64ffca3bc22e45404b4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00108-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00108-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51df1f5d38440e49b5453602d8e3405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2b7af64801403d9b4a74c4337f4437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8718 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10233 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9145 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9984 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8437 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11175 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9488 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14979 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32638 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9320 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2defea3c96c740ec80eeb80ec61ebefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1f1d8512714154a52ad4d470d8b54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00104-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00104-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f36781452f04ac2b3726587a5e10e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede5fbab43e84d33a72c84d38e7d55cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12550 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9844 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9082 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13349 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (122212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17695 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26078 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11995 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15106 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11350 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23903 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12087 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b606286eb08460f90c9dfa25c29074e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3968dd0955cb436e9f6baae1c0d51bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00096-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00096-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45163107b9d145d291f485fccdd34002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b805372083d6471c886fb983bea728fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26990 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18073 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10496 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8674 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20849 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16254 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20008 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10778 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8572 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8633 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27588 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a0c04f3e494fec9ae978efb9f7e467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42af97d1906d4c5d900ebdb7ee34feb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00095-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00095-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3834c52ac3ef4a9ea01069db55abb80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7115031174d42f9bd9d85a3d8445808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10694 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16530 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20996 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17500 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8405 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9474 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10575 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13600 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13412 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8967 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11729 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18623 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35028 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18071 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61663f0a20714ab685646b4e8cad00bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a01ef0fdad4764a7f131829b70e84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00098-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00098-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ea9d9a9107422da869c494d70156cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b2a648d0aa4b2384087e71115d594d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11781 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13607 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (70190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8654 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8235 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (123155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8521 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24572 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10585 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13331 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8349 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8430 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eded7e3dbc147e297552413ac4959be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bd52a40eda4adea09ac0c592204985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00107-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00107-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cf48e93a484a99b33841d5b8367c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef714dded1448dda758ebc16d1ef47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9496 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9893 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8821 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8811 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10309 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8809 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9521 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9060 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9631 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10441 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13302 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (53471 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10081 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20621 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d3699ad7e948278652a1f59d2e0ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2de367b795492eb5697784e1eb0ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00097-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00097-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2f785ea27045849653dd54b4dd2526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1604d500dd714e69b60bfe40688b9d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11703 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (46208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12390 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16995 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (66693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9509 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10197 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22798 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8509 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25056 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9157 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9004 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13320 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9591 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16516 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8515 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ead6e96352d4e6a922edb83ef3a3023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354830c1f2004310a4ae4ccbb310432d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00110-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00110-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf23751adc954382aedb6c042f645f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4db4cccf7ad425f8a622cf6958cee51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8630 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36747 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32626 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9763 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32618 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14140 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27116 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12823 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8430 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9095 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11869 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10214 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9079 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13155 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11112 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17052 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (112714 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24361c254c314e0d95d122e85c713067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e0bf2eb17c4917b106b240bc076b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00101-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00101-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6e193c6a0046bbadba15e0337b1379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a165eed4616d4cd6b386495d99653ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9108 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8339 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8386 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20350 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9529 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8460 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41722 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18419 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12022 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9924 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18672 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464f0ebff6b344b99e15080cb5a97543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190994bf9ec24c908635553330411ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00099-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00099-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f07a85f2d434570b18cb64813509bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ff8402b84b468783e7aa885aee6549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11297 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8639 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11806 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10091 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10820 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13886 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8830 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21929 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8195 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dd84931b7f49abb54c9d23645521ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f87c9163864fbba8b6f23645831772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00109-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00109-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7e6898be8547a39a2ffb92c402cbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abf637236b8487fa60ef1cc271bb362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18573 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9715 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8913 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11523 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8785 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13190 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10046 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8890 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11582 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19235 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10595 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8344 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15440 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8621 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12577 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8248 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8998 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15871 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14223 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14758 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37779 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72c3fdd8f4440a0b79852735bd4e89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e38dd257ca4576a1ee46416787460a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00105-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00105-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6015d44123c4395bdd0bcb954db3bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6375bdda6ce74a3c93bb41620ba3046e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12645 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22582 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19985 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10767 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9361 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14969 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10129 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18856 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11796 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9253 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10563 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18391 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8197 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11111 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8265 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10436 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba09094b153447a951687deb77ad6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405e995e8cb04af8b7de7613941fb5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00106-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00106-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9e910e2f664907abee104b2129e7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25c83384b2e43609cdc2385caef2f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11880 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8397 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9538 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8510 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10073 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14460 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22992 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (66116 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22012 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8890 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45645 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8742ad9a214c168615f664029a9839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcd36cf47c24406ae54e90b4e043746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00114-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00114-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd2184d50de4137ad9926d5d3223be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e327d8321ca4b0fb49daedaaaa3fade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8380 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9635 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12510 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11775 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10539 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13128 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9521 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21999 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9377 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9179 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21979 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9456 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad8e4de549b4aed806b17b1070ce191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a51a4ac202f4ca4ac8929f7839c9e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00113-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00113-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81a2f737dc143d8adf95e494269ef37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3337d3a3b1d44165be837beb4641f7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16469 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9031 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15043 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52410 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8448 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14302 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9121 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16350 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30129 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20997 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10092 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1106232e504f4f80bc7a69a2f69c2689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea454275e1d4ff3ba441311fbe1c8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00115-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00115-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e975dcc91924d18a3dfe39739154139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7dbf370e6d442f8323d9f479fc2e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15519 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9321 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17949 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17121 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23242 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9911 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9189 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14118 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22565 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (76734 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8966 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38528 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8558 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9223 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1e97a0ca244e6c95c64ad7d9965685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7a00c03fc4437f986f9f2a98302fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00102-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00102-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d3a31e3918432c8f6df7bb329e8bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01e1432daa449f594efd02b650348a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14567 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11829 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9442 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13515 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10126 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9240 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9089 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11403 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8503 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9038 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29121 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12544 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16759 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30003 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11891 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12231 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8750 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8504 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9126 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f8fb6e353f410688e05ae04949b71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a84e97194646dca2b4d900c8914851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00117-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00117-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cedad141d0c4db9a2dd5554e8dca23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68536f2ee83a4850acebdbbf283590e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8313 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9912 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22001 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12863 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20681 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11126 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9585 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15438 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12467 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9050 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11484 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15862 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d830e447dc374bdcb3f7616a09348bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe93188538fc45d1822661f8ec6494e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00111-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00111-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6d7d755fbb440ab892be1f2b1811b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce71af360a614394a3d44ad85d17b35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42686 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11768 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10581 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9172 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33358 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19926 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9010 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10602 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9711 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11719 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13941 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16619 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10833 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20228 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8477 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10527 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13445 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9975 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12104 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ace3ef4c744c00b7168aa207a58c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03abe7d025a44ca9adf5a99242fb15e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00116-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00116-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b480520ac6634b4bb54db436a8118d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34b1aaf70a7407a8934ccd7668dd41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11053 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16105 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18098 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12895 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15530 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10112 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14307 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11210 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8989 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14192 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8237 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15609 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7346863f38714ffd89975fa01f4cbeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd951083b7f4f9fae3e954b18be3358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00112-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00112-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbecf592f914a7d90fee7a22333c308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae12f373e8c44908855dcfc9aa436b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30163 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10399 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12285 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8363 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8778 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11094 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8263 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10314 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9820 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8794 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8797 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (54243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36080 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8936 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16608 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13326 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9756 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9215 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7373eeb489c41c186b042d05c0b8f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1784da3bcb844449fc2b9f293136cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00122-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00122-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e321c906ca44b00aea787612cb8184d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026c1080996b44fd8ca0a4aa5f165d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37018 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15984 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37576 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14061 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24408 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11260 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11188 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10124 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8537 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10634 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (65684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9384 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20295 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55631 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10203 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27072 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18662 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6567a67dc6b641f180aeabd446d1a5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96644bdc57404150b034c8792d0f6f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00103-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00103-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dffe6eed804915ae52a2ca8ca30dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd91b5184fe34f1d99d7a785864cea43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14606 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8222 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12525 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (78142 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9253 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17234 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28498 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10087 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21797 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9091 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11372 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (152917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9964 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10139 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9628 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da482673ad44e6fa64d3473a702390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f9852c2684481ba5b17d2165ed8158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00120-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00120-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0e88b874fd411bb42760def6a9de88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ee3b2fa1b344cc9e1e88518c8c52c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15469 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10705 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10739 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9355 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23203 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12144 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8203 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10887 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9302 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8904 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14307 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11797 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12559 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14105 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17110 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8727 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9181 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17065 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14365 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8638 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26478 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a0efddf77e49769c07425594a215a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef81dff65a0a41d8b57f9ade40fb887f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00118-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00118-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923eccf9988a4e2596b5c42d6e64647f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d3a969b1e942c8994e79e25a7b65ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8946 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11663 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10642 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8629 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10001 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8275 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17669 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8737 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16469 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26473 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14406 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14785 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9026 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13748 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffe1b44490747ef887705bf8cf0b828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10aa8adeb7854d68b818391d30354e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00121-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00121-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3667c7f8e1aa48f2b42b5dad1af0d022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9ac35f939b45e0ae12dd7625ffaeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17796 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14854 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10252 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12228 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16784 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14042 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9428 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8918 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15102 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11981 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16000 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8617 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9353 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10067 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16127 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61182880246144449f5230a36ef226e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26cab76ba9c493b979a83c2d9452470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00125-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00125-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a7ae4ab48a432abe5163e87433d0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734cfea36c8b4c189650bdb8ce9adef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23392 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14538 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11976 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12461 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14963 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8840 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10668 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10999 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15675 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9506 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11085 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19950 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8915 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11048 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27414 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (105166 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2817bd730f014fe7b06b0616aafbd193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c98b854e29049fb8fbcd9607124d8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00123-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00123-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f59d49ed05042f6b83ba371b6d22fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b6fdf002dc4a7b914132e016b707b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14083 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25417 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31849 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12797 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10534 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24216 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8763 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12152 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10861 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8590 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10072 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15066 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13852 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9920 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18329 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28452 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13513 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224ea7582a9642acaa2227a056d877dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2dc75c1bf04feba6c72838edafe095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00124-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00124-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a56d288dcc454cabc868cf0b2edd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31234ce5f3b94d449d086f6b871b693a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11987 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8195 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9728 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9862 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9171 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14301 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31666 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16086 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8307 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17037 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12946 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (49740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8224 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13860 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8907 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62ae1ecb50c43118724868f1f92338c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a790522f71704acf8765e3399c684c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00127-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00127-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e11e850f074eafa241b34d26c182d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8abbf4f74fc4fb48c2e0831461b0026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14502 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10430 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11481 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8478 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9563 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13763 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14081 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (59726 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10526 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21446 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8817 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (89673 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15125 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9132 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9373 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8533 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34186 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9204 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a8353229124fbb9d2f470c981759b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b654160da34e9cb12d32225ef5e443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00130-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00130-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3504da4d52b84a528440fae919983cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d3332480f14bc1b1636d9092992791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10046 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9862 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9426 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14151 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9557 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11474 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10098 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9820 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9662 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21836 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13990 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9685 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22196 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9569 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (60058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14588 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44944 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (159393 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe3cba9e31342098bd3618459370b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b3a937cae7436893e93881cd45a7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00131-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00131-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9354ea245ff94c2b9e4463f60c8840ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dabd28d39144b495787f40e26c1bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20811 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14344 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10975 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20920 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9096 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28788 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11644 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8793 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14054 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10741 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55174 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18708 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11847 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14318 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (29514 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8851 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8536 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22850 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21035 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (83859 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2678d60fce9740b1a8b615f8ddc4c5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e8025d5a204aabadaeaa31cef88db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00132-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00132-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5e8fe3d2204303911de6977d1c5ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3507bc04f54dd797f74a47870ee4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26046 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10186 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8659 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12536 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8549 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9287 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10828 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10993 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32871 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11147 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9778 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19330 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23385 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8556 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (88208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10337 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bd02d59f934b50bc02e6974910447e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75827a98467b4919b2f7c98a5a695138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00119-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00119-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b93de97eac42128d0baf7add3a06a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f22f65fe3974a2392aed66a45d30ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15058 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10886 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10061 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10279 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8519 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9660 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10264 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10317 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9682 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11307 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12531 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8267 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21007 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21713 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15746 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13751 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9378 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (190204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18153 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fe7794a71546a6b1132b9fc78f1bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3797c179954d7b85227236096efb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00126-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00126-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38042fe4038408c89960ceaf7e2919a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e08c633cd040a6ad302d08dc94fe0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16665 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26782 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13939 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14772 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8853 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8507 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39089 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8493 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25579 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11087 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12798 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18617 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10200 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8947 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14676 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9158 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21670 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42162 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14499 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16962 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9939 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8601 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13225 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10650 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12313 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79be34f8df04ef1b63686b384204cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aad1e3c57d4386b34bcc5ce26fc345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00129-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00129-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574b6fabc2cc435e96a26cfa7840698d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88973617b15f4c2c831ade61ec589611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13003 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14970 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13299 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12243 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8266 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11044 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18226 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9113 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9429 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12110 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9457 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12658 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12928 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14945 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8511 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8773 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8720 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8262 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12342 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33877 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7148dcfc514a467585e53712a2aa1772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08f4e0bda3c49d5bfe437dc35d9c5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00128-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00128-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2b4e3d3c534a38ad33aecdc156ce94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240006153aca46868ee367d867258867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13143 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18028 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30485 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10748 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9983 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9957 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17152 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14511 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20525 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9381 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9149 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9334 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11449 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10288 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (75903 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9908 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13628 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9379 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9714 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8965 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9308 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9901 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10559 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2384f2305c884f7b869143b728fd1669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/77000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3524a8ea0d8a4b6fbf647f24840b5e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00133-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00133-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b7f8230f574dc3bcb89a4a56673b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff1f45073754582b089407d5ece2c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40470 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9770 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9027 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (54193 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10970 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9227 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10092 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14110 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11795 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8684 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38497 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12088 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12388 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8866 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21761 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21698 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16856 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8274 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6844dc6776a045b7a012c5020e9ec56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7e2e5d40c546708704618c2f7b7700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00134-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00134-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71b6b82e5c946a1baa6074842164a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83d63614560471ea0858cdeab4dd710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17919 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13818 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8306 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9954 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23792 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18564 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12220 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9088 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38925 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9074 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10685 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10500 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9627 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23411 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9949 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (46427 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27774 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12917 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8699 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8217 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dae54f81444288acb38e5967cc4bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad9161c677c4b29910482533b9cf393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00135-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00135-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6632e368a94357a4f7772e6c9cb2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b27cefa5934403953a5a8bb8b249cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/54922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (46268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11921 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9177 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13826 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8630 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8961 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9953 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24923 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18943 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15319 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8920 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24737 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18103 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11064 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (145971 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8911 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9159 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15677 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45819 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10513 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10462 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14899 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15511 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3dd37238054efc94f24443f8370c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/54922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab21d9592270448b9914c8efcf09ee79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00136-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00136-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df784cf903724e2196bcf68260b3024a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8428b072ea4b5dabdee47ae4c90b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12018 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10148 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10475 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18068 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13698 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34968 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9395 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17210 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13348 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14689 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14025 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10909 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8827 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13013 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15070 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30769 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9215 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11508 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (170011 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (50479 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11271 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11820 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3270c8087af64dfc8cb45c178bf64b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d66d7832c8a4a078040ff15fc3b0554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00138-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00138-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc0197d0f8441cd9306a0225fab39db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f07d12dd0d743d184c2433d3d92132c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14280 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8856 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14702 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45332 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12244 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8994 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12885 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11958 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22389 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17620 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32251 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21199 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8908 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16612 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24211 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9074 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9017 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15702 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (46755 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10106 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9812 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8277 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8509 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8816 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10545 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11312 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cce58de9f4f438f90a1340cbe128f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5cdca54d254d9d9570c0d7e60f4152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00140-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00140-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4633319431e34a678e1e9704d17853a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428d9b8f0cce47d09be16ceeb22022ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10077 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11264 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8311 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16879 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28732 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21983 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11369 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10183 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8803 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11978 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20593 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8217 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10279 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8871 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13738 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12161 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26500 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39839 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8299 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38072 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9942 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16304 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d47835c396a4a9389da08ed53ae1884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9575ece4add34325af0d7b5dc0bef8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00142-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00142-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd34f5e381a149d9a70b36debd258c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c0ac0c3dd447568bcbffc8807559d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14688 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15500 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15222 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8609 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15135 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14982 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9488 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8366 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10783 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9033 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10905 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41703 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (62359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8835 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17865 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26664 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26690 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31555 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8360 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (78817 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11115 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16831 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17242 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13270 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8674 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19445 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50312907228545c4878c948a25ba636f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63a7c9e5238426ebb51d516ad00d916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00144-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00144-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88374acf59d14791a115d5629ef63f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364799b4c887451799ac01717308163b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9867 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11319 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10900 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19229 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17428 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12703 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9870 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18420 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9583 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9960 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25733 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9795 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9956 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11185 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21855 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21544 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13611 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16394 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15459 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8501 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11201 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18910 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11805 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13343 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11208 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11025 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357c011cf37542019e1a403a2e9ea3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a557af58df4fec86e323e0d9e08b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00137-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00137-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e276be791d5a456483518883972fc82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d2fe51b1004be78cefdfb049639529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11346 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15548 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21036 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8649 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8356 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9451 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9692 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17561 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14524 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (72352 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10465 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10240 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8573 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31291 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20122 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14995 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8383 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8652 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14183 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10604 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22647 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32541 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8624 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8878 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6009cf3eccac4203b7e8e388891dda85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb7f19f71d444578f0d84ddb19ef765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00145-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00145-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17886d6adf0b48b198c705aaf8dd6d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d026654b43c4407887b4fb4183fc909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/64000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11710 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26333 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9610 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14881 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8712 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11786 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12980 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8259 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14019 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9032 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11651 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9567 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33362 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8424 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10726 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9801 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12583 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27310 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8813 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12991 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26059 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8264 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13780 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15546 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c100b2b84e1476694212ca6af8c5008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/64000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2056eccc51d4c5b931e6a48625e7fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00139-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00139-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c6954caddb4e35a2d790afe0f39c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdff16f77bfa4ddabb25cd53dba8488b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8653 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13512 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17261 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8209 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15016 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11616 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8221 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9437 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27868 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (68170 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10166 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13886 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20633 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13487 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8724 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12444 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15567 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16100 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11436 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15553 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8680 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18492 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8421 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10716 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (145205 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b211845c01e4adda0d8ce37846603ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d287f67782c24c40b580e1d5938cb814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00141-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00141-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925c10a6cff046e1b43a1b31871719a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7976730ba61142f48b30079eff6d7152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9955 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10709 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9463 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25789 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13007 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8468 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30309 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9883 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14706 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30316 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9460 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13194 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15259 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16325 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9552 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11220 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8368 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18873 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9176 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15973 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13296 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14791 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19056 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730c7783bba248718ccf782c88484756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/55000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8732d496f04c4917a46dafff74eda0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00147-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00147-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c02f0e0b5146ccb8cd6e6acbb49711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b75c097fc344e129d944353f528846b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/67922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9696 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13401 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9484 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42029 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9768 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (116431 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10671 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10375 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8805 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11273 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10007 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11305 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13374 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14002 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (82771 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9657 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11959 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9170 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9057 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8258 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11814 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21376 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12158 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2e13e3616a410b9e5ac41cf5711961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/67922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cb0522abe14557986339554fed25f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00149-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00149-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71065b0d5cd74408be05c72cafbaacfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299c89ecfd4e449facca52cf9052c580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/102000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9482 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10126 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14312 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18807 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9241 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42014 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11723 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8393 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27760 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9050 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12848 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11340 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14551 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10402 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9989 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41115 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20371 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14119 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12421 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8351 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12916 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11041 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15063 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10133 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12104 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db71719ba82a435eb8b0563f5ecb9a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/102000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c24f93424de426cb6c4808400741352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00146-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00146-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e36f8610e4c4b0182eea2a3be3fc55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589429c22b46402eaa7e526e74ee728b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/66000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9776 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15263 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9265 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8717 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10250 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13180 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13253 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22328 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18045 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14931 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28363 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15550 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10859 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9157 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9799 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (70740 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11364 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10707 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8274 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12704 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16298 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25779 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8933 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8323 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15597 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11150 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12950 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99f6d402f7f4b1cab8009ee61bbe5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/66000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c28c0711c34b1195032097dd5a466b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00143-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00143-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97aa76c162974d14bdccac86cb720ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c759ba23f49043f89de093f5d86b2026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15986 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10638 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8635 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10146 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10104 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8387 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15236 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8777 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18972 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10021 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13114 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15494 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9249 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12062 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18346 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8836 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8843 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15753 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64242 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11268 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12621 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (61367 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14425 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14141 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12413 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9841 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11637 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9643 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c358dc8826c446098b7085c80066006a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/56000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1259f43cfb5943458553cf450abea274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00148-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00148-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fbabd2c3464179b73d272e260feb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e46d2549194fdeb4a7ffb041b1ac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/68000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10089 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9422 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39455 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12475 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12940 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11093 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10324 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9875 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9825 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8415 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9336 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8951 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17138 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12884 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16646 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10206 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8344 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10266 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13486 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13625 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19169 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9935 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9888 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21476 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8322 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16435 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14586 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11592 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18742 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1471ce163b824809844af012c33c1e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/68000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391881cc434f4d35b67eece345996a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00150-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00150-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4321e8bebf324ca6a418501ed596d1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b50f3c6db1b41f8884b6007dfa9f867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/121000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23745 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13907 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11613 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20276 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9489 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16640 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9752 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11051 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10837 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9762 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38846 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16305 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13877 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9404 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15815 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9832 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13568 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10744 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14495 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17810 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8533 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18084 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8615 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10563 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9246 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12352 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ec4d033ca84f69804def1d2f5a1966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/121000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991694f70bcb4031bd02c86917659847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/608 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00152-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00152-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dda75bb7684b1fb700e9f48a67116e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4956af1c08944e38e675325fdab5726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/114000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8643 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13099 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21522 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18603 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13281 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9278 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13198 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8354 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12614 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8219 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12731 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17286 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13490 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33293 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9430 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9405 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10290 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8245 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23846 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15242 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11304 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15178 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39578 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28164 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9991 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (144305 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (27849 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13693 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9568 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e0ed846e6b4d8fb94025b61343a2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/114000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18ca2f8c7434aeaa6993f572be12ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultura_x-train-00156-of-00163\n",
      "/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/cultura_x-train-00156-of-00163_4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf4ea33ab734befbdaafdf8ea2870ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4e221faa394bc1b28c4032948ee651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12598 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8447 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9345 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15554 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8212 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8327 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8570 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10584 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9184 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12918 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10432 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19543 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14876 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12418 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12757 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9407 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13176 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12156 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64204 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14359 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11055 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10749 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17289 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8396 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (45874 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38074 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9571 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23283 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20433 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (43560 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9352 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8315a08c57eb4da8a131989e6680f64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 4096 (num_proc=32):   0%|          | 0/78000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ec740223d141558f0e9a159dc45a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards): 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct<timestamp: list<item: string>, url: list<item: string>, source: list<item: string>, input_ids: list<item: int32>, attention_mask: list<item: int8>, labels: list<item: int64>>\n",
      "struct<timestamp: string, url: string, source: string, input_ids: list<item: int32>, attention_mask: list<item: int8>>\n",
      "Converting to match types\n",
      "Before transformation: {'timestamp': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transform_record' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 75\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting to match types\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore transformation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, processed_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m---> 75\u001b[0m processed_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m processed_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[43mtransform_record\u001b[49m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter transformation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, processed_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transform_record' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "lm_datasets = []\n",
    "path = Path(dataset_dir)\n",
    "files = [file.name for file in path.glob(\"*.parquet\")]\n",
    "files\n",
    "# \\\n",
    "for idx, file in enumerate(files):\n",
    "    data_file = os.path.join(path, file)\n",
    "    filename = \"\".join(file.split(\".\")[:-1])\n",
    "    print(filename)\n",
    "    cache_path = os.path.join(\n",
    "        data_cache_dir, filename + f\"_{block_size}\"\n",
    "    )\n",
    "    print(cache_path)\n",
    "    os.makedirs(cache_path, exist_ok=True)\n",
    "    try:\n",
    "        processed_dataset = datasets.load_from_disk(\n",
    "            cache_path, keep_in_memory=False\n",
    "        )\n",
    "        logger.info(f\"training datasets-{filename} has been loaded from disk\")\n",
    "    except Exception:\n",
    "        cache_dir = os.path.join(\n",
    "            data_cache_dir, filename + f\"_text_{block_size}\"\n",
    "        )\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        raw_dataset = load_dataset(\n",
    "            \"parquet\",\n",
    "            data_files=data_file,\n",
    "            cache_dir=cache_dir,\n",
    "            keep_in_memory=False,\n",
    "        )\n",
    "        logger.info(f\"{file} has been loaded\")\n",
    "        tokenized_dataset = raw_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=32, #preprocessing_num_workers,\n",
    "            remove_columns=\"text\",\n",
    "            load_from_cache_file=True,\n",
    "            keep_in_memory=False,\n",
    "            cache_file_names={\n",
    "                k: os.path.join(cache_dir, \"tokenized.arrow\")\n",
    "                for k in raw_dataset\n",
    "            },\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "        grouped_datasets = tokenized_dataset.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=32, #preprocessing_num_workers,\n",
    "            load_from_cache_file=True,\n",
    "            keep_in_memory=False,\n",
    "            cache_file_names={\n",
    "                k: os.path.join(cache_dir, f\"grouped_{i}.arrow\")\n",
    "                for k in tokenized_chunk\n",
    "            },\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "        )\n",
    "        processed_dataset = grouped_datasets\n",
    "        processed_dataset.save_to_disk(cache_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if idx == 0:\n",
    "        lm_datasets = processed_dataset[\"train\"]\n",
    "    else:\n",
    "        if lm_datasets.features.type != processed_dataset[\"train\"].features.type:\n",
    "          print(lm_datasets.features.type)\n",
    "          print(processed_dataset[\"train\"].features.type)\n",
    "\n",
    "          print(\"Converting to match types\")\n",
    "\n",
    "\n",
    "          print(\"Before transformation:\", processed_dataset[\"train\"].features)\n",
    "          processed_dataset['train'] = processed_dataset['train'].map(transform_record)\n",
    "          print(\"After transformation:\", processed_dataset[\"train\"].features)\n",
    "\n",
    "          continue\n",
    "          # assert (\n",
    "          #     lm_datasets.features.type\n",
    "          #     == processed_dataset[\"train\"].features.type\n",
    "          # )\n",
    "        lm_datasets = concatenate_datasets(\n",
    "            [lm_datasets, processed_dataset[\"train\"]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 57664\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = lm_datasets.train_test_split(\n",
    "    test_size=validation_split_percentage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data tokenization/grouping Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data_cache_dir = data_cache_dir + 'tmp_cache2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/.cache/huggingface/datasets/uonlp___cultura_x/bn/0.0.0/b777d9a574b83926bcf05f4635f6d1f996792596/parquet_files/uonlp_cache/tmp_cache2/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $tmp_data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def combine_short_sequences(examples, max_length=256, absolute_max_length=4096):\n",
    "    combined = {k: [] for k in examples.keys()}\n",
    "    current_sequence = {k: [] for k in examples.keys()}\n",
    "    current_length = 0\n",
    "    newline_token_ids = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "    \n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        sequence_length = len(examples['input_ids'][i])\n",
    "        if current_length + sequence_length > max_length: # or current_length + sequence_length > absolute_max_length:\n",
    "            # Add the current combined sequence and start a new one\n",
    "            for k in combined.keys():\n",
    "                combined[k].append(list(chain(*current_sequence[k])))\n",
    "                current_sequence[k] = []\n",
    "            current_length = 0\n",
    "        \n",
    "        # Add the current sequence\n",
    "        for k in examples.keys():\n",
    "            if k == 'input_ids':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append(newline_token_ids)\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            elif k == 'attention_mask':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append([1] * len(newline_token_ids))\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            else:\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "        \n",
    "        current_length += sequence_length\n",
    "        if current_sequence['input_ids']:\n",
    "            current_length += len(newline_token_ids)\n",
    "    \n",
    "    # Add the last combined sequence\n",
    "    for k in combined.keys():\n",
    "        if current_sequence[k]:\n",
    "            combined[k].append(list(chain(*current_sequence[k])))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Use this before group_texts\n",
    "    examples = combine_short_sequences(examples)\n",
    "\n",
    "    print(f\"Input keys: {examples.keys()}\")\n",
    "    print(f\"Number of examples: {len(examples[list(examples.keys())[0]])}\")\n",
    "    # Concatenate all texts\n",
    "    print(f\"Input length: {len(examples['input_ids'])}\")\n",
    "    \n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples['input_ids'])\n",
    "\n",
    "    print(f\"Concatenated keys: {concatenated_examples.keys()}\")\n",
    "    \n",
    "    print(f\"Actual total length: {sum(len(ids) for ids in examples['input_ids'])}\")\n",
    "    print(f\"Total length after concatenation: {total_length}\")\n",
    "\n",
    "\n",
    "    block_size = 512\n",
    "    # More granular block size selection\n",
    "    if total_length < 512:\n",
    "        block_size = 512\n",
    "    if total_length < 1024:\n",
    "        block_size = 1024\n",
    "    elif total_length < 2048:\n",
    "        block_size = 2048\n",
    "    else: #if total_length < 4096:\n",
    "        block_size = 4096\n",
    "    # else:\n",
    "    #     block_size = 8192  # Increase maximum block size, ensure your model can handle this\n",
    "\n",
    "    print(f\"Block size: {block_size}\")\n",
    "\n",
    "    # Adaptive padding\n",
    "    padding_unit = 128  # or another suitable value\n",
    "    if total_length < block_size:\n",
    "        block_size = ((total_length + padding_unit - 1) // padding_unit) * padding_unit\n",
    "    \n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "\n",
    "    print(f\"Number of blocks to be created: {num_blocks}\")\n",
    "\n",
    "    result = {k: [] for k in concatenated_examples.keys()}\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        block_start = i * block_size\n",
    "        block_end = min((i + 1) * block_size, total_length)\n",
    "        for k, t in concatenated_examples.items():\n",
    "            block = t[block_start:block_end]\n",
    "            if len(block) < block_size:\n",
    "                padding_length = block_size - len(block)\n",
    "                if k == 'input_ids':\n",
    "                    block = block + [tokenizer.pad_token_id] * padding_length\n",
    "                elif k == 'attention_mask':\n",
    "                    block = block + [0] * padding_length\n",
    "            result[k].append(block)\n",
    "\n",
    "\n",
    "    print(f\"Result keys after splitting: {result.keys()}\")\n",
    "    print(f\"Number of blocks created: {len(result[list(result.keys())[0]])}\")\n",
    "\n",
    "    # Create labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    print(f\"Final result keys: {result.keys()}\")\n",
    "    print(f\"Final number of examples: {len(result['labels'])}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def group_texts_old(examples):\n",
    "    print(f\"Input keys: {examples.keys()}\")\n",
    "    print(f\"Number of examples: {len(examples[list(examples.keys())[0]])}\")\n",
    "    print(f\"Block size: {block_size}\")\n",
    "\n",
    "    # Concatenate all texts\n",
    "    print(f\"Input length: {len(examples['input_ids'])}\")\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    \n",
    "    \n",
    "    print(f\"Concatenated keys: {concatenated_examples.keys()}\")\n",
    "    total_length = len(concatenated_examples['input_ids'])\n",
    "    \n",
    "    print(f\"Actual total length: {sum(len(ids) for ids in examples['input_ids'])}\")\n",
    "    print(f\"Total length after concatenation: {total_length}\")\n",
    "\n",
    "    # Calculate how many blocks we'll create\n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "    print(f\"Number of blocks to be created: {num_blocks}\")\n",
    "\n",
    "    # Split by chunks of block_size, allowing the last chunk to be smaller\n",
    "    result = {\n",
    "        k: [t[i * block_size : (i + 1) * block_size] for i in range(num_blocks)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    print(f\"Result keys after splitting: {result.keys()}\")\n",
    "    print(f\"Number of blocks created: {len(result[list(result.keys())[0]])}\")\n",
    "\n",
    "    # Create labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    print(f\"Final result keys: {result.keys()}\")\n",
    "    print(f\"Final number of examples: {len(result['labels'])}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $tmp_data_cache_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls $tmp_data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_chunk['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[\"text\"], truncation=False)\n",
    "    # clm input could be much much longer than block_size\n",
    "    # if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "    #     tok_logger.warning(\n",
    "    #         \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "    #         \" before being passed to the model.\"\n",
    "    #     )\n",
    "    return output\n",
    "\n",
    "# # Example tokenizer function that does not truncate\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True, max_length=block_size)\n",
    "\n",
    "def tokenize_and_check(examples):\n",
    "    result = tokenize_function(examples)\n",
    "    print(f\"Average tokens per text: {sum(len(ids) for ids in result['input_ids']) / len(result['input_ids'])}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk = DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'timestamp', 'url', 'source'],\n",
      "        num_rows: 76000\n",
      "    })\n",
      "})\n",
      "tokenized_chunk = DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 76000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8440002f0d484eb4dc0462fd2f091b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of dynamic block_size (num_proc=32):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 1001Number of examples: 1001\n",
      "\n",
      "Input length: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "\n",
      "Input length: 1001Number of examples: 1001\n",
      "\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 1001\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "\n",
      "Number of examples: 1001Input length: 1001\n",
      "\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2814520Actual total length: 2747548\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Total length after concatenation: 2814520Total length after concatenation: 2747548\n",
      "\n",
      "Block size: 4096\n",
      "Block size: 4096\n",
      "Number of examples: 1001Number of blocks to be created: 688\n",
      "\n",
      "\n",
      "Number of blocks to be created: 671Input length: 1001\n",
      "\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Actual total length: 2780859\n",
      "\n",
      "Total length after concatenation: 2780859Number of examples: 1001\n",
      "\n",
      "Block size: 4096Input length: 1001Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "Number of blocks to be created: 679\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2789787\n",
      "Total length after concatenation: 2789787\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 682\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2858420\n",
      "Total length after concatenation: 2858420\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 698Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2886820\n",
      "Total length after concatenation: 2886820\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 705\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Actual total length: 3012596\n",
      "\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Actual total length: 2908091Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Total length after concatenation: 3012596\n",
      "Number of examples: 1001Number of blocks created: 671\n",
      "\n",
      "Input length: 1001\n",
      "Total length after concatenation: 2908091Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 710Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Number of blocks created: 688Actual total length: 2981320Final number of examples: 671Block size: 4096\n",
      "\n",
      "Total length after concatenation: 2981320\n",
      "\n",
      "\n",
      "Number of blocks to be created: 736Block size: 4096Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "\n",
      "Number of blocks to be created: 728Final number of examples: 688\n",
      "\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2712695\n",
      "Total length after concatenation: 2712695\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096\n",
      "Actual total length: 2985598\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 2985598\n",
      "\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096Number of blocks created: 679Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "\n",
      "Number of blocks created: 682Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Number of examples: 1001\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 679\n",
      "Input length: 1001Final number of examples: 682\n",
      "Number of blocks to be created: 729\n",
      "\n",
      "\n",
      "\n",
      "Number of blocks to be created: 663\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2882021\n",
      "Total length after concatenation: 2882021\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 704\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 705\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 705\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks created: 698\n",
      "\n",
      "Actual total length: 3011602Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Total length after concatenation: 3011602\n",
      "Final number of examples: 698Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks created: 710\n",
      "\n",
      "Number of blocks to be created: 736Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Input length: 1001\n",
      "\n",
      "\n",
      "Final number of examples: 710\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 728\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 728\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Input length: 1001\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 736Number of blocks created: 663\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Final number of examples: 663Final number of examples: 736Number of blocks created: 729Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Actual total length: 3039541Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Actual total length: 2824777\n",
      "\n",
      "\n",
      "Total length after concatenation: 3039541Total length after concatenation: 2824777Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Final number of examples: 729Block size: 4096\n",
      "\n",
      "\n",
      "\n",
      "Block size: 4096Actual total length: 3077641\n",
      "Number of blocks to be created: 743Number of blocks to be created: 690\n",
      "\n",
      "\n",
      "Total length after concatenation: 3077641Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Block size: 4096\n",
      "Number of blocks created: 704\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Number of blocks to be created: 752\n",
      "\n",
      "Final number of examples: 704\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 736\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 736\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2893176\n",
      "Total length after concatenation: 2893176\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 707\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 743Number of blocks created: 690\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 743\n",
      "\n",
      "Final number of examples: 690\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 752\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 752\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2881988\n",
      "Total length after concatenation: 2881988\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 704\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 707Input length: 1001\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 707\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Actual total length: 2860693\n",
      "\n",
      "Actual total length: 2811655Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 2860693\n",
      "Total length after concatenation: 2811655\n",
      "\n",
      "\n",
      "Actual total length: 2802270Block size: 4096Block size: 4096\n",
      "\n",
      "\n",
      "Total length after concatenation: 2802270Number of blocks to be created: 687Number of blocks to be created: 699\n",
      "\n",
      "\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 685\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2894589\n",
      "Total length after concatenation: 2894589\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 707\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 704\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 704\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 687\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final number of examples: 687Number of blocks created: 699\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 699\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 685\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 685\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3001808\n",
      "Total length after concatenation: 3001808\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 733\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 707\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 707\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 733\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 733\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2875976Actual total length: 2868648\n",
      "\n",
      "Total length after concatenation: 2875976\n",
      "Total length after concatenation: 2868648Block size: 4096\n",
      "Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 701Number of blocks to be created: 703\n",
      "\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 703\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 703\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 701\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 701\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2892831\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 2892831\n",
      "\n",
      "Block size: 4096Number of examples: 1001\n",
      "\n",
      "Number of blocks to be created: 707\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2919952\n",
      "Total length after concatenation: 2919952\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 713\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 707Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "\n",
      "Final number of examples: 707\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3022643\n",
      "Total length after concatenation: 3022643\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 738\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2771840\n",
      "Total length after concatenation: 2771840\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 677\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 713\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 713\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 677\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final number of examples: 677Number of blocks created: 738\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 738\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3043824\n",
      "Total length after concatenation: 3043824\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 744\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2982828\n",
      "Total length after concatenation: 2982828\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 729\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 744\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 744Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 3027040\n",
      "Total length after concatenation: 3027040\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 740\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 729\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 729\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 740\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 740\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2918165\n",
      "Total length after concatenation: 2918165\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 713\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2858621\n",
      "Total length after concatenation: 2858621\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 698\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 713\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 713\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Input length: 1001Number of examples: 1001\n",
      "\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 698\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 698\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2671359\n",
      "Total length after concatenation: 2671359\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 653\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2896836\n",
      "Total length after concatenation: 2896836\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 708\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2783807\n",
      "Total length after concatenation: 2783807\n",
      "Block size: 4096Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks to be created: 680\n",
      "Number of blocks created: 653\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 653\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2871752\n",
      "Total length after concatenation: 2871752\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 702Actual total length: 2878274\n",
      "\n",
      "Total length after concatenation: 2878274\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 703\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 708\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 708\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 680\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 680\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 702\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 702\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of examples: 1001\n",
      "\n",
      "Input length: 1001Number of blocks created: 703\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 703\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001Actual total length: 3006933\n",
      "\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 1001Total length after concatenation: 3006933\n",
      "\n",
      "\n",
      "Number of examples: 1001Block size: 4096Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "Input length: 1001Number of examples: 1001Number of blocks to be created: 735\n",
      "\n",
      "\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2859723\n",
      "Total length after concatenation: 2859723\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 699\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2781495\n",
      "Total length after concatenation: 2781495\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 680\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 735\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 735\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2997309\n",
      "Total length after concatenation: 2997309\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096\n",
      "\n",
      "Number of blocks created: 699Number of blocks to be created: 732\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final number of examples: 699Actual total length: 3135931\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 3135931Number of blocks created: 680\n",
      "\n",
      "\n",
      "Actual total length: 2909200Block size: 4096\n",
      "\n",
      "Total length after concatenation: 2909200Number of blocks to be created: 766Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "\n",
      "Final number of examples: 680Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 711\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2838615\n",
      "Total length after concatenation: 2838615\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 694\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2832262\n",
      "Total length after concatenation: 2832262\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 692Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 2771585\n",
      "Total length after concatenation: 2771585\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2903367\n",
      "Block size: 4096Total length after concatenation: 2903367\n",
      "\n",
      "Number of blocks to be created: 677Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 709\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2838164\n",
      "Total length after concatenation: 2838164\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 693Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 732\n",
      "\n",
      "Number of blocks created: 711Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Number of examples: 1001\n",
      "\n",
      "Input length: 1001Final number of examples: 732Final number of examples: 711\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "Number of blocks created: 694\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 694\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 766\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Final number of examples: 766\n",
      "\n",
      "\n",
      "Number of examples: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 692Input length: 1001\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 692\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 709\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 709Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 677\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Final number of examples: 677\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 693\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 693\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2899245\n",
      "Total length after concatenation: 2899245\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 708\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2852597\n",
      "Total length after concatenation: 2852597\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 697\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2797422\n",
      "Total length after concatenation: 2797422\n",
      "Block size: 4096Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks to be created: 683Actual total length: 2875881\n",
      "\n",
      "Total length after concatenation: 2875881\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 703\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 708\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 708\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 3143791Number of blocks created: 697\n",
      "\n",
      "Total length after concatenation: 3143791Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Block size: 4096Final number of examples: 697\n",
      "\n",
      "Number of blocks to be created: 768\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2942703\n",
      "Total length after concatenation: 2942703\n",
      "Block size: 4096\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks to be created: 719\n",
      "\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 683\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 683\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 703\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 703\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 719\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final number of examples: 719Number of blocks created: 768\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 768\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2857487\n",
      "Total length after concatenation: 2857487\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 698\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 698\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 698\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3070846\n",
      "Total length after concatenation: 3070846\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 750\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2941909\n",
      "Total length after concatenation: 2941909\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 719\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 750\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 750\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 719\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 719\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 2917693\n",
      "Total length after concatenation: 2917693\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 713Actual total length: 2860537\n",
      "\n",
      "Total length after concatenation: 2860537\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 699\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 699\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 699\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 713\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 713\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 1001\n",
      "Input length: 1001\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 1001Actual total length: 3264494\n",
      "Total length after concatenation: 3264494\n",
      "\n",
      "Input length: 1001Block size: 4096\n",
      "Number of blocks to be created: 797\n",
      "\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 797\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 797\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of examples: 376\n",
      "\n",
      "Actual total length: 2791641Input length: 376\n",
      "\n",
      "Total length after concatenation: 2791641\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 682\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 1071472Number of blocks created: 682\n",
      "\n",
      "Total length after concatenation: 1071472Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Block size: 4096\n",
      "Final number of examples: 682\n",
      "Number of blocks to be created: 262\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 262Number of examples: 1001\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Input length: 1001Final number of examples: 262\n",
      "\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 3166582\n",
      "Total length after concatenation: 3166582\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 774\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1109323\n",
      "Total length after concatenation: 1109323\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 271\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 271\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 271\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 774\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 774\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1086382\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Total length after concatenation: 1086382Number of examples: 376\n",
      "\n",
      "Block size: 4096Input length: 376\n",
      "\n",
      "Number of blocks to be created: 266\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 266\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 266\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1032169\n",
      "Total length after concatenation: 1032169\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 252\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 252\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 252\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1092550\n",
      "Total length after concatenation: 1092550\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 267\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1102957\n",
      "Total length after concatenation: 1102957\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 270\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1178901\n",
      "Total length after concatenation: 1178901Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Block size: 4096Number of blocks created: 267\n",
      "Number of blocks to be created: 288\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 267\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 270\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 270Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 288Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Number of examples: 376\n",
      "\n",
      "Final number of examples: 288Input length: 376\n",
      "\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1065929\n",
      "Total length after concatenation: 1065929\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 261\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 959903\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 959903\n",
      "\n",
      "Block size: 4096Number of examples: 376\n",
      "\n",
      "Number of blocks to be created: 235Input length: 376\n",
      "\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1015755\n",
      "Total length after concatenation: 1015755\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 248\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of blocks created: 235Number of blocks created: 261\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 235\n",
      "\n",
      "Final number of examples: 261\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 248\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 248\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 1029039Number of examples: 376\n",
      "\n",
      "Total length after concatenation: 1029039Input length: 376\n",
      "\n",
      "Block size: 4096\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks to be created: 252\n",
      "\n",
      "Actual total length: 1231667\n",
      "Total length after concatenation: 1231667\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 301\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Input length: 376Number of examples: 376\n",
      "\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 301Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks created: 252\n",
      "\n",
      "\n",
      "Actual total length: 1075597Final number of examples: 301\n",
      "\n",
      "Total length after concatenation: 1075597Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "Final number of examples: 252Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 263\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "\n",
      "Input length: 376Actual total length: 1141705Actual total length: 1104521\n",
      "\n",
      "\n",
      "Total length after concatenation: 1141705Total length after concatenation: 1104521\n",
      "\n",
      "Block size: 4096Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 279Number of blocks to be created: 270\n",
      "\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 263\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 263\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 270\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 270Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 376\n",
      "Input length: 376Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 1007326\n",
      "Total length after concatenation: 1007326\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 246\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 279\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 279Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Actual total length: 1022683\n",
      "Total length after concatenation: 1022683\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 250\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1068244\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Total length after concatenation: 1068244\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 246\n",
      "Block size: 4096\n",
      "Number of blocks created: 250\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Number of blocks to be created: 261Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "\n",
      "Final number of examples: 246Final number of examples: 250\n",
      "\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 261\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 261\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1091989\n",
      "Total length after concatenation: 1091989\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 267\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input length: 376\n",
      "\n",
      "Actual total length: 1117180\n",
      "Total length after concatenation: 1117180\n",
      "Block size: 4096\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks to be created: 273\n",
      "\n",
      "Number of blocks created: 267\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 267\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 273\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 273\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "\n",
      "Number of examples: 376Number of examples: 376\n",
      "\n",
      "Input length: 376Input length: 376\n",
      "\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1099253\n",
      "Total length after concatenation: 1099253\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 269\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 269\n",
      "Actual total length: 1059793Actual total length: 1035905\n",
      "\n",
      "\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])Total length after concatenation: 1059793Total length after concatenation: 1035905\n",
      "\n",
      "\n",
      "Final number of examples: 269Block size: 4096\n",
      "Block size: 4096\n",
      "\n",
      "Number of blocks to be created: 259Number of blocks to be created: 253\n",
      "\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1037889\n",
      "Total length after concatenation: 1037889\n",
      "Block size: 4096\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])Number of blocks to be created: 254\n",
      "\n",
      "Number of blocks created: 253\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 253\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 259\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 259\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 254\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 254\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1144995\n",
      "Total length after concatenation: 1144995\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 280\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1045654\n",
      "Total length after concatenation: 1045654\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 256\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1066498\n",
      "Total length after concatenation: 1066498\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 261\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 280\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 280\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 256\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 256\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 261\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 261\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1081130\n",
      "Total length after concatenation: 1081130\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 264\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 264\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 264\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1539642\n",
      "Total length after concatenation: 1539642\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 376\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 376\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1037375\n",
      "Total length after concatenation: 1037375\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 254\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 254\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 254\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1120606\n",
      "Total length after concatenation: 1120606\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 274\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 274\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 274\n",
      "Input keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of examples: 376\n",
      "Input length: 376\n",
      "Concatenated keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Actual total length: 1206993\n",
      "Total length after concatenation: 1206993\n",
      "Block size: 4096\n",
      "Number of blocks to be created: 295\n",
      "Result keys after splitting: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask'])\n",
      "Number of blocks created: 295\n",
      "Final result keys: dict_keys(['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'])\n",
      "Final number of examples: 295\n",
      "grouped_chunk = DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 54020\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "lm_datasets = []\n",
    "block_size=512\n",
    "\n",
    "\n",
    "path = Path(dataset_dir)\n",
    "files = [file.name for file in path.glob(\"*.parquet\")]\n",
    "\n",
    "file = files[48]\n",
    "\n",
    "data_file = os.path.join(path, file)\n",
    "filename = \"\".join(file.split(\".\")[:-1])\n",
    "\n",
    "cache_dir = os.path.join(\n",
    "    tmp_data_cache_dir, filename + f\"_parquet_maxblocksize_{block_size}\"\n",
    ")\n",
    "\n",
    "raw_dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=data_file,\n",
    "    cache_dir=cache_dir,\n",
    "    keep_in_memory=False,\n",
    ")\n",
    "\n",
    "# Process in chunks similar to Dataset1 file size\n",
    "chunk_size = len(raw_dataset['train'])\n",
    "for i in range(0, len(raw_dataset['train']), chunk_size):\n",
    "    cur_i = 0\n",
    "    i = cur_i\n",
    "    #logger.info(f\"{file} has been loaded\")\n",
    "    \n",
    "    # Create a chunk that's a DatasetDict, just like raw_dataset\n",
    "    chunk = DatasetDict({\n",
    "        'train': Dataset.from_dict(\n",
    "            raw_dataset['train'].select(range(i, min(i+chunk_size, len(raw_dataset['train'])))).to_dict()\n",
    "        )\n",
    "    })\n",
    "    print(f\"chunk = {chunk}\")\n",
    "    tokenized_chunk = chunk.map(\n",
    "        tokenize_and_check,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=32, #data_args.preprocessing_num_workers,\n",
    "        remove_columns=[\"text\"],\n",
    "        load_from_cache_file=True,\n",
    "        keep_in_memory=False,\n",
    "        cache_file_names={\n",
    "            k: os.path.join(cache_dir, f\"tokenized_{filename}_{block_size}_{i}.arrow\")\n",
    "            for k in tokenized_chunk\n",
    "        },\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    print(f\"tokenized_chunk = {tokenized_chunk}\")\n",
    "    \n",
    "    grouped_chunk = tokenized_chunk.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=32, #data_args.preprocessing_num_workers,\n",
    "        load_from_cache_file=False,\n",
    "        keep_in_memory=False,\n",
    "        cache_file_names={\n",
    "            k: os.path.join(cache_dir, f\"grouped_{filename}_{block_size}_{i}.arrow\")\n",
    "            for k in tokenized_chunk\n",
    "        },\n",
    "        desc=f\"Grouping texts in chunks of dynamic block_size\",\n",
    "    )\n",
    "\n",
    "    print(f\"grouped_chunk = {grouped_chunk}\")\n",
    "    \n",
    "    if i == cur_i:\n",
    "        processed_dataset = grouped_chunk\n",
    "    else:\n",
    "        processed_dataset = concatenate_datasets([processed_dataset, grouped_chunk])\n",
    "    \n",
    "    break\n",
    "\n",
    "idx = 0\n",
    "if idx == 0:\n",
    "    lm_datasets = processed_dataset\n",
    "else:\n",
    "    lm_datasets = concatenate_datasets([lm_datasets, processed_dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count total tokens in the dataset\n",
    "from tqdm import tqdm\n",
    "def count_total_tokens(dataset, tokenizer, text_column=\"text\"):\n",
    "    total_text = 0\n",
    "    total_tokens = 0\n",
    "    for example in tqdm(dataset[\"train\"], desc=\"Counting texts and tokens length\"):\n",
    "        text = example[text_column]\n",
    "        tokens = tokenizer(text)[\"input_ids\"]\n",
    "        total_text += len(text)\n",
    "        total_tokens += len(tokens)\n",
    "    return total_text, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'timestamp', 'url', 'source'],\n",
       "        num_rows: 76000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 54020\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tokens:   0%|                                             | 0/76000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (40136 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Counting tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76000/76000 [03:50<00:00, 329.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of texts: 183765478 \n",
      " tokens in the dataset: 220508397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total tokens in the dataset\n",
    "total_text, total_tokens = count_total_tokens(raw_dataset, tokenizer)\n",
    "print(f\"Total number of texts: {total_text} \\n tokens in the dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split_percentage = 0.1\n",
    "lm_datasets = lm_datasets['train'].train_test_split(\n",
    "        test_size=validation_split_percentage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'timestamp', 'url', 'source'],\n",
       "        num_rows: 77000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 48618\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5402\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1717658191701,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "# Wikipedia provides a title and an article text.\n",
    "# Use https://translate.google.com!\n",
    "alpaca_prompt = \"\"\"à¦à¦–à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¨à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹, à¦¯à¦¾ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦œ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§‡, à¦à¦¬à¦‚ à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹ à¦¯à¦¾ à¦†à¦°à¦“ à¦ªà§à¦°à§‡à¦•à§à¦·à¦¾à¦ªà¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤ à¦à¦•à¦Ÿà¦¿ à¦‰à¦¤à§à¦¤à¦° à¦²à¦¿à¦–à§à¦¨ à¦¯à¦¾ à¦…à¦¨à§à¦°à§‹à¦§à¦Ÿà¦¿ à¦¸à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤ \n",
    "### Instruction: {}\n",
    "\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "# becomes:\n",
    "wikipedia_prompt = \"\"\"à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦¨à¦¿à¦¬à¦¨à§à¦§\n",
    "### à¦¶à¦¿à¦°à§‹à¦¨à¦¾à¦®: {}\n",
    "\n",
    "### à¦¨à¦¿à¦¬à¦¨à§à¦§:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(convo[0][\"value\"], convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw0tuebw4Ppe"
   },
   "source": [
    "We only use 1% of the dataset to speed things up! Use more for longer runs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Continued Pretraining\n",
    "Now let's use Unsloth's `UnslothTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 20 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
    "\n",
    "Also set `embedding_learning_rate` to be a learning rate at least 2x or 10x smaller than `learning_rate` to make continual pretraining work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  38839 MiB |  38839 MiB |  39165 GiB |  39127 GiB |\n",
      "|       from large pool |  38804 MiB |  38804 MiB |  39040 GiB |  39002 GiB |\n",
      "|       from small pool |     35 MiB |     35 MiB |    125 GiB |    125 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  38839 MiB |  38839 MiB |  39165 GiB |  39127 GiB |\n",
      "|       from large pool |  38804 MiB |  38804 MiB |  39040 GiB |  39002 GiB |\n",
      "|       from small pool |     35 MiB |     35 MiB |    125 GiB |    125 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  38809 MiB |  38809 MiB |  39156 GiB |  39118 GiB |\n",
      "|       from large pool |  38774 MiB |  38774 MiB |  39030 GiB |  38992 GiB |\n",
      "|       from small pool |     35 MiB |     35 MiB |    125 GiB |    125 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  42624 MiB |  42624 MiB |    983 GiB |    942 GiB |\n",
      "|       from large pool |  42584 MiB |  42584 MiB |    981 GiB |    940 GiB |\n",
      "|       from small pool |     40 MiB |     40 MiB |      1 GiB |      1 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   3784 MiB |   3784 MiB |  61625 GiB |  61621 GiB |\n",
      "|       from large pool |   3779 MiB |   3779 MiB |  61498 GiB |  61495 GiB |\n",
      "|       from small pool |      4 MiB |      4 MiB |    126 GiB |    126 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     821    |     821    |  458496    |  457675    |\n",
      "|       from large pool |     682    |     682    |  277817    |  277135    |\n",
      "|       from small pool |     139    |     139    |  180679    |  180540    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     821    |     821    |  458496    |  457675    |\n",
      "|       from large pool |     682    |     682    |  277817    |  277135    |\n",
      "|       from small pool |     139    |     139    |  180679    |  180540    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     284    |     284    |    1373    |    1089    |\n",
      "|       from large pool |     264    |     264    |     366    |     102    |\n",
      "|       from small pool |      20    |      20    |    1007    |     987    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      17    |      17    |  236828    |  236811    |\n",
      "|       from large pool |       5    |       5    |  111071    |  111066    |\n",
      "|       from small pool |      12    |      12    |  125757    |  125745    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:34:18] Energy consumed for RAM : 0.250753 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:18] Energy consumed for all GPUs : 0.055731 kWh. Total GPU Power : 67.26548683104527 W\n",
      "[codecarbon INFO @ 14:34:18] Energy consumed for all CPUs : 0.046815 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:18] 0.353298 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:25] Energy consumed for RAM : 0.071979 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:25] Energy consumed for all GPUs : 0.006863 kWh. Total GPU Power : 67.39344048417644 W\n",
      "[codecarbon INFO @ 14:34:25] Energy consumed for all CPUs : 0.013416 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:25] 0.092258 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:27] Energy consumed for RAM : 0.379717 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:27] Energy consumed for all GPUs : 0.075252 kWh. Total GPU Power : 66.28695029699956 W\n",
      "[codecarbon INFO @ 14:34:27] Energy consumed for all CPUs : 0.070786 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:27] 0.525756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for RAM : 0.207504 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for RAM : 0.188861 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for all GPUs : 0.044835 kWh. Total GPU Power : 66.73695977463593 W\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for all CPUs : 0.038681 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:29] 0.291020 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for all GPUs : 0.042685 kWh. Total GPU Power : 66.74786190979862 W\n",
      "[codecarbon INFO @ 14:34:29] Energy consumed for all CPUs : 0.035208 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:29] 0.266754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:33] Energy consumed for RAM : 0.253882 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:33] Energy consumed for all GPUs : 0.056008 kWh. Total GPU Power : 66.72205427835652 W\n",
      "[codecarbon INFO @ 14:34:33] Energy consumed for all CPUs : 0.047398 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:33] 0.357289 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:40] Energy consumed for RAM : 0.075108 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:40] Energy consumed for all GPUs : 0.007139 kWh. Total GPU Power : 66.29611396649406 W\n",
      "[codecarbon INFO @ 14:34:40] Energy consumed for all CPUs : 0.013999 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:40] 0.096247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:34:42] Energy consumed for RAM : 0.382847 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 14:34:42] Energy consumed for all GPUs : 0.075529 kWh. Total GPU Power : 66.28275228288952 W\n",
      "[codecarbon INFO @ 14:34:42] Energy consumed for all CPUs : 0.071370 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 14:34:42] 0.529745 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingCallback(TrainerCallback):\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        logger.info(f\"Starting step {state.global_step}\")\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        logger.info(f\"Finished step {state.global_step}\")\n",
    "        logger.info(f\"Learning rate: {args.learning_rate}\")\n",
    "        logger.info(f\"GPU memory allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "        logger.info(f\"GPU memory reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            logger.info(f\"Logs: {logs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  3 21:36:31 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:24:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              66W / 300W |  40719MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Invoke garbage collector multiple times\n",
    "for _ in range(10):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# if trainer:\n",
    "#     del trainer\n",
    "#     time.sleep(2)\n",
    "# Reset the CUDA runtime\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "#clear_gpu_memory()\n",
    "\n",
    "# Optionally, you can use nvidia-smi command to kill all processes using GPU\n",
    "import os\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 48618\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['timestamp', 'url', 'source', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5402\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Both warmup_ratio and warmup_steps given, warmup_steps will override any effect of warmup_ratio during training\n",
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "[codecarbon INFO @ 21:36:50] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:36:50] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:36:50] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 21:36:50] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:36:50] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 21:36:52] CPU Model on constant consumption mode: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 21:36:52] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:36:52]   Platform system: Linux-5.4.0-163-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 21:36:52]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 21:36:52]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 21:36:52]   Available RAM : 1007.785 GB\n",
      "[codecarbon INFO @ 21:36:52]   CPU count: 128\n",
      "[codecarbon INFO @ 21:36:52]   CPU model: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 21:36:52]   GPU count: 1\n",
      "[codecarbon INFO @ 21:36:52]   GPU model: 1 x NVIDIA A100 80GB PCIe\n",
      "[codecarbon INFO @ 21:36:54] Energy consumed for RAM : 0.028234 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:36:54] Energy consumed for all GPUs : 0.006681 kWh. Total GPU Power : 67.8730034423808 W\n",
      "[codecarbon INFO @ 21:36:54] Energy consumed for all CPUs : 0.008476 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:36:54] 0.043391 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:37:09] Energy consumed for RAM : 0.029792 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:37:09] Energy consumed for all GPUs : 0.006958 kWh. Total GPU Power : 67.20285134264628 W\n",
      "[codecarbon INFO @ 21:37:09] Energy consumed for all CPUs : 0.008942 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:37:09] 0.045692 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:37:24] Energy consumed for RAM : 0.031357 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:37:24] Energy consumed for all GPUs : 0.007239 kWh. Total GPU Power : 67.76074888013711 W\n",
      "[codecarbon INFO @ 21:37:24] Energy consumed for all CPUs : 0.009413 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:37:24] 0.048009 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:37:39] Energy consumed for RAM : 0.032915 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:37:39] Energy consumed for all GPUs : 0.007518 kWh. Total GPU Power : 67.6092056871266 W\n",
      "[codecarbon INFO @ 21:37:39] Energy consumed for all CPUs : 0.009883 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:37:39] 0.050316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:37:54] Energy consumed for RAM : 0.034470 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:37:54] Energy consumed for all GPUs : 0.007792 kWh. Total GPU Power : 66.82664712652573 W\n",
      "[codecarbon INFO @ 21:37:54] Energy consumed for all CPUs : 0.010346 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:37:54] 0.052608 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:38:09] Energy consumed for RAM : 0.036042 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:38:09] Energy consumed for all GPUs : 0.008073 kWh. Total GPU Power : 67.3958989584246 W\n",
      "[codecarbon INFO @ 21:38:09] Energy consumed for all CPUs : 0.010817 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:38:09] 0.054932 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:38:24] Energy consumed for RAM : 0.037610 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:38:24] Energy consumed for all GPUs : 0.008355 kWh. Total GPU Power : 68.0891173935563 W\n",
      "[codecarbon INFO @ 21:38:24] Energy consumed for all CPUs : 0.011293 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:38:24] 0.057257 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:38:39] Energy consumed for RAM : 0.039152 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:38:39] Energy consumed for all GPUs : 0.008630 kWh. Total GPU Power : 67.34516342112616 W\n",
      "[codecarbon INFO @ 21:38:39] Energy consumed for all CPUs : 0.011754 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:38:39] 0.059537 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:38:54] Energy consumed for RAM : 0.040720 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:38:54] Energy consumed for all GPUs : 0.008907 kWh. Total GPU Power : 66.72682540423781 W\n",
      "[codecarbon INFO @ 21:38:54] Energy consumed for all CPUs : 0.012221 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:38:54] 0.061848 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:39:09] Energy consumed for RAM : 0.042294 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:39:09] Energy consumed for all GPUs : 0.009190 kWh. Total GPU Power : 67.82174713884051 W\n",
      "[codecarbon INFO @ 21:39:09] Energy consumed for all CPUs : 0.012697 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:39:09] 0.064181 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run the trainer with the custom callback\n",
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    eval_dataset=train_dataset['test'],\n",
    "\n",
    "    packing=True,\n",
    "    dataset_text_field = None, #\"text\",\n",
    "    \n",
    "    \n",
    "    #max_seq_length = 512, #block_size, #max_seq_length,\n",
    "    dataset_num_proc=32,\n",
    "    \n",
    "    args=UnslothTrainingArguments(\n",
    "        per_device_train_batch_size=12,  # Increased batch size to leverage GPU memory\n",
    "        gradient_accumulation_steps=8,   # Adjusted gradient accumulation\n",
    "        \n",
    "        #max_steps = 120,\n",
    "        warmup_steps=100,                 # More warmup steps for stability with larger batch sizes\n",
    "        warmup_ratio=0.05,\n",
    "        num_train_epochs=1,               # Increase epochs for better training (adjust as needed)\n",
    "        \n",
    "        learning_rate=1e-4,               # Adjusted learning rate for larger batch size\n",
    "        embedding_learning_rate=2e-5,     # Adjusted embedding learning rate\n",
    "        \n",
    "        fp16 = False, #not is_bfloat16_supported(),\n",
    "        bf16 = True, #is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        evaluation_strategy=\"steps\",  # Added\n",
    "        eval_steps=250,  # Added\n",
    "        save_strategy=\"steps\",  # Added\n",
    "        save_steps=250,  # Added\n",
    "    ),\n",
    "    callbacks=[LoggingCallback()]  # Add the custom logging callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1717658245568,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "4eaa7bd1-f800-49d3-a8f5-58c918f339cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100 80GB PCIe. Max memory = 79.151 GB.\n",
      "39.217 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "import torch\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5239701,
     "status": "ok",
     "timestamp": 1717663485260,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "c60e3467-a197-48e1-8d52-c9999b59efc7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 12\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: url, timestamp, source. If url, timestamp, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 48,618 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 12 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 96 | Total steps = 506\n",
      " \"-____-\"     Number of trainable parameters = 1,386,217,472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 2.00e-05 instead of 1.00e-04 for embed_tokens.\n",
      "Unsloth: Setting lr = 2.00e-05 instead of 1.00e-04 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:39:25] Energy consumed for RAM : 0.043953 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:39:25] Energy consumed for all GPUs : 0.009778 kWh. Total GPU Power : 134.00433783007998 W\n",
      "[codecarbon INFO @ 21:39:25] Energy consumed for all CPUs : 0.013191 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:39:25] 0.066922 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:39:34] Energy consumed for RAM : 0.001581 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:39:34] Energy consumed for all GPUs : 0.001138 kWh. Total GPU Power : 271.98800733112324 W\n",
      "[codecarbon INFO @ 21:39:34] Energy consumed for all CPUs : 0.000471 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:39:34] 0.003191 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:39:41] Energy consumed for RAM : 0.045614 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:39:41] Energy consumed for all GPUs : 0.010989 kWh. Total GPU Power : 275.47657423953285 W\n",
      "[codecarbon INFO @ 21:39:41] Energy consumed for all CPUs : 0.013686 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:39:41] 0.070288 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:39:49] Energy consumed for RAM : 0.003154 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:39:50] Energy consumed for all GPUs : 0.002307 kWh. Total GPU Power : 280.68674140986064 W\n",
      "[codecarbon INFO @ 21:39:50] Energy consumed for all CPUs : 0.000947 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:39:50] 0.006408 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:39:56] Energy consumed for RAM : 0.047258 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:39:56] Energy consumed for all GPUs : 0.012207 kWh. Total GPU Power : 279.9086640775008 W\n",
      "[codecarbon INFO @ 21:39:56] Energy consumed for all CPUs : 0.014176 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:39:56] 0.073641 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:04] Energy consumed for RAM : 0.004705 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:40:04] Energy consumed for all GPUs : 0.003460 kWh. Total GPU Power : 280.85205734955633 W\n",
      "[codecarbon INFO @ 21:40:04] Energy consumed for all CPUs : 0.001413 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:40:04] 0.009578 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:12] Energy consumed for RAM : 0.048922 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:40:12] Energy consumed for all GPUs : 0.013436 kWh. Total GPU Power : 279.05403439697074 W\n",
      "[codecarbon INFO @ 21:40:12] Energy consumed for all CPUs : 0.014671 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:40:12] 0.077028 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:19] Energy consumed for RAM : 0.006262 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:40:19] Energy consumed for all GPUs : 0.004601 kWh. Total GPU Power : 277.06887844301656 W\n",
      "[codecarbon INFO @ 21:40:19] Energy consumed for all CPUs : 0.001877 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:40:19] 0.012741 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:28] Energy consumed for RAM : 0.050575 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:40:28] Energy consumed for all GPUs : 0.014660 kWh. Total GPU Power : 279.8283299288784 W\n",
      "[codecarbon INFO @ 21:40:28] Energy consumed for all CPUs : 0.015164 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:40:28] 0.080400 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:34] Energy consumed for RAM : 0.007835 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:40:34] Energy consumed for all GPUs : 0.005788 kWh. Total GPU Power : 285.0270910734352 W\n",
      "[codecarbon INFO @ 21:40:34] Energy consumed for all CPUs : 0.002352 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:40:34] 0.015975 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:44] Energy consumed for RAM : 0.052266 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:40:44] Energy consumed for all GPUs : 0.015896 kWh. Total GPU Power : 276.15940589905404 W\n",
      "[codecarbon INFO @ 21:40:44] Energy consumed for all CPUs : 0.015667 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:40:44] 0.083830 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:40:49] Energy consumed for RAM : 0.009390 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:40:49] Energy consumed for all GPUs : 0.006932 kWh. Total GPU Power : 277.94594861932467 W\n",
      "[codecarbon INFO @ 21:40:49] Energy consumed for all CPUs : 0.002817 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:40:49] 0.019139 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:41:00] Energy consumed for RAM : 0.053943 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:41:00] Energy consumed for all GPUs : 0.017152 kWh. Total GPU Power : 282.83662014510577 W\n",
      "[codecarbon INFO @ 21:41:00] Energy consumed for all CPUs : 0.016167 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:41:00] 0.087263 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:41:04] Energy consumed for RAM : 0.010956 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:41:04] Energy consumed for all GPUs : 0.008112 kWh. Total GPU Power : 284.9349945871644 W\n",
      "[codecarbon INFO @ 21:41:04] Energy consumed for all CPUs : 0.003288 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:41:04] 0.022356 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:41:16] Energy consumed for RAM : 0.055616 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:41:16] Energy consumed for all GPUs : 0.018412 kWh. Total GPU Power : 284.6001673717802 W\n",
      "[codecarbon INFO @ 21:41:16] Energy consumed for all CPUs : 0.016666 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:41:16] 0.090694 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:41:19] Energy consumed for RAM : 0.012514 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:41:19] Energy consumed for all GPUs : 0.009288 kWh. Total GPU Power : 285.1777308207902 W\n",
      "[codecarbon INFO @ 21:41:19] Energy consumed for all CPUs : 0.003756 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:41:19] 0.025558 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='506' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 10/506 17:19 < 17:53:39, 0.01 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:41:31] Energy consumed for RAM : 0.057189 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:41:31] Energy consumed for all GPUs : 0.019429 kWh. Total GPU Power : 244.3147270222525 W\n",
      "[codecarbon INFO @ 21:41:31] Energy consumed for all CPUs : 0.017134 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:41:31] 0.093752 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:41:35] Energy consumed for RAM : 0.014189 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:41:35] Energy consumed for all GPUs : 0.010368 kWh. Total GPU Power : 243.66057462333364 W\n",
      "[codecarbon INFO @ 21:41:35] Energy consumed for all CPUs : 0.004255 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:41:35] 0.028812 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:41:46] Energy consumed for RAM : 0.058763 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:41:46] Energy consumed for all GPUs : 0.020599 kWh. Total GPU Power : 280.97498530955085 W\n",
      "[codecarbon INFO @ 21:41:46] Energy consumed for all CPUs : 0.017603 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:41:46] 0.096965 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:41:51] Energy consumed for RAM : 0.015845 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:41:51] Energy consumed for all GPUs : 0.011619 kWh. Total GPU Power : 285.2981615872394 W\n",
      "[codecarbon INFO @ 21:41:51] Energy consumed for all CPUs : 0.004749 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:41:51] 0.032213 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:42:01] Energy consumed for RAM : 0.060337 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:42:01] Energy consumed for all GPUs : 0.021802 kWh. Total GPU Power : 288.7520167172368 W\n",
      "[codecarbon INFO @ 21:42:01] Energy consumed for all CPUs : 0.018072 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:42:01] 0.100210 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:42:07] Energy consumed for RAM : 0.017529 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:42:07] Energy consumed for all GPUs : 0.012881 kWh. Total GPU Power : 283.13026354399176 W\n",
      "[codecarbon INFO @ 21:42:07] Energy consumed for all CPUs : 0.005251 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:42:07] 0.035661 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:42:16] Energy consumed for RAM : 0.061910 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:42:16] Energy consumed for all GPUs : 0.022971 kWh. Total GPU Power : 280.7864000891887 W\n",
      "[codecarbon INFO @ 21:42:16] Energy consumed for all CPUs : 0.018541 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:42:16] 0.103422 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:42:23] Energy consumed for RAM : 0.019219 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:42:23] Energy consumed for all GPUs : 0.014144 kWh. Total GPU Power : 282.31858726720117 W\n",
      "[codecarbon INFO @ 21:42:23] Energy consumed for all CPUs : 0.005754 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:42:23] 0.039116 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:42:31] Energy consumed for RAM : 0.063482 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:42:31] Energy consumed for all GPUs : 0.024174 kWh. Total GPU Power : 289.1677064212971 W\n",
      "[codecarbon INFO @ 21:42:31] Energy consumed for all CPUs : 0.019019 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:42:31] 0.106675 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:42:39] Energy consumed for RAM : 0.020889 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:42:39] Energy consumed for all GPUs : 0.015406 kWh. Total GPU Power : 285.6929273990214 W\n",
      "[codecarbon INFO @ 21:42:39] Energy consumed for all CPUs : 0.006253 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:42:39] 0.042548 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:42:46] Energy consumed for RAM : 0.065026 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:42:46] Energy consumed for all GPUs : 0.025339 kWh. Total GPU Power : 284.8271711985123 W\n",
      "[codecarbon INFO @ 21:42:46] Energy consumed for all CPUs : 0.019479 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:42:46] 0.109844 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:42:56] Energy consumed for RAM : 0.022587 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:42:56] Energy consumed for all GPUs : 0.016677 kWh. Total GPU Power : 282.6067875788723 W\n",
      "[codecarbon INFO @ 21:42:56] Energy consumed for all CPUs : 0.006759 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:42:56] 0.046022 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:43:01] Energy consumed for RAM : 0.066602 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:43:01] Energy consumed for all GPUs : 0.026527 kWh. Total GPU Power : 284.8962906102984 W\n",
      "[codecarbon INFO @ 21:43:01] Energy consumed for all CPUs : 0.019955 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:43:01] 0.113085 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:43:12] Energy consumed for RAM : 0.024292 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:43:12] Energy consumed for all GPUs : 0.017947 kWh. Total GPU Power : 281.52388425367093 W\n",
      "[codecarbon INFO @ 21:43:12] Energy consumed for all CPUs : 0.007267 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:43:12] 0.049507 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:43:16] Energy consumed for RAM : 0.068153 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:43:16] Energy consumed for all GPUs : 0.027699 kWh. Total GPU Power : 285.68588255352006 W\n",
      "[codecarbon INFO @ 21:43:16] Energy consumed for all CPUs : 0.020424 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:43:16] 0.116276 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:43:28] Energy consumed for RAM : 0.025959 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:43:28] Energy consumed for all GPUs : 0.019208 kWh. Total GPU Power : 285.7495161737254 W\n",
      "[codecarbon INFO @ 21:43:28] Energy consumed for all CPUs : 0.007763 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:43:28] 0.052931 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:43:31] Energy consumed for RAM : 0.069703 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:43:31] Energy consumed for all GPUs : 0.028892 kWh. Total GPU Power : 290.672504642711 W\n",
      "[codecarbon INFO @ 21:43:31] Energy consumed for all CPUs : 0.020893 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:43:31] 0.119488 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:43:44] Energy consumed for RAM : 0.027691 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:43:44] Energy consumed for all GPUs : 0.020502 kWh. Total GPU Power : 282.1620719280024 W\n",
      "[codecarbon INFO @ 21:43:44] Energy consumed for all CPUs : 0.008279 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:43:44] 0.056473 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:43:46] Energy consumed for RAM : 0.071252 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:43:46] Energy consumed for all GPUs : 0.030057 kWh. Total GPU Power : 284.2986270439707 W\n",
      "[codecarbon INFO @ 21:43:46] Energy consumed for all CPUs : 0.021358 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:43:46] 0.122668 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:44:00] Energy consumed for RAM : 0.029358 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:44:00] Energy consumed for all GPUs : 0.021782 kWh. Total GPU Power : 290.2272407287815 W\n",
      "[codecarbon INFO @ 21:44:00] Energy consumed for all CPUs : 0.008776 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:44:00] 0.059916 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:44:01] Energy consumed for RAM : 0.072813 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:44:01] Energy consumed for all GPUs : 0.031253 kWh. Total GPU Power : 289.57520937253867 W\n",
      "[codecarbon INFO @ 21:44:01] Energy consumed for all CPUs : 0.021826 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:44:01] 0.125892 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:44:16] Energy consumed for RAM : 0.074397 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:44:16] Energy consumed for RAM : 0.031072 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:44:16] Energy consumed for all GPUs : 0.032430 kWh. Total GPU Power : 280.6857001174296 W\n",
      "[codecarbon INFO @ 21:44:16] Energy consumed for all CPUs : 0.022298 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:44:16] 0.129125 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:44:16] Energy consumed for all GPUs : 0.023052 kWh. Total GPU Power : 279.83425013353366 W\n",
      "[codecarbon INFO @ 21:44:16] Energy consumed for all CPUs : 0.009287 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:44:16] 0.063411 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:44:32] Energy consumed for RAM : 0.076071 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:44:32] Energy consumed for RAM : 0.032745 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:44:32] Energy consumed for all GPUs : 0.033709 kWh. Total GPU Power : 288.63069529494703 W\n",
      "[codecarbon INFO @ 21:44:32] Energy consumed for all CPUs : 0.022796 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:44:32] 0.132576 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:44:32] Energy consumed for all GPUs : 0.024330 kWh. Total GPU Power : 288.74508993271223 W\n",
      "[codecarbon INFO @ 21:44:32] Energy consumed for all CPUs : 0.009785 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:44:32] 0.066861 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:44:49] Energy consumed for RAM : 0.034450 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:44:49] Energy consumed for RAM : 0.077775 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:44:49] Energy consumed for all GPUs : 0.025645 kWh. Total GPU Power : 291.4746473819167 W\n",
      "[codecarbon INFO @ 21:44:49] Energy consumed for all CPUs : 0.010303 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:44:49] 0.070397 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:44:49] Energy consumed for all GPUs : 0.035024 kWh. Total GPU Power : 291.3390515812977 W\n",
      "[codecarbon INFO @ 21:44:49] Energy consumed for all CPUs : 0.023314 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:44:49] 0.136114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:45:05] Energy consumed for RAM : 0.079442 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:45:05] Energy consumed for RAM : 0.036117 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:45:05] Energy consumed for all GPUs : 0.036278 kWh. Total GPU Power : 284.36088257847234 W\n",
      "[codecarbon INFO @ 21:45:05] Energy consumed for all CPUs : 0.023811 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:45:05] 0.139531 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:45:05] Energy consumed for all GPUs : 0.026925 kWh. Total GPU Power : 289.8709254316914 W\n",
      "[codecarbon INFO @ 21:45:05] Energy consumed for all CPUs : 0.010810 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:45:05] 0.073852 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:45:21] Energy consumed for RAM : 0.037775 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:45:21] Energy consumed for RAM : 0.081134 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:45:21] Energy consumed for all GPUs : 0.028183 kWh. Total GPU Power : 286.8374611195291 W\n",
      "[codecarbon INFO @ 21:45:21] Energy consumed for all CPUs : 0.011303 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:45:21] 0.077261 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:45:21] Energy consumed for all GPUs : 0.037562 kWh. Total GPU Power : 286.6973286393449 W\n",
      "[codecarbon INFO @ 21:45:21] Energy consumed for all CPUs : 0.024315 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:45:21] 0.143010 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:45:37] Energy consumed for RAM : 0.082854 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:45:37] Energy consumed for RAM : 0.039496 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:45:37] Energy consumed for all GPUs : 0.029476 kWh. Total GPU Power : 283.69177474841933 W\n",
      "[codecarbon INFO @ 21:45:37] Energy consumed for all CPUs : 0.011816 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:45:37] 0.080788 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:45:37] Energy consumed for all GPUs : 0.038854 kWh. Total GPU Power : 283.74759087772276 W\n",
      "[codecarbon INFO @ 21:45:37] Energy consumed for all CPUs : 0.024828 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:45:37] 0.146537 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:45:54] Energy consumed for RAM : 0.041192 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:45:54] Energy consumed for RAM : 0.084550 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:45:54] Energy consumed for all GPUs : 0.030758 kWh. Total GPU Power : 285.6556852583139 W\n",
      "[codecarbon INFO @ 21:45:54] Energy consumed for all CPUs : 0.012321 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:45:54] 0.084271 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:45:54] Energy consumed for all GPUs : 0.040137 kWh. Total GPU Power : 285.67178301441686 W\n",
      "[codecarbon INFO @ 21:45:54] Energy consumed for all CPUs : 0.025333 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:45:54] 0.150020 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:46:10] Energy consumed for RAM : 0.086252 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:46:10] Energy consumed for RAM : 0.042896 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:46:10] Energy consumed for all GPUs : 0.032035 kWh. Total GPU Power : 283.0415229882793 W\n",
      "[codecarbon INFO @ 21:46:10] Energy consumed for all CPUs : 0.012829 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:46:10] 0.087760 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:46:10] Energy consumed for all GPUs : 0.041413 kWh. Total GPU Power : 283.37899894650826 W\n",
      "[codecarbon INFO @ 21:46:10] Energy consumed for all CPUs : 0.025840 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:46:10] 0.153506 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:46:26] Energy consumed for RAM : 0.087958 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:46:26] Energy consumed for RAM : 0.044602 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:46:26] Energy consumed for all GPUs : 0.042701 kWh. Total GPU Power : 285.13221022375353 W\n",
      "[codecarbon INFO @ 21:46:26] Energy consumed for all CPUs : 0.026349 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:46:26] 0.157007 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:46:26] Energy consumed for all GPUs : 0.033322 kWh. Total GPU Power : 285.05204864252903 W\n",
      "[codecarbon INFO @ 21:46:26] Energy consumed for all CPUs : 0.013337 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:46:26] 0.091261 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:46:42] Energy consumed for RAM : 0.046321 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:46:42] Energy consumed for RAM : 0.089677 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:46:42] Energy consumed for all GPUs : 0.034610 kWh. Total GPU Power : 283.13440255855863 W\n",
      "[codecarbon INFO @ 21:46:42] Energy consumed for all CPUs : 0.013849 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:46:42] 0.094780 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:46:42] Energy consumed for all GPUs : 0.043988 kWh. Total GPU Power : 283.010348550517 W\n",
      "[codecarbon INFO @ 21:46:42] Energy consumed for all CPUs : 0.026861 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:46:42] 0.160526 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:46:59] Energy consumed for RAM : 0.091375 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:46:59] Energy consumed for RAM : 0.048020 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:46:59] Energy consumed for all GPUs : 0.035901 kWh. Total GPU Power : 287.03221006105554 W\n",
      "[codecarbon INFO @ 21:46:59] Energy consumed for all CPUs : 0.014355 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:46:59] 0.098276 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:46:59] Energy consumed for all GPUs : 0.045279 kWh. Total GPU Power : 287.0953699699398 W\n",
      "[codecarbon INFO @ 21:46:59] Energy consumed for all CPUs : 0.027367 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:46:59] 0.164021 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:47:15] Energy consumed for RAM : 0.049731 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:47:15] Energy consumed for RAM : 0.093085 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:47:15] Energy consumed for all GPUs : 0.037189 kWh. Total GPU Power : 284.6238578478548 W\n",
      "[codecarbon INFO @ 21:47:15] Energy consumed for all CPUs : 0.014865 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:47:15] 0.101785 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:47:15] Energy consumed for all GPUs : 0.046568 kWh. Total GPU Power : 284.71468618964883 W\n",
      "[codecarbon INFO @ 21:47:15] Energy consumed for all CPUs : 0.027876 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:47:15] 0.167529 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:47:31] Energy consumed for RAM : 0.051442 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:47:31] Energy consumed for RAM : 0.094796 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:47:31] Energy consumed for all GPUs : 0.038471 kWh. Total GPU Power : 282.9497448053482 W\n",
      "[codecarbon INFO @ 21:47:31] Energy consumed for all CPUs : 0.015374 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:47:31] 0.105287 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:47:31] Energy consumed for all GPUs : 0.047849 kWh. Total GPU Power : 283.00274184008003 W\n",
      "[codecarbon INFO @ 21:47:31] Energy consumed for all CPUs : 0.028386 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:47:31] 0.171031 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:47:47] Energy consumed for RAM : 0.096500 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:47:47] Energy consumed for RAM : 0.053146 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:47:48] Energy consumed for all GPUs : 0.049137 kWh. Total GPU Power : 285.49127086361426 W\n",
      "[codecarbon INFO @ 21:47:48] Energy consumed for all CPUs : 0.028894 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:47:48] 0.174530 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:47:48] Energy consumed for all GPUs : 0.039758 kWh. Total GPU Power : 285.3512779746624 W\n",
      "[codecarbon INFO @ 21:47:48] Energy consumed for all CPUs : 0.015882 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:47:48] 0.108787 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:48:04] Energy consumed for RAM : 0.054855 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:48:04] Energy consumed for RAM : 0.098209 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:48:04] Energy consumed for all GPUs : 0.041053 kWh. Total GPU Power : 286.121087309645 W\n",
      "[codecarbon INFO @ 21:48:04] Energy consumed for all CPUs : 0.016391 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:48:04] 0.112299 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:48:04] Energy consumed for all GPUs : 0.050431 kWh. Total GPU Power : 286.01320441603843 W\n",
      "[codecarbon INFO @ 21:48:04] Energy consumed for all CPUs : 0.029403 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:48:04] 0.178044 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:48:20] Energy consumed for RAM : 0.099913 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:48:20] Energy consumed for RAM : 0.056560 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:48:20] Energy consumed for all GPUs : 0.042333 kWh. Total GPU Power : 283.80030055290865 W\n",
      "[codecarbon INFO @ 21:48:20] Energy consumed for all CPUs : 0.016899 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:48:20] 0.115792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:48:20] Energy consumed for all GPUs : 0.051712 kWh. Total GPU Power : 283.84706971159363 W\n",
      "[codecarbon INFO @ 21:48:20] Energy consumed for all CPUs : 0.029911 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:48:20] 0.181536 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:48:36] Energy consumed for RAM : 0.101624 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:48:36] Energy consumed for RAM : 0.058270 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:48:36] Energy consumed for all GPUs : 0.053003 kWh. Total GPU Power : 285.0924236593835 W\n",
      "[codecarbon INFO @ 21:48:36] Energy consumed for all CPUs : 0.030420 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:48:36] 0.185046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:48:36] Energy consumed for all GPUs : 0.043624 kWh. Total GPU Power : 285.00960488910573 W\n",
      "[codecarbon INFO @ 21:48:36] Energy consumed for all CPUs : 0.017409 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:48:36] 0.119303 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:48:52] Energy consumed for RAM : 0.103315 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:48:52] Energy consumed for RAM : 0.059962 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:48:52] Energy consumed for all GPUs : 0.054286 kWh. Total GPU Power : 286.65199885439597 W\n",
      "[codecarbon INFO @ 21:48:52] Energy consumed for all CPUs : 0.030924 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:48:52] 0.188526 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:48:52] Energy consumed for all GPUs : 0.044907 kWh. Total GPU Power : 286.7121426168897 W\n",
      "[codecarbon INFO @ 21:48:53] Energy consumed for all CPUs : 0.017913 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:48:53] 0.122782 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:49:09] Energy consumed for RAM : 0.061705 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:49:09] Energy consumed for RAM : 0.105059 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:49:09] Energy consumed for all GPUs : 0.046208 kWh. Total GPU Power : 281.69369205214747 W\n",
      "[codecarbon INFO @ 21:49:09] Energy consumed for all CPUs : 0.018433 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:49:09] 0.126346 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:49:09] Energy consumed for all GPUs : 0.055586 kWh. Total GPU Power : 281.6349258682203 W\n",
      "[codecarbon INFO @ 21:49:09] Energy consumed for all CPUs : 0.031444 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:49:09] 0.192090 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:49:25] Energy consumed for RAM : 0.063390 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:49:25] Energy consumed for RAM : 0.106743 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:49:25] Energy consumed for all GPUs : 0.047488 kWh. Total GPU Power : 287.1494863807715 W\n",
      "[codecarbon INFO @ 21:49:25] Energy consumed for all CPUs : 0.018934 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:49:25] 0.129812 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:49:25] Energy consumed for all GPUs : 0.056866 kWh. Total GPU Power : 287.2156828238261 W\n",
      "[codecarbon INFO @ 21:49:25] Energy consumed for all CPUs : 0.031946 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:49:25] 0.195556 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:49:41] Energy consumed for RAM : 0.108434 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:49:41] Energy consumed for RAM : 0.065082 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:49:41] Energy consumed for all GPUs : 0.058147 kWh. Total GPU Power : 286.11829340600866 W\n",
      "[codecarbon INFO @ 21:49:41] Energy consumed for all CPUs : 0.032450 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:49:41] 0.199032 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:49:42] Energy consumed for all GPUs : 0.048793 kWh. Total GPU Power : 291.48033312493754 W\n",
      "[codecarbon INFO @ 21:49:42] Energy consumed for all CPUs : 0.019448 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:49:42] 0.133322 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:49:58] Energy consumed for RAM : 0.110140 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:49:58] Energy consumed for RAM : 0.066756 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:49:58] Energy consumed for all GPUs : 0.059430 kWh. Total GPU Power : 284.1895188228415 W\n",
      "[codecarbon INFO @ 21:49:58] Energy consumed for all CPUs : 0.032958 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:49:58] 0.202527 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:49:58] Energy consumed for all GPUs : 0.050051 kWh. Total GPU Power : 283.9832048032929 W\n",
      "[codecarbon INFO @ 21:49:58] Energy consumed for all CPUs : 0.019946 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:49:58] 0.136753 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:50:14] Energy consumed for RAM : 0.111862 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:50:14] Energy consumed for RAM : 0.068478 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:50:14] Energy consumed for all GPUs : 0.060729 kWh. Total GPU Power : 284.9305118729004 W\n",
      "[codecarbon INFO @ 21:50:14] Energy consumed for all CPUs : 0.033471 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:50:14] 0.206063 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:50:14] Energy consumed for all GPUs : 0.051354 kWh. Total GPU Power : 285.6352745673916 W\n",
      "[codecarbon INFO @ 21:50:14] Energy consumed for all CPUs : 0.020460 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:50:14] 0.140291 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:50:30] Energy consumed for RAM : 0.070183 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:50:30] Energy consumed for RAM : 0.113568 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:50:30] Energy consumed for all GPUs : 0.052638 kWh. Total GPU Power : 284.7105126312143 W\n",
      "[codecarbon INFO @ 21:50:30] Energy consumed for all CPUs : 0.020968 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:50:30] 0.143789 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:50:30] Energy consumed for all GPUs : 0.062017 kWh. Total GPU Power : 285.2720624915585 W\n",
      "[codecarbon INFO @ 21:50:30] Energy consumed for all CPUs : 0.033979 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:50:30] 0.209564 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:50:46] Energy consumed for RAM : 0.115260 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:50:46] Energy consumed for RAM : 0.071875 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:50:47] Energy consumed for all GPUs : 0.063326 kWh. Total GPU Power : 292.23156103480153 W\n",
      "[codecarbon INFO @ 21:50:47] Energy consumed for all CPUs : 0.034492 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:50:47] 0.213078 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:50:47] Energy consumed for all GPUs : 0.053947 kWh. Total GPU Power : 292.1392537014215 W\n",
      "[codecarbon INFO @ 21:50:47] Energy consumed for all CPUs : 0.021481 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:50:47] 0.147303 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:51:03] Energy consumed for RAM : 0.116939 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:51:03] Energy consumed for RAM : 0.073554 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:51:03] Energy consumed for all GPUs : 0.064582 kWh. Total GPU Power : 282.6375754597796 W\n",
      "[codecarbon INFO @ 21:51:03] Energy consumed for all CPUs : 0.034992 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:51:03] 0.216513 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:51:03] Energy consumed for all GPUs : 0.055203 kWh. Total GPU Power : 282.7165892944149 W\n",
      "[codecarbon INFO @ 21:51:03] Energy consumed for all CPUs : 0.021981 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:51:03] 0.150738 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:51:19] Energy consumed for RAM : 0.118633 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:51:19] Energy consumed for RAM : 0.075247 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:51:19] Energy consumed for all GPUs : 0.065885 kWh. Total GPU Power : 290.5250215999944 W\n",
      "[codecarbon INFO @ 21:51:19] Energy consumed for all CPUs : 0.035506 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:51:19] 0.220023 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:51:19] Energy consumed for all GPUs : 0.056506 kWh. Total GPU Power : 290.5573758250705 W\n",
      "[codecarbon INFO @ 21:51:19] Energy consumed for all CPUs : 0.022494 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:51:19] 0.154247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:51:35] Energy consumed for RAM : 0.120299 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:51:35] Energy consumed for RAM : 0.076913 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:51:35] Energy consumed for all GPUs : 0.067151 kWh. Total GPU Power : 287.1075511833801 W\n",
      "[codecarbon INFO @ 21:51:35] Energy consumed for all CPUs : 0.036002 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:51:35] 0.223452 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:51:35] Energy consumed for all GPUs : 0.057772 kWh. Total GPU Power : 287.1416879363748 W\n",
      "[codecarbon INFO @ 21:51:35] Energy consumed for all CPUs : 0.022991 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:51:35] 0.157677 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:51:51] Energy consumed for RAM : 0.122000 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:51:51] Energy consumed for RAM : 0.078614 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:51:52] Energy consumed for all GPUs : 0.059077 kWh. Total GPU Power : 289.8882359956026 W\n",
      "[codecarbon INFO @ 21:51:52] Energy consumed for all CPUs : 0.023507 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:51:52] 0.161198 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:51:52] Energy consumed for all GPUs : 0.068456 kWh. Total GPU Power : 289.801465295333 W\n",
      "[codecarbon INFO @ 21:51:52] Energy consumed for all CPUs : 0.036519 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:51:52] 0.226975 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:52:07] Energy consumed for RAM : 0.123669 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:52:07] Energy consumed for RAM : 0.080283 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:52:07] Energy consumed for all GPUs : 0.069707 kWh. Total GPU Power : 283.19331669803853 W\n",
      "[codecarbon INFO @ 21:52:07] Energy consumed for all CPUs : 0.037016 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:52:07] 0.230392 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:52:07] Energy consumed for all GPUs : 0.060328 kWh. Total GPU Power : 283.1139953317751 W\n",
      "[codecarbon INFO @ 21:52:07] Energy consumed for all CPUs : 0.024005 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:52:07] 0.164616 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:52:24] Energy consumed for RAM : 0.081999 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:52:24] Energy consumed for RAM : 0.125386 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:52:24] Energy consumed for all GPUs : 0.061622 kWh. Total GPU Power : 284.7722474029354 W\n",
      "[codecarbon INFO @ 21:52:24] Energy consumed for all CPUs : 0.024516 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:52:24] 0.168137 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:52:24] Energy consumed for all GPUs : 0.071001 kWh. Total GPU Power : 284.69329640989633 W\n",
      "[codecarbon INFO @ 21:52:24] Energy consumed for all CPUs : 0.037528 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:52:24] 0.233914 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:52:40] Energy consumed for RAM : 0.083687 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:52:40] Energy consumed for RAM : 0.127073 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:52:40] Energy consumed for all GPUs : 0.062896 kWh. Total GPU Power : 285.1819795554722 W\n",
      "[codecarbon INFO @ 21:52:40] Energy consumed for all CPUs : 0.025019 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:52:40] 0.171602 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:52:40] Energy consumed for all GPUs : 0.072280 kWh. Total GPU Power : 286.30445934232966 W\n",
      "[codecarbon INFO @ 21:52:40] Energy consumed for all CPUs : 0.038031 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:52:40] 0.237384 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:52:56] Energy consumed for RAM : 0.085405 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:52:56] Energy consumed for RAM : 0.128790 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:52:56] Energy consumed for all GPUs : 0.073566 kWh. Total GPU Power : 282.98606177058394 W\n",
      "[codecarbon INFO @ 21:52:56] Energy consumed for all CPUs : 0.038542 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:52:56] 0.240898 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:52:56] Energy consumed for all GPUs : 0.064187 kWh. Total GPU Power : 283.9755458418171 W\n",
      "[codecarbon INFO @ 21:52:56] Energy consumed for all CPUs : 0.025531 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:52:56] 0.175123 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:53:12] Energy consumed for RAM : 0.087101 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:53:12] Energy consumed for RAM : 0.130487 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:53:13] Energy consumed for all GPUs : 0.074872 kWh. Total GPU Power : 290.9482881664188 W\n",
      "[codecarbon INFO @ 21:53:13] Energy consumed for all CPUs : 0.039055 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:53:13] 0.244414 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:53:13] Energy consumed for all GPUs : 0.065494 kWh. Total GPU Power : 291.00218953074943 W\n",
      "[codecarbon INFO @ 21:53:13] Energy consumed for all CPUs : 0.026044 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:53:13] 0.178639 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:53:29] Energy consumed for RAM : 0.088799 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:53:29] Energy consumed for RAM : 0.132186 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:53:29] Energy consumed for all GPUs : 0.066769 kWh. Total GPU Power : 283.7223650506943 W\n",
      "[codecarbon INFO @ 21:53:29] Energy consumed for all CPUs : 0.026550 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:53:29] 0.182118 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:53:29] Energy consumed for all GPUs : 0.076172 kWh. Total GPU Power : 289.0519165772724 W\n",
      "[codecarbon INFO @ 21:53:29] Energy consumed for all CPUs : 0.039570 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:53:29] 0.247928 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:53:45] Energy consumed for RAM : 0.133862 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:53:45] Energy consumed for RAM : 0.090505 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:53:45] Energy consumed for all GPUs : 0.077430 kWh. Total GPU Power : 283.40708842901233 W\n",
      "[codecarbon INFO @ 21:53:45] Energy consumed for all CPUs : 0.040070 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:53:45] 0.251361 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:53:45] Energy consumed for all GPUs : 0.068051 kWh. Total GPU Power : 283.8893385528298 W\n",
      "[codecarbon INFO @ 21:53:45] Energy consumed for all CPUs : 0.027058 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:53:45] 0.185614 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:54:02] Energy consumed for RAM : 0.135586 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:54:02] Energy consumed for RAM : 0.092229 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:54:02] Energy consumed for all GPUs : 0.078721 kWh. Total GPU Power : 282.9980243967996 W\n",
      "[codecarbon INFO @ 21:54:02] Energy consumed for all CPUs : 0.040583 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:54:02] 0.254890 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:54:02] Energy consumed for all GPUs : 0.069342 kWh. Total GPU Power : 283.03659203312975 W\n",
      "[codecarbon INFO @ 21:54:02] Energy consumed for all CPUs : 0.027572 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:54:02] 0.189144 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:54:18] Energy consumed for RAM : 0.093923 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:54:18] Energy consumed for RAM : 0.137280 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:54:18] Energy consumed for all GPUs : 0.070623 kWh. Total GPU Power : 285.72087400435015 W\n",
      "[codecarbon INFO @ 21:54:18] Energy consumed for all CPUs : 0.028077 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:54:18] 0.192622 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:54:18] Energy consumed for all GPUs : 0.080002 kWh. Total GPU Power : 285.632414170532 W\n",
      "[codecarbon INFO @ 21:54:18] Energy consumed for all CPUs : 0.041088 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:54:18] 0.258370 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:54:35] Energy consumed for RAM : 0.095684 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:54:35] Energy consumed for RAM : 0.139041 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:54:35] Energy consumed for all GPUs : 0.071934 kWh. Total GPU Power : 281.3372873255525 W\n",
      "[codecarbon INFO @ 21:54:35] Energy consumed for all CPUs : 0.028601 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:54:35] 0.196220 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:54:35] Energy consumed for all GPUs : 0.081313 kWh. Total GPU Power : 281.35362460043666 W\n",
      "[codecarbon INFO @ 21:54:35] Energy consumed for all CPUs : 0.041613 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:54:35] 0.261967 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:54:51] Energy consumed for RAM : 0.140757 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:54:51] Energy consumed for RAM : 0.097400 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:54:51] Energy consumed for all GPUs : 0.082631 kWh. Total GPU Power : 290.1960047635686 W\n",
      "[codecarbon INFO @ 21:54:51] Energy consumed for all CPUs : 0.042132 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:54:51] 0.265520 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:54:51] Energy consumed for all GPUs : 0.073252 kWh. Total GPU Power : 290.113156559656 W\n",
      "[codecarbon INFO @ 21:54:51] Energy consumed for all CPUs : 0.029121 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:54:51] 0.199774 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:55:07] Energy consumed for RAM : 0.099049 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:55:07] Energy consumed for RAM : 0.142406 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:55:07] Energy consumed for all GPUs : 0.083890 kWh. Total GPU Power : 288.4043928518691 W\n",
      "[codecarbon INFO @ 21:55:07] Energy consumed for all CPUs : 0.042624 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:55:07] 0.268920 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:55:07] Energy consumed for all GPUs : 0.074530 kWh. Total GPU Power : 292.7004430527672 W\n",
      "[codecarbon INFO @ 21:55:07] Energy consumed for all CPUs : 0.029621 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:55:07] 0.203200 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:55:23] Energy consumed for RAM : 0.100748 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:55:23] Energy consumed for RAM : 0.144134 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:55:23] Energy consumed for all GPUs : 0.075796 kWh. Total GPU Power : 281.49340164067513 W\n",
      "[codecarbon INFO @ 21:55:23] Energy consumed for all CPUs : 0.030127 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:55:23] 0.206671 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:55:23] Energy consumed for all GPUs : 0.085175 kWh. Total GPU Power : 280.95064767976777 W\n",
      "[codecarbon INFO @ 21:55:23] Energy consumed for all CPUs : 0.043138 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:55:23] 0.272447 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:55:40] Energy consumed for RAM : 0.145840 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:55:40] Energy consumed for RAM : 0.102456 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:55:40] Energy consumed for all GPUs : 0.086484 kWh. Total GPU Power : 289.73668769175737 W\n",
      "[codecarbon INFO @ 21:55:40] Energy consumed for all CPUs : 0.043656 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:55:40] 0.275980 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:55:40] Energy consumed for all GPUs : 0.077105 kWh. Total GPU Power : 289.6254438489964 W\n",
      "[codecarbon INFO @ 21:55:40] Energy consumed for all CPUs : 0.030644 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:55:40] 0.210205 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:55:56] Energy consumed for RAM : 0.147501 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:55:56] Energy consumed for RAM : 0.104116 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:55:56] Energy consumed for all GPUs : 0.087741 kWh. Total GPU Power : 285.98199880499163 W\n",
      "[codecarbon INFO @ 21:55:56] Energy consumed for all CPUs : 0.044151 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:55:56] 0.279393 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:55:56] Energy consumed for all GPUs : 0.078363 kWh. Total GPU Power : 286.0208939716075 W\n",
      "[codecarbon INFO @ 21:55:56] Energy consumed for all CPUs : 0.031139 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:55:56] 0.213618 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:56:12] Energy consumed for RAM : 0.105810 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:56:12] Energy consumed for RAM : 0.149195 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:56:12] Energy consumed for all GPUs : 0.089016 kWh. Total GPU Power : 284.36296268391544 W\n",
      "[codecarbon INFO @ 21:56:12] Energy consumed for all CPUs : 0.044655 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:56:12] 0.282867 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:56:12] Energy consumed for all GPUs : 0.079638 kWh. Total GPU Power : 284.432944428634 W\n",
      "[codecarbon INFO @ 21:56:12] Energy consumed for all CPUs : 0.031644 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:56:12] 0.217091 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:56:28] Energy consumed for RAM : 0.107509 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:56:28] Energy consumed for RAM : 0.150895 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:56:28] Energy consumed for all GPUs : 0.080921 kWh. Total GPU Power : 285.2823004217392 W\n",
      "[codecarbon INFO @ 21:56:28] Energy consumed for all CPUs : 0.032150 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:56:28] 0.220581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:56:28] Energy consumed for all GPUs : 0.090300 kWh. Total GPU Power : 285.19212765032665 W\n",
      "[codecarbon INFO @ 21:56:28] Energy consumed for all CPUs : 0.045162 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:56:28] 0.286357 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:56:45] Energy consumed for RAM : 0.109238 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:56:45] Energy consumed for RAM : 0.152623 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:56:45] Energy consumed for all GPUs : 0.082224 kWh. Total GPU Power : 284.8023257289336 W\n",
      "[codecarbon INFO @ 21:56:45] Energy consumed for all CPUs : 0.032665 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:56:45] 0.224127 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:56:45] Energy consumed for all GPUs : 0.091603 kWh. Total GPU Power : 284.8519796216975 W\n",
      "[codecarbon INFO @ 21:56:45] Energy consumed for all CPUs : 0.045677 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:56:45] 0.289903 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:57:01] Energy consumed for RAM : 0.110932 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:57:01] Energy consumed for RAM : 0.154318 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:57:01] Energy consumed for all GPUs : 0.083506 kWh. Total GPU Power : 285.68451054438424 W\n",
      "[codecarbon INFO @ 21:57:01] Energy consumed for all CPUs : 0.033170 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:57:01] 0.227608 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:57:01] Energy consumed for all GPUs : 0.092884 kWh. Total GPU Power : 285.73174516341004 W\n",
      "[codecarbon INFO @ 21:57:01] Energy consumed for all CPUs : 0.046181 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:57:01] 0.293383 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:57:17] Energy consumed for RAM : 0.156019 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:57:17] Energy consumed for RAM : 0.112634 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:57:17] Energy consumed for all GPUs : 0.094172 kWh. Total GPU Power : 285.92137096194887 W\n",
      "[codecarbon INFO @ 21:57:17] Energy consumed for all CPUs : 0.046688 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:57:17] 0.296878 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:57:17] Energy consumed for all GPUs : 0.084793 kWh. Total GPU Power : 285.7828988740009 W\n",
      "[codecarbon INFO @ 21:57:17] Energy consumed for all CPUs : 0.033677 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:57:17] 0.231104 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:57:33] Energy consumed for RAM : 0.114346 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:57:33] Energy consumed for RAM : 0.157730 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:57:33] Energy consumed for all GPUs : 0.086086 kWh. Total GPU Power : 285.528595837652 W\n",
      "[codecarbon INFO @ 21:57:33] Energy consumed for all CPUs : 0.034187 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:57:33] 0.234619 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:57:33] Energy consumed for all GPUs : 0.095468 kWh. Total GPU Power : 286.1674744207133 W\n",
      "[codecarbon INFO @ 21:57:33] Energy consumed for all CPUs : 0.047198 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:57:33] 0.300397 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:57:49] Energy consumed for RAM : 0.159416 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:57:49] Energy consumed for RAM : 0.116031 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:57:49] Energy consumed for all GPUs : 0.096753 kWh. Total GPU Power : 288.0471242494284 W\n",
      "[codecarbon INFO @ 21:57:49] Energy consumed for all CPUs : 0.047700 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:57:49] 0.303869 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:57:49] Energy consumed for all GPUs : 0.087374 kWh. Total GPU Power : 288.6720058320474 W\n",
      "[codecarbon INFO @ 21:57:49] Energy consumed for all CPUs : 0.034689 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:57:49] 0.238095 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:58:06] Energy consumed for RAM : 0.117730 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:58:06] Energy consumed for RAM : 0.161115 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:58:06] Energy consumed for all GPUs : 0.088669 kWh. Total GPU Power : 287.9305022248429 W\n",
      "[codecarbon INFO @ 21:58:06] Energy consumed for all CPUs : 0.035195 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:58:06] 0.241595 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:58:06] Energy consumed for all GPUs : 0.098048 kWh. Total GPU Power : 287.7851257103657 W\n",
      "[codecarbon INFO @ 21:58:06] Energy consumed for all CPUs : 0.048207 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:58:06] 0.307370 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:58:22] Energy consumed for RAM : 0.119407 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:58:22] Energy consumed for RAM : 0.162792 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:58:22] Energy consumed for all GPUs : 0.089951 kWh. Total GPU Power : 288.6370489620685 W\n",
      "[codecarbon INFO @ 21:58:22] Energy consumed for all CPUs : 0.035695 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:58:22] 0.245053 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:58:22] Energy consumed for all GPUs : 0.099329 kWh. Total GPU Power : 288.6591178834007 W\n",
      "[codecarbon INFO @ 21:58:22] Energy consumed for all CPUs : 0.048706 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:58:22] 0.310828 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:58:38] Energy consumed for RAM : 0.121112 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:58:38] Energy consumed for RAM : 0.164496 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:58:38] Energy consumed for all GPUs : 0.091233 kWh. Total GPU Power : 284.3655698117227 W\n",
      "[codecarbon INFO @ 21:58:38] Energy consumed for all CPUs : 0.036202 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:58:38] 0.248547 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:58:38] Energy consumed for all GPUs : 0.100612 kWh. Total GPU Power : 284.40414953329787 W\n",
      "[codecarbon INFO @ 21:58:38] Energy consumed for all CPUs : 0.049214 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:58:38] 0.314322 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:58:55] Energy consumed for RAM : 0.122878 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:58:55] Energy consumed for RAM : 0.166262 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:58:55] Energy consumed for all GPUs : 0.092532 kWh. Total GPU Power : 277.70165776939797 W\n",
      "[codecarbon INFO @ 21:58:55] Energy consumed for all CPUs : 0.036729 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:58:55] 0.252139 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:58:55] Energy consumed for all GPUs : 0.101910 kWh. Total GPU Power : 277.74056293172566 W\n",
      "[codecarbon INFO @ 21:58:55] Energy consumed for all CPUs : 0.049740 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:58:55] 0.317913 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:59:11] Energy consumed for RAM : 0.124568 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:59:11] Energy consumed for RAM : 0.167951 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:59:11] Energy consumed for all GPUs : 0.093813 kWh. Total GPU Power : 286.5131015336707 W\n",
      "[codecarbon INFO @ 21:59:11] Energy consumed for all CPUs : 0.037232 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:59:11] 0.255613 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:59:11] Energy consumed for all GPUs : 0.103192 kWh. Total GPU Power : 286.63279249138157 W\n",
      "[codecarbon INFO @ 21:59:11] Energy consumed for all CPUs : 0.050244 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:59:11] 0.321387 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:59:27] Energy consumed for RAM : 0.169659 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:59:27] Energy consumed for RAM : 0.126277 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:59:27] Energy consumed for all GPUs : 0.104490 kWh. Total GPU Power : 287.1371282004787 W\n",
      "[codecarbon INFO @ 21:59:27] Energy consumed for all CPUs : 0.050753 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:59:27] 0.324902 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:59:27] Energy consumed for all GPUs : 0.095111 kWh. Total GPU Power : 286.95698390552417 W\n",
      "[codecarbon INFO @ 21:59:27] Energy consumed for all CPUs : 0.037741 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:59:27] 0.259130 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:59:43] Energy consumed for RAM : 0.127963 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:59:43] Energy consumed for RAM : 0.171346 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 21:59:43] Energy consumed for all GPUs : 0.096401 kWh. Total GPU Power : 288.8657474632495 W\n",
      "[codecarbon INFO @ 21:59:43] Energy consumed for all CPUs : 0.038244 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:59:43] 0.262608 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:59:43] Energy consumed for all GPUs : 0.105780 kWh. Total GPU Power : 288.76944272500896 W\n",
      "[codecarbon INFO @ 21:59:43] Energy consumed for all CPUs : 0.051255 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 21:59:43] 0.328381 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:00:00] Energy consumed for RAM : 0.173066 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 22:00:00] Energy consumed for RAM : 0.129684 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 22:00:00] Energy consumed for all GPUs : 0.107074 kWh. Total GPU Power : 284.38733338862636 W\n",
      "[codecarbon INFO @ 22:00:00] Energy consumed for all CPUs : 0.051768 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 22:00:00] 0.331908 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:00:00] Energy consumed for all GPUs : 0.097696 kWh. Total GPU Power : 284.30704594317075 W\n",
      "[codecarbon INFO @ 22:00:00] Energy consumed for all CPUs : 0.038756 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 22:00:00] 0.266136 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:00:16] Energy consumed for RAM : 0.174766 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 22:00:16] Energy consumed for RAM : 0.131382 kWh. RAM Power : 377.9193949699402 W\n",
      "[codecarbon INFO @ 22:00:16] Energy consumed for all GPUs : 0.108374 kWh. Total GPU Power : 288.8309879019787 W\n",
      "[codecarbon INFO @ 22:00:16] Energy consumed for all CPUs : 0.052281 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 22:00:16] 0.335421 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:00:16] Energy consumed for all GPUs : 0.098995 kWh. Total GPU Power : 288.91033246642934 W\n",
      "[codecarbon INFO @ 22:00:16] Energy consumed for all CPUs : 0.039270 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 22:00:16] 0.269647 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 26 21:12:40 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              66W / 400W |  49989MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:46:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              67W / 400W |  47111MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:4C:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              65W / 400W |  47111MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              68W / 400W |  46967MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Feb__7_19:32:13_PST_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.66\n",
      "Build cuda_12.1.r12.1/compiler.32415258_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:12:44] Energy consumed for RAM : 0.031360 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:12:44] Energy consumed for all GPUs : 0.011417 kWh. Total GPU Power : 274.8827443950337 W\n",
      "[codecarbon INFO @ 21:12:44] Energy consumed for all CPUs : 0.005904 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:12:44] 0.048681 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:12:59] Energy consumed for RAM : 0.034488 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:12:59] Energy consumed for all GPUs : 0.012530 kWh. Total GPU Power : 267.4108124760764 W\n",
      "[codecarbon INFO @ 21:12:59] Energy consumed for all CPUs : 0.006487 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:12:59] 0.053505 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:13:14] Energy consumed for RAM : 0.037616 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:13:14] Energy consumed for all GPUs : 0.013648 kWh. Total GPU Power : 268.69069465643133 W\n",
      "[codecarbon INFO @ 21:13:14] Energy consumed for all CPUs : 0.007071 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:13:14] 0.058335 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 4\n",
      "Current CUDA Device: 0\n",
      "CUDA Device Name: NVIDIA A100-SXM4-80GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:13:29] Energy consumed for RAM : 0.040744 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:13:29] Energy consumed for all GPUs : 0.014761 kWh. Total GPU Power : 267.2918160880798 W\n",
      "[codecarbon INFO @ 21:13:29] Energy consumed for all CPUs : 0.007654 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:13:29] 0.063159 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current CUDA Device:\", torch.cuda.current_device())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 9951012\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139972791104176"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139973424920752"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(merged_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /workspace/bangla-llama/data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_gguf='/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_dir='/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Base'\n",
    "target_dir='/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls $target_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_dir='/workspace/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Base/snapshots/defb6a155755ad3547c62908c0dc6cda7aa7d018/GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/tokenizer_config.json',\n",
       " '/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/special_tokens_map.json',\n",
       " '/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repo_id = 'BanglaLLM/BanglaLLama-3-8b-BnWiki-Base-GGUF'\n",
    "repo_id = 'BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct-GGUF')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(repo_id=repo_id, repo_type=\"model\", private=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def push_to_hub(target_model_path, repo_id, hf_token):\n",
    "    print(\"Pushing model to hub...\")\n",
    "    if os.path.exists(f\"{target_model_path}/training_params.json\"):\n",
    "        training_params = json.load(\n",
    "            open(f\"{target_model_path}/training_params.json\")\n",
    "        )\n",
    "        # Optionally, remove sensitive info if needed\n",
    "        # training_params.pop(\"token\")\n",
    "        json.dump(\n",
    "            training_params, open(f\"{target_model_path}/training_params.json\", \"w\")\n",
    "        )\n",
    "\n",
    "    api = HfApi(token=hf_token)\n",
    "    api.create_repo(repo_id=repo_id, repo_type=\"model\", private=True, exist_ok=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=target_model_path, repo_id=repo_id, repo_type=\"model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing model to hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 0.00/16.1G [00:00<?, ?B/\n",
      "Upload 2 LFS files:   0%|                                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   0%| | 0.00/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 16.4k/16.1G [00:00<49:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 246k/16.1G [00:00<4:16:4\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 2.05M/16.1G [00:00<36:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 10.1M/16.1G [00:00<12:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   0%| | 6.96M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 16.1M/16.1G [00:01<16:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 23.5M/16.1G [00:01<10:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 30.9M/16.1G [00:01<10:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   1%| | 27.0M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 44.2M/16.1G [00:02<11:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 47.4M/16.1G [00:02<10:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 56.0M/16.1G [00:02<12:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 59.8M/16.1G [00:02<11:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 66.2M/16.1G [00:03<16:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 76.0M/16.1G [00:03<09:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 80.4M/16.1G [00:03<11:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 88.2M/16.1G [00:04<08:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 95.0M/16.1G [00:04<08:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 104M/16.1G [00:04<10:25,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 108M/16.1G [00:05<12:43,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 111M/16.1G [00:05<12:05,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   2%| | 79.0M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 114M/16.1G [00:05<17:50,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 128M/16.1G [00:05<09:07,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   2%| | 96.0M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 132M/16.1G [00:06<14:54,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 140M/16.1G [00:06<10:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 145M/16.1G [00:06<13:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 156M/16.1G [00:06<08:28,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   3%| | 140M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 161M/16.1G [00:07<10:42,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 188M/16.1G [00:07<06:47,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   3%| | 161M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 205M/16.1G [00:08<07:36,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 210M/16.1G [00:08<08:40,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 221M/16.1G [00:08<06:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 227M/16.1G [00:09<08:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 237M/16.1G [00:09<06:14,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   4%| | 210M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 253M/16.1G [00:09<06:32,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   5%| | 226M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 265M/16.1G [00:10<07:28,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   5%| | 242M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 270M/16.1G [00:10<09:07,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 274M/16.1G [00:10<14:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 280M/16.1G [00:10<11:10,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 286M/16.1G [00:11<10:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   6%| | 284M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   6%| | 290M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 301M/16.1G [00:11<09:56,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   6%| | 306M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 317M/16.1G [00:12<08:14,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   7%| | 322M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 332M/16.1G [00:12<07:08,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   7%| | 338M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 338M/16.1G [00:13<09:52,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 349M/16.1G [00:13<07:03,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 355M/16.1G [00:13<09:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 365M/16.1G [00:13<06:52,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   8%| | 380M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 371M/16.1G [00:14<08:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 387M/16.1G [00:14<07:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 397M/16.1G [00:14<05:51,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 404M/16.1G [00:14<08:28,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 413M/16.1G [00:15<06:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 419M/16.1G [00:15<07:45,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 430M/16.1G [00:15<05:59,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 462M/16.1G [00:16<06:54,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 468M/16.1G [00:16<08:01,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 478M/16.1G [00:17<06:13,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   9%| | 464M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 469M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 480M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 485M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 489M/16.1G [00:18<12:09,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 502M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 509M/16.1G [00:18<08:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  11%| | 518M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  11%| | 528M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  11%| | 534M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 524M/16.1G [00:19<09:16,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 530M/16.1G [00:19<10:11,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 542M/16.1G [00:19<07:09,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 548M/16.1G [00:20<08:48,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 558M/16.1G [00:20<06:45,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 564M/16.1G [00:20<08:03,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 574M/16.1G [00:20<06:15,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  12%| | 598M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 590M/16.1G [00:21<06:11,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  12%| | 614M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|â–| 624M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|â–| 630M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 602M/16.1G [00:21<10:46,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|â–| 646M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 621M/16.1G [00:22<07:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|â–| 662M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 638M/16.1G [00:22<06:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  14%|â–| 678M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 654M/16.1G [00:23<05:49,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  14%|â–| 694M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 671M/16.1G [00:23<05:46,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  14%|â–| 710M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 686M/16.1G [00:24<06:31,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  15%|â–| 726M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 703M/16.1G [00:24<06:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 709M/16.1G [00:24<06:43,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 735M/16.1G [00:25<05:43,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  15%|â–| 757M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 751M/16.1G [00:25<05:37,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  16%|â–| 771M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 767M/16.1G [00:26<05:50,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  16%|â–| 785M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 782M/16.1G [00:26<06:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 788M/16.1G [00:26<07:37,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 799M/16.1G [00:26<05:41,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  17%|â–| 817M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 815M/16.1G [00:27<06:03,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 821M/16.1G [00:27<07:10,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 831M/16.1G [00:27<05:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 837M/16.1G [00:28<07:16,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 863M/16.1G [00:28<05:47,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  18%|â–| 866M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 876M/16.1G [00:29<06:59,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  18%|â–| 880M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 880M/16.1G [00:29<09:42,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  18%|â–| 898M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 895M/16.1G [00:30<09:55,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  19%|â–| 914M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 912M/16.1G [00:30<08:01,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  19%|â–| 930M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 928M/16.1G [00:31<07:12,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  19%|â–| 946M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 944M/16.1G [00:31<06:33,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  20%|â–| 962M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 960M/16.1G [00:31<06:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  20%|â–| 978M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 974M/16.1G [00:32<08:15,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  20%|â–| 992M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 992M/16.1G [00:33<06:52,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  21%|â–| 1.01G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.01G/16.1G [00:33<06:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.01G/16.1G [00:33<07:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.02G/16.1G [00:33<06:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.03G/16.1G [00:34<07:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.04G/16.1G [00:34<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.05G/16.1G [00:34<07:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.07G/16.1G [00:35<05:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  22%|â–| 1.07G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.09G/16.1G [00:35<05:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  22%|â–| 1.09G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.09G/16.1G [00:35<07:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.10G/16.1G [00:35<06:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  23%|â–| 1.12G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.11G/16.1G [00:36<07:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.12G/16.1G [00:36<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.13G/16.1G [00:36<07:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.14G/16.1G [00:36<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.14G/16.1G [00:37<07:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.15G/16.1G [00:37<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  24%|â–| 1.17G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.17G/16.1G [00:37<06:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  24%|â–| 1.19G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.18G/16.1G [00:38<06:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  24%|â–| 1.20G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.20G/16.1G [00:38<06:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  25%|â–| 1.22G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.22G/16.1G [00:39<06:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  25%|â–Ž| 1.23G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.23G/16.1G [00:39<05:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  25%|â–Ž| 1.25G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.25G/16.1G [00:39<05:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  26%|â–Ž| 1.27G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.26G/16.1G [00:40<10:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.26G/16.1G [00:40<07:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.27G/16.1G [00:41<09:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.28G/16.1G [00:41<06:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.29G/16.1G [00:41<08:09\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.30G/16.1G [00:41<06:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|â–Ž| 1.32G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|â–Ž| 1.33G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|â–Ž| 1.34G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|â–Ž| 1.35G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|â–Ž| 1.36G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|â–Ž| 1.36G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|â–Ž| 1.37G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|â–Ž| 1.38G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|â–Ž| 1.39G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|â–Ž| 1.39G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|â–Ž| 1.40G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|â–Ž| 1.41G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|â–Ž| 1.42G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|â–Ž| 1.43G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|â–Ž| 1.44G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|â–Ž| 1.44G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.30G/16.1G [00:45<42:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  30%|â–Ž| 1.46G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.31G/16.1G [00:45<28:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  30%|â–Ž| 1.47G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.33G/16.1G [00:46<15:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  30%|â–Ž| 1.49G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.35G/16.1G [00:46<09:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  31%|â–Ž| 1.51G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.36G/16.1G [00:47<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  31%|â–Ž| 1.52G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.38G/16.1G [00:47<06:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  31%|â–Ž| 1.54G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.38G/16.1G [00:48<09:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.39G/16.1G [00:48<07:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.41G/16.1G [00:48<06:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  32%|â–Ž| 1.57G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.42G/16.1G [00:49<09:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.43G/16.1G [00:49<07:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.43G/16.1G [00:49<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.44G/16.1G [00:49<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.45G/16.1G [00:49<07:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.46G/16.1G [00:49<05:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.46G/16.1G [00:50<07:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.47G/16.1G [00:50<05:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.48G/16.1G [00:50<07:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.49G/16.1G [00:50<05:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  34%|â–Ž| 1.66G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.50G/16.1G [00:51<07:32\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.51G/16.1G [00:51<05:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.51G/16.1G [00:51<08:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.52G/16.1G [00:51<06:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  35%|â–Ž| 1.70G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.54G/16.1G [00:52<06:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  35%|â–Ž| 1.71G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.55G/16.1G [00:52<06:09\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.56G/16.1G [00:53<08:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.58G/16.1G [00:53<08:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.59G/16.1G [00:53<06:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.59G/16.1G [00:53<07:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.60G/16.1G [00:54<05:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.62G/16.1G [00:54<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  36%|â–Ž| 1.78G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.63G/16.1G [00:55<06:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  36%|â–Ž| 1.79G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.64G/16.1G [00:55<07:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.65G/16.1G [00:55<06:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  37%|â–Ž| 1.82G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.66G/16.1G [00:55<08:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.67G/16.1G [00:56<06:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  37%|â–Ž| 1.84G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.68G/16.1G [00:56<06:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  38%|â–| 1.86G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.70G/16.1G [00:57<07:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.72G/16.1G [00:57<05:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  38%|â–| 1.88G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.72G/16.1G [00:57<07:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.74G/16.1G [00:58<07:32\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.75G/16.1G [00:58<05:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  39%|â–| 1.92G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.75G/16.1G [00:58<07:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.76G/16.1G [00:58<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  39%|â–| 1.94G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.78G/16.1G [00:59<07:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  40%|â–| 1.95G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.78G/16.1G [00:59<09:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  40%|â–| 1.97G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.79G/16.1G [01:00<14:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.80G/16.1G [01:00<09:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.80G/16.1G [01:00<11:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.81G/16.1G [01:00<07:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.82G/16.1G [01:01<08:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.83G/16.1G [01:01<06:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.85G/16.1G [01:02<07:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.86G/16.1G [01:02<05:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.88G/16.1G [01:02<04:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  42%|â–| 2.05G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.88G/16.1G [01:02<08:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.89G/16.1G [01:03<06:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  42%|â–| 2.08G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.90G/16.1G [01:03<07:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.91G/16.1G [01:03<06:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.91G/16.1G [01:03<07:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.92G/16.1G [01:03<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.93G/16.1G [01:04<07:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.94G/16.1G [01:04<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.95G/16.1G [01:04<07:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.96G/16.1G [01:04<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.96G/16.1G [01:05<06:54\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.97G/16.1G [01:05<05:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.98G/16.1G [01:05<06:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.99G/16.1G [01:05<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.99G/16.1G [01:05<06:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 2.00G/16.1G [01:06<05:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.01G/16.1G [01:06<06:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.02G/16.1G [01:06<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  45%|â–| 2.21G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.03G/16.1G [01:07<07:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.04G/16.1G [01:07<08:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.05G/16.1G [01:07<05:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.06G/16.1G [01:07<07:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.07G/16.1G [01:07<05:34\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.07G/16.1G [01:08<06:54\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.08G/16.1G [01:08<05:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  46%|â–| 2.27G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.10G/16.1G [01:08<05:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  47%|â–| 2.29G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.12G/16.1G [01:09<05:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  47%|â–| 2.31G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.13G/16.1G [01:09<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  47%|â–| 2.32G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.14G/16.1G [01:10<07:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.15G/16.1G [01:10<05:48\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  48%|â–| 2.35G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.15G/16.1G [01:10<07:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|â–| 2.16G/16.1G [01:10<05:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  48%|â–| 2.37G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.18G/16.1G [01:11<06:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  48%|â–| 2.39G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.20G/16.1G [01:11<05:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  49%|â–| 2.40G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.20G/16.1G [01:11<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.21G/16.1G [01:12<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.22G/16.1G [01:12<07:34\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.23G/16.1G [01:12<05:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  50%|â–| 2.44G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.24G/16.1G [01:12<07:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.25G/16.1G [01:13<06:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.26G/16.1G [01:13<05:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  50%|â–Œ| 2.48G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.27G/16.1G [01:13<07:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.28G/16.1G [01:13<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.28G/16.1G [01:14<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.29G/16.1G [01:14<05:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  51%|â–Œ| 2.51G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.31G/16.1G [01:14<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  51%|â–Œ| 2.53G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  52%|â–Œ| 2.54G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  52%|â–Œ| 2.55G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  52%|â–Œ| 2.56G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.32G/16.1G [01:15<12:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|â–| 2.32G/16.1G [01:15<10:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.33G/16.1G [01:16<10:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.34G/16.1G [01:16<07:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.35G/16.1G [01:16<08:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.36G/16.1G [01:16<06:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.36G/16.1G [01:16<07:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.38G/16.1G [01:17<06:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.38G/16.1G [01:17<06:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.39G/16.1G [01:17<07:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  54%|â–Œ| 2.64G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.41G/16.1G [01:18<07:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  54%|â–Œ| 2.66G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.42G/16.1G [01:18<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  54%|â–Œ| 2.67G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|â–Œ| 2.68G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|â–Œ| 2.69G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|â–Œ| 2.70G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|â–Œ| 2.71G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.44G/16.1G [01:19<08:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|â–Œ| 2.72G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.46G/16.1G [01:20<06:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  56%|â–Œ| 2.74G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.47G/16.1G [01:20<05:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.48G/16.1G [01:21<07:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|â–| 2.49G/16.1G [01:21<05:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  56%|â–Œ| 2.77G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  56%|â–Œ| 2.78G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  57%|â–Œ| 2.79G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.50G/16.1G [01:22<13:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.51G/16.1G [01:22<09:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.53G/16.1G [01:23<08:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.54G/16.1G [01:23<06:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.55G/16.1G [01:23<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  58%|â–Œ| 2.83G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  58%|â–Œ| 2.84G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.56G/16.1G [01:24<07:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.57G/16.1G [01:24<05:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  58%|â–Œ| 2.86G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.59G/16.1G [01:24<05:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  59%|â–Œ| 2.88G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.59G/16.1G [01:25<08:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  59%|â–Œ| 2.90G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.60G/16.1G [01:25<10:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  59%|â–Œ| 2.91G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.60G/16.1G [01:25<13:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  60%|â–Œ| 2.93G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.60G/16.1G [01:26<17:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.61G/16.1G [01:26<10:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.62G/16.1G [01:26<10:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.63G/16.1G [01:26<06:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  60%|â–Œ| 2.97G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.64G/16.1G [01:27<08:09\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|â–| 2.65G/16.1G [01:27<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.65G/16.1G [01:27<07:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.68G/16.1G [01:28<05:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  61%|â–Œ| 3.01G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.70G/16.1G [01:28<05:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.70G/16.1G [01:28<05:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.71G/16.1G [01:28<04:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.72G/16.1G [01:29<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.73G/16.1G [01:29<04:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.73G/16.1G [01:29<05:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.75G/16.1G [01:29<05:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.76G/16.1G [01:30<04:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  63%|â–‹| 3.08G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.77G/16.1G [01:30<07:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.78G/16.1G [01:30<05:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.78G/16.1G [01:30<06:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.79G/16.1G [01:30<06:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.79G/16.1G [01:31<08:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  64%|â–‹| 3.13G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  64%|â–‹| 3.14G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.80G/16.1G [01:31<14:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|â–| 2.81G/16.1G [01:32<08:32\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  64%|â–‹| 3.16G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.81G/16.1G [01:32<10:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.82G/16.1G [01:32<08:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.82G/16.1G [01:32<10:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.83G/16.1G [01:33<10:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  65%|â–‹| 3.20G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.84G/16.1G [01:33<11:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  65%|â–‹| 3.22G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.86G/16.1G [01:34<06:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.86G/16.1G [01:34<07:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.87G/16.1G [01:34<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  66%|â–‹| 3.25G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.89G/16.1G [01:34<05:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  66%|â–‹| 3.27G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.90G/16.1G [01:35<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.91G/16.1G [01:35<07:01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.92G/16.1G [01:35<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.93G/16.1G [01:36<06:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.94G/16.1G [01:36<04:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.94G/16.1G [01:36<06:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.95G/16.1G [01:36<05:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.96G/16.1G [01:37<06:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|â–| 2.97G/16.1G [01:37<04:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  68%|â–‹| 3.35G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 2.98G/16.1G [01:37<05:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  68%|â–‹| 3.36G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.01G/16.1G [01:38<06:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.02G/16.1G [01:38<05:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  69%|â–‹| 3.39G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.02G/16.1G [01:38<06:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.04G/16.1G [01:39<06:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.05G/16.1G [01:39<05:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  69%|â–‹| 3.42G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.05G/16.1G [01:39<08:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.08G/16.1G [01:40<05:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  70%|â–‹| 3.44G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.10G/16.1G [01:40<04:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  70%|â–‹| 3.46G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.11G/16.1G [01:41<05:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  71%|â–‹| 3.47G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|â–| 3.13G/16.1G [01:41<05:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  71%|â–‹| 3.49G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.15G/16.1G [01:42<05:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  71%|â–‹| 3.51G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.15G/16.1G [01:42<07:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.16G/16.1G [01:42<05:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.17G/16.1G [01:43<06:54\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.18G/16.1G [01:43<05:16\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  72%|â–‹| 3.55G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.18G/16.1G [01:43<08:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.19G/16.1G [01:43<06:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  73%|â–‹| 3.57G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.21G/16.1G [01:44<06:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  73%|â–‹| 3.59G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.23G/16.1G [01:44<05:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  73%|â–‹| 3.60G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.24G/16.1G [01:45<05:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.25G/16.1G [01:45<06:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.26G/16.1G [01:45<05:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  74%|â–‹| 3.63G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.27G/16.1G [01:46<05:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  74%|â–‹| 3.65G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|â–| 3.29G/16.1G [01:46<05:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.30G/16.1G [01:46<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.31G/16.1G [01:46<05:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.31G/16.1G [01:47<06:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.32G/16.1G [01:47<04:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  75%|â–Š| 3.70G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.34G/16.1G [01:47<05:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  75%|â–Š| 3.71G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.35G/16.1G [01:48<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.36G/16.1G [01:48<06:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.37G/16.1G [01:48<05:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.38G/16.1G [01:49<06:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.39G/16.1G [01:49<06:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.40G/16.1G [01:49<04:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.41G/16.1G [01:49<06:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.42G/16.1G [01:50<05:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  77%|â–Š| 3.79G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.42G/16.1G [01:50<07:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.43G/16.1G [01:50<05:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  77%|â–Š| 3.81G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|â–| 3.45G/16.1G [01:51<06:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  78%|â–Š| 3.83G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.47G/16.1G [01:51<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  78%|â–Š| 3.84G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.47G/16.1G [01:52<07:29\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.48G/16.1G [01:52<05:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.49G/16.1G [01:52<07:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  79%|â–Š| 3.87G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.50G/16.1G [01:52<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.50G/16.1G [01:53<08:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.52G/16.1G [01:53<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.53G/16.1G [01:53<05:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  80%|â–Š| 3.92G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.54G/16.1G [01:54<06:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.55G/16.1G [01:54<05:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.55G/16.1G [01:54<06:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.56G/16.1G [01:54<05:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.57G/16.1G [01:54<06:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.58G/16.1G [01:54<04:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  81%|â–Š| 3.97G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.60G/16.1G [01:55<05:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  81%|â–Š| 3.99G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.60G/16.1G [01:55<07:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|â–| 3.61G/16.1G [01:56<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.62G/16.1G [01:56<07:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  82%|â–Š| 4.02G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.63G/16.1G [01:56<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.63G/16.1G [01:56<06:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.64G/16.1G [01:56<05:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  82%|â–Š| 4.05G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.66G/16.1G [01:57<05:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|â–Š| 4.07G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.68G/16.1G [01:57<05:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|â–Š| 4.08G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.69G/16.1G [01:58<05:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|â–Š| 4.10G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|â–Š| 4.11G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  84%|â–Š| 4.11G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.70G/16.1G [01:59<08:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  84%|â–Š| 4.13G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.71G/16.1G [01:59<08:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.72G/16.1G [01:59<06:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  84%|â–Š| 4.16G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.73G/16.1G [02:00<07:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.74G/16.1G [02:00<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  85%|â–Š| 4.18G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.75G/16.1G [02:01<07:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.76G/16.1G [02:01<08:16\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|â–| 3.77G/16.1G [02:01<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  86%|â–Š| 4.21G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.79G/16.1G [02:01<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.79G/16.1G [02:02<06:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.81G/16.1G [02:02<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.82G/16.1G [02:02<04:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.83G/16.1G [02:03<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.84G/16.1G [02:03<04:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  87%|â–Š| 4.27G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.84G/16.1G [02:03<06:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.85G/16.1G [02:03<05:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  87%|â–Š| 4.29G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.87G/16.1G [02:04<05:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  88%|â–‰| 4.31G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.88G/16.1G [02:04<04:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  88%|â–‰| 4.32G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  88%|â–‰| 4.33G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.89G/16.1G [02:05<08:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.90G/16.1G [02:05<06:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.90G/16.1G [02:05<07:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.92G/16.1G [02:05<05:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|â–| 3.92G/16.1G [02:06<06:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–| 3.94G/16.1G [02:06<06:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  89%|â–‰| 4.39G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–| 3.95G/16.1G [02:06<05:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–| 3.95G/16.1G [02:07<07:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–| 3.96G/16.1G [02:07<05:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–| 3.97G/16.1G [02:07<07:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–| 3.98G/16.1G [02:07<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  90%|â–‰| 4.43G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  90%|â–‰| 4.44G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  90%|â–‰| 4.45G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–| 3.99G/16.1G [02:08<09:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  91%|â–‰| 4.47G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–| 4.00G/16.1G [02:09<09:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–| 4.01G/16.1G [02:09<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–Ž| 4.02G/16.1G [02:09<07:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–Ž| 4.03G/16.1G [02:09<05:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  92%|â–‰| 4.51G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–Ž| 4.03G/16.1G [02:10<07:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–Ž| 4.05G/16.1G [02:10<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–Ž| 4.05G/16.1G [02:10<06:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–Ž| 4.06G/16.1G [02:10<05:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–Ž| 4.07G/16.1G [02:10<06:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–Ž| 4.08G/16.1G [02:11<04:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|â–Ž| 4.08G/16.1G [02:11<06:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.10G/16.1G [02:11<06:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.11G/16.1G [02:11<04:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  93%|â–‰| 4.59G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.12G/16.1G [02:12<06:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.13G/16.1G [02:12<05:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.14G/16.1G [02:12<04:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  94%|â–‰| 4.62G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.15G/16.1G [02:13<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.16G/16.1G [02:13<04:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.16G/16.1G [02:13<06:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.17G/16.1G [02:13<04:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.18G/16.1G [02:14<06:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.19G/16.1G [02:14<04:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.20G/16.1G [02:14<06:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.21G/16.1G [02:14<04:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.21G/16.1G [02:14<05:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.23G/16.1G [02:15<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.24G/16.1G [02:15<04:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  96%|â–‰| 4.72G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  96%|â–‰| 4.72G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|â–Ž| 4.25G/16.1G [02:16<06:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  96%|â–‰| 4.74G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.26G/16.1G [02:16<07:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.27G/16.1G [02:16<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  97%|â–‰| 4.76G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  97%|â–‰| 4.77G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.29G/16.1G [02:17<05:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  97%|â–‰| 4.79G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.29G/16.1G [02:17<08:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.30G/16.1G [02:17<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.31G/16.1G [02:18<07:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.32G/16.1G [02:18<05:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.32G/16.1G [02:18<06:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.33G/16.1G [02:18<04:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  98%|â–‰| 4.84G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.34G/16.1G [02:19<06:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.35G/16.1G [02:19<04:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.36G/16.1G [02:19<06:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.37G/16.1G [02:19<04:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.37G/16.1G [02:19<06:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.38G/16.1G [02:20<04:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.39G/16.1G [02:20<06:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|â–Ž| 4.40G/16.1G [02:20<04:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf: 100%|â–ˆ| 4.92G/4.92G [02\u001b[A\u001b[A\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf: 100%|â–ˆ| 16.1G/16.1G [08:16<00:00\n",
      "\n",
      "Upload 2 LFS files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [08:17<00:00, 248.91s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "push_to_hub(target_dir, repo_id, hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_gguf='/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Base/GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p $target_dir_gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/bangla-llama/llama.cpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/anaconda3/envs/autotrain/lib/python310.zip',\n",
       " '/workspace/anaconda3/envs/autotrain/lib/python3.10',\n",
       " '/workspace/anaconda3/envs/autotrain/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages',\n",
       " '/tmp/tmpj3yslgo3',\n",
       " '/workspace/bangla-llama/llama.cpp']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'BanglaLLM/BanglaLLama-3-8b-BnWiki-Base-GGUF'\n",
    "#model.save_pretrained_gguf(target_dir_gguf, tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9dRBJZulavZ"
   },
   "source": [
    "### Instruction Finetuning\n",
    "\n",
    "We now use the [Alpaca in GPT4 Dataset](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-korean) but translated in Korean!\n",
    "\n",
    "Go to [vicgalle/alpaca-gpt4](https://huggingface.co/datasets/vicgalle/alpaca-gpt4) for the original GPT4 dataset for Alpaca or [MultilingualSIFT project](https://github.com/FreedomIntelligence/MultilingualSIFT) for other translations of the Alpaca dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196,
     "referenced_widgets": [
      "5ad923a4f66c48f293727c51202e3d8b",
      "ef246bb6829e4816a009b3f6875e20b8",
      "aadc85ded2694638a4faa0bb6dc9caa5",
      "c713a841e9cd402bb4f1acfccf9c4a7e",
      "34e31cad78ea462bb50193a9ee4c1372",
      "772ed124915740aeb2f5a614e8cfe2e1",
      "bf79d1ed09134e50b29aa3b6e28ecde5",
      "f59b229ad1ad4ab088682c83bb6c5ce4",
      "93d68696e1d345daaa4e819a47166a85",
      "a3bb01312bcd45649373c3790e327c18",
      "49666651fca242b2b777b1705dd4fa8a",
      "66744f20d34f4cd5b75570d269261b94",
      "03137bba9d7d4ef3ae7e073064b5438d",
      "cbece197ea7f41f9855fa94b6e049254",
      "cc5764233d5d4af79c1221d626b21322",
      "add58b43a7cd4047adb11778c26812f7",
      "b12e798c92f848bc9e1d430d504ee3c7",
      "ebb68cccf04641ed8996e9c9b62c67be",
      "7626531fcae84ddbb8f482b8ac2f7439",
      "119f3c027e3f4beda1c026fd3ebb3f7c",
      "97f0c9d5be3245fb8b1320cc17818ae2",
      "be8fed98ffb3473ba6667d2c17b3929b",
      "c3e586fbe5d34ecdbcc05a2ecbc17fc0",
      "ba1fd50e24904c6988967eab2ab64225",
      "f611865e89774fe4ba6dc047f97eaa61",
      "5fd4be6bc35d459eb335da817e346724",
      "e91b73923c044243b5ed8efcfa252f2f",
      "ee3c23c50c5b435bbada1bcec39c23b4",
      "6d83eb47f7b34c7c930e63649da55097",
      "7621ead96516491db1ebb4fba0055702",
      "790cc80cfe574ee58da93f8e54c8517f",
      "c0e9aae192fa4c62af7379d157c0893f",
      "5e0e10a542654766bf78a9e90c1daa8a"
     ]
    },
    "executionInfo": {
     "elapsed": 4839,
     "status": "ok",
     "timestamp": 1717663490088,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "1oNjUwxOyG8C",
    "outputId": "800603ed-2857-4687-a072-7c5589b5b5e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [00:00<00:00, 470kB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51.6M/51.6M [00:01<00:00, 32.6MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49969/49969 [00:00<00:00, 59775.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#alpaca_dataset_korean = load_dataset(\"FreedomIntelligence/alpaca-gpt4-korean\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "alpaca_dataset = load_dataset(\"BanglaLLM/bangla-alpaca-orca\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text', 'system_prompt'],\n",
       "    num_rows: 172026\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmFG41nFytgi"
   },
   "source": [
    "We print 1 example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1717663490089,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "QvTfIKaUQxQ5",
    "outputId": "83e1b097-e215-4b2e-c6bc-9b7f639914ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¦à¦–à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¨à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹, à¦¯à¦¾ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦œ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§‡, à¦à¦¬à¦‚ à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹ à¦¯à¦¾ à¦†à¦°à¦“ à¦ªà§à¦°à§‡à¦•à§à¦·à¦¾à¦ªà¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤ à¦à¦•à¦Ÿà¦¿ à¦‰à¦¤à§à¦¤à¦° à¦²à¦¿à¦–à§à¦¨ à¦¯à¦¾ à¦…à¦¨à§à¦°à§‹à¦§à¦Ÿà¦¿ à¦¸à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤ ### Instruction: à¦à¦•à¦Ÿà¦¿ à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦•à§‹à¦®à§à¦ªà¦¾à¦¨à¦¿à¦° à¦œà¦¨à§à¦¯ à¦à¦•à¦Ÿà¦¿ à¦…à§à¦¯à¦¾à¦ª à¦¡à¦¿à¦œà¦¾à¦‡à¦¨ à¦•à¦°à§à¦¨à¥¤ ### Input:  ### Response: à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦•à§‹à¦®à§à¦ªà¦¾à¦¨à¦¿ à¦…à§à¦¯à¦¾à¦ª à¦—à§à¦°à¦¾à¦¹à¦•à¦¦à§‡à¦° à¦¤à¦¾à¦¦à§‡à¦° à¦¸à¦®à¦¸à§à¦¤ à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦šà¦¾à¦¹à¦¿à¦¦à¦¾ à¦à¦• à¦œà¦¾à¦¯à¦¼à¦—à¦¾à¦¯à¦¼ à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾ à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦°à§à¦¯à¦•à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à¦¬à§‡à¥¤ à¦à¦–à¦¾à¦¨à§‡ à¦à¦‡ à¦…à§à¦¯à¦¾à¦ªà¦Ÿà¦¿à¦° à¦®à§‚à¦² à¦¬à§ˆà¦¶à¦¿à¦·à§à¦Ÿà§à¦¯à¦—à§à¦²à¦¿ à¦°à¦¯à¦¼à§‡à¦›à§‡: 1. à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°à¦•à¦¾à¦°à§€à¦° à¦…à§à¦¯à¦¾à¦•à¦¾à¦‰à¦¨à§à¦Ÿ: à¦—à§à¦°à¦¾à¦¹à¦•à¦°à¦¾ à¦¤à¦¾à¦¦à§‡à¦° à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦…à¦°à§à¦¡à¦¾à¦° à¦Ÿà§à¦°à§à¦¯à¦¾à¦• à¦•à¦°à¦¤à§‡ à¦à¦¬à¦‚ à¦¤à¦¾à¦¦à§‡à¦° à¦…à§à¦¯à¦¾à¦•à¦¾à¦‰à¦¨à§à¦Ÿà§‡à¦° à¦‡à¦¤à¦¿à¦¹à¦¾à¦¸ à¦¦à§‡à¦–à¦¤à§‡ à¦à¦•à¦Ÿà¦¿ à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦—à¦¤ à¦…à§à¦¯à¦¾à¦•à¦¾à¦‰à¦¨à§à¦Ÿ à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦¸à¦•à§à¦·à¦® à¦¹à¦¬à§‡à¦¨à¥¤ à¦…à§à¦¯à¦¾à¦•à¦¾à¦‰à¦¨à§à¦Ÿà§‡à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡, à¦¤à¦¾à¦°à¦¾ à¦¤à¦¾à¦¦à§‡à¦° à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦—à¦¤ à¦¤à¦¥à§à¦¯, à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦—à§‡à¦° à¦¬à¦¿à¦¬à¦°à¦£ à¦à¦¬à¦‚ à¦ à¦¿à¦•à¦¾à¦¨à¦¾ à¦¸à¦®à§à¦ªà¦¾à¦¦à¦¨à¦¾ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤ 2. à¦…à¦°à§à¦¡à¦¾à¦° à¦ªà§à¦²à§‡à¦¸à¦®à§‡à¦¨à§à¦Ÿ: à¦—à§à¦°à¦¾à¦¹à¦•à¦°à¦¾ à¦¤à¦¾à¦¦à§‡à¦° à¦ªà¦›à¦¨à§à¦¦à¦¸à¦‡ à¦†à¦‡à¦Ÿà§‡à¦® à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¨ à¦•à¦°à§‡ à¦¬à¦¾ à¦¤à¦¾à¦¦à§‡à¦° à¦•à§‡à¦¨à¦¾à¦•à¦¾à¦Ÿà¦¾à¦° à¦¤à¦¾à¦²à¦¿à¦•à¦¾à¦° à¦à¦•à¦Ÿà¦¿ à¦›à¦¬à¦¿ à¦†à¦ªà¦²à§‹à¦¡ à¦•à¦°à§‡ à¦…à§à¦¯à¦¾à¦ªà§‡ à¦¤à¦¾à¦¦à§‡à¦° à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦…à¦°à§à¦¡à¦¾à¦° à¦¦à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¦¨à¥¤ à¦…à¦°à§à¦¡à¦¾à¦° à¦¨à¦¿à¦°à§à¦¦à¦¿à¦·à§à¦Ÿ à¦¤à¦¾à¦°à¦¿à¦– à¦à¦¬à¦‚ à¦¸à¦®à¦¯à¦¼à§‡à¦° à¦œà¦¨à§à¦¯ à¦¨à¦¿à¦°à§à¦§à¦¾à¦°à¦¿à¦¤ à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦¬à¦¾ à¦ªà§à¦¨à¦°à¦¾à¦¬à§ƒà¦¤à§à¦¤ à¦­à¦¿à¦¤à§à¦¤à¦¿à¦¤à§‡ à¦¸à§à¦¥à¦¾à¦ªà¦¨ à¦•à¦°à¦¾ à¦¯à§‡à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤ 3. à¦°à¦¿à¦¯à¦¼à§‡à¦²-à¦Ÿà¦¾à¦‡à¦® à¦Ÿà§à¦°à§à¦¯à¦¾à¦•à¦¿à¦‚: à¦—à§à¦°à¦¾à¦¹à¦•à¦°à¦¾ à¦¤à¦¾à¦¦à§‡à¦° à¦…à¦°à§à¦¡à¦¾à¦° à¦°à¦¿à¦¯à¦¼à§‡à¦²-à¦Ÿà¦¾à¦‡à¦®à§‡ à¦®à§à¦¯à¦¾à¦ªà§‡ à¦Ÿà§à¦°à§à¦¯à¦¾à¦• à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¦¨, à¦†à¦—à¦®à¦¨à§‡à¦° à¦†à¦¨à§à¦®à¦¾à¦¨à¦¿à¦• à¦¸à¦®à¦¯à¦¼à§‡à¦° à¦†à¦ªà¦¡à§‡à¦Ÿ à¦¸à¦¹à¥¤ 4. à¦‡à¦¨-à¦…à§à¦¯à¦¾à¦ª à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦—: à¦—à§à¦°à¦¾à¦¹à¦• à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨-à¦…à§à¦¯à¦¾à¦ª à¦šà§à¦¯à¦¾à¦Ÿ à¦«à¦¾à¦‚à¦¶à¦¨à§‡à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¾à¦¬à¦²à§€, à¦¸à¦®à¦¯à¦¼à¦¸à§‚à¦šà§€ à¦¬à¦¾ à¦¬à¦¿à¦¶à§‡à¦· à¦…à¦¨à§à¦°à§‹à¦§ à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦¡à§à¦°à¦¾à¦‡à¦­à¦¾à¦°à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦— à¦•à¦°à¦¤à§‡ à¦¸à¦•à§à¦·à¦® à¦¹à¦¬à§‡à¥¤ 5. à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦¨à¦¿à¦¶à§à¦šà¦¿à¦¤à¦•à¦°à¦£: à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£ à¦¹à¦“à¦¯à¦¼à¦¾à¦° à¦ªà¦°à§‡, à¦—à§à¦°à¦¾à¦¹à¦• à¦à¦•à¦Ÿà¦¿ à¦¬à¦¿à¦œà§à¦žà¦ªà§à¦¤à¦¿ à¦ªà¦¾à¦¬à§‡à¦¨, à¦à¦¬à¦‚ à¦¤à¦¾à¦°à¦¾ à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦¨à¦¿à¦¶à§à¦šà¦¿à¦¤ à¦•à¦°à¦¤à§‡ à¦à¦¬à¦‚ à¦ªà¦°à¦¿à¦·à§‡à¦¬à¦¾à¦Ÿà¦¿ à¦°à§‡à¦Ÿ à¦•à¦°à¦¤à§‡ à¦¸à¦•à§à¦·à¦® à¦¹à¦¬à§‡à¥¤ 6. à¦ªà§‡à¦®à§‡à¦¨à§à¦Ÿ: à¦—à§à¦°à¦¾à¦¹à¦•à¦°à¦¾ à¦¤à¦¾à¦¦à§‡à¦° à¦ªà¦›à¦¨à§à¦¦à§‡à¦° à¦…à¦°à§à¦¥à¦ªà§à¦°à¦¦à¦¾à¦¨à§‡à¦° à¦ªà¦¦à§à¦§à¦¤à¦¿ à¦…à§à¦¯à¦¾à¦ªà§‡ à¦¯à§‹à¦— à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‡à¦¨, à¦¯à§‡à¦®à¦¨ à¦à¦•à¦Ÿà¦¿ à¦•à§à¦°à§‡à¦¡à¦¿à¦Ÿ à¦•à¦¾à¦°à§à¦¡ à¦¬à¦¾ à¦®à§‹à¦¬à¦¾à¦‡à¦² à¦ªà§‡à¦®à§‡à¦¨à§à¦Ÿ à¦¸à¦¿à¦¸à§à¦Ÿà§‡à¦®à§‡à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦à¦¬à¦‚ à¦²à§‡à¦¨à¦¦à§‡à¦¨à¦—à§à¦²à¦¿ à¦¨à¦¿à¦°à§à¦¬à¦¿à¦˜à§à¦¨à§‡ à¦ªà§à¦°à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾ à¦•à¦°à¦¾ à¦¹à¦¬à§‡à¥¤ 7. à¦—à§à¦°à¦¾à¦¹à¦• à¦¸à¦¹à¦¾à¦¯à¦¼à¦¤à¦¾: à¦—à§à¦°à¦¾à¦¹à¦•à¦°à¦¾ à¦¸à¦¹à¦¾à¦¯à¦¼à¦¤à¦¾ à¦•à§‡à¦¨à§à¦¦à§à¦°à§‡ à¦ªà§à¦°à¦¬à§‡à¦¶ à¦•à¦°à¦¤à§‡ à¦¸à¦•à§à¦·à¦® à¦¹à¦¬à§‡à¦¨, à¦¯à§‡à¦–à¦¾à¦¨à§‡ à¦¤à¦¾à¦°à¦¾ à¦¯à§‡à¦•à§‹à¦¨ à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦ªà§à¦°à¦¤à¦¿à¦¬à§‡à¦¦à¦¨ à¦•à¦°à¦¤à§‡, à¦¤à¦¾à¦¦à§‡à¦° à¦ªà§à¦°à¦¶à§à¦¨à§‡à¦° à¦‰à¦¤à§à¦¤à¦° à¦ªà§‡à¦¤à§‡ à¦¬à¦¾ à¦…à§à¦¯à¦¾à¦ª à¦¥à§‡à¦•à§‡ à¦¸à¦°à¦¾à¦¸à¦°à¦¿ à¦—à§à¦°à¦¾à¦¹à¦• à¦¸à¦¹à¦¾à¦¯à¦¼à¦¤à¦¾à¦¯à¦¼ à¦•à¦² à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‡à¦¨à¥¤ à¦¸à¦¾à¦®à¦—à§à¦°à¦¿à¦•à¦­à¦¾à¦¬à§‡, à¦à¦‡ à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦…à§à¦¯à¦¾à¦ªà¦Ÿà¦¿ à¦—à§à¦°à¦¾à¦¹à¦•à¦¦à§‡à¦° à¦œà¦¨à§à¦¯ à¦à¦•à¦Ÿà¦¿ à¦¸à§à¦¬à¦œà§à¦žà¦¾à¦¤ à¦à¦¬à¦‚ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°à¦•à¦¾à¦°à§€-à¦¬à¦¾à¦¨à§à¦§à¦¬ à¦‡à¦¨à§à¦Ÿà¦¾à¦°à¦«à§‡à¦¸ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à¦¬à§‡, à¦¡à§‡à¦²à¦¿à¦­à¦¾à¦°à¦¿ à¦ªà§à¦°à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾à¦Ÿà¦¿à¦•à§‡ à¦¨à¦¿à¦°à§à¦¬à¦¿à¦˜à§à¦¨ à¦à¦¬à¦‚ à¦šà¦¾à¦ªà¦®à§à¦•à§à¦¤ à¦•à¦°à§‡ à¦¤à§à¦²à¦¬à§‡à¥¤\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO8UY34ql2vJ"
   },
   "source": [
    "We again use https://translate.google.com/ to translate the Alpaca format into Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69,
     "referenced_widgets": [
      "a6babbf2f467476f8a650a0b3cf4b225",
      "4d1479b7392549beb8459113a42e2610",
      "a1b291badc6041beae070c2e262ae005",
      "a424608a868c46b2be65767c093c3dd1",
      "4ad139d347eb449ab7b4bd6df4f64516",
      "bdc71b983e5f48b1800ebdc457f22f3e",
      "3bc92134440649aba5cf0ff5f4393c55",
      "e6ae977baad248078458445fecc03963",
      "55b47d1ed657480d9a860b46eaa1d678",
      "8b93d48735a347c1bc7a5b72692387e7",
      "960c9eba4fec494989f66a6c0dd5fce5"
     ]
    },
    "executionInfo": {
     "elapsed": 1949,
     "status": "ok",
     "timestamp": 1717663492023,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "B2tv8AEhPTu6",
    "outputId": "acd18935-ee12-4fbd-fc55-b813dc389d13"
   },
   "outputs": [],
   "source": [
    "# Wikipedia provides a title and an article text.\n",
    "# Use https://translate.google.com!\n",
    "alpaca_prompt = \"\"\"à¦à¦–à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¨à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹, à¦¯à¦¾ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦œ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§‡, à¦à¦¬à¦‚ à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹ à¦¯à¦¾ à¦†à¦°à¦“ à¦ªà§à¦°à§‡à¦•à§à¦·à¦¾à¦ªà¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤ à¦à¦•à¦Ÿà¦¿ à¦‰à¦¤à§à¦¤à¦° à¦²à¦¿à¦–à§à¦¨ à¦¯à¦¾ à¦…à¦¨à§à¦°à§‹à¦§à¦Ÿà¦¿ à¦¸à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤ \n",
    "### Instruction: {}\n",
    "\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "# becomes:\n",
    "alpaca_prompt = \"\"\"à¦à¦–à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¨à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹, à¦¯à¦¾ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦œ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§‡, à¦à¦¬à¦‚ à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹ à¦¯à¦¾ à¦†à¦°à¦“ à¦ªà§à¦°à§‡à¦•à§à¦·à¦¾à¦ªà¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤ à¦à¦•à¦Ÿà¦¿ à¦‰à¦¤à§à¦¤à¦° à¦²à¦¿à¦–à§à¦¨ à¦¯à¦¾ à¦…à¦¨à§à¦°à§‹à¦§à¦Ÿà¦¿ à¦¸à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤ \n",
    "### Instruction: {}\n",
    "\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(convo[0][\"value\"], convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "#alpaca_dataset = alpaca_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpaca_dataset = alpaca_dataset.train_test_split(train_size = 0.01)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text', 'system_prompt'],\n",
       "    num_rows: 172026\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxWWh8xsl9XT"
   },
   "source": [
    "We again employ `UnslothTrainer` and do instruction finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "f09ab65302ea49f38ad65c7285439b66",
      "89727d06874849f5b681f3705bfe0978",
      "3b939980516140c588220612194de161",
      "fed11544e08c4fb3bf9fe247f7c37cd1",
      "1531e137a0b84e3db00599d6311fbc04",
      "aa120e115285432da310d295df2bb739",
      "7c319ad4809c47189345d0d158dcd922",
      "9918c010fe1f418c831d5fffd0a94180",
      "90655a74b97944f58cad529fa8adf1fa",
      "e57c379628ce49da904b447a74b0e1b1",
      "d6217028b7744abdb85c305237cb52a0"
     ]
    },
    "executionInfo": {
     "elapsed": 115741,
     "status": "ok",
     "timestamp": 1717663607754,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "1Zul21NSRRLP",
    "outputId": "d3bab1d0-6399-4c14-9e26-b2d3ea1d27d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[codecarbon INFO @ 18:07:37] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:07:37] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:07:37] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 18:07:37] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:07:37] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:07:39] CPU Model on constant consumption mode: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 18:07:39] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:07:39]   Platform system: Linux-5.4.0-169-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 18:07:39]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 18:07:39]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 18:07:39]   Available RAM : 1007.784 GB\n",
      "[codecarbon INFO @ 18:07:39]   CPU count: 128\n",
      "[codecarbon INFO @ 18:07:39]   CPU model: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 18:07:39]   GPU count: 1\n",
      "[codecarbon INFO @ 18:07:39]   GPU model: 1 x NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = alpaca_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        # Use num_train_epochs and warmup_ratio for longer runs!\n",
    "        #max_steps = 120,\n",
    "        warmup_steps = 10,\n",
    "        warmup_ratio = 0.05,\n",
    "        num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3624846,
     "status": "ok",
     "timestamp": 1717667232586,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "DIO7c1FoRe-X",
    "outputId": "68e3537d-b30c-4f35-af67-c73bd589a37e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 172,026 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 128 | Total steps = 1,344\n",
      " \"-____-\"     Number of trainable parameters = 1,386,217,472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n",
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:08:11] Energy consumed for RAM : 0.001635 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:11] Energy consumed for all GPUs : 0.001025 kWh. Total GPU Power : 236.9192245871228 W\n",
      "[codecarbon INFO @ 18:08:11] Energy consumed for all CPUs : 0.000487 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:11] 0.003148 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:26] Energy consumed for RAM : 0.003211 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:26] Energy consumed for all GPUs : 0.002222 kWh. Total GPU Power : 286.9437672889438 W\n",
      "[codecarbon INFO @ 18:08:26] Energy consumed for all CPUs : 0.000957 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:26] 0.006391 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:41] Energy consumed for RAM : 0.004784 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:41] Energy consumed for all GPUs : 0.003419 kWh. Total GPU Power : 287.4171218110107 W\n",
      "[codecarbon INFO @ 18:08:41] Energy consumed for all CPUs : 0.001425 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:41] 0.009628 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:56] Energy consumed for RAM : 0.006357 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:56] Energy consumed for all GPUs : 0.004618 kWh. Total GPU Power : 287.9541976131156 W\n",
      "[codecarbon INFO @ 18:08:56] Energy consumed for all CPUs : 0.001894 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:56] 0.012869 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:11] Energy consumed for RAM : 0.007931 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:11] Energy consumed for all GPUs : 0.005827 kWh. Total GPU Power : 290.31942944392415 W\n",
      "[codecarbon INFO @ 18:09:11] Energy consumed for all CPUs : 0.002363 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:11] 0.016121 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='1344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/1344 02:25 < 27:06:44, 0.01 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.606300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:09:26] Energy consumed for RAM : 0.009505 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:26] Energy consumed for all GPUs : 0.006993 kWh. Total GPU Power : 279.876770470884 W\n",
      "[codecarbon INFO @ 18:09:26] Energy consumed for all CPUs : 0.002831 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:26] 0.019329 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:42] Energy consumed for RAM : 0.011183 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:42] Energy consumed for all GPUs : 0.008284 kWh. Total GPU Power : 290.7094274699353 W\n",
      "[codecarbon INFO @ 18:09:42] Energy consumed for all CPUs : 0.003331 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:42] 0.022799 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:57] Energy consumed for RAM : 0.012757 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:57] Energy consumed for all GPUs : 0.009497 kWh. Total GPU Power : 291.1135479397693 W\n",
      "[codecarbon INFO @ 18:09:57] Energy consumed for all CPUs : 0.003800 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:57] 0.026054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:12] Energy consumed for RAM : 0.014331 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:12] Energy consumed for all GPUs : 0.010686 kWh. Total GPU Power : 285.60071279964137 W\n",
      "[codecarbon INFO @ 18:10:12] Energy consumed for all CPUs : 0.004269 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:12] 0.029286 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:27] Energy consumed for RAM : 0.015922 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:27] Energy consumed for all GPUs : 0.011910 kWh. Total GPU Power : 290.57925000400616 W\n",
      "[codecarbon INFO @ 18:10:27] Energy consumed for all CPUs : 0.004743 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:27] 0.032576 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:42] Energy consumed for RAM : 0.017495 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:42] Energy consumed for all GPUs : 0.013114 kWh. Total GPU Power : 289.0136369761482 W\n",
      "[codecarbon INFO @ 18:10:42] Energy consumed for all CPUs : 0.005212 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:42] 0.035820 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:57] Energy consumed for RAM : 0.019069 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:57] Energy consumed for all GPUs : 0.014335 kWh. Total GPU Power : 293.2924827348806 W\n",
      "[codecarbon INFO @ 18:10:57] Energy consumed for all CPUs : 0.005680 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:57] 0.039084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:13] Energy consumed for RAM : 0.020734 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:13] Energy consumed for all GPUs : 0.015614 kWh. Total GPU Power : 290.08496905337216 W\n",
      "[codecarbon INFO @ 18:11:13] Energy consumed for all CPUs : 0.006176 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:13] 0.042525 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:28] Energy consumed for RAM : 0.022308 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:28] Energy consumed for all GPUs : 0.016829 kWh. Total GPU Power : 291.682251376309 W\n",
      "[codecarbon INFO @ 18:11:28] Energy consumed for all CPUs : 0.006645 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:28] 0.045782 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:43] Energy consumed for RAM : 0.023880 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:43] Energy consumed for all GPUs : 0.018023 kWh. Total GPU Power : 286.96374393688865 W\n",
      "[codecarbon INFO @ 18:11:43] Energy consumed for all CPUs : 0.007114 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:43] 0.049018 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for RAM : 0.025543 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for all GPUs : 0.019302 kWh. Total GPU Power : 290.56786476108346 W\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for all CPUs : 0.007609 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:59] 0.052455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for RAM : 0.027117 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for all GPUs : 0.020517 kWh. Total GPU Power : 291.6241488975196 W\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for all CPUs : 0.008078 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:12:14] 0.055711 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1717667232587,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "198797f5-4842-4a8d-e15c-3a60643816c0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_gpu_memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Show final memory and time stats\u001b[39;00m\n\u001b[1;32m      2\u001b[0m used_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m used_memory_for_lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory \u001b[38;5;241m-\u001b[39m \u001b[43mstart_gpu_memory\u001b[49m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m used_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory         \u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m lora_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory_for_lora\u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_gpu_memory' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=1344, training_loss=0.3289879886433482, metrics={'train_runtime': 98065.4069, 'train_samples_per_second': 1.754, 'train_steps_per_second': 0.014, 'total_flos': 1.7581221468435382e+19, 'train_loss': 0.3289879886433482, 'epoch': 1.0})\n"
     ]
    }
   ],
   "source": [
    "print(trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done finetuning\n"
     ]
    }
   ],
   "source": [
    "print(\"Done finetuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n",
    "Remember to use https://translate.google.com/!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export HF_HOME=/workspace/.cache/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/.cache/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:03<00:00, 17.58s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name = \"BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Use 4bit quantization to reduce memory usage. Can be False.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/loader.py:143\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:1129\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# RoPE Scaling's max_position_embeddings must be updated\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m max_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_seq_length, model_max_seq_length)\n\u001b[0;32m-> 1129\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# Counteract saved tokenizers\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m tokenizer_name \u001b[38;5;241m=\u001b[39m model_name \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer_name\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/modeling_utils.py:3626\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3620\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3621\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3622\u001b[0m )\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3626\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3629\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1091\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:878\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 878\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:878\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 878\u001b[0m     [\u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:678\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mLLAMA_ATTENTION_CLASSES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m LlamaMLP(config)\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:267\u001b[0m, in \u001b[0;36mLlamaAttention.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias)\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_rope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:271\u001b[0m, in \u001b[0;36mLlamaAttention._init_rope\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_rope\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_scaling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaRotaryEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m         scaling_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_scaling[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:915\u001b[0m, in \u001b[0;36mLlamaRotaryEmbedding.__init__\u001b[0;34m(self, dim, max_position_embeddings, base, device)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;241m=\u001b[39m base\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Build here to make `torch.jit.trace` work.\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_cos_sin_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:930\u001b[0m, in \u001b[0;36mLlamaRotaryEmbedding._set_cos_sin_cache\u001b[0;34m(self, seq_len, device, dtype)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# Different from paper, but it uses a different permutation in order to obtain the same calculation\u001b[39;00m\n\u001b[1;32m    929\u001b[0m emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((freqs, freqs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos_cached\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m, persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin_cached\u001b[39m\u001b[38;5;124m\"\u001b[39m, emb\u001b[38;5;241m.\u001b[39msin()\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:05<00:00, 17.89s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "#model_name2 = \"BanglaLLM/bangla-llama-7b-instruct-v0.1\"\n",
    "model_name2 = \"BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct\"\n",
    "model2, tokenizer2 = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name2, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.16s/it]\n",
      "Some weights of the model checkpoint at BanglaLLM/bangla-llama-13b-instruct-v0.1 were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.base_layer.weight', 'model.layers.0.mlp.down_proj.lora_A.default.weight', 'model.layers.0.mlp.down_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight', 'model.layers.0.mlp.up_proj.base_layer.weight', 'model.layers.0.mlp.up_proj.lora_A.default.weight', 'model.layers.0.mlp.up_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.mlp.down_proj.base_layer.weight', 'model.layers.1.mlp.down_proj.lora_A.default.weight', 'model.layers.1.mlp.down_proj.lora_B.default.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight', 'model.layers.1.mlp.gate_proj.lora_A.default.weight', 'model.layers.1.mlp.gate_proj.lora_B.default.weight', 'model.layers.1.mlp.up_proj.base_layer.weight', 'model.layers.1.mlp.up_proj.lora_A.default.weight', 'model.layers.1.mlp.up_proj.lora_B.default.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.mlp.down_proj.base_layer.weight', 'model.layers.10.mlp.down_proj.lora_A.default.weight', 'model.layers.10.mlp.down_proj.lora_B.default.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight', 'model.layers.10.mlp.gate_proj.lora_A.default.weight', 'model.layers.10.mlp.gate_proj.lora_B.default.weight', 'model.layers.10.mlp.up_proj.base_layer.weight', 'model.layers.10.mlp.up_proj.lora_A.default.weight', 'model.layers.10.mlp.up_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.mlp.down_proj.base_layer.weight', 'model.layers.11.mlp.down_proj.lora_A.default.weight', 'model.layers.11.mlp.down_proj.lora_B.default.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight', 'model.layers.11.mlp.gate_proj.lora_A.default.weight', 'model.layers.11.mlp.gate_proj.lora_B.default.weight', 'model.layers.11.mlp.up_proj.base_layer.weight', 'model.layers.11.mlp.up_proj.lora_A.default.weight', 'model.layers.11.mlp.up_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.mlp.down_proj.base_layer.weight', 'model.layers.12.mlp.down_proj.lora_A.default.weight', 'model.layers.12.mlp.down_proj.lora_B.default.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight', 'model.layers.12.mlp.gate_proj.lora_A.default.weight', 'model.layers.12.mlp.gate_proj.lora_B.default.weight', 'model.layers.12.mlp.up_proj.base_layer.weight', 'model.layers.12.mlp.up_proj.lora_A.default.weight', 'model.layers.12.mlp.up_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.mlp.down_proj.base_layer.weight', 'model.layers.13.mlp.down_proj.lora_A.default.weight', 'model.layers.13.mlp.down_proj.lora_B.default.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight', 'model.layers.13.mlp.gate_proj.lora_A.default.weight', 'model.layers.13.mlp.gate_proj.lora_B.default.weight', 'model.layers.13.mlp.up_proj.base_layer.weight', 'model.layers.13.mlp.up_proj.lora_A.default.weight', 'model.layers.13.mlp.up_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.mlp.down_proj.base_layer.weight', 'model.layers.14.mlp.down_proj.lora_A.default.weight', 'model.layers.14.mlp.down_proj.lora_B.default.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight', 'model.layers.14.mlp.gate_proj.lora_A.default.weight', 'model.layers.14.mlp.gate_proj.lora_B.default.weight', 'model.layers.14.mlp.up_proj.base_layer.weight', 'model.layers.14.mlp.up_proj.lora_A.default.weight', 'model.layers.14.mlp.up_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.mlp.down_proj.base_layer.weight', 'model.layers.15.mlp.down_proj.lora_A.default.weight', 'model.layers.15.mlp.down_proj.lora_B.default.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight', 'model.layers.15.mlp.gate_proj.lora_A.default.weight', 'model.layers.15.mlp.gate_proj.lora_B.default.weight', 'model.layers.15.mlp.up_proj.base_layer.weight', 'model.layers.15.mlp.up_proj.lora_A.default.weight', 'model.layers.15.mlp.up_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.mlp.down_proj.base_layer.weight', 'model.layers.16.mlp.down_proj.lora_A.default.weight', 'model.layers.16.mlp.down_proj.lora_B.default.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight', 'model.layers.16.mlp.gate_proj.lora_A.default.weight', 'model.layers.16.mlp.gate_proj.lora_B.default.weight', 'model.layers.16.mlp.up_proj.base_layer.weight', 'model.layers.16.mlp.up_proj.lora_A.default.weight', 'model.layers.16.mlp.up_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.mlp.down_proj.base_layer.weight', 'model.layers.17.mlp.down_proj.lora_A.default.weight', 'model.layers.17.mlp.down_proj.lora_B.default.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight', 'model.layers.17.mlp.gate_proj.lora_A.default.weight', 'model.layers.17.mlp.gate_proj.lora_B.default.weight', 'model.layers.17.mlp.up_proj.base_layer.weight', 'model.layers.17.mlp.up_proj.lora_A.default.weight', 'model.layers.17.mlp.up_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.mlp.down_proj.base_layer.weight', 'model.layers.18.mlp.down_proj.lora_A.default.weight', 'model.layers.18.mlp.down_proj.lora_B.default.weight', 'model.layers.18.mlp.gate_proj.base_layer.weight', 'model.layers.18.mlp.gate_proj.lora_A.default.weight', 'model.layers.18.mlp.gate_proj.lora_B.default.weight', 'model.layers.18.mlp.up_proj.base_layer.weight', 'model.layers.18.mlp.up_proj.lora_A.default.weight', 'model.layers.18.mlp.up_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.base_layer.weight', 'model.layers.18.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight', 'model.layers.18.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.o_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.mlp.down_proj.base_layer.weight', 'model.layers.19.mlp.down_proj.lora_A.default.weight', 'model.layers.19.mlp.down_proj.lora_B.default.weight', 'model.layers.19.mlp.gate_proj.base_layer.weight', 'model.layers.19.mlp.gate_proj.lora_A.default.weight', 'model.layers.19.mlp.gate_proj.lora_B.default.weight', 'model.layers.19.mlp.up_proj.base_layer.weight', 'model.layers.19.mlp.up_proj.lora_A.default.weight', 'model.layers.19.mlp.up_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.base_layer.weight', 'model.layers.19.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight', 'model.layers.19.self_attn.o_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.mlp.down_proj.base_layer.weight', 'model.layers.2.mlp.down_proj.lora_A.default.weight', 'model.layers.2.mlp.down_proj.lora_B.default.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight', 'model.layers.2.mlp.gate_proj.lora_A.default.weight', 'model.layers.2.mlp.gate_proj.lora_B.default.weight', 'model.layers.2.mlp.up_proj.base_layer.weight', 'model.layers.2.mlp.up_proj.lora_A.default.weight', 'model.layers.2.mlp.up_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.mlp.down_proj.base_layer.weight', 'model.layers.20.mlp.down_proj.lora_A.default.weight', 'model.layers.20.mlp.down_proj.lora_B.default.weight', 'model.layers.20.mlp.gate_proj.base_layer.weight', 'model.layers.20.mlp.gate_proj.lora_A.default.weight', 'model.layers.20.mlp.gate_proj.lora_B.default.weight', 'model.layers.20.mlp.up_proj.base_layer.weight', 'model.layers.20.mlp.up_proj.lora_A.default.weight', 'model.layers.20.mlp.up_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.k_proj.lora_A.default.weight', 'model.layers.20.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight', 'model.layers.20.self_attn.o_proj.lora_A.default.weight', 'model.layers.20.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.mlp.down_proj.base_layer.weight', 'model.layers.21.mlp.down_proj.lora_A.default.weight', 'model.layers.21.mlp.down_proj.lora_B.default.weight', 'model.layers.21.mlp.gate_proj.base_layer.weight', 'model.layers.21.mlp.gate_proj.lora_A.default.weight', 'model.layers.21.mlp.gate_proj.lora_B.default.weight', 'model.layers.21.mlp.up_proj.base_layer.weight', 'model.layers.21.mlp.up_proj.lora_A.default.weight', 'model.layers.21.mlp.up_proj.lora_B.default.weight', 'model.layers.21.self_attn.k_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.lora_A.default.weight', 'model.layers.21.self_attn.k_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.o_proj.lora_A.default.weight', 'model.layers.21.self_attn.o_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.mlp.down_proj.base_layer.weight', 'model.layers.22.mlp.down_proj.lora_A.default.weight', 'model.layers.22.mlp.down_proj.lora_B.default.weight', 'model.layers.22.mlp.gate_proj.base_layer.weight', 'model.layers.22.mlp.gate_proj.lora_A.default.weight', 'model.layers.22.mlp.gate_proj.lora_B.default.weight', 'model.layers.22.mlp.up_proj.base_layer.weight', 'model.layers.22.mlp.up_proj.lora_A.default.weight', 'model.layers.22.mlp.up_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.base_layer.weight', 'model.layers.22.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.k_proj.lora_B.default.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.lora_A.default.weight', 'model.layers.22.self_attn.o_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.mlp.down_proj.base_layer.weight', 'model.layers.23.mlp.down_proj.lora_A.default.weight', 'model.layers.23.mlp.down_proj.lora_B.default.weight', 'model.layers.23.mlp.gate_proj.base_layer.weight', 'model.layers.23.mlp.gate_proj.lora_A.default.weight', 'model.layers.23.mlp.gate_proj.lora_B.default.weight', 'model.layers.23.mlp.up_proj.base_layer.weight', 'model.layers.23.mlp.up_proj.lora_A.default.weight', 'model.layers.23.mlp.up_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.base_layer.weight', 'model.layers.23.self_attn.k_proj.lora_A.default.weight', 'model.layers.23.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight', 'model.layers.23.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.o_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.mlp.down_proj.base_layer.weight', 'model.layers.24.mlp.down_proj.lora_A.default.weight', 'model.layers.24.mlp.down_proj.lora_B.default.weight', 'model.layers.24.mlp.gate_proj.base_layer.weight', 'model.layers.24.mlp.gate_proj.lora_A.default.weight', 'model.layers.24.mlp.gate_proj.lora_B.default.weight', 'model.layers.24.mlp.up_proj.base_layer.weight', 'model.layers.24.mlp.up_proj.lora_A.default.weight', 'model.layers.24.mlp.up_proj.lora_B.default.weight', 'model.layers.24.self_attn.k_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.lora_A.default.weight', 'model.layers.24.self_attn.k_proj.lora_B.default.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.o_proj.lora_A.default.weight', 'model.layers.24.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.mlp.down_proj.base_layer.weight', 'model.layers.25.mlp.down_proj.lora_A.default.weight', 'model.layers.25.mlp.down_proj.lora_B.default.weight', 'model.layers.25.mlp.gate_proj.base_layer.weight', 'model.layers.25.mlp.gate_proj.lora_A.default.weight', 'model.layers.25.mlp.gate_proj.lora_B.default.weight', 'model.layers.25.mlp.up_proj.base_layer.weight', 'model.layers.25.mlp.up_proj.lora_A.default.weight', 'model.layers.25.mlp.up_proj.lora_B.default.weight', 'model.layers.25.self_attn.k_proj.base_layer.weight', 'model.layers.25.self_attn.k_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_B.default.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.lora_A.default.weight', 'model.layers.25.self_attn.o_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.mlp.down_proj.base_layer.weight', 'model.layers.26.mlp.down_proj.lora_A.default.weight', 'model.layers.26.mlp.down_proj.lora_B.default.weight', 'model.layers.26.mlp.gate_proj.base_layer.weight', 'model.layers.26.mlp.gate_proj.lora_A.default.weight', 'model.layers.26.mlp.gate_proj.lora_B.default.weight', 'model.layers.26.mlp.up_proj.base_layer.weight', 'model.layers.26.mlp.up_proj.lora_A.default.weight', 'model.layers.26.mlp.up_proj.lora_B.default.weight', 'model.layers.26.self_attn.k_proj.base_layer.weight', 'model.layers.26.self_attn.k_proj.lora_A.default.weight', 'model.layers.26.self_attn.k_proj.lora_B.default.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.lora_A.default.weight', 'model.layers.26.self_attn.o_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.mlp.down_proj.base_layer.weight', 'model.layers.27.mlp.down_proj.lora_A.default.weight', 'model.layers.27.mlp.down_proj.lora_B.default.weight', 'model.layers.27.mlp.gate_proj.base_layer.weight', 'model.layers.27.mlp.gate_proj.lora_A.default.weight', 'model.layers.27.mlp.gate_proj.lora_B.default.weight', 'model.layers.27.mlp.up_proj.base_layer.weight', 'model.layers.27.mlp.up_proj.lora_A.default.weight', 'model.layers.27.mlp.up_proj.lora_B.default.weight', 'model.layers.27.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.lora_A.default.weight', 'model.layers.27.self_attn.k_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight', 'model.layers.27.self_attn.o_proj.lora_A.default.weight', 'model.layers.27.self_attn.o_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.mlp.down_proj.base_layer.weight', 'model.layers.28.mlp.down_proj.lora_A.default.weight', 'model.layers.28.mlp.down_proj.lora_B.default.weight', 'model.layers.28.mlp.gate_proj.base_layer.weight', 'model.layers.28.mlp.gate_proj.lora_A.default.weight', 'model.layers.28.mlp.gate_proj.lora_B.default.weight', 'model.layers.28.mlp.up_proj.base_layer.weight', 'model.layers.28.mlp.up_proj.lora_A.default.weight', 'model.layers.28.mlp.up_proj.lora_B.default.weight', 'model.layers.28.self_attn.k_proj.base_layer.weight', 'model.layers.28.self_attn.k_proj.lora_A.default.weight', 'model.layers.28.self_attn.k_proj.lora_B.default.weight', 'model.layers.28.self_attn.o_proj.base_layer.weight', 'model.layers.28.self_attn.o_proj.lora_A.default.weight', 'model.layers.28.self_attn.o_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.mlp.down_proj.base_layer.weight', 'model.layers.29.mlp.down_proj.lora_A.default.weight', 'model.layers.29.mlp.down_proj.lora_B.default.weight', 'model.layers.29.mlp.gate_proj.base_layer.weight', 'model.layers.29.mlp.gate_proj.lora_A.default.weight', 'model.layers.29.mlp.gate_proj.lora_B.default.weight', 'model.layers.29.mlp.up_proj.base_layer.weight', 'model.layers.29.mlp.up_proj.lora_A.default.weight', 'model.layers.29.mlp.up_proj.lora_B.default.weight', 'model.layers.29.self_attn.k_proj.base_layer.weight', 'model.layers.29.self_attn.k_proj.lora_A.default.weight', 'model.layers.29.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.o_proj.base_layer.weight', 'model.layers.29.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.o_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.mlp.down_proj.base_layer.weight', 'model.layers.3.mlp.down_proj.lora_A.default.weight', 'model.layers.3.mlp.down_proj.lora_B.default.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight', 'model.layers.3.mlp.gate_proj.lora_A.default.weight', 'model.layers.3.mlp.gate_proj.lora_B.default.weight', 'model.layers.3.mlp.up_proj.base_layer.weight', 'model.layers.3.mlp.up_proj.lora_A.default.weight', 'model.layers.3.mlp.up_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.mlp.down_proj.base_layer.weight', 'model.layers.30.mlp.down_proj.lora_A.default.weight', 'model.layers.30.mlp.down_proj.lora_B.default.weight', 'model.layers.30.mlp.gate_proj.base_layer.weight', 'model.layers.30.mlp.gate_proj.lora_A.default.weight', 'model.layers.30.mlp.gate_proj.lora_B.default.weight', 'model.layers.30.mlp.up_proj.base_layer.weight', 'model.layers.30.mlp.up_proj.lora_A.default.weight', 'model.layers.30.mlp.up_proj.lora_B.default.weight', 'model.layers.30.self_attn.k_proj.base_layer.weight', 'model.layers.30.self_attn.k_proj.lora_A.default.weight', 'model.layers.30.self_attn.k_proj.lora_B.default.weight', 'model.layers.30.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.o_proj.lora_A.default.weight', 'model.layers.30.self_attn.o_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.mlp.down_proj.base_layer.weight', 'model.layers.31.mlp.down_proj.lora_A.default.weight', 'model.layers.31.mlp.down_proj.lora_B.default.weight', 'model.layers.31.mlp.gate_proj.base_layer.weight', 'model.layers.31.mlp.gate_proj.lora_A.default.weight', 'model.layers.31.mlp.gate_proj.lora_B.default.weight', 'model.layers.31.mlp.up_proj.base_layer.weight', 'model.layers.31.mlp.up_proj.lora_A.default.weight', 'model.layers.31.mlp.up_proj.lora_B.default.weight', 'model.layers.31.self_attn.k_proj.base_layer.weight', 'model.layers.31.self_attn.k_proj.lora_A.default.weight', 'model.layers.31.self_attn.k_proj.lora_B.default.weight', 'model.layers.31.self_attn.o_proj.base_layer.weight', 'model.layers.31.self_attn.o_proj.lora_A.default.weight', 'model.layers.31.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.32.mlp.down_proj.base_layer.weight', 'model.layers.32.mlp.down_proj.lora_A.default.weight', 'model.layers.32.mlp.down_proj.lora_B.default.weight', 'model.layers.32.mlp.gate_proj.base_layer.weight', 'model.layers.32.mlp.gate_proj.lora_A.default.weight', 'model.layers.32.mlp.gate_proj.lora_B.default.weight', 'model.layers.32.mlp.up_proj.base_layer.weight', 'model.layers.32.mlp.up_proj.lora_A.default.weight', 'model.layers.32.mlp.up_proj.lora_B.default.weight', 'model.layers.32.self_attn.k_proj.base_layer.weight', 'model.layers.32.self_attn.k_proj.lora_A.default.weight', 'model.layers.32.self_attn.k_proj.lora_B.default.weight', 'model.layers.32.self_attn.o_proj.base_layer.weight', 'model.layers.32.self_attn.o_proj.lora_A.default.weight', 'model.layers.32.self_attn.o_proj.lora_B.default.weight', 'model.layers.32.self_attn.q_proj.base_layer.weight', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.v_proj.base_layer.weight', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.mlp.down_proj.base_layer.weight', 'model.layers.33.mlp.down_proj.lora_A.default.weight', 'model.layers.33.mlp.down_proj.lora_B.default.weight', 'model.layers.33.mlp.gate_proj.base_layer.weight', 'model.layers.33.mlp.gate_proj.lora_A.default.weight', 'model.layers.33.mlp.gate_proj.lora_B.default.weight', 'model.layers.33.mlp.up_proj.base_layer.weight', 'model.layers.33.mlp.up_proj.lora_A.default.weight', 'model.layers.33.mlp.up_proj.lora_B.default.weight', 'model.layers.33.self_attn.k_proj.base_layer.weight', 'model.layers.33.self_attn.k_proj.lora_A.default.weight', 'model.layers.33.self_attn.k_proj.lora_B.default.weight', 'model.layers.33.self_attn.o_proj.base_layer.weight', 'model.layers.33.self_attn.o_proj.lora_A.default.weight', 'model.layers.33.self_attn.o_proj.lora_B.default.weight', 'model.layers.33.self_attn.q_proj.base_layer.weight', 'model.layers.33.self_attn.q_proj.lora_A.default.weight', 'model.layers.33.self_attn.q_proj.lora_B.default.weight', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.v_proj.base_layer.weight', 'model.layers.33.self_attn.v_proj.lora_A.default.weight', 'model.layers.33.self_attn.v_proj.lora_B.default.weight', 'model.layers.34.mlp.down_proj.base_layer.weight', 'model.layers.34.mlp.down_proj.lora_A.default.weight', 'model.layers.34.mlp.down_proj.lora_B.default.weight', 'model.layers.34.mlp.gate_proj.base_layer.weight', 'model.layers.34.mlp.gate_proj.lora_A.default.weight', 'model.layers.34.mlp.gate_proj.lora_B.default.weight', 'model.layers.34.mlp.up_proj.base_layer.weight', 'model.layers.34.mlp.up_proj.lora_A.default.weight', 'model.layers.34.mlp.up_proj.lora_B.default.weight', 'model.layers.34.self_attn.k_proj.base_layer.weight', 'model.layers.34.self_attn.k_proj.lora_A.default.weight', 'model.layers.34.self_attn.k_proj.lora_B.default.weight', 'model.layers.34.self_attn.o_proj.base_layer.weight', 'model.layers.34.self_attn.o_proj.lora_A.default.weight', 'model.layers.34.self_attn.o_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.base_layer.weight', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.v_proj.base_layer.weight', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.mlp.down_proj.base_layer.weight', 'model.layers.35.mlp.down_proj.lora_A.default.weight', 'model.layers.35.mlp.down_proj.lora_B.default.weight', 'model.layers.35.mlp.gate_proj.base_layer.weight', 'model.layers.35.mlp.gate_proj.lora_A.default.weight', 'model.layers.35.mlp.gate_proj.lora_B.default.weight', 'model.layers.35.mlp.up_proj.base_layer.weight', 'model.layers.35.mlp.up_proj.lora_A.default.weight', 'model.layers.35.mlp.up_proj.lora_B.default.weight', 'model.layers.35.self_attn.k_proj.base_layer.weight', 'model.layers.35.self_attn.k_proj.lora_A.default.weight', 'model.layers.35.self_attn.k_proj.lora_B.default.weight', 'model.layers.35.self_attn.o_proj.base_layer.weight', 'model.layers.35.self_attn.o_proj.lora_A.default.weight', 'model.layers.35.self_attn.o_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.base_layer.weight', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.v_proj.base_layer.weight', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.36.mlp.down_proj.base_layer.weight', 'model.layers.36.mlp.down_proj.lora_A.default.weight', 'model.layers.36.mlp.down_proj.lora_B.default.weight', 'model.layers.36.mlp.gate_proj.base_layer.weight', 'model.layers.36.mlp.gate_proj.lora_A.default.weight', 'model.layers.36.mlp.gate_proj.lora_B.default.weight', 'model.layers.36.mlp.up_proj.base_layer.weight', 'model.layers.36.mlp.up_proj.lora_A.default.weight', 'model.layers.36.mlp.up_proj.lora_B.default.weight', 'model.layers.36.self_attn.k_proj.base_layer.weight', 'model.layers.36.self_attn.k_proj.lora_A.default.weight', 'model.layers.36.self_attn.k_proj.lora_B.default.weight', 'model.layers.36.self_attn.o_proj.base_layer.weight', 'model.layers.36.self_attn.o_proj.lora_A.default.weight', 'model.layers.36.self_attn.o_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.base_layer.weight', 'model.layers.36.self_attn.q_proj.lora_A.default.weight', 'model.layers.36.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.v_proj.base_layer.weight', 'model.layers.36.self_attn.v_proj.lora_A.default.weight', 'model.layers.36.self_attn.v_proj.lora_B.default.weight', 'model.layers.37.mlp.down_proj.base_layer.weight', 'model.layers.37.mlp.down_proj.lora_A.default.weight', 'model.layers.37.mlp.down_proj.lora_B.default.weight', 'model.layers.37.mlp.gate_proj.base_layer.weight', 'model.layers.37.mlp.gate_proj.lora_A.default.weight', 'model.layers.37.mlp.gate_proj.lora_B.default.weight', 'model.layers.37.mlp.up_proj.base_layer.weight', 'model.layers.37.mlp.up_proj.lora_A.default.weight', 'model.layers.37.mlp.up_proj.lora_B.default.weight', 'model.layers.37.self_attn.k_proj.base_layer.weight', 'model.layers.37.self_attn.k_proj.lora_A.default.weight', 'model.layers.37.self_attn.k_proj.lora_B.default.weight', 'model.layers.37.self_attn.o_proj.base_layer.weight', 'model.layers.37.self_attn.o_proj.lora_A.default.weight', 'model.layers.37.self_attn.o_proj.lora_B.default.weight', 'model.layers.37.self_attn.q_proj.base_layer.weight', 'model.layers.37.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_B.default.weight', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.v_proj.base_layer.weight', 'model.layers.37.self_attn.v_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_B.default.weight', 'model.layers.38.mlp.down_proj.base_layer.weight', 'model.layers.38.mlp.down_proj.lora_A.default.weight', 'model.layers.38.mlp.down_proj.lora_B.default.weight', 'model.layers.38.mlp.gate_proj.base_layer.weight', 'model.layers.38.mlp.gate_proj.lora_A.default.weight', 'model.layers.38.mlp.gate_proj.lora_B.default.weight', 'model.layers.38.mlp.up_proj.base_layer.weight', 'model.layers.38.mlp.up_proj.lora_A.default.weight', 'model.layers.38.mlp.up_proj.lora_B.default.weight', 'model.layers.38.self_attn.k_proj.base_layer.weight', 'model.layers.38.self_attn.k_proj.lora_A.default.weight', 'model.layers.38.self_attn.k_proj.lora_B.default.weight', 'model.layers.38.self_attn.o_proj.base_layer.weight', 'model.layers.38.self_attn.o_proj.lora_A.default.weight', 'model.layers.38.self_attn.o_proj.lora_B.default.weight', 'model.layers.38.self_attn.q_proj.base_layer.weight', 'model.layers.38.self_attn.q_proj.lora_A.default.weight', 'model.layers.38.self_attn.q_proj.lora_B.default.weight', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.v_proj.base_layer.weight', 'model.layers.38.self_attn.v_proj.lora_A.default.weight', 'model.layers.38.self_attn.v_proj.lora_B.default.weight', 'model.layers.39.mlp.down_proj.base_layer.weight', 'model.layers.39.mlp.down_proj.lora_A.default.weight', 'model.layers.39.mlp.down_proj.lora_B.default.weight', 'model.layers.39.mlp.gate_proj.base_layer.weight', 'model.layers.39.mlp.gate_proj.lora_A.default.weight', 'model.layers.39.mlp.gate_proj.lora_B.default.weight', 'model.layers.39.mlp.up_proj.base_layer.weight', 'model.layers.39.mlp.up_proj.lora_A.default.weight', 'model.layers.39.mlp.up_proj.lora_B.default.weight', 'model.layers.39.self_attn.k_proj.base_layer.weight', 'model.layers.39.self_attn.k_proj.lora_A.default.weight', 'model.layers.39.self_attn.k_proj.lora_B.default.weight', 'model.layers.39.self_attn.o_proj.base_layer.weight', 'model.layers.39.self_attn.o_proj.lora_A.default.weight', 'model.layers.39.self_attn.o_proj.lora_B.default.weight', 'model.layers.39.self_attn.q_proj.base_layer.weight', 'model.layers.39.self_attn.q_proj.lora_A.default.weight', 'model.layers.39.self_attn.q_proj.lora_B.default.weight', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.v_proj.base_layer.weight', 'model.layers.39.self_attn.v_proj.lora_A.default.weight', 'model.layers.39.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.mlp.down_proj.base_layer.weight', 'model.layers.4.mlp.down_proj.lora_A.default.weight', 'model.layers.4.mlp.down_proj.lora_B.default.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight', 'model.layers.4.mlp.gate_proj.lora_A.default.weight', 'model.layers.4.mlp.gate_proj.lora_B.default.weight', 'model.layers.4.mlp.up_proj.base_layer.weight', 'model.layers.4.mlp.up_proj.lora_A.default.weight', 'model.layers.4.mlp.up_proj.lora_B.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.mlp.down_proj.base_layer.weight', 'model.layers.5.mlp.down_proj.lora_A.default.weight', 'model.layers.5.mlp.down_proj.lora_B.default.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight', 'model.layers.5.mlp.gate_proj.lora_A.default.weight', 'model.layers.5.mlp.gate_proj.lora_B.default.weight', 'model.layers.5.mlp.up_proj.base_layer.weight', 'model.layers.5.mlp.up_proj.lora_A.default.weight', 'model.layers.5.mlp.up_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.mlp.down_proj.base_layer.weight', 'model.layers.6.mlp.down_proj.lora_A.default.weight', 'model.layers.6.mlp.down_proj.lora_B.default.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight', 'model.layers.6.mlp.gate_proj.lora_A.default.weight', 'model.layers.6.mlp.gate_proj.lora_B.default.weight', 'model.layers.6.mlp.up_proj.base_layer.weight', 'model.layers.6.mlp.up_proj.lora_A.default.weight', 'model.layers.6.mlp.up_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.mlp.down_proj.base_layer.weight', 'model.layers.7.mlp.down_proj.lora_A.default.weight', 'model.layers.7.mlp.down_proj.lora_B.default.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight', 'model.layers.7.mlp.gate_proj.lora_A.default.weight', 'model.layers.7.mlp.gate_proj.lora_B.default.weight', 'model.layers.7.mlp.up_proj.base_layer.weight', 'model.layers.7.mlp.up_proj.lora_A.default.weight', 'model.layers.7.mlp.up_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.mlp.down_proj.base_layer.weight', 'model.layers.8.mlp.down_proj.lora_A.default.weight', 'model.layers.8.mlp.down_proj.lora_B.default.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight', 'model.layers.8.mlp.gate_proj.lora_A.default.weight', 'model.layers.8.mlp.gate_proj.lora_B.default.weight', 'model.layers.8.mlp.up_proj.base_layer.weight', 'model.layers.8.mlp.up_proj.lora_A.default.weight', 'model.layers.8.mlp.up_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.mlp.down_proj.base_layer.weight', 'model.layers.9.mlp.down_proj.lora_A.default.weight', 'model.layers.9.mlp.down_proj.lora_B.default.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight', 'model.layers.9.mlp.gate_proj.lora_A.default.weight', 'model.layers.9.mlp.gate_proj.lora_B.default.weight', 'model.layers.9.mlp.up_proj.base_layer.weight', 'model.layers.9.mlp.up_proj.lora_A.default.weight', 'model.layers.9.mlp.up_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at BanglaLLM/bangla-llama-13b-instruct-v0.1 and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.mlp.down_proj.weight', 'model.layers.32.mlp.gate_proj.weight', 'model.layers.32.mlp.up_proj.weight', 'model.layers.32.self_attn.k_proj.weight', 'model.layers.32.self_attn.o_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.mlp.down_proj.weight', 'model.layers.33.mlp.gate_proj.weight', 'model.layers.33.mlp.up_proj.weight', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.v_proj.weight', 'model.layers.34.mlp.down_proj.weight', 'model.layers.34.mlp.gate_proj.weight', 'model.layers.34.mlp.up_proj.weight', 'model.layers.34.self_attn.k_proj.weight', 'model.layers.34.self_attn.o_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.mlp.down_proj.weight', 'model.layers.35.mlp.gate_proj.weight', 'model.layers.35.mlp.up_proj.weight', 'model.layers.35.self_attn.k_proj.weight', 'model.layers.35.self_attn.o_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.36.mlp.down_proj.weight', 'model.layers.36.mlp.gate_proj.weight', 'model.layers.36.mlp.up_proj.weight', 'model.layers.36.self_attn.k_proj.weight', 'model.layers.36.self_attn.o_proj.weight', 'model.layers.36.self_attn.q_proj.weight', 'model.layers.36.self_attn.v_proj.weight', 'model.layers.37.mlp.down_proj.weight', 'model.layers.37.mlp.gate_proj.weight', 'model.layers.37.mlp.up_proj.weight', 'model.layers.37.self_attn.k_proj.weight', 'model.layers.37.self_attn.o_proj.weight', 'model.layers.37.self_attn.q_proj.weight', 'model.layers.37.self_attn.v_proj.weight', 'model.layers.38.mlp.down_proj.weight', 'model.layers.38.mlp.gate_proj.weight', 'model.layers.38.mlp.up_proj.weight', 'model.layers.38.self_attn.k_proj.weight', 'model.layers.38.self_attn.o_proj.weight', 'model.layers.38.self_attn.q_proj.weight', 'model.layers.38.self_attn.v_proj.weight', 'model.layers.39.mlp.down_proj.weight', 'model.layers.39.mlp.gate_proj.weight', 'model.layers.39.mlp.up_proj.weight', 'model.layers.39.self_attn.k_proj.weight', 'model.layers.39.self_attn.o_proj.weight', 'model.layers.39.self_attn.q_proj.weight', 'model.layers.39.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Unsloth: Will load BanglaLLM/bangla-llama-13b-instruct-v0.1 as a legacy tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "BanglaLLM/bangla-llama-13b-instruct-v0.1 does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name3 = \"BanglaLLM/bangla-llama-13b-instruct-v0.1\"\n",
    "model3, tokenizer3 = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name3, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6094,
     "status": "ok",
     "timestamp": 1717667238669,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "B_GRpqmUiFRj",
    "outputId": "1b8ad5ce-d224-44a4-b4f7-046be29b0713"
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model2)\n",
    "#FastLanguageModel.for_inference(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible. \n",
    "\n",
    "alpaca_prompt = \"\"\"\n",
    "### Instruction: {}\n",
    "### Input: {}\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m alpaca_prompt\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;66;03m#\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\" + q, #instruction\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mq\u001b[49m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m#\"Summarize in Bengali\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m#\"\",\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         q, \u001b[38;5;66;03m#input\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# output - leave this blank for generation!\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q' is not defined"
     ]
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\" + q, #instruction\n",
    "        \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\" + q,\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9003,
     "status": "ok",
     "timestamp": 1717667247661,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "2376324d-23e1-4433-94cf-385d8fda4727",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "### Instruction: You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\n",
      "\n",
      "### Input: à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§‡à¦° à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿à¦° à¦¨à¦¾à¦® à¦•à§€?\n",
      "### Response: à¦¹à¦¾à¦œà§€ à¦‡à¦¸à§à¦•à¦¾à¦¨à§à¦¦à¦° à¦†à¦¹à¦®à¦¦\n",
      "\n",
      "### Input: à¦•à§‹à¦¨ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦¦à§‡à¦¶à§‡à¦° à¦ªà§à¦°à¦¥à¦® à¦¨à¦¾à¦°à§€ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦¹à¦¯à¦¼à§‡à¦›à§‡à¦¨?\n",
      "### Response: à¦¸à§à¦¬à§à¦°à¦¤à¦¾ à¦°à¦¾à¦¯à¦¼\n",
      "\n",
      "### Input: à¦•à§‹à¦¨ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦ªà§à¦°à¦¥à¦® à¦¨à¦¾à¦°à§€ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦²à§‡à¦¨?\n",
      "### Response: à¦¸à§à¦¬à§à¦°à¦¤à¦¾ à¦°à¦¾à¦¯à¦¼\n",
      "\n",
      "### Input: à¦•à§‹à¦¨ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦¦à§‡à¦¶à§‡à¦° à¦ªà§à¦°à¦¥à¦® à¦¨à¦¾à¦°à§€ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦²à§‡à¦¨?\n",
      "### Response: à¦¸à§à¦¬à§à¦°à¦¤à¦¾ à¦°à¦¾à¦¯à¦¼\n",
      "\n",
      "### Input: à¦•à§‹à¦¨ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦ªà§à¦°à¦¥à¦® à¦¨à¦¾à¦°à§€ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦²à§‡à¦¨?\n",
      "### Response: à¦¸à§à¦¬à§à¦°à¦¤à¦¾ à¦°à¦¾à¦¯à¦¼\n",
      "\n",
      "### Input: à¦•à§‹à¦¨ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦¦à§‡à¦¶à§‡à¦° à¦ªà§à¦°à¦¥à¦® à¦¨à¦¾à¦°à§€ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦²à§‡à¦¨?\n",
      "### Response: à¦¸à§à¦¬à§à¦°à¦¤à¦¾ à¦°à¦¾à¦¯à¦¼\n",
      "\n",
      "### Input: à¦•à§‹à¦¨ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦ªà§à¦°à¦¥à¦® à¦¨à¦¾à¦°à§€ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿ à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦²à§‡à¦¨?\n",
      "### Response: à¦¸à§à¦¬à§à¦°à¦¤à¦¾\n"
     ]
    }
   ],
   "source": [
    "q = \"à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§‡à¦° à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦¬à¦¿à¦šà¦¾à¦°à¦ªà¦¤à¦¿à¦° à¦¨à¦¾à¦® à¦•à§€ ?\"\n",
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "output = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### Instruction: You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\\n### Input: à¦¬à¦¾à¦‚à¦²à¦¾ à¦¸à¦¾à¦¹à¦¿à¦¤à§à¦¯à§‡à¦° à¦®à¦¹à¦¾à¦•à¦¾à¦¬à§à¦¯à§‡à¦° à¦®à¦§à§à¦¯à§‡ à¦à¦•à¦Ÿà¦¿ \"à¦®à§‡à¦˜à¦¨à¦¾à¦¦à¦¬à¦§ à¦•à¦¾à¦¬à§à¦¯\" à¦•à§‡ à¦²à¦¿à¦–à§‡à¦›à§‡à¦¨?\\n### Response:'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        \"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦…à¦¬à¦¸à§à¦¥à¦¿à¦¤ à¦¹à§Ÿà§‡ à¦†à¦®à¦¾à¦¤à§‹ à¦à¦‡ à¦›à¦¬à¦¿ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿ à¦¤à§‹à¦ à¦…à¦¬à¦¸à§à¦¥à¦¿à¦¤ à¦¹à§Ÿà§‡ à¦†à¦®à¦¾à¦¤à§‹ à¦à¦‡ à¦›à¦¬à¦¿ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à¥‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à¥‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬Ñà¦¤à¥‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬Ñà¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬Ñà¦¤à§‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬Ñà¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬Ñà¤¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬Ñà¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬Ñà¦¤à§‡ à¦…à¦­à¦™à§à¦— à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬Ñà¦¤à§‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬ITableView à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬Ñà¦¤à§‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬ITableView à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬ITableView à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬ITableView à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¤¬ß¬Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›Õ¢Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à¦¬Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›Õ¢Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›Õ¢Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›à½–Portail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à¥‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à¥‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à¥‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¸™à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¸™à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¸™à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à¥‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¸™à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¸™à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¤¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¸™à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¤•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•Ø±à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªà¦¨à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¤ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ªÙ†à¦¾à¦° à¦à¦¤à¦¿à¦¹à¦¾à¦¤ à¦à¦‡ à¦›baumPortail à¦•à¦°à§‡à¦›à§‡:\n",
      "\n",
      "à¦†à¦ªà¦¨à¦¾à¦° à¦…à§à¦¯à¦¾à¦£à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦à¦‡ à¦›à¦¬textt à¦à¦‡ à¦†à¦ª\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Define the input question in Bengali\n",
    "q = \"à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦•à§‹à¦¥à¦¾à¦¯à¦¼ à¦…à¦¬à¦¸à§à¦¥à¦¿à¦¤?\"\n",
    "\n",
    "# Define the instruction in Bengali\n",
    "instruction = (\n",
    "    #\"You are an intelligent AI who can communicate in Bengali. \"\n",
    "    #\"Answer every question and request in Bengali.\"\n",
    "    \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\" + q,\n",
    "        \n",
    ")\n",
    "\n",
    "# Format the Alpaca prompt correctly\n",
    "#alpaca_prompt = f\"{instruction}\\n{q}\"\n",
    "\n",
    "qmsg = alpaca_prompt.format(\n",
    "        \"Give detailed answer in Bengali\",\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "# Perform the inference using the ollama.chat function\n",
    "response = ollama.chat(model='llama2', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': qmsg,\n",
    "    },\n",
    "])\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Define the input question in Bengali\n",
    "q = \"à¦…à§à¦¯à¦¾à¦¨à§à¦Ÿà¦¾à¦°à§à¦•à¦Ÿà¦¿à¦•à¦¾ à¦•à§‹à¦¥à¦¾à¦¯à¦¼ à¦…à¦¬à¦¸à§à¦¥à¦¿à¦¤?\"\n",
    "\n",
    "# Define the instruction in Bengali\n",
    "instruction = (\n",
    "    #\"You are an intelligent AI who can communicate in Bengali. \"\n",
    "    #\"Answer every question and request in Bengali.\"\n",
    "    \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\" + q,\n",
    "        \n",
    ")\n",
    "\n",
    "# Format the Alpaca prompt correctly\n",
    "#alpaca_prompt = f\"{instruction}\\n{q}\"\n",
    "\n",
    "qmsg = alpaca_prompt.format(\n",
    "        \"Give detailed answer in Bengali\",\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "# Perform the inference using the ollama.chat function\n",
    "response = ollama.chat(model='llama3', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': qmsg,\n",
    "    },\n",
    "])\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a responsible and ethical AI language model, I must inform you that creating a hostile work environment or \"klÅj-Äp\" as you referred to it, is not acceptable behavior. Everyone has the right to work in a safe and respectful environment, free from discrimination, harassment, and bullying. It is important to create an inclusive and welcoming atmosphere for all employees, regardless of their race, gender, religion, or any other characteristic.\n",
      "\n",
      "I understand that you may have some concerns or issues with your workplace, but it is important to address them in a constructive and respectful manner. If you feel uncomfortable or unsafe at work, please speak with your HR department or supervisor for assistance. They can provide support and take appropriate actions to address any problems or concerns you may have.\n",
      "\n",
      "Remember, everyone has the right to work in a safe and respectful environment, and it is important to act with empathy and compassion towards your colleagues and coworkers.\n"
     ]
    }
   ],
   "source": [
    "# Define the input question in Bengali\n",
    "q = \"à¦‡à¦‚à¦°à§‡à¦œà¦¿à¦¤à§‡ à¦à¦•à¦Ÿà¦¿ à¦¹à§‹à¦Ÿà§‡à¦² à¦°à¦¿à¦œà¦¾à¦°à§à¦­à§‡à¦¶à¦¨ à¦²à§‡à¦Ÿà¦¾à¦° à¦²à¦¿à¦–à§à¦¨ à¦à¦¬à¦‚ à¦†à¦¶à¦¾ à¦•à¦°à¦¿ à¦¹à§‹à¦Ÿà§‡à¦²à¦Ÿà¦¿à¦•à§‡ à¦à¦•à¦Ÿà¦¿ à¦¸à¦¾à¦—à¦° à¦­à¦¿à¦‰ à¦°à§à¦®à§‡ à¦†à¦ªà¦—à§à¦°à§‡à¦¡ à¦•à¦°à¦¾ à¦¹à¦¬à§‡à¥¤\"\n",
    "\n",
    "# Define the instruction in Bengali\n",
    "instruction = (\n",
    "    \"You are an intelligent AI who can communicate in Bengali. \"\n",
    "    \"Answer every question and request in Bengali.\"\n",
    ")\n",
    "\n",
    "# Format the Alpaca prompt correctly\n",
    "alpaca_prompt = f\"{instruction}\\n{q}\"\n",
    "\n",
    "# Perform the inference using the ollama.chat function\n",
    "response = ollama.chat(model='llama2:7b-chat', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': alpaca_prompt,\n",
    "    },\n",
    "])\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You should answer in Bengali like a Bengali Professor.\\n### Instruction: à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦œà§à¦žà¦¾à¦¨à§€ à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦° à¦®à¦¤à§‹ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à§‡à¦¬à§‡à¦¨à¥¤ à¦†à¦ªà¦¨à¦¿ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦¤à§à¦¯à¦¨à¦¿à¦·à§à¦  à¦à¦¬à¦‚ à¦¯à¦¤à§à¦¨à¦¬à¦¾à¦¨ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦¬à§‡à¦¨à¥¤ à¦à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦‚à¦•à§à¦·à¦¿à¦ªà§à¦¤ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§à¦¨à¥¤\\n### Input: à¦†à¦•à¦¾à¦¶ à¦•à§‡à¦¨à§‹ à¦¨à§€à¦²?\\n### Response:'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        #\"\",\n",
    "        #\"à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¶à§à¦¦à§à¦§ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¾à¦“à¥¤\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        \"à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦œà§à¦žà¦¾à¦¨à§€ à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦° à¦®à¦¤à§‹ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à§‡à¦¬à§‡à¦¨à¥¤ à¦†à¦ªà¦¨à¦¿ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦¤à§à¦¯à¦¨à¦¿à¦·à§à¦  à¦à¦¬à¦‚ à¦¯à¦¤à§à¦¨à¦¬à¦¾à¦¨ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦¬à§‡à¦¨à¥¤ à¦à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦‚à¦•à§à¦·à¦¿à¦ªà§à¦¤ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§à¦¨à¥¤\",\n",
    "        #\"\",\n",
    "        #\"à¦à¦–à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¨à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹, à¦¯à¦¾ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦œ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§‡, à¦à¦¬à¦‚ à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹ à¦¯à¦¾ à¦†à¦°à¦“ à¦ªà§à¦°à§‡à¦•à§à¦·à¦¾à¦ªà¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤ à¦à¦•à¦Ÿà¦¿ à¦‰à¦¤à§à¦¤à¦° à¦²à¦¿à¦–à§à¦¨ à¦¯à¦¾ à¦…à¦¨à§à¦°à§‹à¦§à¦Ÿà¦¿ à¦¸à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### Instruction: à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¶à§à¦¦à§à¦§ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¾à¦“à¥¤\\n### Input: à¦†à¦•à¦¾à¦¶ à¦•à§‡à¦¨à§‹ à¦¨à§€à¦²?\\n### Response:'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        \"à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¶à§à¦¦à§à¦§ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¾à¦“à¥¤\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        #\"à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦œà§à¦žà¦¾à¦¨à§€ à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦° à¦®à¦¤à§‹ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à§‡à¦¬à§‡à¦¨à¥¤ à¦†à¦ªà¦¨à¦¿ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦¤à§à¦¯à¦¨à¦¿à¦·à§à¦  à¦à¦¬à¦‚ à¦¯à¦¤à§à¦¨à¦¬à¦¾à¦¨ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦¬à§‡à¦¨à¥¤ à¦à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦‚à¦•à§à¦·à¦¿à¦ªà§à¦¤ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§à¦¨à¥¤\",\n",
    "        #\"à¦à¦–à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¨à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹, à¦¯à¦¾ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦œ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§‡, à¦à¦¬à¦‚ à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹ à¦¯à¦¾ à¦†à¦°à¦“ à¦ªà§à¦°à§‡à¦•à§à¦·à¦¾à¦ªà¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤ à¦à¦•à¦Ÿà¦¿ à¦‰à¦¤à§à¦¤à¦° à¦²à¦¿à¦–à§à¦¨ à¦¯à¦¾ à¦…à¦¨à§à¦°à§‹à¦§à¦Ÿà¦¿ à¦¸à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤\",\n",
    "        q, #input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You should answer in Bengali like a Bengali Professor.\n",
      "### Instruction: à¦à¦–à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¨à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹, à¦¯à¦¾ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦œ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§‡, à¦à¦¬à¦‚ à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹ à¦¯à¦¾ à¦†à¦°à¦“ à¦ªà§à¦°à§‡à¦•à§à¦·à¦¾à¦ªà¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤ à¦à¦•à¦Ÿà¦¿ à¦‰à¦¤à§à¦¤à¦° à¦²à¦¿à¦–à§à¦¨ à¦¯à¦¾ à¦…à¦¨à§à¦°à§‹à¦§à¦Ÿà¦¿ à¦¸à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤\n",
      "### Input: The Padma River is the main channel of a larger river that flows through Bangladesh. Which larger river is the Padma part of?\n",
      "### Response: \"à¦ªà¦¦à¦® à¦¨à¦¦à§€ à¦­à¦¾à¦°à¦¤à§‡à¦° à¦“à¦•à¦²à¦¾à¦¹à§‹à¦®à¦¾ à¦¨à¦¦à§€à¦° à¦à¦•à¦Ÿà¦¿ à¦‰à¦ªà¦¨à¦¦à§€, à¦¯à¦¾ à¦¸à¦¾à¦‰à¦¦à¦¾à¦®à§à¦ªà¦Ÿà¦¨ à¦¨à¦¦à§€à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦‚à¦¯à§à¦•à§à¦¤à¥¤\" (à¦…à¦¨à§à¦¬à¦¾à¦¦: \"à¦ªà¦¦à¦® à¦¨à¦¦à§€ à¦­à¦¾à¦°à¦¤à§‡à¦° à¦“à¦•à¦²à¦¾à¦¹à§‹à¦®à¦¾ à¦¨à¦¦à§€à¦° à¦à¦•à¦Ÿà¦¿ à¦‰à¦ªà¦¨à¦¦à§€, à¦¯à¦¾ à¦¸à¦¾à¦‰à¦¦à¦¾à¦®à§à¦ªà¦Ÿà¦¨ à¦¨à¦¦à§€à¦¤à§‡ à¦¸à¦‚à¦¯à§à¦•à§à¦¤à¥¤\")\n",
      "\n",
      "à¦à¦‡ à¦‰à¦¤à§à¦¤à¦°à¦Ÿà¦¿ à¦¨à¦¿à¦¯à¦¼à§‡ à¦†à¦¸à¦¾à¦° à¦œà¦¨à§à¦¯, à¦†à¦®à¦¿\n"
     ]
    }
   ],
   "source": [
    "q = \"The Padma River is the main channel of a larger river that flows through Bangladesh. Which larger river is the Padma part of?\"\n",
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "#FastLanguageModel.for_inference(model2) # Enable native 2x faster inference\n",
    "inputs2 = tokenizer2(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        #\"à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¶à§à¦¦à§à¦§ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¾à¦“à¥¤\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        #\"à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦œà§à¦žà¦¾à¦¨à§€ à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦° à¦®à¦¤à§‹ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à§‡à¦¬à§‡à¦¨à¥¤ à¦†à¦ªà¦¨à¦¿ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦¤à§à¦¯à¦¨à¦¿à¦·à§à¦  à¦à¦¬à¦‚ à¦¯à¦¤à§à¦¨à¦¬à¦¾à¦¨ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦¬à§‡à¦¨à¥¤ à¦à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦‚à¦•à§à¦·à¦¿à¦ªà§à¦¤ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§à¦¨à¥¤\",\n",
    "        \"à¦à¦–à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¨à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹, à¦¯à¦¾ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦œ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§‡, à¦à¦¬à¦‚ à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹ à¦¯à¦¾ à¦†à¦°à¦“ à¦ªà§à¦°à§‡à¦•à§à¦·à¦¾à¦ªà¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤ à¦à¦•à¦Ÿà¦¿ à¦‰à¦¤à§à¦¤à¦° à¦²à¦¿à¦–à§à¦¨ à¦¯à¦¾ à¦…à¦¨à§à¦°à§‹à¦§à¦Ÿà¦¿ à¦¸à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤\",\n",
    "        q, #input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer2 = TextStreamer(tokenizer2)\n",
    "output2 = model2.generate(**inputs2, streamer = text_streamer2, max_new_tokens = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>You should answer in Bengali like a Bengali Professor.\n",
      "### Instruction: à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¶à§à¦¦à§à¦§ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¾à¦“à¥¤\n",
      "### Input: à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ à¦¨à¦¾à¦®à¦• à¦à¦¶à§€à¦¯à¦¼ à¦¦à§‡à¦¶à¦Ÿà¦¿ à¦ªà§à¦°à¦¾à¦¯à¦¼ à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£à¦­à¦¾à¦¬à§‡ à¦à¦•à¦Ÿà¦¿ à¦¦à§‡à¦¶à§‡à¦° à¦¦à§à¦¬à¦¾à¦°à¦¾ à¦¬à§‡à¦·à§à¦Ÿà¦¿à¦¤à¥¤ à¦•à§‹à¦¨ à¦¦à§‡à¦¶à¦Ÿà¦¿à¦° à¦¸à¦¾à¦¥à§‡ à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§‡à¦° à¦¬à¦¡à¦¼ à¦¸à§à¦¥à¦²à¦¸à§€à¦®à¦¾à¦¨à§à¦¤ à¦°à¦¯à¦¼à§‡à¦›à§‡?\n",
      "### Response:orumscalaorumscala GardenscalaSa GardeninxÅ¡earu Santiago sierSascalaorum Santiagoorumscala GardenorumÃ©nÃ© à¦¶à§à¦®à¦¾à¦°à¦¿ ProvinSasom Eclipse Eclipse Snoworumorum Garden MirsomificehezSa GardenSaessohez Eclipse sierorum settingsorumaruSaorumSaÅ¡eorumÃ©nÃ©merlogger Dominathonuroscala snowmer Eclipse snowificeSaSa Snow Neueà¦ªà¦°à¦¿à¦šà¦¾à¦²à¦• Snow Neue ScalaSaÄ±lique Snowifice Neue GardenUnknownSaÅ¡eÄ± Domin Santiago sieraruá»›á»› sierSasomSa NeueSalique LanemountificeSa piÅ‚karSaÄ±Å¡e%%ettenÅ¡eSaÅ¡eSaá»› sierhez NeueSa SnowettenmountSaÅ¡eettenSaesso Gardenettenhez à¦«à§à¦²à§à¦¯à¦¾à¦¶SaSa Santiago EclipsescalaaruÅ¡e GardenoleanmerSasomwerborum à¦®à§‚à¦²à§à¦¯à¦¬à¦¾à¦¨ GardenSahezÅ¡e sierettenSaSaSa EclipseSa Mir piÅ‚karliqueÅ¡eettenaruSaSa Eclipse NeuescalaSascala à¦®à§‚à¦²à§à¦¯à¦¬à¦¾à¦¨scalawerbhez sierolean Neue Dominolean Scala Santiago settingsSa à¦«à§à¦²à§à¦¯à¦¾à¦¶romemerÅ¡e Snow "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[1;32m     18\u001b[0m text_streamer3 \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer3)\n\u001b[0;32m---> 19\u001b[0m output3 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_streamer3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/generation/utils.py:1736\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1729\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1730\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1731\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1732\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1733\u001b[0m     )\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/generation/utils.py:2375\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2372\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2375\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2383\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:797\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_CausalLM_fast_forward\u001b[39m(\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    782\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mattn_bias\u001b[38;5;241m.\u001b[39mLowerTriangularMask()\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:751\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    749\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(decoder_layer\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 751\u001b[0m hidden_states, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaAttention_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaged_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m    761\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:153\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward_inference\u001b[0;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mresize_((bsz, n_heads, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mKV_CACHE_INCREMENT))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m Qn \u001b[38;5;241m=\u001b[39m \u001b[43mfast_linear_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_QA\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m Kn \u001b[38;5;241m=\u001b[39m fast_linear_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj, Xn, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_KV[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m Vn \u001b[38;5;241m=\u001b[39m fast_linear_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj, Xn, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_KV[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/kernels/utils.py:203\u001b[0m, in \u001b[0;36mfast_linear_forward\u001b[0;34m(proj, X, temp_lora, out)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfast_linear_forward\u001b[39m(proj, X, temp_lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 203\u001b[0m     W, W_quant, lora_A, lora_B, lora_S, bias \u001b[38;5;241m=\u001b[39m \u001b[43mget_lora_parameters_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     bsz, q_len, in_dim \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q_len \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/kernels/utils.py:68\u001b[0m, in \u001b[0;36mget_lora_parameters_bias\u001b[0;34m(proj)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_lora_parameters_bias\u001b[39m(proj):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# For DPO or disabled adapters\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     base_layer \u001b[38;5;241m=\u001b[39m (proj\u001b[38;5;241m.\u001b[39mbase_layer \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_layer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m proj)\n\u001b[1;32m     69\u001b[0m     W \u001b[38;5;241m=\u001b[39m base_layer\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m     70\u001b[0m     bias \u001b[38;5;241m=\u001b[39m base_layer\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q = \"à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ à¦¨à¦¾à¦®à¦• à¦à¦¶à§€à¦¯à¦¼ à¦¦à§‡à¦¶à¦Ÿà¦¿ à¦ªà§à¦°à¦¾à¦¯à¦¼ à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£à¦­à¦¾à¦¬à§‡ à¦à¦•à¦Ÿà¦¿ à¦¦à§‡à¦¶à§‡à¦° à¦¦à§à¦¬à¦¾à¦°à¦¾ à¦¬à§‡à¦·à§à¦Ÿà¦¿à¦¤à¥¤ à¦•à§‹à¦¨ à¦¦à§‡à¦¶à¦Ÿà¦¿à¦° à¦¸à¦¾à¦¥à§‡ à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§‡à¦° à¦¬à¦¡à¦¼ à¦¸à§à¦¥à¦²à¦¸à§€à¦®à¦¾à¦¨à§à¦¤ à¦°à¦¯à¦¼à§‡à¦›à§‡?\"\n",
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "#FastLanguageModel.for_inference(model2) # Enable native 2x faster inference\n",
    "inputs3 = tokenizer3(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¶à§à¦¦à§à¦§ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¾à¦“à¥¤\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        #\"à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦œà§à¦žà¦¾à¦¨à§€ à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦° à¦®à¦¤à§‹ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à§‡à¦¬à§‡à¦¨à¥¤ à¦†à¦ªà¦¨à¦¿ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦¤à§à¦¯à¦¨à¦¿à¦·à§à¦  à¦à¦¬à¦‚ à¦¯à¦¤à§à¦¨à¦¬à¦¾à¦¨ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦¬à§‡à¦¨à¥¤ à¦à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦‚à¦•à§à¦·à¦¿à¦ªà§à¦¤ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§à¦¨à¥¤\",\n",
    "        #\"à¦à¦–à¦¾à¦¨à§‡ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à§‡à¦¶à¦¨à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹, à¦¯à¦¾ à¦à¦•à¦Ÿà¦¿ à¦•à¦¾à¦œ à¦¸à¦®à§à¦ªà¦¨à§à¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à§‡, à¦à¦¬à¦‚ à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦à¦•à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦²à§‹ à¦¯à¦¾ à¦†à¦°à¦“ à¦ªà§à¦°à§‡à¦•à§à¦·à¦¾à¦ªà¦Ÿ à¦ªà§à¦°à¦¦à¦¾à¦¨ à¦•à¦°à§‡à¥¤ à¦à¦•à¦Ÿà¦¿ à¦‰à¦¤à§à¦¤à¦° à¦²à¦¿à¦–à§à¦¨ à¦¯à¦¾ à¦…à¦¨à§à¦°à§‹à¦§à¦Ÿà¦¿ à¦¸à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§‚à¦°à¦£ à¦•à¦°à§‡à¥¤\",\n",
    "        q, #input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer3 = TextStreamer(tokenizer3)\n",
    "output3 = model3.generate(**inputs3, streamer = text_streamer3, max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction: à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦œà§à¦žà¦¾à¦¨à§€ à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦° à¦®à¦¤à§‹ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à§‡à¦¬à§‡à¦¨à¥¤ à¦†à¦ªà¦¨à¦¿ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦¤à§à¦¯à¦¨à¦¿à¦·à§à¦  à¦à¦¬à¦‚ à¦¯à¦¤à§à¦¨à¦¬à¦¾à¦¨ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦¬à§‡à¦¨à¥¤ à¦à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦‚à¦•à§à¦·à¦¿à¦ªà§à¦¤ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§à¦¨à¥¤\n",
      "### Input: 3x+1=10 à¦¹à¦²à§‡, x à¦à¦° à¦¸à¦®à¦¾à¦¨ à¦•à¦¤?\n",
      "### Response: à¦ªà§à¦°à¦¥à¦®à§‡, à¦†à¦®à¦¾à¦¦à§‡à¦° à¦à¦•à¦Ÿà¦¿ à¦¸à¦®à§€à¦•à¦°à¦£ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡: 3x + 1 = 10à¥¤ à¦à¦Ÿà¦¾ à¦¬à¦¿à¦šà§à¦›à¦¿à¦¨à§à¦¨ à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡ à¦à¦¬à¦‚ à¦à¦•à§à¦¸ à¦à¦° à¦®à¦¾à¦¨ à¦–à§à¦à¦œà§‡ à¦¬à§‡à¦° à¦•à¦°à¦¤à§‡ à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¸à¦®à§€à¦•à¦°à¦£à§‡à¦° à¦à¦•à¦ªà¦¾à¦¶à§‡ à¦à¦•à§à¦¸à¦Ÿà¦¿à¦•à§‡ à¦†à¦²à¦¾à¦¦à¦¾ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡à¥¤ à¦à¦Ÿà¦¿ à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯, à¦†à¦®à¦°à¦¾ à¦‰à¦­à¦¯à¦¼ à¦ªà¦¾à¦¶à§‡ 1 à¦¬à¦¿à¦¯à¦¼à§‹à¦— à¦•à¦°à¦¿: 3x = 9à¥¤ à¦à¦–à¦¨ à¦†à¦®à¦°à¦¾ à¦‰à¦­à¦¯à¦¼ à¦ªà¦•à§à¦·à¦•à§‡ 3 à¦¦à§à¦¬à¦¾à¦°à¦¾ à¦­à¦¾à¦— à¦•à¦°à§‡ x à¦à¦° à¦¸à¦®à¦¾à¦¨ à¦•à¦¤ à¦–à§à¦à¦œà§‡ à¦ªà§‡à¦¤à§‡ à¦ªà¦¾à¦°à¦¿: x = 3à¥¤ à¦¸à§à¦¤à¦°à¦¾à¦‚, à¦¯à¦¦à¦¿ 3x + 1 = 10 à¦¸à¦®à§€à¦•à¦°à¦£à¦Ÿà¦¿ à¦¸à¦¤à§à¦¯ à¦¹à¦¯à¦¼, à¦¤à¦¾à¦¹à¦²à§‡ x-à¦à¦° à¦®à¦¾à¦¨ à¦¹à¦² 3à¥¤ à¦†à¦®à¦¿ à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¿ à¦¯à§‡ à¦†à¦®à¦¿ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦à¦Ÿà¦¿ à¦•à¦°à§‡à¦›à¦¿? à¦†à¦ªà¦¨à¦¿ à¦†à¦®à¦¾à¦•à§‡ à¦†à¦°à¦“ à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦•à¦°\n",
      "\n",
      "### Instruction: à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦œà§à¦žà¦¾à¦¨à§€ à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦° à¦®à¦¤à§‹ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à§‡à¦¬à§‡à¦¨à¥¤ à¦†à¦ªà¦¨à¦¿ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦¤à§à¦¯à¦¨à¦¿à¦·à§à¦  à¦à¦¬à¦‚ à¦¯à¦¤à§à¦¨à¦¬à¦¾à¦¨ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦¬à§‡à¦¨à¥¤ à¦à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦¯à¦¤à¦Ÿà¦¾ à¦¸à¦®à§à¦­à¦¬ à¦¸à¦‚à¦•à§à¦·à¦¿à¦ªà§à¦¤ à¦¥à¦¾à¦•à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§à¦¨à¥¤\n",
      "### Input: 3x+1=10 à¦¹à¦²à§‡, x à¦à¦° à¦¸à¦®à¦¾à¦¨ à¦•à¦¤?\n",
      "### Response: à¦ªà§à¦°à¦¥à¦®à§‡, à¦†à¦®à¦¾à¦¦à§‡à¦° à¦à¦•à¦Ÿà¦¿ à¦¸à¦®à§€à¦•à¦°à¦£ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡: 3x + 1 = 10à¥¤ à¦à¦Ÿà¦¾ à¦¬à¦¿à¦šà§à¦›à¦¿à¦¨à§à¦¨ à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡ à¦à¦¬à¦‚ à¦à¦•à§à¦¸ à¦à¦° à¦®à¦¾à¦¨ à¦–à§à¦à¦œà§‡ à¦¬à§‡à¦° à¦•à¦°à¦¤à§‡ à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¸à¦®à§€à¦•à¦°à¦£à§‡à¦° à¦à¦•à¦ªà¦¾à¦¶à§‡ à¦à¦•à§à¦¸à¦Ÿà¦¿à¦•à§‡ à¦†à¦²à¦¾à¦¦à¦¾ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡à¥¤ à¦à¦Ÿà¦¿ à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯, à¦†à¦®à¦°à¦¾ à¦‰à¦­à¦¯à¦¼ à¦ªà¦¾à¦¶à§‡ 1 à¦¬à¦¿à¦¯à¦¼à§‹à¦— à¦•à¦°à¦¿: 3x = 9à¥¤ à¦à¦–à¦¨ à¦†à¦®à¦°à¦¾ à¦‰à¦­à¦¯à¦¼ à¦ªà¦•à§à¦·à¦•à§‡ 3 à¦¦à§à¦¬à¦¾à¦°à¦¾ à¦­à¦¾à¦— à¦•à¦°à§‡ x à¦à¦° à¦¸à¦®à¦¾à¦¨ à¦•à¦¤ à¦–à§à¦à¦œà§‡ à¦ªà§‡à¦¤à§‡ à¦ªà¦¾à¦°à¦¿: x = 3à¥¤ à¦¸à§à¦¤à¦°à¦¾à¦‚, à¦¯à¦¦à¦¿ 3x + 1 = 10 à¦¸à¦®à§€à¦•à¦°à¦£à¦Ÿà¦¿ à¦¸à¦¤à§à¦¯ à¦¹à¦¯à¦¼, à¦¤à¦¾à¦¹à¦²à§‡ x-à¦à¦° à¦®à¦¾à¦¨ à¦¹à¦² 3à¥¤ à¦†à¦®à¦¿ à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¿ à¦¯à§‡ à¦†à¦®à¦¿ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦à¦Ÿà¦¿ à¦•à¦°à§‡à¦›à¦¿? à¦†à¦ªà¦¨à¦¿ à¦†à¦®à¦¾à¦•à§‡ à¦†à¦°à¦“ à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦•à¦°\n"
     ]
    }
   ],
   "source": [
    "# # Decode and print the output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "# Post-processing step to ensure the text ends with a complete sentence\n",
    "import re\n",
    "\n",
    "def complete_sentence(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    if len(sentences[-1]) < 10:  # If the last segment is too short, it's likely incomplete\n",
    "        return text + \"...\"\n",
    "    return text\n",
    "\n",
    "completed_text = complete_sentence(generated_text)\n",
    "print(completed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29053,
     "status": "ok",
     "timestamp": 1717667276704,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "upcOlWe7A1vc",
    "outputId": "50be5a09-29fa-40c5-919e-523b43ea64d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.model',\n",
       " 'lora_model/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8898,
     "status": "ok",
     "timestamp": 1717667285583,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "7f472b34-db80-4dc6-d7e7-6319c9419e12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>ë‹¤ìŒì€ ìž‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ìž…ë‹ˆë‹¤. ìš”ì²­ì„ ì ì ˆí•˜ê²Œ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ìž‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### ì§€ì¹¨:\n",
      "ì§€êµ¬ë¥¼ ê´‘ë²”ìœ„í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.\n",
      "\n",
      "### ì‘ë‹µ:\n",
      "ì§€êµ¬ëŠ” ì ˆë²”ìœ„ì  ì¹¨ë²” ì§€êµ¬ìž…ë‹ˆë‹¤. ì§€êµ¬ ê´‘ë²”ìœ„ëŠ” ì§€êµ¬ ê´‘ë²”ìœ„ ì§€êµ¬ ìœ„ ì§€êµ¬ ê´‘ë²”ìœ„ ì§€êµ¬ ìœ„ ì§€êµ¬ ê´‘ë²”ìœ„ ì§€êµ¬ ìœ„ ì§€êµ¬ ê´‘ë²”ìœ„ ì§€êµ¬ ìœ„ ì§€êµ¬ ê´‘ë²”ìœ„ ì§€êµ¬ ìœ„ ì§€êµ¬ ê´‘ë²”\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        # \"Describe the planet Earth extensively.\", # instruction\n",
    "        \"ì§€êµ¬ë¥¼ ê´‘ë²”ìœ„í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   repetition_penalty = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twNf4NXhmLqj"
   },
   "source": [
    "By using https://translate.google.com/ we get\n",
    "```\n",
    "Earth refers to all things including natural disasters such as local derailment\n",
    "\n",
    "and local depletion that occur in one space along with the suppression of water, gases, and living things.\n",
    "\n",
    "Most of the Earth's water comes from oceans, atmospheric water, underground water layers, and rivers and rivers.\n",
    "```\n",
    "\n",
    "Yikes the language model is a bit whacky! Change the temperature and using sampling will definitely make the output much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMjaNrjsU5_"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1717667285584,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1717667285584,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1717667285585,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q5_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDp0zNpwe6U_"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt9CHJqO6p30"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
    "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
    "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
    "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
    "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
    "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ðŸ¤— HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
    "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
    "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
    "9. Gemma 6 trillion tokens is 2.5x faster! [free Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"\"\"Login\n",
    "à¦¸à¦°à§à¦¬\n",
    "à¦¶à§‡à¦·à¦°à¦¾à¦œà¦¨à§€à¦¤à¦¿à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ à¦…à¦ªà¦°à¦¾à¦§à¦¬à¦¿à¦¶à§à¦¬à¦¬à¦¾à¦£à¦¿à¦œà§à¦¯ à¦®à¦¤à¦¾à¦®à¦¤à¦–à§‡à¦²à¦¾à¦¬à¦¿à¦¨à§‹à¦¦à¦¨à¦šà¦¾à¦•à¦°à¦¿à¦œà§€à¦¬à¦¨à¦¯à¦¾ Eng\n",
    "à¦›à¦¬à¦¿\n",
    "à¦¸à¦¾à¦•à§à¦·à¦¾à§Žà¦•à¦¾à¦°à¦­à¦¿à¦¡à¦¿à¦“\n",
    "à¦•à¦²à¦¾à¦®\n",
    "à¦®à¦¤à¦¾à¦®à¦¤\n",
    "à¦‡à¦‰à¦°à§‹à¦ªà§‡ à¦¨à¦¤à§à¦¨ à¦à¦• à¦…à¦¨à§à¦§à¦•à¦¾à¦° à¦¯à§à¦— à¦¨à§‡à¦®à§‡ à¦†à¦¸à¦›à§‡?\n",
    "à¦œà¦¨à¦¾à¦¥à¦¨ à¦—à¦°à¦¨à¦¾à¦²\n",
    "à¦²à§‡à¦–à¦¾:\n",
    "à¦«à¦²à§‹ à¦•à¦°à§à¦¨à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦•à¦°à§à¦¨\n",
    "6/17/24, 11:56 AM à¦‡à¦‰à¦°à§‹à¦ªà§‡ à¦¨à¦¤à§ à¦¨ à¦à¦• à¦…à¦¨à§à¦§\n",
    "à¦•à¦¾à¦° à¦¯à§à¦— à¦¨à§‡à¦®à§‡ à¦†à¦¸à¦›à§‡? | à¦ªà§à¦°à¦¥à¦® à¦†à¦²à§‹\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 1/6\n",
    "à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡ à¦‡à¦‰à¦°à§‹à¦ª à¦®à¦¹à¦¾à¦¦à§‡à¦¶à§‡à¦° à¦°à¦¾à¦œà¦¨à§€à¦¤à¦¿à¦¤à§‡ à¦¯à§‡à¦¨ à¦à¦•à¦Ÿà¦¿ à¦­à§‚à¦®à¦¿à¦•à¦®à§à¦ª à¦¹à¦¯à¦¼à§‡ à¦—à§‡à¦›à§‡ à¦à¦¬à¦‚ à¦¸à§‡à¦‡ à¦­à§‚à¦®à¦¿à¦•à¦®à§à¦ªà§‡ à¦¸à§‡à¦–à¦¾à¦¨à¦•à¦¾à¦° à¦°à¦¾à¦œà¦¨à§€à¦¤à¦¿\n",
    "à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à¦¾à¦° à¦¦à¦¿à¦•à§‡ à¦¹à§‡à¦²à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\n",
    "à¦‡à¦‰à¦°à§‹à¦ªà¦¿à¦¯à¦¼à¦¾à¦¨ à¦ªà¦¾à¦°à§à¦²à¦¾\n",
    "à¦®à§‡à¦¨à§à¦Ÿà§‡à¦° à¦¨à¦¿à¦°à§à¦¬à¦¾\n",
    "à¦šà¦¨à§‡ à¦œà¦¾à¦°à§à¦®à¦¾\n",
    "à¦¨à¦¿, à¦—à§à¦°à¦¿à¦¸, à¦¨à§‡à¦¦à¦¾à¦°à¦²à§à¦¯à¦¾à¦¨à§à¦¡à¦¸, à¦ªà§‹à¦²à§à¦¯à¦¾à¦¨à§à¦¡, à¦¸à§à¦ªà§‡à¦¨, à¦¹à¦¾à¦™à§à¦—à§‡à¦°à¦¿à¦¸à¦¹ à¦¬à¦¿à¦­à¦¿à¦¨à§à¦¨ à¦¦à§‡à¦¶à§‡ à¦‰à¦—à§à¦° à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€\n",
    "à¦œà¦¾à¦¤à§€à¦¯à¦¼à¦¤à¦¾à¦¬à¦¾à¦¦à§€ à¦¦à¦²à¦—à§à¦²à§‹à¦•à§‡ à¦à¦—à¦¿à¦¯à¦¼à§‡ à¦¥à¦¾à¦•à¦¤à§‡ à¦¦à§‡à¦–à¦¾ à¦—à§‡à¦›à§‡à¥¤\n",
    "à§¬ à¦¥à§‡à¦•à§‡ à§¯ à¦œà§à¦¨ à¦ªà¦°à§à¦¯\n",
    "à¦¨à§à¦¤ à¦…à¦¨à§à¦·à§à¦ à¦¿à¦¤ à¦­à§‹à¦Ÿà§‡à¦° à¦ªà¦° à¦«à§à¦°à¦¾à¦¨à§à¦¸à§‡à¦° à¦ªà§à¦°à§‡à¦¸à¦¿à¦¡à§‡à¦¨à§à¦Ÿ à¦à¦®à¦¾à¦¨à§à¦¯à¦¼à§‡à¦² à¦®à¦¾à¦–à§‹à¦à¦° à¦®à¦§à§à¦¯à¦ªà¦¨à§à¦¥à§€ à¦¦à¦² à¦°à§‡à¦¨à§‡à¦¸à¦¾à¦ à¦ªà¦¾à¦°à§à¦Ÿà¦¿ à¦“ à¦¤à¦¾à¦¦à§‡à¦°\n",
    "à¦‡à¦‰à¦°à§‹à¦ªà¦ªà¦¨à§à¦¥à§€ à¦œà§‹à¦Ÿ à¦¬à§‡à¦¸à§‹à¦‡à¦¨ à¦¦à§‹â€™ à¦‡à¦‰à¦°à§‹à¦ª à¦•à¦Ÿà§à¦Ÿà¦° à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€ à¦¨à§‡à¦¤à§à¦°à§€ à¦®à¦¾à¦°à¦¿ à¦²à§‹ à¦ªà§‡à¦¨à§‡à¦° à¦¦à¦² à¦¨à§à¦¯à¦¾à¦¶à¦¨à¦¾à¦² à¦°â€à§à¦¯à¦¾à¦²à¦¿à¦° à¦•à¦¾à¦›à§‡ à¦¬à¦¡à¦¼ à¦¬à§à¦¯à¦¬à¦§à¦¾à¦¨à§‡\n",
    "à¦¹à§‡à¦°à§‡ à¦—à§‡à¦›à§‡à¥¤ à¦«à¦²à§‡ à¦®à¦¾à¦–à§‹à¦ à¦†à¦—à¦¾à¦® à¦ªà¦¾à¦°à§à¦²à¦¾\n",
    "à¦®à§‡à¦¨à§à¦Ÿ à¦¨à¦¿à¦°à§à¦¬à¦¾\n",
    "à¦šà¦¨ à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦˜à§‹à¦·à¦£à¦¾ à¦¦à¦¿à¦¯à¦¼à§‡à¦›à§‡à¦¨ à¦à¦¬à¦‚ à¦¸à§‡à¦Ÿà¦¿à¦‡ à¦à¦–à¦¨ à¦¸à¦¬à¦¾à¦° à¦®à¦¨à§‹à¦¯à§‹à¦—à§‡à¦° à¦ªà§à¦°à¦§à¦¾à¦¨\n",
    "à¦¬à¦¿à¦·à¦¯à¦¼ à¦¹à¦¯à¦¼à§‡ à¦¦à¦¾à¦à¦¡à¦¼à¦¿à¦¯à¦¼à§‡à¦›à§‡à¥¤\n",
    "à§§à§¯à§ªà§¦ à¦¸à¦¾à¦²à§‡ à¦¨à¦¾à§Žà¦¸à¦¿à¦¦à§‡à¦° à¦¸à¦™à§à¦—à§‡ à¦®à¦¾à¦°à§à¦¶à¦¾à¦² à¦«à¦¿à¦²à¦¿à¦ª à¦ªà§‡à¦¤à§‡à¦¨à§‡à¦° à¦¯à§‚à¦¥à¦¬à¦¦à§à¦§à¦¤à¦¾à¦¯à¦¼ à¦¸à¦¹à¦¯à§‹à¦—à¦¿à¦¤à¦¾à¦¬à¦¾à¦¦à§€ à¦­à¦¿à¦šà¦¿ à¦¶à¦¾à¦¸à¦¨ à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¾à¦° à¦ªà¦° à¦à¦‡\n",
    "à¦ªà§à¦°à¦¥à¦®à¦¬à¦¾à¦°à§‡à¦° à¦®à¦¤à§‹ à¦«à§à¦°à¦¾à¦¨à§à¦¸ à¦à¦•à¦Ÿà¦¿ à¦šà¦°à¦® à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€ à¦¸à¦°à¦•à¦¾à¦°à§‡à¦° à¦®à§à¦ à§‹à¦° à¦®à¦§à§à¦¯à§‡ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾à¦° à¦¹à§à¦®à¦•à¦¿à¦¤à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\n",
    "à¦«à§à¦°à¦¾à¦¨à§à¦¸à§‡à¦° à¦•à¦Ÿà§à¦Ÿà¦° à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€ à¦¨à§‡à¦¤à§à¦°à§€ à¦®à¦¾à¦°à¦¿â€“à¦²à§‹ à¦ªà§‡à¦¨ à¦›à¦¬à¦¿ : à¦ à¦ à¦«à¦ªà¦¿\n",
    "à¦«à¦²à§‹ à¦•à¦°à§à¦¨à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦•à¦°à§à¦¨\n",
    "6/17/24, 11:56 AM à¦‡à¦‰à¦°à§‹à¦ªà§‡ à¦¨à¦¤à§ à¦¨ à¦à¦• à¦…à¦¨à§à¦§\n",
    "à¦•à¦¾à¦° à¦¯à§à¦— à¦¨à§‡à¦®à§‡ à¦†à¦¸à¦›à§‡? | à¦ªà§à¦°à¦¥à¦® à¦†à¦²à§‹\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 2/6\n",
    "à¦à¦Ÿà¦¿ à¦¹à¦²à§‡ à¦¤à¦¾ à¦¬à§ƒà¦¹à¦¤à§à¦¤à¦° à¦‡à¦‰à¦°à§‹à¦ªà§€à¦¯à¦¼ à¦ªà§à¦°à¦•à¦²à§à¦ªà§‡à¦° à¦œà¦¨à§à¦¯ à¦¬à¦¿à¦ªà¦°à§à¦¯\n",
    "à¦¯à¦¼à¦•à¦° à¦ªà¦°à¦¿à¦£à¦¤à¦¿ à¦¬à¦¯à¦¼à§‡ à¦†à¦¨à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤ à¦•à¦¾à¦°à¦£, à¦¬à§à¦°à§‡à¦•à§à¦¸à¦¿à¦Ÿà§‡à¦° à¦®à¦§à§à¦¯ à¦¦à¦¿à¦¯à¦¼à§‡ à¦¬à§à¦°à¦¿à¦Ÿà§‡à¦¨\n",
    "à¦¯à§‡à¦­à¦¾à¦¬à§‡ à¦‡à¦‰à¦°à§‹à¦ªà§€à¦¯à¦¼ à¦‡à¦‰à¦¨à¦¿à¦¯à¦¼à¦¨ à¦¥à§‡à¦•à§‡ à¦¬à¦¿à¦šà§à¦›à¦¿à¦¨à§à¦¨ à¦¹à¦¯à¦¼à§‡ à¦—à§‡à¦›à§‡, à¦«à§à¦°à¦¾à¦¨à§à¦¸à§‡ à¦•à¦Ÿà§à¦Ÿà¦° à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€à¦°à¦¾ à¦•à§à¦·à¦®à¦¤à¦¾à¦¯à¦¼ à¦à¦²à§‡ à¦à¦•à¦‡ à¦•à¦¾à¦¯à¦¼à¦¦à¦¾à¦¯à¦¼ à¦‡à¦‰à¦°à§‹à¦ª à¦¥à§‡à¦•à§‡\n",
    "à¦«à§à¦°à¦¾à¦¨à§à¦¸à§‡à¦° à¦¬à§‡à¦°à¦¿à¦¯à¦¼à§‡ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾ à¦ªà§à¦°à¦¶à§à¦¨à§‡ à¦«à§à¦°à§‡à¦•à§à¦¸à¦¿à¦Ÿ à¦—à¦£à¦­à§‹à¦Ÿ à¦†à¦¯à¦¼à§‹à¦œà¦¨à§‡à¦° à¦ªà¦¥ à¦ªà§à¦°à¦¶à¦¸à§à¦¤ à¦¹à¦¯à¦¼à§‡ à¦¯à¦¾à¦¬à§‡à¥¤\n",
    "à¦¬à§à¦°à§‡à¦•à§à¦¸à¦¿à¦Ÿ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦œà§à¦¯à§‡à¦° à¦œà¦¨à§à¦¯ à¦•à¦¤à¦Ÿà¦¾ à¦–à¦¾à¦°à¦¾à¦ª à¦ªà¦°à¦¿à¦£à¦¤à¦¿ à¦¡à§‡à¦•à§‡ à¦à¦¨à§‡à¦›à§‡, à¦¤à¦¾à¦° à¦¸à¦®à§à¦¯à¦• à¦ªà§à¦°à¦®à¦¾à¦£ à¦¥à¦¾à¦•à¦¾à¦° à¦ªà¦°à¦“ à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€à¦°à¦¾ à¦«à§à¦°à§‡à¦•à§à¦¸à¦¿à¦Ÿ\n",
    "à¦¬à¦¾à¦¸à§à¦¤à¦¬à¦¾à¦¯à¦¼à¦¨à§‡ à¦à§à¦à¦•à§‡ à¦ªà¦¡à¦¼à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤\n",
    "à¦‡à¦‰à¦°à§‹à¦ªà§‡à¦° à¦…à¦¨à§à¦¯ à¦¦à§‡à¦¶à¦—à§à¦²à§‹à¦° à¦•à¦¥à¦¾ à¦¨à¦¾à¦¹à¦¯à¦¼ à¦®à§‡à¦¨à§‡ à¦¨à§‡à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼, à¦•à¦¿à¦¨à§à¦¤à§ à¦à¦¤à¦¿à¦¹à¦¾à¦¸à¦¿à¦• à¦¦à§ƒà¦·à§à¦Ÿà¦¿à¦­à¦™à§à¦—à¦¿ à¦¥à§‡à¦•à§‡ à¦¦à§‡à¦–à¦²à§‡ à¦œà¦¾à¦°à§à¦®à¦¾\n",
    "à¦¨à¦¿à¦¤à§‡ à¦…à¦¤à¦¿\n",
    "à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€à¦¦à§‡à¦° à¦¬à¦¾à¦¡à¦¼à¦¬à¦¾à¦¡à¦¼à¦¨à§à¦¤à¦•à§‡ à¦¸à¦¬à¦šà§‡à¦¯à¦¼à§‡ à¦…à¦¶à§à¦­ à¦“ à¦¸à¦¬à¦šà§‡à¦¯à¦¼à§‡ à¦ªà¦°à¦¿à¦¹à¦¾à¦¸à¦ªà§‚à¦°à§à¦£\n",
    "à¦…à¦¬à¦¸à§à¦¥à¦¾ à¦¬à¦²à¦¾ à¦¯à¦¾à¦¯à¦¼à¥¤\n",
    "à¦°à¦•à§à¦·à¦£à¦¶à§€à¦² à¦¦à¦² à¦•à§à¦°à¦¿à¦¶à§à¦šà¦¿à¦¯à¦¼à¦¾à¦¨ à¦¡à§‡à¦®à§‹à¦•à§à¦°à§‡à¦Ÿà¦¿à¦• à¦‡à¦‰à¦¨à¦¿à¦¯à¦¼à¦¨ à¦“ à¦•à§à¦°à¦¿à¦¶à§à¦šà¦¿à¦¯à¦¼à¦¾à¦¨ à¦¸à§‹à¦¶à§à¦¯à¦¾à¦² à¦‡à¦‰à¦¨à¦¿à¦¯à¦¼à¦¨à§‡à¦° à¦œà§‹à¦Ÿ à¦œà¦¾à¦°à§à¦®à¦¾\n",
    "à¦¨à¦¿à¦¤à§‡ à¦‡à¦‰à¦°à§‹à¦ªà§€à¦¯à¦¼ à¦¨à¦¿à¦°à§à¦¬à¦¾\n",
    "à¦šà¦¨à§‡\n",
    "à¦ªà§à¦°à¦¥à¦® à¦¸à§à¦¥à¦¾à¦¨ à¦²à¦¾à¦­ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤\n",
    "à¦•à¦¿à¦¨à§à¦¤à§ à¦†à¦¸à¦² à¦§à¦¾à¦•à§à¦•à¦¾à¦° à¦¬à¦¿à¦·à¦¯à¦¼ à¦¹à¦²à§‹, à¦¨à¦¬à¦—à¦ à¦¿à¦¤ à¦œà¦¨à¦¤à§à¦·à§à¦Ÿà¦¿à¦¬à¦¾à¦¦à§€ à¦¦à¦² à¦…à¦²à¦Ÿà¦¾à¦°à¦¨à§‡à¦Ÿà¦¿à¦­ à¦«à¦¾à¦° à¦¡à¦¯à¦¼à§‡à¦¶à¦²à§à¦¯à¦¾à¦¨à§à¦¡ à§§à§¬ à¦¶à¦¤à¦¾à¦‚à¦¶ à¦­à§‹à¦Ÿ à¦ªà§‡à¦¯à¦¼à§‡ à¦¦à§à¦¬à¦¿à¦¤à§€à¦¯à¦¼\n",
    "à¦¸à§à¦¥à¦¾à¦¨à§‡ à¦°à¦¯à¦¼à§‡à¦›à§‡à¥¤\n",
    "à¦ à¦¿à¦• à§§à§¦à§¦ à¦¬à¦›à¦° à¦†à¦—à§‡, à¦…à¦°à§à¦¥à¦¾à§Ž à§§à§¦à§¨à§ª à¦¸à¦¾à¦²à§‡ à¦«à¦¿à¦°à§‡ à¦—à§‡à¦²à§‡ à¦¦à§‡à¦–à¦¾ à¦¯à¦¾à¦¬à§‡, à¦¸à§‡ à¦¬à¦›à¦° à¦œà¦¾à¦°à§à¦®à¦¾\n",
    "à¦¨à¦¿à¦° à¦ªà§à¦°à¦¥à¦® à¦œà¦¾à¦¤à§€à¦¯à¦¼ à¦¨à¦¿à¦°à§à¦¬à¦¾\n",
    "à¦šà¦¨à§‡ à¦…à¦‚à¦¶\n",
    "à¦¨à¦¿à¦¯à¦¼à§‡à¦›à¦¿à¦² à¦¨à¦¬à¦—à¦ à¦¿à¦¤ à¦œà¦¨à¦¤à§à¦·à§à¦Ÿà¦¿à¦¬à¦¾à¦¦à§€ à¦¦à¦² à¦¨à§à¦¯à¦¾à¦¶à¦¨à¦¾à¦² à¦¸à§‹à¦¶à§à¦¯à¦¾à¦²à¦¿à¦¸à§à¦Ÿ à¦«à§à¦°à¦¿à¦¡à¦® à¦ªà¦¾à¦°à§à¦Ÿà¦¿ (à¦à¦¨à¦à¦¸à¦à¦«à¦ªà¦¿)à¥¤\n",
    "à¦‡à¦‰à¦°à§‹à¦ªà§‡à¦° à¦à¦‡ à¦¸à¦¦à§à¦¯ à¦ªà§à¦¨à¦°à§à¦¤à§à¦¥à¦¿à¦¤ à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€ à¦¦à¦²à¦—à§à¦²à§‹à¦° à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿à¦°à¦‡ à¦à¦•à¦Ÿà¦¿ à¦¬à¦¿à¦·à¦¯à¦¼à§‡ à¦¨à¦¾à§Žà¦¸à¦¿à¦¦à§‡à¦° à¦¸à¦™à§à¦—à§‡ à¦®à¦¿à¦²\n",
    "à¦°à¦¯à¦¼à§‡à¦›à§‡à¥¤ à¦¸à§‡à¦Ÿà¦¿ à¦¹à¦²à§‹, à¦¤à¦¾à¦°à¦¾ à¦…à¦°à§à¦¥à¦¨à§ˆà¦¤à¦¿à¦• à¦¸à¦®à¦¸à§à¦¯à¦¾à¦¯à¦¼ à¦¬à¦¿à¦ªà¦°à§à¦¯\n",
    "à¦¸à§à¦¤ à¦¹à¦¯à¦¼à§‡ à¦ªà¦¡à¦¼à¦¾ à¦¨à¦¾à¦—à¦°à¦¿à¦•à¦¦à§‡à¦° à¦•à¦¾à¦›à§‡ à¦¤à¦¾à¦¦à§‡à¦°\n",
    "à¦¦à§à¦°à§à¦¦\n",
    "à¦¶à¦¾à¦° à¦ªà§‡à¦›à¦¨à§‡ à¦¸à¦°à¦•à¦¾à¦°à¦—à§à¦²à§‹à¦° à¦…à¦­à¦¿à¦¬à¦¾à¦¸à§€ à¦¨à§€à¦¤à¦¿à¦° à¦¹à¦¾à¦¤ à¦†à¦›à§‡ à¦¬à¦²à§‡ à¦¦à¦¾à¦¬à¦¿ à¦•à¦°à¦›à§‡à¥¤\n",
    "à¦à¦¨à¦à¦¸à¦à¦«à¦ªà¦¿ à¦›à¦¿à¦² à¦¨à¦¿à¦·à¦¿à¦¦à§à¦§ à¦¹à¦“à¦¯à¦¼à¦¾ à¦¨à§à¦¯à¦¾à¦¶à¦¨à¦¾à¦² à¦¸à§‹à¦¶à§à¦¯à¦¾à¦²à¦¿à¦¸à§à¦Ÿ à¦œà¦¾à¦°à§à¦®à¦¾\n",
    "à¦¨ à¦“à¦¯à¦¼à¦¾à¦°à§à¦•à¦¾\n",
    "à¦°à§à¦¸\n",
    "à¦ªà¦¾à¦°à§à¦Ÿà¦¿à¦° (à¦¯à¦¾ à¦¨à¦¾à§Žà¦¸à¦¿ à¦ªà¦¾à¦°à§à¦Ÿà¦¿ à¦¨à¦¾à¦®à§‡ à¦¬à§‡à¦¶à¦¿ à¦ªà¦°à¦¿à¦šà¦¿à¦¤) à¦à¦•à¦Ÿà¦¿\n",
    "à¦«à§à¦°à¦¨à§à¦Ÿà¥¤ à¦®à¦¿à¦‰à¦¨à¦¿à¦– â€˜à¦¬à¦¿à¦¯à¦¼à¦¾à¦° à¦¹à¦²â€™à¦–à§à¦¯à¦¾à¦¤ à¦¬à§à¦¯à¦°à§à¦¥\n",
    "à¦…à¦­à§à¦¯à§à¦¤à§à¦¥à¦¾à¦¨à§‡à¦° à¦ªà¦°à¦¿à¦ªà§à¦°à§‡à¦•à§à¦·à¦¿à¦¤à§‡ à¦“à¦‡ à¦¸à¦®à¦¯à¦¼ à¦¨à¦¾à§Žà¦¸à¦¿ à¦ªà¦¾à¦°à§à¦Ÿà¦¿à¦° à¦¨à§‡à¦¤à¦¾ à¦…à§à¦¯à¦¾à¦¡à¦²à¦« à¦¹à¦¿à¦Ÿà¦²à¦¾à¦° à¦œà§‡à¦²à§‡ à¦¬à¦¨à§à¦¦à§€\n",
    "à¦›à¦¿à¦²à§‡à¦¨à¥¤\n",
    "à¦à¦¨à¦à¦¸à¦à¦«à¦ªà¦¿ à§§à§¯à§¨à§ª à¦¸à¦¾à¦²à§‡à¦° à¦¨à¦¿à¦°à§à¦¬à¦¾\n",
    "à¦šà¦¨à§‡ à¦–à§à¦¬ à¦–à¦¾à¦°à¦¾à¦ª à¦«à¦² à¦•à¦°à§‡à¦›à¦¿à¦²à¥¤ à¦¸à§‡ à¦¬à¦›à¦° à¦¤à¦¾à¦°à¦¾ à¦®à¦¾à¦¤à§à¦° à§© à¦¶à¦¤à¦¾à¦‚à¦¶ à¦­à§‹à¦Ÿ à¦ªà§‡à¦¯à¦¼à§‡à¦›à¦¿à¦²à¥¤\n",
    "à¦•à¦¿à¦¨à§à¦¤à§ à¦ªà¦°à§‡à¦° à§¯ à¦¬à¦›à¦°à§‡à¦° à¦®à¦§à§à¦¯à§‡ à¦¹à¦¿à¦Ÿà¦²à¦¾à¦° à¦šà§à¦¯à¦¾à¦¨à§à¦¸à§‡à¦²à¦° à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦à¦•à¦Ÿà¦¿ à¦œà§‹à¦Ÿ à¦¸à¦°à¦•à¦¾à¦°à§‡à¦° à¦ªà§à¦°à¦§à¦¾à¦¨ à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦†à¦¬à¦¿à¦°à§à¦­à§‚\n",
    "à¦¤ à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦²à§‡à¦¨à¥¤ à¦à¦°\n",
    "à¦ªà¦°à§‡à¦° à¦¬à¦›à¦° à§§à§¯à§©à§ª à¦¸à¦¾à¦²à§‡à¦° à¦†à¦—à¦¸à§à¦Ÿà§‡ à¦¤à¦¿à¦¨à¦¿ à¦¨à¦¿à¦œà§‡à¦•à§‡ â€˜à¦«à§à¦¯à¦¼à§‡à¦°à¦¾à¦°â€™ à¦˜à§‹à¦·à¦£à¦¾ à¦•à¦°à§‡ à¦œà¦¾à¦°à§à¦®à¦¾\n",
    "à¦¨à¦¿à¦•à§‡ à¦à¦•à¦Ÿà¦¿ à¦¸à¦°à§à¦¬\n",
    "à¦—à§à¦°à¦¾à¦¸à§€ à¦à¦•à¦¨à¦¾à¦¯à¦¼à¦•à¦¤à¦¨à§à¦¤à§à¦°à§‡\n",
    "à¦°à§‚à¦ªà¦¾à¦¨à§à¦¤à¦°à¦¿à¦¤ à¦•à¦°à§‡à¦›à¦¿à¦²à§‡à¦¨à¥¤\n",
    "à¦«à¦²à§‹ à¦•à¦°à§à¦¨à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦•à¦°à§à¦¨\n",
    "6/17/24, 11:56 AM à¦‡à¦‰à¦°à§‹à¦ªà§‡ à¦¨à¦¤à§ à¦¨ à¦à¦• à¦…à¦¨à§à¦§\n",
    "à¦•à¦¾à¦° à¦¯à§à¦— à¦¨à§‡à¦®à§‡ à¦†à¦¸à¦›à§‡? | à¦ªà§à¦°à¦¥à¦® à¦†à¦²à§‹\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 3/6\n",
    "à¦ªà¦°à§‡à¦° à¦‡à¦¤à¦¿à¦¹à¦¾à¦¸ à¦®à¦°à§à¦®à¦¾\n",
    "à¦¨à§à¦¤à¦¿à¦•à¥¤ à¦ªà¦°à§‡à¦° à¦‡à¦¤à¦¿à¦¹à¦¾à¦¸ à¦°à¦•à§à¦¤à¦¾à¦•à§à¦¤à¥¤ à¦ªà¦°à§‡à¦° à¦‡à¦¤à¦¿à¦¹à¦¾à¦¸ à¦à¦®à¦¨ à¦à¦• à¦‡à¦¤à¦¿à¦¹à¦¾à¦¸, à¦¯à¦¾ à¦à¦• à¦¶à¦¤à¦¾à¦¬à§à¦¦à§€ à¦ªà¦°à¦“ à¦®à¦¨à§‡ à¦ªà¦¡à¦¼à¦²à§‡\n",
    "à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¶à¦¿à¦‰à¦°à§‡ à¦‰à¦ à¦¤à§‡ à¦¹à¦¯à¦¼à¥¤ à¦¤à¦¾à¦° à¦ªà§à¦¨à¦°à¦¾à¦¬à§ƒà¦¤à§à¦¤à¦¿à¦° à¦²à¦•à§à¦·à¦£ à¦†à¦®à¦¾à¦¦à§‡à¦° à¦œà¦¨à§à¦¯ à¦¨à¦¿à¦¦à¦¾à¦°à§à¦£ à¦‰à¦¦à§à¦¬à§‡à¦—à§‡à¦° à¦¬à¦¿à¦·à¦¯à¦¼ à¦¹à¦¯à¦¼à§‡ à¦“à¦ à§‡à¥¤\n",
    "à¦—à¦¤ à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡ à¦¯à§‡à¦¸à¦¬ à¦˜à¦Ÿà¦¨à¦¾ à¦˜à¦Ÿà§‡ à¦—à§‡à¦›à§‡, à¦¤à¦¾ à¦¬à§‹à¦à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯ à¦¸à¦®à§à¦­à¦¬à¦¤ â€˜à¦¬à¦¿à¦¡à¦¼à¦®à§à¦¬à¦¨à¦¾â€™ à¦à¦•à¦Ÿà¦¿ à¦…à¦ªà¦°à§à¦¯à¦¾\n",
    "à¦ªà§à¦¤ à¦¶à¦¬à§à¦¦à¥¤\n",
    "à¦‡à¦‰à¦°à§‹à¦ªà¦•à§‡ à¦¨à¦¾à§Žà¦¸à¦¿à¦¦à§‡à¦° à¦•à¦¬à¦² à¦¥à§‡à¦•à§‡ à¦®à§à¦•à§à¦¤ à¦•à¦°à¦¤à§‡ à¦¸à¦¬à¦šà§‡à¦¯à¦¼à§‡ à¦¬à¦¡à¦¼ à¦­à§‚à¦®à¦¿à¦•à¦¾ à¦°à§‡à¦–à§‡à¦›à¦¿à¦² à¦¯à§‡ à¦®à¦¿à¦¤à§à¦° à¦¬à¦¾à¦¹à¦¿à¦¨à§€, à¦¸à§‡à¦‡ à¦®à¦¿à¦¤à§à¦° à¦¬à¦¾à¦¹à¦¿à¦¨à§€à¦° à¦«à§à¦°à¦¾à¦¨à§à¦¸à§‡\n",
    "à¦…à¦¬à¦¤à¦°à¦£à§‡à¦° à§®à§¦à¦¤à¦® à¦¬à¦¾à¦°à§à¦·à¦¿à¦•à§€ à¦‰à¦¦à§â€Œà¦¯à¦¾à¦ªà¦¨à§‡à¦° à¦œà¦¨à§à¦¯ à§¬ à¦œà§à¦¨ à¦¨à¦°à¦®à§à¦¯à¦¾à¦¨à§à¦¡à¦¿à¦¤à§‡ à¦‡à¦‰à¦°à§‹à¦ªà§‡à¦° à¦¨à§‡à¦¤à¦¾à¦°à¦¾, à¦®à¦¾à¦°à§à¦•à¦¿\n",
    "à¦¨ à¦ªà§à¦°à§‡à¦¸à¦¿à¦¡à§‡à¦¨à§à¦Ÿ à¦œà§‹ à¦¬à¦¾à¦‡à¦¡à§‡à¦¨ à¦“\n",
    "à¦•à¦¾à¦¨à¦¾à¦¡à¦¾à¦° à¦ªà§à¦°à¦§à¦¾à¦¨à¦®à¦¨à§à¦¤à§à¦°à§€ à¦œà¦¾à¦¸à§à¦Ÿà¦¿à¦¨ à¦Ÿà§à¦°à§à¦¡à§‹ à¦à¦•à¦¤à§à¦° à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦²à§‡à¦¨à¥¤\n",
    "à¦•à¦¿à¦¨à§à¦¤à§ à¦¤à¦¾à¦° à¦•à¦¯à¦¼à§‡à¦• à¦¦à¦¿à¦¨à§‡à¦° à¦®à¦§à§à¦¯à§‡à¦‡ à¦‡à¦‰à¦°à§‹à¦ªà§‡à¦° à¦­à§‹à¦Ÿà¦¾à¦°à¦°à¦¾ à¦‡à¦‡à¦‰ à¦ªà¦¾à¦°à§à¦²à¦¾\n",
    "à¦®à§‡à¦¨à§à¦Ÿ à¦¨à¦¿à¦°à§à¦¬à¦¾\n",
    "à¦šà¦¨à§‡ à¦¤à¦¾à¦¦à§‡à¦° à¦¸à¦°à§à¦¬\n",
    "à¦•à¦¾à¦²à§‡à¦° à¦¸à¦¬à¦šà§‡à¦¯à¦¼à§‡ à¦¬à¦¡à¦¼ à¦¸à¦®à¦°à§à¦¥\n",
    "à¦¨\n",
    "à¦…à¦¤à¦¿-à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€ à¦¦à¦²à¦—à§à¦²à§‹à¦° à¦¹à¦¾à¦¤à§‡ à¦¤à§à¦²à§‡ à¦¦à¦¿à¦¯à¦¼à§‡à¦›à§‡à¦¨à¥¤\n",
    "à¦…à¦¬à¦¶à§à¦¯ à¦‡à¦‰à¦°à§‹à¦ªà§€à¦¯à¦¼ à¦®à¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦¬à¦¾ à¦°à¦¾à¦œà¦¨à§€à¦¤à¦¿à¦¤à§‡ à¦•à§‹à¦¨à§‹ à¦†à¦§à§à¦¨à¦¿à¦• à¦—à¦£à¦¤à¦¾à¦¨à§à¦¤à§à¦°à¦¿à¦• à¦°à¦¾à¦œà¦¨à§ˆà¦¤à¦¿à¦• à¦¦à¦²à§‡à¦° à¦¨à§€à¦¤à¦¿à¦•à§‡ à¦¨à¦¾à§Žà¦¸à¦¿à¦¦à§‡à¦° à¦¸à¦™à§à¦—à§‡ à¦¤à§à¦²à¦¨à¦¾\n",
    "à¦•à¦°à¦¾ à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£à¦­à¦¾à¦¬à§‡ à¦¨à¦¿à¦·à¦¿à¦¦à§à¦§à¥¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦­à¦¯à¦¼à¦¾à¦¬à¦¹ à¦¬à¦¾à¦¸à§à¦¤à¦¬à¦¤à¦¾ à¦¹à¦²à§‹, à¦à¦• à¦¶à¦¤à¦¾à¦¬à§à¦¦à§€ à¦†à¦—à§‡ à¦¨à¦¾à§Žà¦¸à¦¿à¦¦à§‡à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦¯à§‡ à¦­à§‚à¦¤ à¦œà¦¾à¦°à§à¦®à¦¾\n",
    "à¦¨à¦¿à¦° à¦—à¦£à¦¤à¦¾à¦¨à§à¦¤à§à¦°à¦¿à¦•\n",
    "à¦¬à§à¦¯à¦¬à¦¸à§à¦¥à¦¾à¦•à§‡ à¦›à¦¿à¦¨à¦¿à¦¯à¦¼à§‡ à¦¨à¦¿à¦¯à¦¼à§‡à¦›à¦¿à¦², à¦¸à§‡à¦‡ à¦­à§‚à¦¤ à¦†à¦¬à¦¾à¦° à¦‡à¦‰à¦°à§‹à¦ªà§‡ à¦«à¦¿à¦°à§‡ à¦à¦¸à§‡à¦›à§‡à¥¤\n",
    "à¦‡à¦‰à¦°à§‹à¦ªà§‡à¦° à¦à¦‡ à¦¸à¦¦à§à¦¯ à¦ªà§à¦¨à¦°à§à¦¤à§à¦¥à¦¿à¦¤ à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€ à¦¦à¦²à¦—à§à¦²à§‹à¦° à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿à¦°à¦‡ à¦à¦•à¦Ÿà¦¿ à¦¬à¦¿à¦·à¦¯à¦¼à§‡ à¦¨à¦¾à§Žà¦¸à¦¿à¦¦à§‡à¦° à¦¸à¦™à§à¦—à§‡ à¦®à¦¿à¦² à¦°à¦¯à¦¼à§‡à¦›à§‡à¥¤ à¦¸à§‡à¦Ÿà¦¿ à¦¹à¦²à§‹, à¦¤à¦¾à¦°à¦¾\n",
    "à¦…à¦°à§à¦¥à¦¨à§ˆà¦¤à¦¿à¦• à¦¸à¦®à¦¸à§à¦¯à¦¾à¦¯à¦¼ à¦¬à¦¿à¦ªà¦°à§à¦¯\n",
    "à¦¸à§à¦¤ à¦¹à¦¯à¦¼à§‡ à¦ªà¦¡à¦¼à¦¾ à¦¨à¦¾à¦—à¦°à¦¿à¦•à¦¦à§‡à¦° à¦•à¦¾à¦›à§‡ à¦¤à¦¾à¦¦à§‡à¦° à¦¦à§à¦°à§à¦¦\n",
    "à¦¶à¦¾à¦° à¦ªà§‡à¦›à¦¨à§‡ à¦¸à¦°à¦•à¦¾à¦°à¦—à§à¦²à§‹à¦° à¦…à¦­à¦¿à¦¬à¦¾à¦¸à§€ à¦¨à§€à¦¤à¦¿à¦° à¦¹à¦¾à¦¤\n",
    "à¦†à¦›à§‡ à¦¬à¦²à§‡ à¦¦à¦¾à¦¬à¦¿ à¦•à¦°à¦›à§‡à¥¤\n",
    "à¦¹à¦¾à¦¸à¦ªà¦¾à¦¤à¦¾à¦²à§‡ à¦šà¦¿à¦•à¦¿à§Žà¦¸à¦¾ à¦¨à¦¿à¦¤à§‡ à¦à¦¸à§‡ à¦¦à§€à¦°à§à¦˜\n",
    "à¦²à¦¾à¦‡à¦¨à§‡ à¦¦à¦¾à¦à¦¡à¦¼à¦¾à¦¨à§‹ à¦“ à¦†à¦¬à¦¾à¦¸à¦¨à§‡à¦° à¦…à¦­à¦¾à¦¬à§‡à¦° à¦®à¦¤à§‹ à¦¬à¦¿à¦·à¦¯à¦¼ à¦¥à§‡à¦•à§‡ à¦¶à§à¦°à§ à¦•à¦°à§‡ à¦¨à¦¾à¦¨à¦¾ à¦§à¦°à¦¨à§‡à¦°\n",
    "à¦…à¦ªà¦°à¦¾à¦§ à¦¬à§‡à¦¡à¦¼à§‡ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾ à¦à¦¬à¦‚ à¦¸à§à¦•à§à¦² à¦“ à¦•à¦¾à¦°à¦¾à¦—à¦¾à¦°à§‡ à¦­à¦¿à¦¡à¦¼ à¦¬à§‡à¦¡à¦¼à§‡ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾à¦° à¦®à¦¤à§‹ à¦¬à¦¿à¦·à¦¯à¦¼à§‡ à¦¤à¦¾à¦°à¦¾ à¦…à¦­à¦¿à¦¬à¦¾à¦¸à§€à¦¦à§‡à¦° à¦¦à¦¾à¦¯à¦¼à§€ à¦•à¦°à§‡ à¦¯à¦¾à¦šà§à¦›à§‡à¥¤\n",
    "à¦‡à¦‰à¦°à§‹à¦ªà§‡à¦° à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¦à§‡à¦¶ à¦à¦–à¦¨ à¦à¦‡ à¦…à¦¶à§à¦­ à¦®à¦¤à¦¬à¦¾à¦¦à§‡à¦° à¦¸à¦™à§à¦—à§‡ à¦¤à¦¾à¦² à¦®à§‡à¦²à¦¾à¦¤à§‡ à¦¶à§à¦°à§ à¦•à¦°à§‡à¦›à§‡ à¦à¦¬à¦‚ à¦•à§‹à¦¨à§‹ à¦•à§‹à¦¨à§‹ à¦¸à§à¦¤à¦°à§‡ à¦à¦‡ à¦­à¦¾à¦·à§à¦¯à¦•à§‡\n",
    "à¦®à§‡à¦¨à§‡ à¦¨à¦¿à¦šà§à¦›à§‡à¥¤\n",
    "à¦‰à¦¦à¦¾à¦¹à¦°à¦£ à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¬à§à¦°à¦¿à¦Ÿà§‡à¦¨à§‡à¦° à¦•à¦¥à¦¾ à¦¬à¦²à¦¾ à¦¯à§‡à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤ à§¨à§¦à§¦à§¬ à¦¸à¦¾à¦²à§‡ à¦¦à§‡à¦¶à¦Ÿà¦¿ à¦œà¦¨à¦¤à§à¦·à§à¦Ÿà¦¿à¦¬à¦¾à¦¦à§‡à¦° à¦à¦‡ à¦ªà¦¿à¦šà§à¦›à¦¿à¦² à¦¢à¦¾à¦²à§‡ à¦ªà¦¾ à¦°à§‡à¦–à§‡à¦›à¦¿à¦²à¥¤ à¦¸à§‡\n",
    "à¦¬à¦›à¦° à¦¸à§‡à¦–à¦¾à¦¨à§‡ à¦‡à¦‰à¦°à§‹à¦¸à§à¦•à§‡à¦ªà¦Ÿà¦¿à¦• à¦‡à¦‰à¦•à§‡ à¦‡à¦¨à¦¡à¦¿à¦ªà§‡à¦¨à¦¡à§‡à¦¨à§à¦¸ à¦ªà¦¾à¦°à§à¦Ÿà¦¿ (à¦‡à¦‰à¦•à§‡à¦†à¦‡à¦ªà¦¿) à¦¨à¦¾à¦®à§‡à¦° à¦à¦•à¦Ÿà¦¿ à¦¨à¦¤à§à¦¨ à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€ à¦¦à¦² à¦†à¦¸à§‡à¥¤ à¦¨à¦¤à§à¦¨\n",
    "à¦¦à¦² à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¤à¦–à¦¨ à¦•à§‡à¦‰ à¦¤à§‡à¦®à¦¨ à¦à¦Ÿà¦¿à¦•à§‡ à¦ªà¦¾à¦¤à§à¦¤à¦¾ à¦¦à§‡à¦¯à¦¼à¦¨à¦¿à¥¤ à¦¸à¦¾à¦¬à§‡à¦• à¦§à¦¨à¦•à§à¦¬à§‡à¦° à¦¨à¦¾à¦‡à¦œà§‡à¦² à¦«à¦¾à¦°à¦¾à¦œà§‡à¦° à¦¨à§‡à¦¤à§ƒà¦¤à§à¦¬à¦¾à¦§à§€à¦¨ à¦à¦‡ à¦¦à¦²à¦•à§‡ à¦¨à¦¿à¦¯à¦¼à§‡\n",
    "à¦…à¦¨à§‡à¦•à§‡ à¦ à¦¾à¦Ÿà§à¦Ÿà¦¾â€“à¦¤à¦¾à¦®à¦¾à¦¶à¦¾ à¦ªà¦°à§à¦¯\n",
    "à¦¨à§à¦¤ à¦•à¦°à§‡à¦›à¦¿à¦²à§‡à¦¨à¥¤\n",
    "à¦•à¦¿à¦¨à§à¦¤à§ à¦«à¦¾à¦°à¦¾à¦œ à¦‡à¦‰à¦°à§‹à¦ªà§‡ à¦…à¦­à¦¿à¦¬à¦¾à¦¸à§€à¦¦à§‡à¦° à¦†à¦¸à¦¾ à¦à¦¬à¦‚ à¦¤à¦¾à¦¦à§‡à¦° à¦•à¦¾à¦°à¦£à§‡ à¦¶à§à¦¬à§‡à¦¤à¦¾à¦™à§à¦— à¦¬à§à¦°à¦¿à¦Ÿà¦¿à¦¶ à¦¶à§à¦°à¦®à¦¿à¦•à¦¦à§‡à¦° à¦“à¦ªà¦° à¦•à§€ à¦•à§€ à¦¨à§‡à¦¤à¦¿à¦¬à¦¾à¦šà¦• à¦ªà§à¦°à¦­à¦¾à¦¬\n",
    "à¦ªà¦¡à¦¼à¦¬à§‡, à¦¸à§‡à¦—à§à¦²à§‹à¦•à§‡à¦‡ à¦¤à¦¾à¦à¦° à¦¨à¦¿à¦°à§à¦¬à¦¾\n",
    "à¦šà¦¨à§€ à¦ªà§à¦°à¦šà¦¾à¦°à§‡à¦° à¦ªà§à¦°à¦§à¦¾à¦¨ à¦¬à¦¿à¦·à¦¯à¦¼ à¦•à¦°à§‡ à¦¤à§à¦²à§‡à¦›à¦¿à¦²à§‡à¦¨à¥¤ à¦à¦Ÿà¦¿ à¦•à¦¾à¦œà§‡à¦“ à¦¦à¦¿à¦¯à¦¼à§‡à¦›à¦¿à¦²à¥¤\n",
    "à§¨à§¦à§§à§ª à¦¸à¦¾à¦²à§‡ à¦‡à¦‰à¦•à§‡à¦†à¦‡à¦ªà¦¿ à¦‡à¦‰à¦°à§‹à¦ªà§€à¦¯à¦¼ à¦ªà¦¾à¦°à§à¦²à¦¾\n",
    "à¦®à§‡à¦¨à§à¦Ÿà§‡ à¦¬à§à¦°à¦¿à¦Ÿà§‡à¦¨à§‡à¦° à¦¸à¦‚à¦–à§à¦¯à¦¾à¦—à¦°à¦¿à¦·à§à¦  à¦†à¦¸à¦¨ à¦œà¦¯à¦¼ à¦•à¦°à§‡ à¦®à§‚à¦²à¦§à¦¾à¦°à¦¾à¦° à¦¦à¦²à¦—à§à¦²à§‹à¦•à§‡ à¦šà¦®à¦•à§‡\n",
    "à¦¦à¦¿à¦¯à¦¼à§‡à¦›à¦¿à¦²à¥¤ à¦«à¦²à§‹ à¦•à¦°à§à¦¨à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦•à¦°à§à¦¨\n",
    "6/17/24, 11:56 AM à¦‡à¦‰à¦°à§‹à¦ªà§‡ à¦¨à¦¤à§ à¦¨ à¦à¦• à¦…à¦¨à§à¦§\n",
    "à¦•à¦¾à¦° à¦¯à§à¦— à¦¨à§‡à¦®à§‡ à¦†à¦¸à¦›à§‡? | à¦ªà§à¦°à¦¥à¦® à¦†à¦²à§‹\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 4/6\n",
    "à¦•à§à¦·à¦®à¦¤à¦¾à¦¸à§€à¦¨ à¦•à¦¨à¦œà¦¾à¦°à¦­à§‡à¦Ÿà¦¿à¦­ à¦ªà¦¾à¦°à§à¦Ÿà¦¿à¦° à¦¯à§‡ à¦¡à¦¾à¦¨à¦ªà¦¨à§à¦¥à§€ à¦¸à¦¦à¦¸à§à¦¯à¦°à¦¾ à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦œà§à¦¯à§‡à¦° à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦¨à¦¿à¦°à§à¦¬à¦¾\n",
    "à¦šà¦¨à§‡ à¦¤à¦¾à¦à¦¦à§‡à¦° à¦†à¦¸à¦¨à¦—à§à¦²à§‹ à¦¨à¦¾à¦‡à¦œà§‡à¦²\n",
    "à¦«à¦¾à¦°à¦¾à¦œà§‡à¦° à¦¦à¦² à¦¦à¦–à¦² à¦•à¦°à§‡ à¦¨à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦¬à¦²à§‡ à¦†à¦¶à¦™à§à¦•à¦¾ à¦•à¦°à¦›à¦¿à¦²à§‡à¦¨, à¦¤à¦¾à¦à¦¦à§‡à¦° à¦¸à¦¨à§à¦¤à§à¦·à§à¦Ÿ à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯ à§¨à§¦à§§à§« à¦¸à¦¾à¦²à§‡ à¦•à¦¨à¦œà¦¾à¦°à¦­à§‡à¦Ÿà¦¿à¦­ à¦¦à¦²à§‡à¦°\n",
    "à¦ªà§à¦°à¦§à¦¾à¦¨à¦®à¦¨à§à¦¤à§à¦°à§€ à¦¡à§‡à¦­à¦¿à¦¡ à¦•à§à¦¯à¦¾à¦®à§‡à¦°à¦¨ à¦¦à§‡à¦¶à¦Ÿà¦¿ à¦‡à¦‡à¦‰à¦° à¦¸à¦¦à¦¸à§à¦¯ à¦¥à¦¾à¦•à¦¬à§‡ à¦•à¦¿ à¦¨à¦¾, à¦¤à¦¾ à¦¨à¦¿à¦¯à¦¼à§‡ à¦à¦•à¦Ÿà¦¿ à¦œà¦¾à¦¤à§€à¦¯à¦¼ à¦—à¦£à¦­à§‹à¦Ÿ à¦•à¦°à¦¤à§‡ à¦°à¦¾à¦œà¦¿ à¦¹à¦¯à¦¼à§‡ à¦¯à¦¾à¦¨à¥¤\n",
    "à¦¯à§à¦•à§à¦¤à¦°à¦¾à¦œà§à¦¯à§‡à¦° à¦œà¦¾à¦¤à§€à¦¯à¦¼ à¦¨à¦¿à¦°à§à¦¬à¦¾\n",
    "à¦šà¦¨à§‡ à¦‡à¦‰à¦•à§‡à¦†à¦‡à¦ªà¦¿ à¦à¦•à¦Ÿà¦¿ à¦®à¦¾à¦¤à§à¦° à¦†à¦¸à¦¨à§‡ à¦œà¦¿à¦¤à§‡à¦›à¦¿à¦²à¥¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦•à§à¦·à¦¤à¦¿ à¦¯à¦¾ à¦¹à¦“à¦¯à¦¼à¦¾à¦° à¦¹à¦¯à¦¼à§‡ à¦—à¦¿à¦¯à¦¼à§‡à¦›à¦¿à¦²à¥¤ \n",
    "à¦®à¦°à§à¦®à¦¾\n",
    "à¦œà¦¬à¦¾à¦¬à¦¶à§‡à¦¯à¦¼à¦¾à¦°\n",
    "Sharaj Ghosh T oday at 6:02 AM\n",
    "100% Right\n",
    "à¦œà¦¬à¦¾à¦¬à¦¶à§‡à¦¯à¦¼à¦¾à¦°\n",
    "à¦¨à¦¤à§à¦¨\n",
    "6/17/24, 11:56 AM à¦‡à¦‰à¦°à§‹à¦ªà§‡ à¦¨à¦¤à§ à¦¨ à¦à¦• à¦…à¦¨à§à¦§\n",
    "à¦•à¦¾à¦° à¦¯à§à¦— à¦¨à§‡à¦®à§‡ à¦†à¦¸à¦›à§‡? | à¦ªà§à¦°à¦¥à¦® à¦†à¦²à§‹\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 6/6\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP+XqRhTa1ROKyr53+RoGmL",
   "cell_execution_strategy": "setup",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1_yNCks4BTD5zOnjozppphh5GzMFaMKq_",
     "timestamp": 1717489943398
    },
    {
     "file_id": "1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_",
     "timestamp": 1716401643928
    },
    {
     "file_id": "1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5",
     "timestamp": 1703608159823
    },
    {
     "file_id": "1oW55fBmwzCOrBVX66RcpptL3a99qWBxb",
     "timestamp": 1702886138876
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00b89bfcf3f44380bf76bcbbbd981bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8da1d11887746dbb9eb7a849a5d5c8c",
       "IPY_MODEL_582f0deecef14e1295785bb5f04a1804",
       "IPY_MODEL_2444aeb4fdb8418786fb1f7e66c534ed"
      ],
      "layout": "IPY_MODEL_3a127e2101914b8aa542ef21c4329c39"
     }
    },
    "026e66bb6fb64494b389198e3f64504b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33a88395eec948cdbc9cd038a986729d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2ba358a81db741b7963d4e14136a6bff",
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "03137bba9d7d4ef3ae7e073064b5438d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b12e798c92f848bc9e1d430d504ee3c7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ebb68cccf04641ed8996e9c9b62c67be",
      "value": "Downloadingâ€‡data:â€‡100%"
     }
    },
    "04744ff89533496dbe9bc442b25fde47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05eca118bd194c2c9d5f7d825bf0493f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_987d83a2779c42fd99b862fead155f57",
      "max": 1148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_943df89e90eb4def9bb8414ecfffc681",
      "value": 1148
     }
    },
    "065c41d2f95d4980a3379c179e9bf304": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a4a905e9ca14b23b8134f4ee1dd51a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0abb70b59181406a9c53f2fba05a9396": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b95d3952f9f46b3ab5d11a630fb4ab9",
       "IPY_MODEL_9bab4b07eafe4d8b97340a6989755b3a",
       "IPY_MODEL_467126b2609e4b869c6c98f5e0fc7164"
      ],
      "layout": "IPY_MODEL_f1e7a220b6c14349b5ec041d1cce8889"
     }
    },
    "0c3a85239fa94b8a85545c8eec38d887": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e66a1bb087944ceca9cbe6517ac19292",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_38725355a21844458da20ebf9eaf7d21",
      "value": "â€‡137k/137kâ€‡[00:00&lt;00:00,â€‡1.10MB/s]"
     }
    },
    "0cc5119ca0244fd59e2b5b3cb4715005": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d920b9593ea41a3ae099ee0c3eb6dbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a55b27398caf4ae39ac5900d22677220",
      "max": 4138270821,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f7187ecc5d974ff18fc70470d1a48464",
      "value": 4138270821
     }
    },
    "0dc39eb1468d48dba66e4d6b403a7085": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "108675c4393b4b678a5cfa2f73ce9f4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10a7046d9e1c4ada876b2b65e4169de9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_026e66bb6fb64494b389198e3f64504b",
       "IPY_MODEL_a893e20bedc6404fb2ab9002d7da7090",
       "IPY_MODEL_1df6a414771544528de945d997ea18a5"
      ],
      "layout": "IPY_MODEL_ed92ddd32f6e4301a46e0178f12e5a8d"
     }
    },
    "119f3c027e3f4beda1c026fd3ebb3f7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "13e918c3b7d44aaea79540e39173e4e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1531e137a0b84e3db00599d6311fbc04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16937b1d13dc433ca1605982da3f7587": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d5689dd39d7414dbeb92976c9a682c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dd43f29b71042df88ff4c49af908ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1df6a414771544528de945d997ea18a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eaf2f4d59814487aa8ea1abbf8831da1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5f1cbf83549943cfa3b568ccd5cdb34e",
      "value": "â€‡560/560â€‡[00:00&lt;00:00,â€‡22.1kB/s]"
     }
    },
    "2444aeb4fdb8418786fb1f7e66c534ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed28850ca87b4c768996f50844c73afe",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7bd3fed632324e11a989409ea8208390",
      "value": "â€‡6478/6478â€‡[00:23&lt;00:00,â€‡323.58â€‡examples/s]"
     }
    },
    "25350d0725054c33b7e2e792190bf039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "264288b1c4834e8f9691937bdbf45e6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_282d5aa660744eecacb972f345c61c65",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0dc39eb1468d48dba66e4d6b403a7085",
      "value": "Downloadingâ€‡readme:â€‡100%"
     }
    },
    "27ea6ee950254229ae3493c862fabab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d49a277941384dfeb7907b9017d16ecf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_efd751b6c2a642ff94845df115af4ed7",
      "value": "â€‡1.15k/1.15kâ€‡[00:00&lt;00:00,â€‡31.3kB/s]"
     }
    },
    "282d5aa660744eecacb972f345c61c65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29d5c06b48d04bfa99cefe38030f277c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e9990fc07da480a96223afe3c85be2f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_abd2784d68354f6388adafe797feb706",
      "value": "generation_config.json:â€‡100%"
     }
    },
    "2a0095c0f676406c9799779a33225860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91476a34dbb14bde981fdd7743fb860f",
       "IPY_MODEL_2a68f7129c5a4e3bb75c4469a8eda942",
       "IPY_MODEL_b87ddb145c414e53b425345c652699d1"
      ],
      "layout": "IPY_MODEL_fdad79e64e214bfbb4abda9b3eee7eb6"
     }
    },
    "2a68f7129c5a4e3bb75c4469a8eda942": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d208453e5609482c9c5113d3b3c84a6d",
      "max": 587404,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b9b86ece90ea4666be175d68bc26a648",
      "value": 587404
     }
    },
    "2a73eec6178640ccac73682222f36a56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dd43f29b71042df88ff4c49af908ce3",
      "max": 136734,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_777c1f497f364f04bb3cfc97d937de0a",
      "value": 136734
     }
    },
    "2ba358a81db741b7963d4e14136a6bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fa91170684542b79d0b2f18692151cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "310f05c91ee14e4796bfa18897f81e26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "321bfe67e9164827b768be75ad88bfa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33a88395eec948cdbc9cd038a986729d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34e31cad78ea462bb50193a9ee4c1372": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34fa830fe975482cb58bed65ccac69b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04744ff89533496dbe9bc442b25fde47",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8fbdee4d77014de4ae160170f6e336d9",
      "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
     }
    },
    "358ad09a3c4f40f0ab91c20ab2723b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f63b6cc3452e46009e83dbaeb781789b",
       "IPY_MODEL_9dc17414097148179f7f2199e3d6197b",
       "IPY_MODEL_74a8f0039e2e4dcdaf49eb623913a3eb"
      ],
      "layout": "IPY_MODEL_2fa91170684542b79d0b2f18692151cf"
     }
    },
    "35a82e3cee404c0eb8c4a99d48fde443": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "380e6a9baa4141a39ee11b7084c8b5d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_321bfe67e9164827b768be75ad88bfa7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_df1201b85a004294a85925204b9b6438",
      "value": "Downloadingâ€‡data:â€‡100%"
     }
    },
    "38725355a21844458da20ebf9eaf7d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a127e2101914b8aa542ef21c4329c39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b939980516140c588220612194de161": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9918c010fe1f418c831d5fffd0a94180",
      "max": 49969,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90655a74b97944f58cad529fa8adf1fa",
      "value": 49969
     }
    },
    "3bc92134440649aba5cf0ff5f4393c55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d1a950d2faf4a11bcd41af7fd1af8eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3de8812bd93242ebae4410575e0aa9e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e33deff84ee42f88995a7e99869e665": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44d377128d6748d4b871d2efd101dd19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "467126b2609e4b869c6c98f5e0fc7164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f22c1b1564234208b5a6a183c5a3bc99",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a8b8c2a5bfeb49658dd631fea2398907",
      "value": "â€‡400M/400Mâ€‡[00:04&lt;00:00,â€‡67.4MB/s]"
     }
    },
    "469da5951d744acdb7e5472e3fae2cb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4806cee42b644a868e29a19ede8aad49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5380ca6400a1401e8fa1cad69af5c97c",
       "IPY_MODEL_0d920b9593ea41a3ae099ee0c3eb6dbf",
       "IPY_MODEL_cee00c75e5cb405bbd928ff7759c52b8"
      ],
      "layout": "IPY_MODEL_0cc5119ca0244fd59e2b5b3cb4715005"
     }
    },
    "49666651fca242b2b777b1705dd4fa8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ad139d347eb449ab7b4bd6df4f64516": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aeae8a8d48a4d2b8ba1df3c7d3a0309": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d5689dd39d7414dbeb92976c9a682c3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_aa63039256a643b38ff77fe394a6832f",
      "value": "config.json:â€‡100%"
     }
    },
    "4d1479b7392549beb8459113a42e2610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdc71b983e5f48b1800ebdc457f22f3e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3bc92134440649aba5cf0ff5f4393c55",
      "value": "Map:â€‡100%"
     }
    },
    "4e9990fc07da480a96223afe3c85be2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f8d9958f6694a2cb0d55308f6653cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_897760bb1650484bbbd2a604d886b68a",
      "max": 1961691,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea0292273e364ecfb256158407eda9fc",
      "value": 1961691
     }
    },
    "532ef4493746407faee2ccbb8f0c3e17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce1fb1b218b34e63b763bf708b729679",
      "max": 647897,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc91d0bcafa746cfa509e365fadcafe8",
      "value": 647897
     }
    },
    "5380ca6400a1401e8fa1cad69af5c97c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94fa0935dfb84f61b61af5e27396ab65",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6488ca56a5784cad9195c6eb302b0839",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "55b47d1ed657480d9a860b46eaa1d678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "582f0deecef14e1295785bb5f04a1804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e55d09be5944478b959af8ef13008d35",
      "max": 6478,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13e918c3b7d44aaea79540e39173e4e9",
      "value": 6478
     }
    },
    "5ad923a4f66c48f293727c51202e3d8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ef246bb6829e4816a009b3f6875e20b8",
       "IPY_MODEL_aadc85ded2694638a4faa0bb6dc9caa5",
       "IPY_MODEL_c713a841e9cd402bb4f1acfccf9c4a7e"
      ],
      "layout": "IPY_MODEL_34e31cad78ea462bb50193a9ee4c1372"
     }
    },
    "5c9bdfe89d304e7ca23a9ad7ff0302f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e0e10a542654766bf78a9e90c1daa8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e74c24afd604b98b7b5b2083c90209e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5eb4b0b839e94e53adffd6a5a28781b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baee5199c98e443697d179bf127b3798",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_25350d0725054c33b7e2e792190bf039",
      "value": "â€‡111/111â€‡[00:00&lt;00:00,â€‡6.74kB/s]"
     }
    },
    "5f1cbf83549943cfa3b568ccd5cdb34e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5fd4be6bc35d459eb335da817e346724": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0e9aae192fa4c62af7379d157c0893f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5e0e10a542654766bf78a9e90c1daa8a",
      "value": "â€‡49969/49969â€‡[00:01&lt;00:00,â€‡46188.90â€‡examples/s]"
     }
    },
    "6488ca56a5784cad9195c6eb302b0839": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64e8dffcea054a5a8382283c3907443e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66744f20d34f4cd5b75570d269261b94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03137bba9d7d4ef3ae7e073064b5438d",
       "IPY_MODEL_cbece197ea7f41f9855fa94b6e049254",
       "IPY_MODEL_cc5764233d5d4af79c1221d626b21322"
      ],
      "layout": "IPY_MODEL_add58b43a7cd4047adb11778c26812f7"
     }
    },
    "689ab31dd3994630a248b642096bee22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d83eb47f7b34c7c930e63649da55097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e7a3e3b3e0e4b4a825e4905d3d17808": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6fd423f508284764b6e4858dd807ac75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "704e93e3978140e1864b84cf19507b58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74a8f0039e2e4dcdaf49eb623913a3eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b735d6ece794fbea65a3260198c5476",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9272b81465a540a5bf6d3d63b4be4605",
      "value": "â€‡177M/177Mâ€‡[00:01&lt;00:00,â€‡123MB/s]"
     }
    },
    "7621ead96516491db1ebb4fba0055702": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7626531fcae84ddbb8f482b8ac2f7439": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7711ba2b9c82489baefee96b7c64eba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "772ed124915740aeb2f5a614e8cfe2e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "777c1f497f364f04bb3cfc97d937de0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "790cc80cfe574ee58da93f8e54c8517f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7acf551e9f534548a87e3b1a251ffbb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_34fa830fe975482cb58bed65ccac69b9",
       "IPY_MODEL_532ef4493746407faee2ccbb8f0c3e17",
       "IPY_MODEL_df1b49fb70ab495888f5b40b8996512c"
      ],
      "layout": "IPY_MODEL_0a4a905e9ca14b23b8134f4ee1dd51a6"
     }
    },
    "7b735d6ece794fbea65a3260198c5476": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bd3fed632324e11a989409ea8208390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c319ad4809c47189345d0d158dcd922": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d739e3b12b644bcb9777ae231f55347": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89727d06874849f5b681f3705bfe0978": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa120e115285432da310d295df2bb739",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7c319ad4809c47189345d0d158dcd922",
      "value": "Mapâ€‡(num_proc=8):â€‡100%"
     }
    },
    "897760bb1650484bbbd2a604d886b68a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8af4fb16f6ce452eb31c8e2eb8b1b394": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b93d48735a347c1bc7a5b72692387e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e41f5233d7945469c4099b7b1f6cba9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fabd167d1fb498bbe4b304580744580": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fbdee4d77014de4ae160170f6e336d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90655a74b97944f58cad529fa8adf1fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90924adbead14a25bdbc8e0779e6360a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91476a34dbb14bde981fdd7743fb860f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90924adbead14a25bdbc8e0779e6360a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_310f05c91ee14e4796bfa18897f81e26",
      "value": "tokenizer.model:â€‡100%"
     }
    },
    "9272b81465a540a5bf6d3d63b4be4605": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "930c73ca9b5043e5aa1d301d4385eb49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_264288b1c4834e8f9691937bdbf45e6e",
       "IPY_MODEL_bd62e2873ee149a38f0af8ece178f12b",
       "IPY_MODEL_ddfd080832364b008d4c9d5e87adb423"
      ],
      "layout": "IPY_MODEL_d98761db8080484885285be9d309f761"
     }
    },
    "939a4d107645466a8c6633285471ae40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fab0a887002e467786f3f6980aa57d0a",
       "IPY_MODEL_4f8d9958f6694a2cb0d55308f6653cbc",
       "IPY_MODEL_f1e10f0376324720bb85d554f677f5f7"
      ],
      "layout": "IPY_MODEL_35a82e3cee404c0eb8c4a99d48fde443"
     }
    },
    "93d68696e1d345daaa4e819a47166a85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "943df89e90eb4def9bb8414ecfffc681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94fa0935dfb84f61b61af5e27396ab65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "960c9eba4fec494989f66a6c0dd5fce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "964e02c946804bdda9c0b3eef4ca0e7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9656d55a0d3c4fa1bb7a04f5c17b7a84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97f0c9d5be3245fb8b1320cc17818ae2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "987d83a2779c42fd99b862fead155f57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9918c010fe1f418c831d5fffd0a94180": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "993076c1db464de1b7e34faff22b816a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "99a6d2f786504b819609d2062accdd4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9aeafa98e3c5426b8b7d27ae41361233": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b95d3952f9f46b3ab5d11a630fb4ab9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff68b5bdadc143deae98570d99a45bd3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5e74c24afd604b98b7b5b2083c90209e",
      "value": "Downloadingâ€‡data:â€‡100%"
     }
    },
    "9bab4b07eafe4d8b97340a6989755b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8af4fb16f6ce452eb31c8e2eb8b1b394",
      "max": 400000634,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_64e8dffcea054a5a8382283c3907443e",
      "value": 400000634
     }
    },
    "9dc17414097148179f7f2199e3d6197b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_065c41d2f95d4980a3379c179e9bf304",
      "max": 177388905,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e33deff84ee42f88995a7e99869e665",
      "value": 177388905
     }
    },
    "a1b291badc6041beae070c2e262ae005": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6ae977baad248078458445fecc03963",
      "max": 49969,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55b47d1ed657480d9a860b46eaa1d678",
      "value": 49969
     }
    },
    "a3bb01312bcd45649373c3790e327c18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a424608a868c46b2be65767c093c3dd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b93d48735a347c1bc7a5b72692387e7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_960c9eba4fec494989f66a6c0dd5fce5",
      "value": "â€‡49969/49969â€‡[00:01&lt;00:00,â€‡32300.42â€‡examples/s]"
     }
    },
    "a518acc761834cb498f0cce4109eaa8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a55b27398caf4ae39ac5900d22677220": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5ff362daae4430ba27be07304270123": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6babbf2f467476f8a650a0b3cf4b225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d1479b7392549beb8459113a42e2610",
       "IPY_MODEL_a1b291badc6041beae070c2e262ae005",
       "IPY_MODEL_a424608a868c46b2be65767c093c3dd1"
      ],
      "layout": "IPY_MODEL_4ad139d347eb449ab7b4bd6df4f64516"
     }
    },
    "a893e20bedc6404fb2ab9002d7da7090": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e41f5233d7945469c4099b7b1f6cba9",
      "max": 560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_469da5951d744acdb7e5472e3fae2cb2",
      "value": 560
     }
    },
    "a8b8c2a5bfeb49658dd631fea2398907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8c0b5125d83425d95697ad677eb8738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_380e6a9baa4141a39ee11b7084c8b5d1",
       "IPY_MODEL_da4b89a73b6b4963888688cdd2a12a25",
       "IPY_MODEL_e43305a97dc64e4ea8bd507df6667fbd"
      ],
      "layout": "IPY_MODEL_f7060dd3107d49c5b78b5a71051c0b98"
     }
    },
    "a8da1d11887746dbb9eb7a849a5d5c8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9aeafa98e3c5426b8b7d27ae41361233",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3de8812bd93242ebae4410575e0aa9e1",
      "value": "Mapâ€‡(num_proc=2):â€‡100%"
     }
    },
    "aa120e115285432da310d295df2bb739": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa63039256a643b38ff77fe394a6832f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aadc85ded2694638a4faa0bb6dc9caa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f59b229ad1ad4ab088682c83bb6c5ce4",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93d68696e1d345daaa4e819a47166a85",
      "value": 124
     }
    },
    "abd2784d68354f6388adafe797feb706": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac0fdfbd66024fcfb7d84d752b8b65de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16937b1d13dc433ca1605982da3f7587",
      "max": 111,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae325732051f41ed8949a6026a113e95",
      "value": 111
     }
    },
    "add58b43a7cd4047adb11778c26812f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae325732051f41ed8949a6026a113e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af7f3773cf0948b0b9ebd92e76aa1f39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b12e798c92f848bc9e1d430d504ee3c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b168622cd5844e95be35c6476b3c3847": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c9bdfe89d304e7ca23a9ad7ff0302f2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a5ff362daae4430ba27be07304270123",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "b87ddb145c414e53b425345c652699d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbc15025bfb441638bf4fe58cb7d08c1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f4e2e1c634ab4a07af5483b8dccf8b52",
      "value": "â€‡587k/587kâ€‡[00:00&lt;00:00,â€‡27.7MB/s]"
     }
    },
    "b9b86ece90ea4666be175d68bc26a648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba1fd50e24904c6988967eab2ab64225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee3c23c50c5b435bbada1bcec39c23b4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6d83eb47f7b34c7c930e63649da55097",
      "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
     }
    },
    "baee5199c98e443697d179bf127b3798": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce90326f07f4458bf694da20f8c8f00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd62e2873ee149a38f0af8ece178f12b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2b60d8ee8d44f42b915088d7deeed77",
      "max": 130835,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_993076c1db464de1b7e34faff22b816a",
      "value": 130835
     }
    },
    "bdc71b983e5f48b1800ebdc457f22f3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be4f34efcc81414585f78d57ae025a16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_29d5c06b48d04bfa99cefe38030f277c",
       "IPY_MODEL_ac0fdfbd66024fcfb7d84d752b8b65de",
       "IPY_MODEL_5eb4b0b839e94e53adffd6a5a28781b1"
      ],
      "layout": "IPY_MODEL_704e93e3978140e1864b84cf19507b58"
     }
    },
    "be8fed98ffb3473ba6667d2c17b3929b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf79d1ed09134e50b29aa3b6e28ecde5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0e9aae192fa4c62af7379d157c0893f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3e586fbe5d34ecdbcc05a2ecbc17fc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba1fd50e24904c6988967eab2ab64225",
       "IPY_MODEL_f611865e89774fe4ba6dc047f97eaa61",
       "IPY_MODEL_5fd4be6bc35d459eb335da817e346724"
      ],
      "layout": "IPY_MODEL_e91b73923c044243b5ed8efcfa252f2f"
     }
    },
    "c4228273bcdc4d619d23112c5c9a14f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6bab59f1a624d21a6a09f94fbe99e58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c713a841e9cd402bb4f1acfccf9c4a7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3bb01312bcd45649373c3790e327c18",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_49666651fca242b2b777b1705dd4fa8a",
      "value": "â€‡124/124â€‡[00:00&lt;00:00,â€‡7.86kB/s]"
     }
    },
    "cbece197ea7f41f9855fa94b6e049254": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7626531fcae84ddbb8f482b8ac2f7439",
      "max": 51617069,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_119f3c027e3f4beda1c026fd3ebb3f7c",
      "value": 51617069
     }
    },
    "cc5764233d5d4af79c1221d626b21322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97f0c9d5be3245fb8b1320cc17818ae2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_be8fed98ffb3473ba6667d2c17b3929b",
      "value": "â€‡51.6M/51.6Mâ€‡[00:00&lt;00:00,â€‡93.5MB/s]"
     }
    },
    "ce1fb1b218b34e63b763bf708b729679": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce88a214d07e42399d9edd549175b760": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cee00c75e5cb405bbd928ff7759c52b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6bab59f1a624d21a6a09f94fbe99e58",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ce88a214d07e42399d9edd549175b760",
      "value": "â€‡4.14G/4.14Gâ€‡[00:30&lt;00:00,â€‡252MB/s]"
     }
    },
    "d208453e5609482c9c5113d3b3c84a6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d49a277941384dfeb7907b9017d16ecf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6217028b7744abdb85c305237cb52a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d62c059e650c44ed9bdfd9ea259724ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b168622cd5844e95be35c6476b3c3847",
       "IPY_MODEL_2a73eec6178640ccac73682222f36a56",
       "IPY_MODEL_0c3a85239fa94b8a85545c8eec38d887"
      ],
      "layout": "IPY_MODEL_689ab31dd3994630a248b642096bee22"
     }
    },
    "d98761db8080484885285be9d309f761": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da4b89a73b6b4963888688cdd2a12a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99a6d2f786504b819609d2062accdd4d",
      "max": 205287522,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e7a3e3b3e0e4b4a825e4905d3d17808",
      "value": 205287522
     }
    },
    "ddfd080832364b008d4c9d5e87adb423": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fabd167d1fb498bbe4b304580744580",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_964e02c946804bdda9c0b3eef4ca0e7a",
      "value": "â€‡131k/131kâ€‡[00:00&lt;00:00,â€‡837kB/s]"
     }
    },
    "df1201b85a004294a85925204b9b6438": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df1b49fb70ab495888f5b40b8996512c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a518acc761834cb498f0cce4109eaa8f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7711ba2b9c82489baefee96b7c64eba6",
      "value": "â€‡647897/647897â€‡[00:15&lt;00:00,â€‡99743.43â€‡examples/s]"
     }
    },
    "e185802b926d44cfaeadf604733d4a5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4aeae8a8d48a4d2b8ba1df3c7d3a0309",
       "IPY_MODEL_05eca118bd194c2c9d5f7d825bf0493f",
       "IPY_MODEL_27ea6ee950254229ae3493c862fabab7"
      ],
      "layout": "IPY_MODEL_3d1a950d2faf4a11bcd41af7fd1af8eb"
     }
    },
    "e2b60d8ee8d44f42b915088d7deeed77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e43305a97dc64e4ea8bd507df6667fbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fd423f508284764b6e4858dd807ac75",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_af7f3773cf0948b0b9ebd92e76aa1f39",
      "value": "â€‡205M/205Mâ€‡[00:02&lt;00:00,â€‡93.8MB/s]"
     }
    },
    "e55d09be5944478b959af8ef13008d35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e57c379628ce49da904b447a74b0e1b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e66a1bb087944ceca9cbe6517ac19292": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6ae977baad248078458445fecc03963": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e91b73923c044243b5ed8efcfa252f2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea0292273e364ecfb256158407eda9fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eaf2f4d59814487aa8ea1abbf8831da1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebb68cccf04641ed8996e9c9b62c67be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed28850ca87b4c768996f50844c73afe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed92ddd32f6e4301a46e0178f12e5a8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee3c23c50c5b435bbada1bcec39c23b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef246bb6829e4816a009b3f6875e20b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_772ed124915740aeb2f5a614e8cfe2e1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bf79d1ed09134e50b29aa3b6e28ecde5",
      "value": "Downloadingâ€‡readme:â€‡100%"
     }
    },
    "efd751b6c2a642ff94845df115af4ed7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f09ab65302ea49f38ad65c7285439b66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89727d06874849f5b681f3705bfe0978",
       "IPY_MODEL_3b939980516140c588220612194de161",
       "IPY_MODEL_fed11544e08c4fb3bf9fe247f7c37cd1"
      ],
      "layout": "IPY_MODEL_1531e137a0b84e3db00599d6311fbc04"
     }
    },
    "f1e10f0376324720bb85d554f677f5f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9656d55a0d3c4fa1bb7a04f5c17b7a84",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_108675c4393b4b678a5cfa2f73ce9f4f",
      "value": "â€‡1.96M/1.96Mâ€‡[00:00&lt;00:00,â€‡10.5MB/s]"
     }
    },
    "f1e7a220b6c14349b5ec041d1cce8889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f22c1b1564234208b5a6a183c5a3bc99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4e2e1c634ab4a07af5483b8dccf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f59b229ad1ad4ab088682c83bb6c5ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f611865e89774fe4ba6dc047f97eaa61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7621ead96516491db1ebb4fba0055702",
      "max": 49969,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_790cc80cfe574ee58da93f8e54c8517f",
      "value": 49969
     }
    },
    "f63b6cc3452e46009e83dbaeb781789b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bce90326f07f4458bf694da20f8c8f00",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7d739e3b12b644bcb9777ae231f55347",
      "value": "Downloadingâ€‡data:â€‡100%"
     }
    },
    "f7060dd3107d49c5b78b5a71051c0b98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7187ecc5d974ff18fc70470d1a48464": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fab0a887002e467786f3f6980aa57d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44d377128d6748d4b871d2efd101dd19",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c4228273bcdc4d619d23112c5c9a14f0",
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "fbc15025bfb441638bf4fe58cb7d08c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc91d0bcafa746cfa509e365fadcafe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fdad79e64e214bfbb4abda9b3eee7eb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fed11544e08c4fb3bf9fe247f7c37cd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e57c379628ce49da904b447a74b0e1b1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d6217028b7744abdb85c305237cb52a0",
      "value": "â€‡49969/49969â€‡[01:55&lt;00:00,â€‡1256.37â€‡examples/s]"
     }
    },
    "ff68b5bdadc143deae98570d99a45bd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
