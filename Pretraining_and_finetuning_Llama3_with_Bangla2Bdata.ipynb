{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49031,
     "status": "ok",
     "timestamp": 1717658104173,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "2eSvM9zX_2d3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" #\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Home Directory: ['/workspace/.cache/huggingface/']\n",
      "Updated PATH: ['/workspace/anaconda3/envs/autotrain/bin:/workspace/anaconda3/envs/autotrain/bin:/workspace/anaconda3/envs/autotrain/bin:/workspace/anaconda3/bin:/workspace/anaconda3/bin:/workspace/anaconda3/condabin:/workspace/anaconda3/bin:/usr/local/cuda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:22:51] Energy consumed for RAM : 0.084772 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 15:22:51] Energy consumed for all GPUs : 0.017829 kWh. Total GPU Power : 67.04652842891704 W\n",
      "[codecarbon INFO @ 15:22:51] Energy consumed for all CPUs : 0.015804 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 15:22:51] 0.118405 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface_hub in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: filelock in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:22:58] Energy consumed for RAM : 0.273759 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 15:22:58] Energy consumed for all GPUs : 0.096732 kWh. Total GPU Power : 66.54437172227186 W\n",
      "[codecarbon INFO @ 15:22:58] Energy consumed for all CPUs : 0.051045 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 15:22:58] 0.421536 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the PIP_CACHE_DIR environment variable\n",
    "os.environ['PIP_CACHE_DIR'] = '/workspace/.pip/'\n",
    "# Set the HF_HOME environment variable to the desired directory\n",
    "os.environ['HF_HOME'] = '/workspace/.cache/huggingface/'\n",
    "\n",
    "# Add /workspace/anaconda3/envs/autotrain/bin/ to the PATH environment variable\n",
    "os.environ['PATH'] = '/workspace/anaconda3/envs/autotrain/bin:' + os.environ['PATH']\n",
    "\n",
    "# Verify the changes\n",
    "hf_home_dir = !echo $HF_HOME\n",
    "path_dirs = !echo $PATH\n",
    "print(\"Hugging Face Home Directory:\", hf_home_dir)\n",
    "print(\"Updated PATH:\", path_dirs)\n",
    "\n",
    "# Install huggingface-cli\n",
    "!pip install huggingface_hub\n",
    "\n",
    "# Verify the installation\n",
    "!huggingface-cli --version\n",
    "\n",
    "# Test the Hugging Face CLI\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  2 16:09:10 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              60W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Delete model and other training-related objects\n",
    "try:\n",
    "    #del model\n",
    "    del trainer\n",
    "    #del tokenizer\n",
    "    #del train_dataset\n",
    "except NameError:\n",
    "    pass  # Ignore if they are not defined\n",
    "\n",
    "# Invoke garbage collector multiple times\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Reset the CUDA runtime\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Optionally, you can use nvidia-smi command to kill all processes using GPU\n",
    "import os\n",
    "os.system('nvidia-smi')\n",
    "\n",
    "# Optionally, restart the notebook kernel (uncomment if necessary)\n",
    "# import IPython\n",
    "# IPython.display.clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /workspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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 --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/.cache/huggingface/\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367,
     "referenced_widgets": [
      "e185802b926d44cfaeadf604733d4a5d",
      "4aeae8a8d48a4d2b8ba1df3c7d3a0309",
      "05eca118bd194c2c9d5f7d825bf0493f",
      "27ea6ee950254229ae3493c862fabab7",
      "3d1a950d2faf4a11bcd41af7fd1af8eb",
      "1d5689dd39d7414dbeb92976c9a682c3",
      "aa63039256a643b38ff77fe394a6832f",
      "987d83a2779c42fd99b862fead155f57",
      "943df89e90eb4def9bb8414ecfffc681",
      "d49a277941384dfeb7907b9017d16ecf",
      "efd751b6c2a642ff94845df115af4ed7",
      "4806cee42b644a868e29a19ede8aad49",
      "5380ca6400a1401e8fa1cad69af5c97c",
      "0d920b9593ea41a3ae099ee0c3eb6dbf",
      "cee00c75e5cb405bbd928ff7759c52b8",
      "0cc5119ca0244fd59e2b5b3cb4715005",
      "94fa0935dfb84f61b61af5e27396ab65",
      "6488ca56a5784cad9195c6eb302b0839",
      "a55b27398caf4ae39ac5900d22677220",
      "f7187ecc5d974ff18fc70470d1a48464",
      "c6bab59f1a624d21a6a09f94fbe99e58",
      "ce88a214d07e42399d9edd549175b760",
      "be4f34efcc81414585f78d57ae025a16",
      "29d5c06b48d04bfa99cefe38030f277c",
      "ac0fdfbd66024fcfb7d84d752b8b65de",
      "5eb4b0b839e94e53adffd6a5a28781b1",
      "704e93e3978140e1864b84cf19507b58",
      "4e9990fc07da480a96223afe3c85be2f",
      "abd2784d68354f6388adafe797feb706",
      "16937b1d13dc433ca1605982da3f7587",
      "ae325732051f41ed8949a6026a113e95",
      "baee5199c98e443697d179bf127b3798",
      "25350d0725054c33b7e2e792190bf039",
      "d62c059e650c44ed9bdfd9ea259724ef",
      "b168622cd5844e95be35c6476b3c3847",
      "2a73eec6178640ccac73682222f36a56",
      "0c3a85239fa94b8a85545c8eec38d887",
      "689ab31dd3994630a248b642096bee22",
      "5c9bdfe89d304e7ca23a9ad7ff0302f2",
      "a5ff362daae4430ba27be07304270123",
      "1dd43f29b71042df88ff4c49af908ce3",
      "777c1f497f364f04bb3cfc97d937de0a",
      "e66a1bb087944ceca9cbe6517ac19292",
      "38725355a21844458da20ebf9eaf7d21",
      "2a0095c0f676406c9799779a33225860",
      "91476a34dbb14bde981fdd7743fb860f",
      "2a68f7129c5a4e3bb75c4469a8eda942",
      "b87ddb145c414e53b425345c652699d1",
      "fdad79e64e214bfbb4abda9b3eee7eb6",
      "90924adbead14a25bdbc8e0779e6360a",
      "310f05c91ee14e4796bfa18897f81e26",
      "d208453e5609482c9c5113d3b3c84a6d",
      "b9b86ece90ea4666be175d68bc26a648",
      "fbc15025bfb441638bf4fe58cb7d08c1",
      "f4e2e1c634ab4a07af5483b8dccf8b52",
      "10a7046d9e1c4ada876b2b65e4169de9",
      "026e66bb6fb64494b389198e3f64504b",
      "a893e20bedc6404fb2ab9002d7da7090",
      "1df6a414771544528de945d997ea18a5",
      "ed92ddd32f6e4301a46e0178f12e5a8d",
      "33a88395eec948cdbc9cd038a986729d",
      "2ba358a81db741b7963d4e14136a6bff",
      "8e41f5233d7945469c4099b7b1f6cba9",
      "469da5951d744acdb7e5472e3fae2cb2",
      "eaf2f4d59814487aa8ea1abbf8831da1",
      "5f1cbf83549943cfa3b568ccd5cdb34e",
      "939a4d107645466a8c6633285471ae40",
      "fab0a887002e467786f3f6980aa57d0a",
      "4f8d9958f6694a2cb0d55308f6653cbc",
      "f1e10f0376324720bb85d554f677f5f7",
      "35a82e3cee404c0eb8c4a99d48fde443",
      "44d377128d6748d4b871d2efd101dd19",
      "c4228273bcdc4d619d23112c5c9a14f0",
      "897760bb1650484bbbd2a604d886b68a",
      "ea0292273e364ecfb256158407eda9fc",
      "9656d55a0d3c4fa1bb7a04f5c17b7a84",
      "108675c4393b4b678a5cfa2f73ce9f4f"
     ]
    },
    "executionInfo": {
     "elapsed": 72427,
     "status": "ok",
     "timestamp": 1717658176589,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "4807874d-7225-42ac-e0d3-fe7e3f2de8cd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8b9b0cdfdf438aa4f1f02f3648f4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "meta-llama/Meta-Llama-3-8B does not have a padding token! Will use pad_token = <|reserved_special_token_250|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    #model_name = \"BanglaLLM/BanglaLLama-3-8b-BnWiki-Base\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"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\" # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/logs/*\n"
     ]
    }
   ],
   "source": [
    "log_file_base = \"/workspace/logs/\"\n",
    "existing_log_files = log_file_base + \"*\"\n",
    "print(existing_log_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file should be at: /workspace/logs/BanglaLLM_2024-07-02_16:11:25.904797.log\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback, TrainingArguments, logging as hf_logging\n",
    "from unsloth import is_bfloat16_supported, UnslothTrainer, UnslothTrainingArguments\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Setup the log file name\n",
    "log_file_base = \"/workspace/logs/BanglaLLM_\"\n",
    "log_file = log_file_base + \"_\".join(str(datetime.datetime.now()).split()) + \".log\"\n",
    "\n",
    "existing_log_files = log_file_base + \"*\"\n",
    "!rm -rf $existing_log_files\n",
    "\n",
    "# Create a dedicated logger\n",
    "logger = logging.getLogger('BanglaLLMLogger')\n",
    "logger.setLevel(logging.DEBUG)  # Set to DEBUG to capture all levels of logs\n",
    "\n",
    "# Create handlers\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatters and add it to the handlers\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Redirect Hugging Face Transformers logging to our logger\n",
    "transformers_logger = hf_logging.get_logger()\n",
    "transformers_logger.setLevel(logging.DEBUG)\n",
    "for handler in logger.handlers:\n",
    "    transformers_logger.addHandler(handler)\n",
    "\n",
    "print(f\"Log file should be at: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "\n",
    "We also add `embed_tokens` and `lm_head` to allow the model to learn out of distribution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15122,
     "status": "ok",
     "timestamp": 1717658191701,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "1f0fc2ba-b073-45e5-eba1-7fc51ff64c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,   # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): ModulesToSaveWrapper(\n",
       "          (original_module): Embedding(128256, 4096)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Embedding(128256, 4096)\n",
       "          )\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    PeftModelForCausalLM,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, get_last_checkpoint\n",
    "from transformers.utils import send_example_telemetry\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bangla_2b_data_path='/workspace/data/Bangla2B+/shards'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.txt\t     3256070.txt  5860926.txt  7814568.txt  cache\n",
      "1302428.txt  3907284.txt  651214.txt   8465782.txt  chunked_dataset\n",
      "1953642.txt  4558498.txt  6512140.txt  9116996.txt  parquet_files\n",
      "2604856.txt  5209712.txt  7163354.txt  9768210.txt\n"
     ]
    }
   ],
   "source": [
    "!ls $bangla_2b_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My inquiries show beyond all question that the family portrait did not lie, and that this fellow was indeed a Baskerville. আমার তদন্তের ফলে প্রশ্নাতীতভাবে একটা সিদ্ধান্ত স্পষ্ট হয়ে উঠেছিল-পারিবারিক প্রতিকৃতি কখনো মিথ্যে বলতে পারে না।\n",
      "\n",
      "বাঙালি ক্রিকেট দেখতে ভালবাসে, খেলতে নয়: সৌরভ\n",
      "এখনও ভালবাসেন ২২ গজ, সবুজ ময়দান দেখলেই মনে হয় লন্ডনের হাইড পার্ক।\n",
      "এক মোবাইল প্রস্তুতকারক সংস্থার হ্যাশট্যাগ 'অন দি গো' অনুষ্ঠানে স্মৃতিমেদুর সৌরভ জানালেন তাঁর ক্রিকেট অনুরাগের কথা।\n",
      "একই সঙ্গে মত জানালেন বাঙালির ক্রিকেট প্রেম নিয়েও।\n",
      "আরও পড়ুন- বিরাট নায়ক হতে পারতেন, শাহরুখ ক্রিকেটার, 'উলটপূরাণ পছন্দ' গাঙ্গুলির\n",
      "'বাঙালি ক্রিকেট দেখতে ভালবাসে, খেলতে নয়'।\n",
      "আপনি সহমত না হলেও 'প্রিন্স অব কলকাতা' ভাবেন এমনটাই।\n",
      "পোস্তপ্রেমী সৌরভ মনে করেন, ক্রিকেট খেলতে গেলে যা ঘাম ঝরাতে হয়, যে পরিমাণ পরিশ্রম করতে হয়, সুখী বাঙালি এই ঝক্কিটাই এখন নেয় না।\n",
      "বরং রসিক বাঙালি অনেক বেশি 'অ্যাক্টিভ' ভোজনে।\n",
      "তবে শহরবাসীর ক্রিকেট অনুরাগ নিয়ে কোনও সন্দেহই নেই মহারাজের।\n",
      "অলি-গলির পাড়া ক্রিকেট থেকে ২২ গজের মহাযুদ্ধ, স্রেফ গলা ফাটিয়েই দলকে জিতিয়ে দেওয়াক ক্ষমতা রাখে কলকাতা।\n",
      "ইডেন তো বটেই এমনকী গোটা দেশের ক্রিকেট অনুরাগ নিয়েই একই মত সৌরভ গাঙ্গুলির। Also Watch\n",
      "\n",
      "রাফাল চুক্তির পরই 'তোফা' অনিল অম্বানীকে, ১১০০ কোটি টাকার কর মকুব!\n",
      "রাফাল-কাণ্ড যেন কিছুতেই পিছু ছাড়ছে না নরেন্দ্র মোদী সরকারের!\n",
      "লোকসভা নির্বাচনের প্রথম দফা ভোটগ্রহণের ঠিক আগে সুপ্রিম কোর্ট জানিয়েছিল, আদালত রাফাল-চুক্তির 'চুরি যাওয়া' নথি খতিয়ে দেখবে।\n",
      "যে নথিতে ছিল, রাফাল যুদ্ধবিমান কেনার জন্য প্রতিরক্ষা মন্ত্রকের দর কষাকষিতে প্রধানমন্ত্রীর দফতর নাক গলিয়েছিল।\n",
      "এ বার দ্বিতীয় দফার ভোটগ্রহণের পাঁচ দিন আগে ফরাসি সংবাদপত্র 'ল্য মোঁদ'-এর রিপোর্ট জানাল, প্রধানমন্ত্রী নরেন্দ্র মোদী ফ্রান্সের দাসো সংস্থার থেকে ৩৬টি রাফাল যুদ্ধবিমান কেনার সিদ্ধান্ত ঘোষণার কয়েক মাস পরেই ফ্রান্স সরকার অনিল অম্বানীর একটি সংস্থাকে ১৪৩.৭ মিলিয়ন ইউরোর পাওনা কর মকুব করে দিয়েছিল।\n",
      "ভারতীয় মুদ্রায় যার পরিমাণ অন্তত ১১০০ কোটি টাকা।\n",
      "২০১৫-র এপ্রিলে মনমোহন সিংহ সরকারের ১২৬টি রাফাল কেনার সিদ্ধান্ত বাতিল করে দিয়ে দাসো-র থেকে ৩৬টি যুদ্ধবিমান কেনার সিদ্ধান্ত ঘোষণা করেন মোদী।\n",
      "ল্য মোঁদ-এর রিপোর্ট অনুযায়ী, তার ছয় মাসের মধ্যেই, অক্টোবরে অনিল অম্বানীর ফরাসি টেলিযোগাযোগ সংস্থা 'রিলায়্যান্স আটলান্টিক ফ্ল্যাগ ফ্রান্স' সংস্থাকে ১৪৩.৭ মিলিয়ন ইউরো মকুব করে দেয় ফ্রান্স।\n",
      "সংবাদপত্রটির দাবি, অথচ ওই বছরের ফেব্রুয়ারিতেই অনিলের সংস্থার বিরুদ্ধে দু'দফায় তদন্ত চালিয়ে থেকে ফ্রান্স সরকার ১৫১ মিলিয়ন ইউরো বকেয়া কর দাবি করেছিল।\n",
      "প্রথমে অনিলের সংস্থা মাত্র ৭.৬ মিলিয়ন ইউরো দিতে চাইলেও তাতে ফ্রান্স সরকার রাজি হয়নি।\n",
      "অথচ পরে তার থেকেও কম অঙ্ক, ৭.৩ মিলিয়ন ইউরোতেই রাজি হয় তারা।\n",
      "দিল্লি দখলের লড়াই, লোকসভা নির্বাচন ২০১৯\n",
      "এই তথ্য আজ রাহুল গাঁধীর কংগ্রেসের হাতে ফের নতুন অস্ত্র দিয়েছে।\n",
      "কংগ্রেস মুখপাত্র রণদীপ সিংহ সুরজেওয়ালার প্রশ্ন, ''প্রধানমন্ত্রী কি অনিল অম্বানীর 'মিডল ম্যান' হিসেবে কাজ করছিলেন?\n",
      "ফ্রান্সের থেকে যুদ্ধবিমান কেনার বিনিময়ে কি অনিল অম্বানীর জন্য এই কর ছাড় আদায় করা হয়েছিল?'' তাঁর কটাক্ষ, ''এ বার তো স্পষ্ট যে, এক জন চৌকিদারই চোর।\n",
      "এ হল মোদী-কৃপা!\n",
      "এই জন্যই বলা হচ্ছে, মোদী হ্যায় তো মুমকিন হ্যায়!'' সিপিএমের সীতারাম ইয়েচুরির অভিযোগ, ''মানুষের টাকায় মাত্রাছাড়া দামে রাফাল কেনা হয়েছে।\n",
      "যাতে সেই টাকায় এক জন ব্যবসায়ীর সুবিধা হয়, ফ্রান্স সরকার তাঁর কর মকুব করে দিতে পারে।'\n",
      "' বিরোধীরা বলছেন, রাফালের সঙ্গে ১১০০ কোটি করছাড়ের 'পুরস্কার' পেয়েছেন অনিল!\n",
      "ইয়েচুরির অভিযোগ, ''বায়ুসেনাকে টপকে রাফাল দুর্নীতিতে সরাসরি যুক্ত ছিল প্রধানমন্ত্রীর দফতর।\n",
      "জাতীয় নিরাপত্তা উপদেষ্টা বেআইনি ভাবে প্যারিসে বসে দর কষাকষি করেছিলেন।'\n",
      "' সুরজেওয়ালা ও ইয়েচুরি মনে করিয়ে দিয়েছেন, প্রাক্তন ফরাসি প্রেসিডেন্ট ওলাঁদ আগেই বলেছেন যে, মোদীই দাবি তুলেছিলেন, রাফাল-চুক্তির বরাত অনিল অম্বানীর সংস্থাকে দিতে হবে।\n",
      "কর ছাড়ের কথা স্বীকার করেও অনিল অম্বানীর রিলায়্যান্স কমিউনিকেশনস-এর মুখপাত্র জানান, এতে তাঁরা কোনও বাড়তি সুবিধা পাননি।\n",
      "রিলায়্যান্স আটলান্টিক ফ্ল্যাগ ফ্রান্সের ক্ষতির পরিমাণ ছিল ২০ কোটি টাকা।\n",
      "ফ্রান্সের কর দফতর ১০ বছরের জন্য প্রায় ১১০০ কোটি টাকা দাবি করে। তা বেআইনি।\n",
      "পরে ফ্রান্সের আইন মেনেই ৫৬ কোটিতে রফা হয় বলে দাবি তাঁর।\n",
      "ফরাসি দূতাবাস রাতে সংবাদ সংস্থাকে জানায়, অনিলের সংস্থাকে করছাড় দেওয়ার ব্যাপারে কোনও রাজনৈতিক হস্তক্ষেপ কাজ করেনি।\n",
      "নয়া অভিযোগ নিয়ে বিজেপি মুখ না খুললেও প্রতিরক্ষা মন্ত্রক বিবৃতি দিয়ে জানিয়েছে, ওই কর মকুবের সঙ্গে রাফাল কেনার সম্পর্ক নেই।\n",
      "কংগ্রেস নেতা সুরজেওয়ালার মন্তব্য, ''এ তো ঠাকুর ঘরে কে, আমি তো কলা খাইনি!\n",
      "প্রতিরক্ষা মন্ত্রক কেন বেসরকারি সংস্থার মুখপাত্র হিসেবে কথা বলছে!'' আইনজীবী প্রশান্ত ভূষণের কটাক্ষ, ''চৌকিদারের বন্ধু সত্যিই ভাগ্যবান!''\n",
      "\n",
      "Not only do such ones fail to find true happiness but, worse still, they will soon pass away with this world and all its selfish desires.- Matthew 6:19, 20; 1 John 2:15-17. এইধরনের ব্যক্তিরা যে শুধু প্রকৃত সুখ পেতেই ব্যর্থ হয় তা নয় কিন্তু আরও শোচনীয় হল যে, খুব শীঘ্রই এই জগৎ ও এর স্বার্থপর আকাঙ্ক্ষার সঙ্গে সঙ্গে তারাও শেষ হয়ে যাবে।- মথি ৬:১৯, ২০; ১ যোহন ২:১৫-১৭.\n",
      "\n",
      "অনেক অলঙ্করণ ইতোমধ্যে নষ্ট হয়ে গেলেও মসজিদের ভেতরে ও বাইরে এখনও প্রচুর পরিমাণে বিদ্যমান। Although much of the ornamentation has already disappeared, quite a bit still exists both in the interior and exterior of the building.\n",
      "\n",
      "অ্যাপের মাধ্যমে জাকাত সদকা দেওয়ার সুবিধা চালু করলো রবি\n",
      "পবিত্র রমজান মাসে গ্রাহকরা যাতে সহজেই দরিদ্র মানুষের পাশে দাঁড়াতে পারেন এজন্য একটি উদ্ভাবনী ডিজিটাল সেবা এনেছে মোবাইল ফোন অপারেটর রবি।\n",
      "এর মাধ্যমে দেশে প্রথমবারের মতো ডিজিটাল উপায়ে জাকাত ও সদকা প্রদান করতে পারবেন রবি'র গ্রাহকরা।\n",
      "ইসলামিক লাইফস্টাইল অ্যাপ নূর ব্যবহার করে গ্রাহকরা নির্ভরযোগ্য কয়েকটি দাতব্য সংস্থায় এ অনুদান দিতে পারবেন।\n",
      "অনলাইন জাকাত ও সদকা ফিচারের পাশাপাশি রবি বিদ্যানন্দ ফাউন্ডেশনের সহায়তায় দেশে প্রথমবারের মতো চালু করতে যাচ্ছে ডিজিটাল ইফতার ভেন্ডিং মেশিন- আমার ইফতার।\n",
      "এর মাধ্যমে রমজান মাসজুড়ে ইফতার পাবে পথশিশু ও সমাজের সুবিধাবঞ্চিত বয়স্ক ব্যক্তিরা।\n",
      "রিচার্জ বান্ডল কেনার মাধ্যমে 'আমার ইফতার' উদ্যোগে অনুদানের সুযোগ পাবেন রবি'র গ্রাহকরা।\n",
      "রাজধানীর একটি হোটেলে সোমবার (৬ মে) এই উদ্যোগটি উদ্বোধন করা হয়।\n",
      "অনুষ্ঠানে প্রধান অতিথি ছিলেন জাতীয় রাজস্ব বোর্ডের (এনবিআর) চেয়ারম্যান মো. মোশাররফ হোসেন ভূঁইয়া।\n",
      "এ সময় রবি'র ব্যবস্থাপনা পরিচালক ও প্রধান নির্বাহী মাহতাব উদ্দিন আহমেদ উপস্থিত ছিলেন।\n",
      "এনবিআর চেয়ারম্যান মো. মোশাররফ হোসেন ভূঁইয়া বলেন, 'অ্যাপসের মাধ্যমে সাধারণ মানুষের দান-অনুদানের যে মহৎ উদ্যোগ রবি নিয়েছে সে জন্য তাদের ধন্যবাদ।\n",
      "এমন মহৎ উদ্যোগ প্রচারের দায়িত্ব প্রাথমিকভাবে রবির হলেও একই সঙ্গে গণমাধ্যম এবং যে পাঁচটি প্রতিষ্ঠান এই অনুদান পাবে তাদেরও এ দায়িত্ব নিতে হবে।'\n",
      "ডিজিটাল ইফতার ভেন্ডিং মেশিনগুলো ঢাকা, চট্টগ্রাম, কক্সবাজার, রংপুর ও রাজশাহীর বিভিন্ন স্থানে স্থাপন করা হবে।\n",
      "বিদ্যানন্দ ফাউন্ডেশনের স্বেচ্ছাসেবকরা সুবিধাবঞ্চিত শিশু ও বয়স্কদের বায়োমেট্রিক পদ্ধতিতে নিবন্ধন করবেন।\n",
      "ভেন্ডিং মেশিনে তাদের আঙুলের ছাপ দিয়ে নির্ধারিত ইফতার সংগ্রহ করতে পারবেন নিবন্ধিত সুবিধাভোগীরা।\n",
      "রবি'র গ্রাহকরা নূর অ্যাপের মাধ্যমে তাদের জন্য প্রযোজ্য জাকাতের পরিমাণ হিসাব করতে এবং ডিজিটাল পেমেন্ট গেটওয়ের মাধ্যমে দিতে পারবেন।\n",
      "নূর অ্যাপের মাধ্যমে সদকা প্রদানেরও সুযোগ পাবেন রবি গ্রাহকরা।\n",
      "জাকাত ও সদকা হিসেবে সংগৃহীত সব অনুদান আহছানিয়া মিশন ক্যানসার অ্যান্ড জেনারেল হাসপাতাল, বিদ্যানন্দ ফাউন্ডেশন, চাইল্ড অ্যান্ড ওল্ড এজ কেয়ার, স্কলার্স স্পেশাল স্কুল ফর স্পেশাল নিডস চিলড্রেন, রহমত-ই-আলম মিশন ও ইসলাম মিশন এতিমখানায় পাঠানো হবে।\n",
      "নূর অ্যাপ প্ল্যাটফর্ম ব্যবহার করে জাকাত বা সদকা পরিশোধের জন্য গ্রাহকদের রবিকে কোনও ধরনের চার্জ দিতে হবে না (তবে কার্ডের মাধ্যমে অর্থ প্রদানের ক্ষেত্রে সংশ্লিষ্ট ব্যাংক বা পেমেন্ট কোম্পানি চার্জ নিতে পারে)।\n",
      "\n",
      "২৪ ঘণ্টায় দেশে করোনায় আক্রান্ত ১৫৪১, মৃত্যু ২২\n",
      "চীনের উহান শহর থেকে সারাবিশ্বে ছড়িয়ে পড়া কারোন ভাইরাস বাংলাদেশেও সংক্রমণ শুরু করেছে।\n",
      "ইতোমধ্যে দেশের ৬৪ জেলায় ছড়িয়ে পড়েছে এই ভাইরাস।\n",
      "গত ২৪ ঘণ্টায় দেশে করোনায় আক্রান্ত হয়ে নতুন করে মারা গেছে ২২ জন।\n",
      "এ নিয়ে দেশে মোট করোনা আক্রান্ত হয়ে মারা গেল ৫৪৪ জন।\n",
      "এছাড়া এই ২৪ ঘণ্টায় দেশে নতুন করে আরও ১৫৪১ জন করোনা রোগী শনাক্ত হয়েছে।\n",
      "এ নিয়ে দেশে মোট করোনাক্রান্তের সংখ্যা ৩৮ হাজার ২৯২ জন।\n",
      "আজ বুধবার (২৭ মে) দুপুরে করোনা নিয়ে নিয়মিত অনলাইন বুলেটিনে স্বাস্থ্য অধিদপ্তরের অতিরিক্ত মহাপরিচালক অধ্যাপক ডা. নাসিমা সুলতানা এসব তথ্য জানান।\n",
      "\n",
      "- তাহলে আমি একাই যাবো। - Then I'll go alone.\n",
      "\n",
      "Without that , the program might already be closed . There 's a good chance that the program would 've been closed without that . কোকো-এন মূলত নয়টি স্বতন্ত্র উদ্যান নিয়ে গঠিত, যার প্রত্যেকটির একটি বিশেষ থিম রয়েছে যেমন বাঁশ, পাইন গাছ, চারা, গ্রীষ্মের গাছ এবং ফুল। কোকো-এন গার্ডেনে মজার থিম আছে, তাই আপনি জানেন আপনি কোথায় যাচ্ছেন।\n",
      "\n",
      "শান্তি ও আমাদের কর্ম!\n",
      "ইসলাম যদি শান্তির ধর্ম হয়, তাহলে সেই শান্তির কতটুকু তাতপর্য আমরা মুসলমানরা ব্যাখ্যা দিতে পারব?\n",
      "থাপ্পড়ের জবাব যেমন থাপ্পড়ে হয়না, তেমনি ভুলের জবাব ভুল দিয়ে হয়না।\n",
      "সমাজে নানান রকমের নানান পেশার মানুষ রয়েছেন, কেউ আমরা এক রকমের নই।\n",
      "নাসিরনগরের এক হিন্দু যুবক আমাদের পবিত্র কাবা শরীফ নিয়ে ব্যাঙ করেছেন আর সেই প্রতিশোধের আগুন জ্বালিয়ে দিলেন কিছু নিরীহ মানুষের মনে।\n",
      "মারামারি ভাংচুর করে নিজেকে প্রমাণিত করে দিলেন আপনি কতটা নিচু মনের অধিকারী।\n",
      "ইসলামের তাতপর্য কি এই?\n",
      "আমাদের আইকন মহাবিশ্বের মহানবী হজরত মুহাম্মদ (সা: ) তার চলমান জীবন বানী উদারতা থেকে আমরা তো এটা শিখিনি...\n",
      "মহানবীর বিদায় ভাষনের সেই কথা গুলো থেকে আমরা কি শিক্ষা গ্রহণ করেছি।\n",
      "একটা মানুষের জন্য আপনারা আজ পুরো জাতিকে ছোট করে ফেললেন।\n",
      "মানববন্ধন কিংবা আইনিভাবে ব্যাবস্থা নেয়া যেতো না?\n",
      "দেশের ভাবমূর্তি আমরা জাতি হিসাবে কেমন তা বারবার আমাদের কিছু সাংবাদিকতা নির্লজ্জতার প্রমান করে দিলেন।\n",
      "করবেই না কেন?\n",
      "ভাইরাল করে নিজেকে বড় কিছু ভাবার সুযোগ তো এটাই।\n",
      "বর্তমান দেশের চলমান পরিস্থিতি নিয়ে কি একবারেও জন্য ভাবেননি।\n",
      "মনে রাখুন আমরা মুসলমানরাই পুরো পৃথিবীর সংখ্যায় অনেক বৃহত্তর জন গুষ্টি ...\n",
      "আমাদের এগিয়ে যেতে হবে শান্তির সাথে।\n",
      "আমাদের দেখে অন্য ধর্মের মানুষ ভালো কিছু শিক্ষা লাভ করুক সেটাই আমাদের কাম্য।\n",
      "হযরত আবু বকর ওসমান ফারুক কিংবা আলীর কাছ থেকে সেই সোনালী সুদিন ছিলো অন্যরকম, তাদের ভালোবাসা দান আর উদারতা ছিল অনেক প্রশাংসানিয়।\n",
      "কেন এতো মাথা গরম কেন এতো রক্তের তেজ।\n",
      "আমরা শান্তি চাই শান্তি প্রতিষ্ঠিত করতে চাই।\n",
      "আমাদের কে দিয়ে কেউ শিখুক।\n",
      "ভাংচুর রক্তাক্ত দিয়ে এই সল্প জীবন চলেনা।\n",
      "মনে কি নেই, পূর্বে বহুজাতি এভাবে নিজের ভুলের কারনে ধংস হয়ে গেছে।\n",
      "দয়া করুন এদের উপর...\n",
      "এরাও এই দেশের একটি অংশ, ভালবাসা থাকুক সবার প্রতি।\n",
      "\n",
      "১৯ বছরের আফগান যুদ্ধে পরাজয় মানল যুক্তরাষ্ট্র\n",
      "শনিবার (২৯ ফেব্রুয়ারি) কাতারের রাজধানী দোহায় মার্কিন পররাষ্ট্রমন্ত্রী মাইক পম্পেও এবং তালেবান নেতাদের উপস্থিতিতে এই চুক্তি সই হয়।\n",
      "চুক্তি অনুযায়ী আগামী ১৪ মাসের মধ্যে আফগানিস্তান ছাড়তে হবে মার্কিন ও ন্যাটো সেনাদের।\n",
      "আন্তর্জাতিক সংবাদমাধ্যম এ ঘটনাকে যুক্তরাষ্ট্র কর্তৃপক্ষ তালেবানের শর্ত মেনে নেয়া হিসেবে আখ্যায়িত করা হয়েছে।\n",
      "যা প্রকৃতপক্ষে আফগান যুদ্ধে মার্কিনিদের চুড়ান্ত পরাজয় হিসেবে দেখা হচ্ছে।\n",
      "দোহায় অনুষ্ঠানে মাইক পম্পেও বলেন, আজ এ চুক্তির মাধ্যমে শান্তির সূচনা হল।\n",
      "আফগানিস্তানে শান্তি ফিরিয়ে আনতে যুক্তরাষ্ট্র সব পক্ষের আন্তরিক সহযোগিতা কামনা করে।\n",
      "তালেবানদের সঙ্গে শান্তি চুক্তি সই করতে শনিবারই কাতারে পৌঁছান পররাষ্ট্রমন্ত্রী মাইক পম্পেও।\n",
      "অপরপক্ষে ৩১ সদস্যের তালেবানের একটি প্রতিনিধি দল আগেই কাতার যান।\n",
      "আরো পড়ুন:তালেবানের মুখপাত্র জবিউল্লাহ মুজাহিদ বার্তাসংস্থা রয়টার্সকে জানিয়েছেন, আমরা আশা করি দুই পক্ষের মধ্যে আলোচনা এবং শান্তি চুক্তি সই হওয়ার সময় যুক্তরাষ্ট্র তার প্রতিশ্রুতির প্রতি অনড় থাকবে।\n",
      "তিনি বলেন, জাতির এ খুশির দিনে আজকে তালেবানের সব সদস্যকে যেকোনো ধরনের হামলা থেকে বিরত থাকার জন্য নির্দেশ দেয়া হয়েছে।\n",
      "চুক্তির আওতায় যুদ্ধবিধ্বস্ত আফগানিস্তান থেকে হাজার হাজার মার্কিন সেনা প্রত্যাহারের পাশাপাশি দেশটিতে স্থায়ী যুদ্ধবিরতি পালনের কথা বলা হয়েছে।\n",
      "\n",
      "সাপের ভয়ে অফিস ছাড়লেন প্রেসিডেন্ট!\n",
      "নির্বাচন, রাজনৈতিক বিরোধ, সামরিক হস্তক্ষেপ-এ রকম নানা কারণে অনেক দেশের প্রেসিডেন্টকে তাঁদের কার্যালয় ছাড়তে হয়েছে।\n",
      "কিন্তু লাইবেরিয়ার প্রেসিডেন্টকে কার্যালয় ছাড়তে হয়েছে সম্পূর্ণ ভিন্ন এক কারণে।\n",
      "বুধবার থেকে বাড়িতে বসে রাষ্ট্রীয় কাজ সারতে বাধ্য হয়েছেন প্রেসিডেন্ট জর্জ ওয়েয়া।\n",
      "আর তার কারণ হলো দুটি কালো সাপ।\n",
      "এর আগেও একবার কার্যালয় ছাড়তে হয়েছে দেশটির প্রেসিডেন্টকে।\n",
      "এরপর আর ফিরতেই পারেননি সেখানে।\n",
      "তার কারণ হলো ২০০৬ সালে এক অগ্নিকাণ্ডে প্রেসিডেন্টের দপ্তর মারাত্মকভাবে ক্ষতিগ্রস্ত হয়।\n",
      "এর পর থেকে লাইবেরিয়ার প্রেসিডেন্ট দেশটির পররাষ্ট্র দপ্তরে বসছেন।\n",
      "প্রেসিডেন্টের কার্যালয়ের গণমাধ্যমবিষয়ক কর্মকর্তা স্মিথ টোবি জানান, বুধবার ভবনটির অভ্যর্থনা এলাকার দেয়ালের একটি গর্ত থেকে দুটি কালো সাপ বের হয়ে আসে।\n",
      "স্থানীয় গণমাধ্যমে কর্মচারীদের ধারণ করা একটি ভিডিওতে সাপ দুটিকে ধীরে ধীরে নড়তে দেখা গেছে।\n",
      "গর্তে ধোঁয়া দিয়ে সাপগুলো বের করে আনার চেষ্টা চলছে।\n",
      "স্মিথ টোবি আরো জানান, ভবনটি বেশ পুরনো।\n",
      "এর পানি বা পয়োনিষ্কাশন ব্যবস্থা থেকে সাপ দুটি এসেছে বলে ধারণা করা হচ্ছে।\n",
      "সাপগুলো এখনো ভবনে আছে কি না তা-ও বোঝা যাচ্ছে না।\n",
      "অন্তত আগামীকাল সোমবার পর্যন্ত সবাইকে দূরে থাকতে বলা হয়েছে।\n",
      "সূত্র : বিবিসি।\n",
      "\n",
      "মামার সেঞ্চুরি\n",
      "বলেন মামা, 'ইশ\n",
      "অল্প ক'টা রানের জন্য সেঞ্চুরিটা মিস।\n",
      "গোটা দশেক চার\n",
      "সাথে আটেক ছয়,\n",
      "একশো হতে আর\n",
      "থাকলো বাকী কয়!!\n",
      "হিসেব কষে নিজে\n",
      "ব্যাট ধরেছি ক্রিজে,\n",
      "লাগছে ভালো কিযে\n",
      "সুখ স্বপনে ভিজে।\n",
      "প্রথম বলে ভীষণ জোড়ে\n",
      "মেরে দিলাম ছয়,\n",
      "মারের চোটে লাল বলটা\n",
      "দুই টুকরো হয়।\n",
      "শূণ্যে উঠে দুই জনেই\n",
      "দুই দিকে দেয় পাড়ি,\n",
      "ছয় দু'গুণে বারোটা রান ভীষণ বাড়াবাড়ি!!\n",
      "এলো নেমে টুকরো দু'টি\n",
      "জমে গেছে ম্যাচ,\n",
      "দুই ফিল্ডার দুই দিকে\n",
      "ধরে নিলো ক্যাচ।\n",
      "নেই কোন আর দাউট\n",
      "এক বলেই দু'বার হলাম\n",
      "আমি শূণ্য রানে আউট।\n",
      "\n",
      "(Genesis, chapters 1 and 2) They also discerned that the Bible holds out two destinies - a heavenly one for the 144,000 anointed footstep followers of Christ and a paradise earth for an unnumbered \"great crowd\" of \"other sheep.\" (আদিপুস্তক ১ ও ২ অধ্যায়) তারা এটাও বুঝতে পারেন যে বাইবেল দুটি গন্তব্য পথের বিষয়ে উল্লেখ করে - একটি হল স্বর্গীয়, খ্রীষ্টের পদানুসরণকারীদের অভিষিক্ত ১,৪৪,০০০ জনের জন্য এবং আর একটি পরমদেশ পৃথিবীতে থাকার জন্য অগণিত \"অপর মেষ\" এর \"বিস্তর লোক।\"\n",
      "\n",
      "মন্দিরটি ক্রমবর্ধমান টেরেস দিয়ে বর্গক্ষেত্রের টেমপ্লেটে সেট করা হয়েছে। এটি ১০ শতকের খ্রিস্টাব্দে বাগানের (বাগানের)এটি ও অন্যান্য মন্দিরগুলিতে কাজ করার জন্য ভারতীয় কারিগরদের দ্বারা নির্মিত হয়েছিল। বাগানের প্রাচীনতম মন্দির হিসাবে, এর শৈলী অনুসরণ এবং অনুপ্রাণিত করা অনেক অন্যান্য বৌদ্ধ কাঠামো দেখা যায় । আরেকটি কিংবদন্তী বলে যে মন্দিরটি অন্য মন্দির থেকে সমস্ত নাটকে সংরক্ষণ করার জন্য নির্মিত হয়েছিল, যাতে বাগানের রাজ্যে বৌদ্ধধর্ম প্রতিষ্ঠিত হয়। The temple is set on a square template with steep-rising upper terraces. It may have been built by Indian artisans brought into Bagan (Pagan), during the 10th century AD, to work on it and other temples. As the oldest temple in Bagan, its style influenced and inspired the numerous other Buddhist structures that followed. Another legend states that the temple was built to store all the nat from other temples, so that Buddhism could get established in Bagan kingdom.\n",
      "\n",
      "Wait a minute! হ্যাঁ, দাঁড়ান!\n",
      "\n",
      "অস্বাভাবিক যৌনতাড়না সম্পন্ন oversexed\n",
      "\n",
      "Family background. পারিবারিক পটভূমি।\n",
      "\n",
      "করোনার আরেকটি রূপ চিহ্নিত, মিলছে পুরুষের শুক্রানুতেও\n",
      "আন্তর্জাতিক ডেস্ক : করোনাভাইরাসে বিপর্যস্ত গোটা বিশ্ব।\n",
      "চীনের উহানে চার মাসে আগে তাণ্ডব শুরু করে এই ভাইরাস।\n",
      "বর্তমানে চীনের গণ্ডি পেরিয়ে বিশ্বব্যাপী ধ্বংসযজ্ঞ চালাচ্ছে করোনা।\n",
      "ইতোমধ্যে এই ভাইরাস বিশ্বের ২১২টি দেশ ও অঞ্চলে ছড়িয়ে পড়েছে।\n",
      "এসব দেশে এখন পর্যন্ত (শুক্রবার বেলা সোয়া ১১টা) আক্রান্ত হয়েছে ৩৯ লাখ ১৭ হাজার ৯৪৪ জন।\n",
      "এর মধ্যে মৃত্যু হয়েছে ২ লাখ ৭০ হাজার ৭৪০ জনের।\n",
      "করোনাভাইরাস এতটাই ভয়ঙ্কর যে বিজ্ঞানীরা এর অস্তিত্ব খুঁজে পেয়েছেন আক্রান্ত পুরুষের শুক্রানুতেও।\n",
      "চীনের গবেষকরা এ তথ্য জানিয়েছেন।\n",
      "সে অনুসারে, শারীরিক সম্পর্কের মাধ্যমে করোনাভাইরাস ছড়ানোর শঙ্কা আরও প্রকট আকারে দেখা দিয়েছে।\n",
      "তবে আক্রান্ত পুরুষের শুক্রানুতে কী পরিমাণ ভাইরাসের উপস্থিতি দেখা গেছে এবং শারীরিক সম্পর্কের ফলে করোনা সংক্রমণ ঘটতে পারে কি না, সে ব্যাপারে গবেষকরা এখনও কোনও তথ্য প্রকাশ করেননি।\n",
      "চিকিৎসা বিষয়ক জার্নাল জামা নেটওয়ার্ক গবেষণাটির ফলাফল প্রকাশ করেছে।\n",
      "চীনের শাংকিউ মিউনিসিপ্যাল হসপিটালের গবেষকরা এ ব্যাপারে গবেষণাটি করেছেন।\n",
      "শুক্রানুতে করোনাভাইরাস থাকার ব্যাপারে গবেষণার ফল এটাই প্রথম পাওয়া গেল।\n",
      "করোনা পজিটিভ হলে নিরাপদ শারীরিক সম্পর্কের বিষয়গুলো কেমন হবে, তা জানার জন্য নতুন গবেষণা দরকার বলে মনে করছেন গবেষকরা।\n",
      "সেই সঙ্গে বলা হয়েছে, এই গবেষণা পরবর্তীতে করোনা সম্পর্কিত শারীরিক সম্পর্কের বিষয়ে গবেষণার পথ উন্মোচন করবে।\n",
      "যুক্তরাষ্ট্রের সেন্টার ফর ডিজিস কন্ট্রোল অ্যান্ড প্রিভেনশনের (সিডিসি) চীফ মেডিকেল অফিসার জন ব্রুকস মনে করেন, এটা চমৎকার ফল।\n",
      "তবে তিনি মনে করেন, এর অর্থ এই নয় যে শুক্রানু সংক্রামক হয়ে উঠবে।\n",
      "তিনি আরও বলেন, যখন আমরা সবখানে এই ভাইরাসটি দেখছি, শরীরের বিভিন্নখানেই এর উপস্থিতি রয়েছে।\n",
      "আক্রান্ত ব্যক্তির অবস্থানস্থলেও এই ভাইরাস পাওয়া যাচ্ছে।\n",
      "সে কারণে এ ব্যাপারে নির্দিষ্ট করে কিছু বলা কঠিন।\n",
      "সিডিসির গবেষকরা মনে করেন, আক্রান্ত ব্যক্তির হাঁচি-কাশি থেকে ছড়ানো ড্রপলেটের মাধ্যমে করোনাভাইরাস অন্যদের শরীরে সংক্রমণ ঘটায়।\n",
      "শারীরিক সম্পর্কের মাধ্যমে আরো নির্দিষ্ট করে বললে শুক্রানুর মাধ্যমে করোনা সংক্রমণের প্রমাণ তারা পাননি।\n",
      "সূত্র: দ্য গার্ডিয়ান, সিএনএন\n",
      "\n",
      "বিমানের অব্যবহৃত টিকেটে ভ্রমণ করা যাবে ২০২১-এর মার্চ পর্যন্ত\n",
      "প্রায় দেড়মাস ধরে বন্ধ আছে দেশের বিমানবন্দর, বন্ধ আছে সবধরনের বিমান চলাচল।\n",
      "এর ফলে অনেক যাত্রীই টিকেট কিনেও এক গন্তব্য থেকে অন্য গন্তব্যে উড়াল দিতে পারেননি।\n",
      "তাদের জন্য বিশেষ অফার এনেছে বিমান বাংলাদেশ এয়ারলাইনস।\n",
      "এক ক্ষুদে বার্তায় বিমান জানায়, কোভিড-১৯ এর কারণে বিমান বাংলাদেশ এয়ারলাইন্সের অব্যবহৃত টিকিটে সম্মানিত যাত্রীগণ আগামী ১৪মার্চ, ২০২১ পর্যন্ত কোনো প্রকার চার্জ ছাড়া ভ্রমণ করতে পারবেন।\n",
      "বিমান আরো জানায়, কেউ যদি আর টিকেটটি ব্যবহার না করতে চায়, সেক্ষেত্রে তিনি এ সময়ের মধ্যে টিকেটের সম্পূর্ণ মূল্য ফেরত নিতেও পারবেন।\n",
      "বিশ্বব্যাপী করোনা ভাইরাস ছড়িয়ে পড়ার পরিপ্রেক্ষিতে দেশের বিমানবন্দরগুলোতে ফ্লাইট চলাচলে ১৬ মে পর্যন্ত নিষেধাজ্ঞা দিয়েছে বাংলাদেশ বেসামরিক বিমান চলাচল কর্তৃপক্ষ (বেবিচক)।\n",
      "তাই বন্ধ আছে বিমানের ফ্লাইটও।\n",
      "তবে বিভিন্ন দেশে আটকেপড়া যাত্রীদের আনা-নেয়ার জন্য বিশেষ ফ্লাইট পরিচালনা, সবজি রপ্তানিসহ বিমানের কার্গো ফ্লাইটগুলো এখনও চালু রয়েছে। এআর/এনএফ/এমকেএইচ\n",
      "\n",
      "Pooth Kalan পোথ কলন\n",
      "\n",
      "সোহাগ পরিবহনের চালক-হেলপার জামিনে মুক্ত\n",
      "ফরিদপুরের মধুখালীতে বেনাপোলগামী সোহাগ পরিবহনের বাসে ডাকাতির মামলায় আটক বাসটির চালক ও তার সহকারীকে জামিন দিয়েছে আদালত। মঙ্গলবার ফরিদপুরের ৪\n",
      "নম্বর আমলী আদালতের হাকিম গোলাম সরোয়ার এ দুই পরিবহন শ্রমিককে ১০ সপ্তাহের অন্তর্বর্তীকালীন জামিনের আদেশ দেন। জামিন\n",
      "পাওয়া দুই শ্রমিক হলেন চালক আয়নাল (৩৮) ও তার সহকারী শাকিল (২৬)। দক্ষিণাঞ্চলের\n",
      "বাস ধর্মঘট প্রত্যাহারের একদিন পর এ দুজনকে মুক্তি দেওয়া হলো। ফরিদপুর\n",
      "আদালতের পিপি খোরশেদুজ্জামান দুলু জানান, ফরিদপুর জেলা বাস মালিক\n",
      "গ্রুপের আহ্বায়ক সাজ্জাদ হোসেন বরকত ও যুগ্ন আহ্বায়ক খন্দকার রাশেদের জিম্মায় তাদের জামিন দেওয়া হয়েছে। আদেশের\n",
      "কপি জেলা কারাগারে পৌঁছার পর সন্ধ্যায় তাদের মুক্তি দেওয়া হয় বলে তিনি জানান। খোরশেদুজ্জামান\n",
      "আরও জানান, মঙ্গলবার এ মামলায়\n",
      "পুলিশের রিমান্ড আবেদনের শুনানি হওয়ার কথা ছিল।\n",
      "কিন্তু মামলার তদন্ত কর্মকর্তা\n",
      "মধুখালী থানার উপ-পরিদর্শক (এসআই) রউফ ইতিপূর্বে আদালতে দেওয়া রিমান্ড আবেদন স্থগিত রাখার আবেদন করে। অন্যদিকে আসামিপক্ষের\n",
      "আইনজীবী নারায়ণ চন্দ্র দাস আসামিদের জামিন আবেদন করেন। তিনি জানান, দুপক্ষের\n",
      "শুনানি শেষে আদালত জেলা বাস মালিক গ্রুপের আহ্বায়ক সাজ্জাদ হোসেন বরকত ও যুগ্ন আহ্বায়ক\n",
      "খন্দকার রাশেদের জিম্মায় ১০ হাজার টাকায় মুচলেকায় তাদের ১০ সপ্তাহের অন্তর্বর্তীকালীন\n",
      "জামিনের আদেশ দেন। গত\n",
      "১৯ মে ঢাকা-খুলনা মহাসড়কে মধুখালীতে বেনাপোলগামী সোহাগ পরিবহনের একটি বাসে সাত ডাকাত যাত্রীদের নগদ সাত\n",
      "হাজার ২শ মার্কিন ডলার, ২২\n",
      "হাজার ভারতীয় রুপি, ১৬ ভরি স্বর্ণালংকার, নগদ\n",
      "প্রায় ৪ লাখ টাকা এবং ২৫টি মোবাইল ফোন লুট করে। এ ঘটনায়\n",
      "বাসটির যাত্রী গাজীপুরের\n",
      "কালিয়াকৈরের পৌরসভার ৮\n",
      "নম্বর ওয়ার্ড কাউন্সিলর সরোয়ার হোসেন (৩৫) বাদী হয়ে ১৯ মে মধুখালী থানায় একটি মামলা দায়ের করেন। ওই মামলায় চালক, তার সহকারী ও\n",
      "সুপারভাইজার শহিদুল ইসলামসহ\n",
      "(৩০) অজ্ঞাত পরিচয় আরও সাতজনকে আসামি করা হয়। ওইদিনই\n",
      "বাসের চালক ও সহকারী\n",
      "গ্রেপ্তার হলেও সুপারভাইজার পলাতক রয়েছেন। এদিকে তাদের গ্রেপ্তারের প্রতিবাদে দক্ষিণাঞ্চলে অনির্দিষ্টকালের\n",
      "পরিবহন ধর্মঘটের ডাক দেয় মালিক-শ্রমিকরা।\n",
      "সোমবার ঢাকায় সরকারের সঙ্গে মালিক-শ্রমিকদের বৈঠকে ধর্মঘট\n",
      "স্থগিত করা হয়।\n",
      "\n",
      "ধাক্কায় নৌকাডুবি নিখোঁজ ২ শিশুর লাশ উদ্ধার\n",
      "ঢাকার সদরঘাটে বুড়িগঙ্গা নদীতে লঞ্চের ধাক্কায় নৌকা ডুবে নিখোঁজ ভাই-বোনের লাশ উদ্ধার করেছেন নৌবাহিনী ও ফায়ার সার্ভিসের ডুবুরিরা।\n",
      "শুক্রবার সকাল ৭টার দিকে সদরঘাটের এক নম্বর পন্টুন বরাবর মাঝ নদীতে পূবালী-৫ লঞ্চের ধাক্কায় নৌকাটি ডুবে যায়।\n",
      "এ সময় মেশকাত (১২) এবং তার বোন নুসরাত (৭) পানিতে তলিয়ে যায়।\n",
      "বেলা সোয়া ১১টা ও সাড়ে ১১টায় তাদের লাশ উদ্ধার করা হয়।\n",
      "সদরঘাট নৌ থানার ওসি মো. রেজাউল জানান, এক নম্বর পন্টুন বরাবর নদী থেকে প্রথামে মিশকাতের লাশ উদ্ধার করেন নৌবাহিনীর ডুবুরিরা।\n",
      "কিছুক্ষণ পর ফায়ার সার্ভিসের ডুবুরিরা এর কাছাকাছি এলাকায় নদী থেকে নুসরাতের লাশ উদ্ধার করেন।\n",
      "ওই দুই শিশুর বাবা বাবুল ফরাজী কেরানীগঞ্জে ব্যবসা করেন।\n",
      "তাদের গ্রামের বাড়ি বরিশালের বাকেরগঞ্জ উপজেলার উত্তমপুর গ্রামে।\n",
      "মেশকাত ও নুসরাত বরিশালে গ্রামের বাড়িতে বেড়ানো শেষে মামা শামীম হাওলাদারের সঙ্গে শুক্রবার ভোরে ঢাকায় ফেরেন।\n",
      "সদরঘাটে পৌঁছানোর পর নৌকায় করে কেরানীগঞ্জের বাসায় ফেরার পথে এ দুর্ঘটনা ঘটে।\n",
      "শামীম হাওলাদার বলেন, নৌকায় আমার বোন জোসনা, ওদের তিন সন্তান মেশকাত, নুসরাত ও এক বছর বয়সী নুসাইবা ছিল।\n",
      "মাঝ নদীতে এমভি পূবালী-৫ লঞ্চের ধাক্কায় নৌকাটি ডুবে যায়।\n",
      "নুসাইবা আমার কোলে ছিল।\n",
      "ধাক্কা লাগার পর দেখি আমরা লঞ্চের নিচে।\n",
      "কোনো রকমে সাঁতরে বের হই।\n",
      "জোসনাও বের হয়।\n",
      "কিন্তু মেশকাত ও নুসরাত ডুবে যায়।\n",
      "খবর পেয়ে মেশকাতদের বাবা বাবুল ফরাজী কেরানীগঞ্জ থেকে সদরঘাটে ছুটে আসেন।\n",
      "পুলিশ ও ঘাটে থাকা নৌকাগুলো তাৎক্ষণিকভাবে খোঁজাখুঁজি শুরু করে।\n",
      "পরে ফায়ার সার্ভিস ও নৌবাহিনীর ডুবুরিরা এবং কোস্ট গার্ড ও আইডব্লিউটিএ-এর উদ্ধারকর্মীরা তল্লাশি অভিযানে অংশ নেন।\n",
      "বিআইডব্লিউটিএ'র পরিবহন পরিদর্শক মো. সেলিম বলেন, পূবালী-৫ লঞ্চটি সদরঘাটে যাত্রী নামিয়ে কেরানীগঞ্জের দিকে যাচ্ছিল।\n",
      "নৌকাটি লঞ্চের পেছন দিয়ে যাওয়ার সময় এ দুর্ঘটনা ঘটে।\n",
      "মাঝি দেখেশুনে চালালে এ বিপদ হত না।\n",
      "\n",
      "পা মাটিতেই রাখছেন কিংসের মেয়েরা\n",
      "ক্রীড়া প্রতিবেদক : লিগের প্রথম পর্বে প্রতিপক্ষ দলগুলোর ওপর একরকম ঝড় বইয়ে দিয়েছেন বসুন্ধরা কিংসের মেয়েরা।\n",
      "ছয় ম্যাচ জিততে ৫৮ বার বল জালে পাঠিয়েছেন তাঁরা।\n",
      "আজ থেকে শুরু হতে যাওয়া দ্বিতীয় পর্বেও একই রকম আধিপত্য প্রত্যাশা তাঁদের কাছ থেকে।\n",
      "যদিও পা মাটিতেই রাখছেন সাবিনা খাতুনরা।\n",
      "করোনার পর যে একটি ম্যাচ তাঁরা খেলেছেন প্রথম পর্বের, সেখানে খেলোয়াড়দের জড়তা চোখ এড়ায়নি কোচ মাহমুদা খাতুনের।\n",
      "দ্বিতীয় পর্বে পুরনো ছন্দে দলটিকে দেখতে হলে খেলোয়াড়দের তাই উন্নতির বিকল্প দেখছেন না তিনি।\n",
      "'দ্বিতীয় পর্বে একই রকম আধিপত্য দেখিয়ে আমরা ম্যাচগুলো জিতে যাব বলা যায় না।\n",
      "কারণ করোনার লম্বা বিরতিতে মেয়েরা নিজেদের সেরা অবস্থা থেকে অনেকটাই পিছিয়ে পড়েছে।\n",
      "ফেরার পর প্রথম ম্যাচটিতেও সেটি বোঝা গেছে।\n",
      "দ্বিতীয় পর্বে ভালো করতে হলে তাই আমাদের আরো উন্নতি করতেই হবে।'- বলছিলেন মাহমুদা।\n",
      "প্রথম পর্বের ওই শেষ ম্যাচটিতে কিংস প্রথম অর্ধে গোলই পায়নি।\n",
      "সেটিই ছিল সবচেয়ে অবাক করা ব্যাপার।\n",
      "দ্বিতীয়ার্ধে অবশ্য প্রতিপক্ষ এফসি উত্তরবঙ্গের প্রতিরোধ ভাঙে।\n",
      "দ্বিতীয় লেগে কিংসের মূল প্রতিপক্ষ হতে পারে নাসরিন স্পোর্ট একাডেমি।\n",
      "তারা কিংসের চেয়ে মাত্র ৩ পয়েন্ট পিছিয়ে আছে।\n",
      "কিংসের কাছেই ওই একটি ম্যাচ হেরেছিল তারা, সেটিও বড় ব্যবধানে।\n",
      "তবে দ্বিতীয় পর্বে নাসরিন শক্তি বাড়িয়েছে।\n",
      "এসএসসি পরীক্ষার কারণে প্রথম পর্বে কোনো দলে নাম না লেখানো সাজেদা খাতুন ও জাতীয় দলে খেলা দুই বোন আনুচিং ও আনাই মোগিনীকে তারা দলে ভিড়িয়েছে।\n",
      "এর পরও অবশ্য প্রতিদ্বন্দ্বিতার কথা জোরগলায় বলা সম্ভব হচ্ছে না ক্লাবটির সাধারণ সম্পাদক নাসরি আক্তারের, 'কিংস এখনো আমাদের চেয়ে অনেক গুণ এগিয়ে।'\n",
      "নতুন খেলোয়াড় নিয়েছে কিংসও।\n",
      "জাপানে জন্ম নেওয়া মাতসুশিমা সুমাইয়াকে তারা এই লেগে দলে নিয়েছে।\n",
      "অনূর্ধ্ব-১৬ দলের হয়ে আলো ছড়ানো সিরাত জাহানকেও নিয়েছে তারা, সঙ্গে যোগ হয়েছে অভিজ্ঞ মাইনু প্রু ও সাবিনা চাকমা।\n",
      "\n",
      "বাংলার মাটিতে সাঁতারে কেউ আমাকে হারাতে পারেনি\n",
      "আন্তর্জাতিক পর্যায়ের সাফল্যের বিস্তৃতিতে বাংলাদেশের কোনো ক্রীড়াবিদই সম্ভবত তাঁর ধারে-কাছে আসেন না।\n",
      "এখন রাজনীতির মারপ্যাঁচে মো. মোশাররফ হোসেন খান থাকেন সুদূর মার্কিন মুলুকে।\n",
      "দেশে এলেন অনেক বছর পর।\n",
      "নোমান মোহাম্মদ গিয়ে হাজির হলেন তাঁর কাছে।\n",
      "সিন্দুক থেকে বেরোল বীরত্বের উপচানো গল্প।\n",
      "প্রশ্ন : কথোপকথন শুরু করতে চাই আপনার অবিশ্বাস্য এক অর্জনের গল্পে।\n",
      "শুনেছি, বাংলাদেশে সাঁতারের জাতীয় চ্যাম্পিয়নশিপে আপনি কখনো দ্বিতীয় হননি।\n",
      "প্রতিবার প্রথম; প্রতিটিতে স্বর্ণপদক।\n",
      "এটি কি সত্যি?\n",
      "মো. মোশাররফ হোসেন খান : সত্যি।\n",
      "বাংলার মাটিতে আমাকে সাঁতারে কেউ কখনো হারাতে পারেনি।\n",
      "প্রতিবার আমি স্বর্ণপদক জিতেছি। প্রতিটি ইভেন্টে।\n",
      "প্রশ্ন : অথচ আপনার ক্যারিয়ার তো দু-এক বছরের নয়!\n",
      "মোশাররফ : নাহ্, দীর্ঘ ক্যারিয়ার।\n",
      "স্বাধীনতার পর থেকে হিসাব করলেও ১৬-১৭ বছরের।\n",
      "১৯৭২ সালে শুরু, শেষ সাঁতরেছি ১৯৮৮ সালের জাতীয় চ্যাম্পিয়নশিপে।\n",
      "প্রশ্ন : এই প্রায় দেড় যুগে জাতীয় চ্যাম্পিয়নশিপে ঠিক কতটি পদক জিতেছেন, মনে আছে?\n",
      "মোশাররফ : খুব মনে আছে। ৯৬টি।\n",
      "১৯৭২ সালে প্রথম জাতীয় চ্যাম্পিয়নশিপে চার ইভেন্টে অংশ নিই।\n",
      "এরপর প্রতিবার অন্তত আট ইভেন্টে।\n",
      "প্রশ্ন : এক প্রতিযোগিতায় সর্বোচ্চ পদক পেয়েছেন কটি?\n",
      "মোশাররফ : ১৯৭৮ সালে প্রথম বাংলাদেশ গেমস হয়।\n",
      "সেখানে ১০ ইভেন্টে নাম দিই; প্রতিটিতে জিতি স্বর্ণপদক।\n",
      "আর প্রতিটিই নতুন জাতীয় রেকর্ড গড়ে।\n",
      "আসলে জাতীয় সাঁতার চ্যাম্পিয়নশিপে ইভেন্টই থাকত ১৫টি।\n",
      "আমি যখন যে দলের হয়ে খেলেছি, ওদের একটাই দাবি - অন্তত আটটি স্বর্ণ এনে দাও।\n",
      "১৫-র মধ্যে আট স্বর্ণ পেলেই তো ওই দল চ্যাম্পিয়ন।\n",
      "আমি তাই আটটি ইভেন্টেই সাধারণত লড়তাম।\n",
      "বলতাম, 'আট স্বর্ণ আমার, বাকি সাতটি ভাগ করে নাও পুরো বাংলাদেশ।'\n",
      "প্রশ্ন : রেকর্ড গড়েছেন কয়বার?\n",
      "মোশাররফ : আমার রেকর্ড আমিই ভাঙতাম বারবার।\n",
      "১৯৮৮ সালে শেষ যেবার প্রতিযোগিতামূলক সাঁতার করি, সেদিনও নতুন রেকর্ড গড়েছি।\n",
      "সব মিলিয়ে এই ৯৬ পদকের মধ্যে নতুন জাতীয় রেকর্ড ৬৪ বার।\n",
      "প্রশ্ন : '৯৬-তে আটকে যাওয়ার পর একটু কি খারাপ লাগে?\n",
      "আর চারটি স্বর্ণপদক জিতলেই তো সেঞ্চুরি হয়ে যেত...\n",
      "মোশাররফ : এক হিসাবে কিন্তু সেঞ্চুরি হয়েছে।\n",
      "আমি ম্যাট্রিক পরীক্ষা দিই ১৯৬৯ সালে।\n",
      "ওই বছরই পূর্ব পাকিস্তানের ১৫তম জাতীয় সাঁতার প্রতিযোগিতায় অংশ নিই প্রথম।\n",
      "চার ইভেন্টে নাম দিয়ে প্রতিটিতে হই প্রথম।\n",
      "১০০ মিটার বাটারফ্লাই, ১০০ মিটার ব্যাকস্ট্রোক ও ২০০ মিটার ব্যক্তিগত মিডলে ও ১০০ মিটার ফ্রিস্টাইল।\n",
      "এর মধ্যে ১০০ মিটার ফ্রিস্টাইলে পুরো পাকিস্তানের নতুন জাতীয় রেকর্ড গড়ে জিতি স্বর্ণপদক।\n",
      "ওই শেষ দিনে ঢাকা স্টেডিয়ামের সুইমিং পুলে এসেছিলেন পূর্ব পাকিস্তানের গভর্নর অ্যাডমিরাল আহসান।\n",
      "নেভির সাদা অফিশিয়াল পোশাক পরে এসেছিলেন।\n",
      "রেকর্ড গড়ার ঘোষণা যখন দেওয়া হয়, উনি ভিআইপি গ্যালারি থেকে হেঁটে ওই পোশাকে আমাকে ভিজে গায়ে জড়িয়ে ধরে অভিনন্দন জানান।\n",
      "নিজের গা ভিজিয়ে ফেলেন।\n",
      "যুদ্ধের আগে ওই একবারই জাতীয় চ্যাম্পিয়নশিপ হয়।\n",
      "এই চারটি স্বর্ণপদক যোগ করলে পদকের সেঞ্চুরি কিন্তু হয়ে যায়।\n",
      "অবশ্য স্বাধীন বাংলাদেশেই সেটি করতে পারতাম; যদি ১৯৭৫ সালের জাতীয় চ্যাম্পিয়নশিপে আমাকে সাঁতরাতে দেওয়া হতো।\n",
      "বঙ্গবন্ধুকে তো সপরিবারে মেরে ফেলা হয় আগস্টে।\n",
      "সেপ্টেম্বরেই ছিল আমাদের জাতীয় চ্যাম্পিয়নশিপ।\n",
      "তখন মার্শাল ল, ওখানে বাংলাদেশ সেনাবাহিনীকে চ্যাম্পিয়ন করাতেই হবে।\n",
      "সে কারণে ঠুনকো অজুহাত দেখিয়ে আমার দল বিটিএমসির নিবন্ধন বাতিল করা হয়।\n",
      "সেবার সাঁতরাতে দিলে স্বাধীন দেশে আমার স্বর্ণপদকের সেঞ্চুরি হয়ে যেত।\n",
      "প্রশ্ন : এ তো বলছেন ঘরোয়া সাফল্যের কথা।\n",
      "আন্তর্জাতিক পর্যায়েও সাঁতারে বাংলাদেশকে অনেক পদক এনে দিয়েছেন আপনি...\n",
      "মোশাররফ : সংখ্যাও মনে আছে, ৪৪টি স্বর্ণপদক।\n",
      "এর বাইরে রৌপ্য, ব্রোঞ্জপদক আছে।\n",
      "সেগুলো গোনায় নাই-বা ধরলাম।\n",
      "প্রশ্ন : কোন কোন প্রতিযোগিতায়?\n",
      "মোশাররফ : সাফ গেমসে আছে।\n",
      "সাফ শুরুর আগে আঞ্চলিক সাঁতার প্রতিযোগিতা হতো।\n",
      "বাংলাদেশ-ভারত-শ্রীলঙ্কাকে নিয়ে ত্রিদেশীয় মিট হতো শ্রীলঙ্কায়।\n",
      "এসব জায়গায় স্বর্ণ পেয়েছি প্রচুর।\n",
      "পাকিস্তানে কায়েদে আজম আন্তর্জাতিক আমন্ত্রণমূলক সাঁতার প্রতিযোগিতায় জিতেছি।\n",
      "কলকাতা থেকে একবারে ছয় স্বর্ণপদক নিয়ে এসেছি।\n",
      "আসলে এ অঞ্চলে সাঁতারুরা আমাকে ব্লকে দেখলেই স্বর্ণপদক নিয়ে আর ভাবত না।\n",
      "ওরা চিন্তা করত রৌপ্য পাওয়া যায় কি না।\n",
      "এ কথাটি আমি জোর দিয়েই বলছি।\n",
      "প্রশ্ন : তাহলে ১৯৮৪ সালে প্রথম সাফে কোনো স্বর্ণপদক পাননি কেন?\n",
      "মোশাররফ : এটি খুব দুঃখজনক ব্যাপার।\n",
      "আমার সর্বনাশটি হয়ে যায় আসলে লস অ্যাঞ্জেলেস অলিম্পিকে যেতে না পারাটাই...\n",
      "প্রশ্ন : অলিম্পিকের সঙ্গে সাফের সম্পর্ক কী?\n",
      "মোশাররফ : সম্পর্ক আছে।\n",
      "সেই অলিম্পিকে বাংলাদেশ থেকে শুরুতে ছয়জনের যাওয়ার কথা ছিল।\n",
      "ইন্টারভিউ বোর্ড করে সবাইকে ডেকে রীতিমতো পরীক্ষা নেওয়া হয়।\n",
      "সেখানে ২০ নম্বরের মধ্যে আমি পাই ১৯।\n",
      "দ্বিতীয় হয় সাইদুর রব; ১১ নম্বর পেয়ে।\n",
      "১০ নম্বর পেয়ে সাইদুর রহমান ডন তৃতীয়।\n",
      "আমি তো যাবই লস অ্যাঞ্জেলেসে।\n",
      "এর মধ্যে হংকংয়ে যাই তিন মাসের ট্রেনিংয়ে।\n",
      "ফিরে ঢাকা বিমানবন্দরে দেখা হকির কিংবদন্তি ইব্রাহিম সাবের ভাইয়ের সঙ্গে।\n",
      "উনি ব্যাংকক যাচ্ছিলেন।\n",
      "আমাকে বললেন, 'মোশাররফ, আপনার তো অলিম্পিক যাওয়া হচ্ছে না।'\n",
      "আমি বলি, 'কী বলেন! কেন?'\n",
      "উনি বলেন, 'বাংলাদেশ থেকে যাবে মাত্র এক ক্রীড়াবিদ।\n",
      "জেনারেল ওয়াহেদ ম্যানেজার হয়ে নিয়ে যাচ্ছেন ডনকে।'\n",
      "জাতীয় ক্রীড়া পরিষদে গিয়ে কথা বলে শুনি ঘটনা সত্যি।\n",
      "পরে জেনেছি এ দুজন কেমন আত্মীয় হন, একই গ্রামে বাড়ি - সে কারণে আমাকে বাদ দিয়ে ডনকে অলিম্পিকে নেওয়া হয়।\n",
      "মনটা এত খারাপ হয়ে যায়!\n",
      "তাত্ক্ষণিক সিদ্ধান্তে সাঁতার থেকে অবসরের ঘোষণা দিই।\n",
      "প্রশ্ন : '৮৪ সাফ গেমসের তখন বাকি কত দিন?\n",
      "মোশাররফ : চার মাস।\n",
      "এই চার মাসে এক দিনও পানিতে নামিনি।\n",
      "সাঁতার ফেডারেশন থেকে খুঁজেছে, সাংবাদিকরা খুঁজেছেন - আমি কাউকে ধরা দিই না।\n",
      "রাগ-দুঃখ-অভিমানে একেবারে মুন্সীগঞ্জে চলে যাই।\n",
      "প্রশ্ন : ওই রাগ-দুঃখ-অভিমান ভাঙে কিভাবে?\n",
      "মোশাররফ : গ্রুপ ক্যাপ্টেন মাহফুজ সাহেব তখন সাঁতার ফেডারেশনের ভাইস প্রেসিডেন্ট।\n",
      "ওদিকে আমি যে বিটিএমসিতে চাকরি করি, ওখানকার বস।\n",
      "উনি সাফ শুরুর সপ্তাহ দুয়েক আগে আমাকে ধরে বলেন, 'নেপালে তোমাকে যেতেই হবে।'\n",
      "এরপর আমাকে ডাকেন সাঁতার ফেডারেশনের প্রেসিডেন্ট আসাফউদ্দৌলা সাহেব।\n",
      "ওনাকে আমি বাবার মতোই শ্রদ্ধা করি।\n",
      "এই দুজনকে 'না' করতে পারি না; কিন্তু পানিতেও আমি নামি না।\n",
      "পরে নেপালে যখন চলেই গেলাম, এরপর সেখানে গিয়ে অনুশীলন শুরু করি আবার।\n",
      "আগের চার মাসে এক দিনও পানিতে নামিনি, সেখানে নেপাল সাফ গেমসে ছয় ইভেন্ট করে ছয়টিতে রৌপ্যপদক পাওয়া মন্দ কী!\n",
      "যদি অনুশীলন করতাম, তাহলে এই ছয়টিতেই স্বর্ণপদক থাকত।\n",
      "প্রশ্ন : ১৯৮৫ সালে ঢাকা সাফ গেমসে অবশ্য স্বর্ণে ভাসিয়ে দিয়েছিলেন।\n",
      "ঘরের মাঠে স্বাগতিক বাংলাদেশ মোট স্বর্ণপদক জেতে ৯টি; এর মধ্যে আপনারই পাঁচটি।\n",
      "আগের আসরের অপ্রাপ্তি ঘুচিয়ে দেওয়ার চ্যালেঞ্জটা কি ছিল সেবার?\n",
      "মোশাররফ : এটি একটি কারণ।\n",
      "আরেক কারণ এরশাদ সাহেব আমাকে ডেকেছিলেন।\n",
      "উনার সঙ্গে আগে থেকেই আমার খুব ভালো সম্পর্ক।\n",
      "তাঁর বাসায় অনেকবার গিয়েছি।\n",
      "১৯৭৭ সালে শ্রীলঙ্কায় এক মিটে চারটি স্বর্ণপদক জেতার পর উনি টেলিগ্রাম পাঠান, 'ওয়েল ডান মোশাররফ, দ্য নেশন ইজ প্রাউড অব ইউ।'\n",
      "তিনি তখন জাতীয় ক্রীড়া পরিষদের চেয়ারম্যান।\n",
      "আর ১৯৮৫ সালে তো দেশের প্রেসিডেন্টই।\n",
      "আমাকে 'তুই' করে বলতেন।\n",
      "ওই সাফের আগে আমাকে ডাকিয়ে নিয়ে বললেন, 'মোশাররফ, জাতি তোর দিকে তাকিয়ে আছে।\n",
      "তুই জাতিকে কিছু দে; জাতিও তোকে ফেরত দেবে।'\n",
      "প্রশ্ন : এতে উজ্জীবিত হন খুব?\n",
      "মোশাররফ : খুবই।\n",
      "আমি এরশাদ সাহেবের কাছ থেকে বাসায় ফিরে বড় একটি ট্রাংকে সব কাপড়চোপড় ভরে আরেক বন্ধুর বাসায় তা রেখে আসি।\n",
      "যাতে কোথাও কোনো দাওয়াতে যেতে না হয়।\n",
      "সাফ শুরুর আগের ওই চার মাসে আমার পোশাক বলতে ছিল কেবল সুইমিংপুলে যাওয়া-আসার ট্র্যাকসুট।\n",
      "সাঁতার-বিশ্রাম, বিশ্রাম-সাঁতার এই ছিল রুটিন।\n",
      "কারণ দেশের প্রেসিডেন্টকে আমি কথা দিয়েছি।\n",
      "সাফে তা রাখতে পারি।\n",
      "১০০ মিটার বাটারফ্লাই, ১০০ মিটার ব্যাকস্ট্রোক, ২০০ মিটার ব্যক্তিগত মিডলে, ৪০০ মিটার ব্যক্তিগত মিডলে এবং ৪ গুণিতক ১০০ মিটার মিডলে রিলেতে জিতি স্বর্ণপদক।\n",
      "সঙ্গে ২০০ মিটার ব্যাকস্ট্রোক ও ২০০ মিটার বাটারফ্লাইতে রৌপ্য।\n",
      "প্রশ্ন : আপনি কথা রেখেছেন; হুসেইন মুহম্মদ এরশাদ তাঁর কথা রেখেছিলেন?\n",
      "মোশাররফ : রেখেছিলেন।\n",
      "সাফ গেমসের পরের বছরই রাষ্ট্রের সর্বোচ্চ পুরস্কার স্বাধীনতা পদক পাই।\n",
      "এটি তো তাও চার-পাঁচ মাস পরের ঘটনা।\n",
      "আর সাফ গেমস শেষ হওয়ার ১০ দিনের মধ্যেই এরশাদ সাহেব আমাকে ডাকেন।\n",
      "খুব প্রশংসা করেন আর বলেন, 'তুই দ্রুত এনএসসির চেয়ারম্যানের সঙ্গে দেখা কর।' দেখা করলাম।\n",
      "উনি একটি ম্যাপ বের করে বলেন, 'মোশাররফ, এই কয়েকটি দোকানের মধ্যে কোনটি তোমার পছন্দ?'\n",
      "সেটি ছিল আউটার স্টেডিয়ামের ম্যাপ।\n",
      "ওখানে দোকান বরাদ্দ হচ্ছিল।\n",
      "আমাকে এক টাকার বিনিময়ে একটি দোকান দেওয়া হয়।\n",
      "পরবর্তী সময়ে বারিধারায় আমাকে একটি প্লটও দেন এরশাদ সাহেব।\n",
      "কিন্তু বিএনপি ক্ষমতায় এসে সে বরাদ্দ বাতিল করে।\n",
      "এ জন্য আমি বলি, এরশাদ সাহেব অন্যদের ক্ষেত্রে যেমনই হোক, ক্রীড়াবিদদের জন্য খুব ভালো ছিলেন।\n",
      "প্রশ্ন : নিজ দেশের সাফে আপনিই ছিলেন হিরো।\n",
      "পুরো দেশ তখন নিশ্চয়ই আপনাকে নিয়ে মেতে ছিল?\n",
      "মোশাররফ : অবশ্যই।\n",
      "মানুষের ভালোবাসা তো পেয়েছিই, সাংবাদিকদেরও।\n",
      "ইত্তেফাকের সাংবাদিক বদি ভাই, দৈনিক বাংলার জামান ভাই কত কিছু লিখেছেন!\n",
      "বিশেষত জামান ভাইয়ের কাছে আমার কৃতজ্ঞতার শেষ নেই।\n",
      "সাফের পর উনি শিরোনাম করেছিলেন '৫ স্বর্ণ, ২ রৌপ্য, ৭ পদক - শাবাশ মোশাররফ, জাতি তোমাকে নিয়ে গর্বিত।'\n",
      "এর আগে ১৯৭৭ সালে প্রথম বাংলাদেশি ক্রীড়াবিদ হিসেবে যখন বিদেশ থেকে স্বর্ণপদক জিতি শ্রীলঙ্কায়, তখনকার একটি ঘটনাও পরে শুনেছি।\n",
      "জামান ভাই চেয়েছিলেন আমার খবরটি প্রথম পৃষ্ঠায় দিতে।\n",
      "কিন্তু অফিস থেকে বলা হচ্ছিল, ওখানে দেবে না।\n",
      "তখন জামান ভাই নিজের পদত্যাগপত্র লিখে বলেন, 'বিদেশে বাংলাদেশের প্রথম পদক জয়ের খবর যদি প্রথম পৃষ্ঠায় না যায়, তাহলে আমি সাংবাদিকতাই করব না।'\n",
      "ওনার জন্যই পরে খবরটি যায় প্রথম পৃষ্ঠায়।\n",
      "প্রশ্ন : এবার একটু আপনার 'সাঁতারের প্রথম পৃষ্ঠা'য় একটু যেতে চাই।\n",
      "সাঁতারে আগ্রহ হলো কিভাবে?\n",
      "মোশাররফ : আমার জন্ম মুন্সীগঞ্জের বিক্রমপুরের ধামারন গ্রামে।\n",
      "বাবা আব্দুল বারী খানের ছিল কাপড়ের ব্যবসা।\n",
      "মা মালেকা বারী মধু সংসার দেখভাল করতেন।\n",
      "আমরা ৫ ভাই, ৫ বোন।\n",
      "আমি ৬ নম্বর।\n",
      "গ্রামের ছেলেদের যা হয় আর কি, সাঁতারটা শেখা হয়ে যায় ছোটবেলাতেই।\n",
      "কার কাছে শিখেছি, তা মনে নেই।\n",
      "তবে মনে আছে, প্রথম প্রতিযোগিতা করি দ্বিতীয় শ্রেণিতে পড়ার সময়।\n",
      "প্রশ্ন : দ্বিতীয় শ্রেণিতে!\n",
      "মোশাররফ : হ্যাঁ।\n",
      "গ্রামে নৌকাবাইচ হবে আর হবে 'এক মাইল সাঁতার'।\n",
      "দূরত্বটা এক মাইল বলে নামটা এমন।\n",
      "তখন আমাকে প্রাইভেট পড়াতেন আব্দুস সোবহান শিকদার স্যার।\n",
      "উনি নিজে স্কুল জীবনে ব্যাকস্ট্রোক সাঁতারে পুরস্কার পেয়েছিলেন।\n",
      "তিনি এক মাইল সাঁতারে আমার নাম দিয়ে দিলেন।\n",
      "এরপর বাড়ির পুকুরে শুরু করালেন প্র্যাকটিস।\n",
      "যতটা সময় প্রাইভেট পড়াতেন, তার চেয়ে বেশি সময় সাঁতার অনুশীলন করাতেন তিনি।\n",
      "প্রশ্ন : প্রাইভেট শিক্ষক সাঁতার শেখাচ্ছেন - ব্যাপারটি বাঙালি মধ্যবিত্ত পরিবারের জন্য মেনে নেওয়া একটু কঠিন ছিল না?\n",
      "মোশাররফ : আমার বাবার কিন্তু উল্টো উত্সাহ ছিল।\n",
      "কারণ উনি নিজে ছিলেন ক্রীড়াবিদ।\n",
      "তারুণ্যে কলকাতায় সাঁতার কেটেছেন, ওয়াটার পোলো লিগে খেলেছেন; কাবাডি খেলে বেড়াতেন পুরো দেশে।\n",
      "১৯২০ সালে নিখিল ভারতের লং জাম্প প্রতিযোগিতায় দ্বিতীয় হয়েছিলেন বলেও আমাকে বলেছেন।\n",
      "খেলাধুলা তাই পরিবারেই ছিল।\n",
      "আর দ্বিতীয় শ্রেণিতে পড়ার সময় ওই এক মাইল সাঁতারে কলেজ-বিশ্ববিদ্যালয়ের ছেলেদের হারিয়ে যখন প্রথম হয়ে গেলাম, তখন আমাকে আর পায় কে!\n",
      "প্রশ্ন : পুরস্কার কী দিয়েছিল, মনে আছে?\n",
      "মোশাররফ : শিল্ড।\n",
      "সেটি উচ্চতায় প্রায় আমার সমান।\n",
      "পুরস্কার দিচ্ছিলেন আমাদের মুন্সীগঞ্জের এসডিও; পরে যে নির্বাচন কমিশনার হয়েছেন আবু হেনা সাহেব, উনি।\n",
      "শিল্ড তুলে দেওয়ার সময় তিনি বলেছিলেন, 'তুমি তো একদম সাপের মতো সাঁতার কাটলে; পানিও টের পেল না।'\n",
      "প্রশ্ন : এরপর?\n",
      "মোশাররফ : এরপর সাঁতারটা ভালোবেসে ফেললাম।\n",
      "বজ যোগিনী জয়কালী হাই স্কুলে পড়ার সময় পূর্ব পাকিস্তান আন্ত স্কুল সাঁতারে চ্যাম্পিয়ন হই আমি।\n",
      "এটি ১৯৬৮ সালের কথা।\n",
      "ফাইনালে অতিথি ছিলেন সব্যসাচী ক্রীড়াবিদ কাজী আবদুল আলিম।\n",
      "১০ ইভেন্টের স্বর্ণপদকজয়ীর জন্য ১০টি কাপ এবং একটি বড় কাপ চ্যাম্পিয়নদের চ্যাম্পিয়নের জন্য।\n",
      "প্রথম পুরস্কার নেওয়ার জন্য যখন মঞ্চে যাই আলীম ভাই আমাকে বললেন, 'মোশাররফ, এক কাজ কর।\n",
      "বারবার আসার দরকার নেই।\n",
      "ওখানে যে ১০টি ট্রফি আছে এবং একটি বড় ট্রফি - সব নিয়ে নাও একবারে।\n",
      "ওই সবই তোমার।'\n",
      "প্রশ্ন : ছোটবেলায় অন্য কোনো খেলা খেলেননি?\n",
      "মোশাররফ : পোলভোল্টে পূর্ব পাকিস্তানের আন্ত স্কুলে দ্বিতীয় হয়েছি।\n",
      "১৫০০ মিটার দৌড়াতাম; খুব একটা ভালো করতে পারিনি অবশ্য।\n",
      "শর্টপুট-ডিসকাস ভালো করতাম।\n",
      "আবার স্কুল দলে ফুটবলও খেলেছি; গোলরক্ষক ছিলাম।\n",
      "প্রশ্ন : আপনার অন্য ভাইয়েরা কি খেলাধুলার সঙ্গে ছিলেন?\n",
      "মোশাররফ : হ্যাঁ, আমরা চার ভাই একসঙ্গে রিলেতে সাঁতরেছি পর্যন্ত।\n",
      "আন্ত মিল প্রতিযোগিতায়।\n",
      "সেখানেও প্রথম হই।\n",
      "আমার ছোট দুই ভাই অনেক আন্তর্জাতিক মিটেও সাঁতরেছে।\n",
      "প্রশ্ন : শুরুর দিকে বলেছিলেন যে, জাতীয় চ্যাম্পিয়নশিপে শেষ যেবার পানিতে নামেন, সেবারও নতুন রেকর্ড করেছিলেন।\n",
      "তখন তাহলে অবসর নিলেন কেন?\n",
      "মোশাররফ : বয়স হয়েছিল তো!\n",
      "১৯৮৪ সালে প্রথম সাফে আমি যখন সাঁতরাই, তখন আমার বয়স ৩২ বছর।\n",
      "৩৩ বছর বয়সে দেশের সাফ গেমসে পাঁচ স্বর্ণপদক জিতেছি।\n",
      "৩৫ বছর বয়সে ১৯৮৭ সালে কলকাতা সাফ গেমসে আবার তিনটি রৌপ্য জিতি।\n",
      "এত বয়সে সাঁতার কাটা খুব কঠিন।\n",
      "তার ওপর তখন ঠিক করি ইংলিশ চ্যানেল পাড়ি দেব।\n",
      "আমি অল্প দূরত্বের সাঁতারু; ইংলিশ চ্যানেল পাড়ি দেওয়ার জন্য লম্বা দূরত্বের সাঁতারের প্রস্তুতি নিতে হবে।\n",
      "সব মিলিয়েই অবসর নিয়ে ফেলি।\n",
      "প্রশ্ন : ইংলিশ চ্যানেল পাড়ি দেওয়ার ভাবনা মাথায় আসে কেন?\n",
      "মোশাররফ : এটি সব সাঁতারুরই স্বপ্ন।\n",
      "আমার স্বপ্নটি বেশি করে ছিল, কারণ স্বাধীন বাংলাদেশে আর কেউ তা করেনি।\n",
      "আমাদের ব্রজেন দাশ ইংলিশ চ্যানেল পাড়ি দেন কয়েকবার; আবদুল মালেকও পাড়ি দেন ১৯৬৩ সালে।\n",
      "কিন্তু তাঁদের রেকর্ডে দেশের নাম কিন্তু পাকিস্তানই লেখা; পূর্ব পাকিস্তান।\n",
      "ওখানে বাংলাদেশের নাম তোলার জেদ ছিল।\n",
      "প্রশ্ন : যেটি বলছিলেন যে, আপনি তো স্বল্প দূরত্বের সাঁতারু।\n",
      "ওই জেদ পূরণের কাজটি নিশ্চয়ই সহজ হয়নি?\n",
      "মোশাররফ : মোটেই না।\n",
      "প্রতিযোগিতামূলক পর্যায়ে প্রথম লম্বা দূরত্বের সাঁতার করি এরশাদ সাহেবের সময়।\n",
      "সদরঘাট থেকে মুন্সীগঞ্জের গজারিয়া হয়ে নারায়ণগঞ্জ টার্মিনাল - অন্তত ৫০ মাইলের কম নয়।\n",
      "দেশে এমন সাঁতার সেবারই প্রথম।\n",
      "সদরঘাটে এরশাদ সাহেব সাঁতার শুরু করে দেন।\n",
      "তখন আমার গায়ে খুব জ্বর।\n",
      "উনি আমার সঙ্গে হাত মেলাতে গিয়ে বলেন, 'তোর গা এত গরম কেন? জ্বর নাকি?'\n",
      "আমি বলি, 'হ্যাঁ স্যার।\n",
      "কিন্তু পানিতে নামলে জ্বর চলে যাবে।'\n",
      "ঠিকই আমি প্রথম হই।\n",
      "গুলশান লেকে ২২ মাইলের আরেক সাঁতারেও প্রথম হই।\n",
      "তখন নিজের মধ্যে ইংলিশ চ্যানেল পাড়ি দেওয়ার স্বপ্নটা আরো পোক্ত হয়।\n",
      "প্রশ্ন : ইংলিশ চ্যানেল পাড়ি দেওয়ার প্রস্তুতিটা ছিল কেমন?\n",
      "মোশাররফ : আমি ছিলাম সোনারগাঁ হোটেলের হেলথ ক্লাবের আজীবন সদস্য।\n",
      "নতুন হোটেল হয়েছে, সেখানে সাঁতার কাটতে যেতাম।\n",
      "দেশের বড় ব্যবসায়ী, সরকারের উচ্চপদস্থ কর্মকর্তা, নায়ক-নায়িকারা সাঁতার কাটতে যেতেন সোনারগাঁওয়ে।\n",
      "বসুন্ধরার মালিক আহমেদ আকবর সোবহান সাহেব যেতেন।\n",
      "নির্মাণ ইন্টারন্যাশনালের কর্ণধার কে জেড ইসলাম সাহেব যেতেন।\n",
      "নায়ক-নায়িকাদের মধ্যে শাবানা, অলিভিয়া, বুলবুল, আলমগীররা।\n",
      "ওখানে একদিন কে জেড ইসলাম সাহেব আমাকে বললেন, 'মোশাররফ, তুমি তো সাঁতারে অনেক কিছু করলে।\n",
      "ইংলিশ চ্যানেল পাড়ি দেওয়ার চেষ্টা করছ না কেন?'\n",
      "আমি বললাম, 'স্যার, অনেক টাকা লাগবে।'\n",
      "উনি কিন্তু একসময় আমার বস ছিলেন।\n",
      "আমি যখন বিটিএমসিতে, উনি তখন ওখানকার ফিন্যান্স ডিরেক্টর।\n",
      "কে জেড ইসলাম সাহেব আবার জানতে চান, 'কত টাকা লাগবে?'\n",
      "আমি বলি, 'পাঁচ লাখ তো লাগবেই।'\n",
      "উনি বলেন, 'তুমি প্রস্তুতি নাও, আমি পাঁচ লাখ টাকা দেব।'\n",
      "প্রশ্ন : এরপর?\n",
      "মোশাররফ : এরপর আমি বাংলাদেশ সাঁতার ফেডারেশনে যোগাযোগ করি।\n",
      "ওরা 'চ্যানেল সুইমিং অ্যাসোসিয়েশন ইংল্যান্ড'-এর সঙ্গে চিঠি আদান-প্রদান করে আমার জন্য অনুমতি নিয়ে আসে।\n",
      "আসাফউদ্দৌলা সাহেব ক্রীড়া মন্ত্রণালয় থেকে আমার জন্য দেড় লাখ টাকা বরাদ্দ দেন।\n",
      "তা নিয়ে আমি বউ-বাচ্চা নিয়ে ইংল্যান্ড চলে যাই।\n",
      "ওখানে গিয়ে কে জেড ইসলাম সাহেবের পরামর্শমতো লন্ডনের ওয়েস্টমিনস্টার ব্যাংকে অ্যাকাউন্ট করে তাঁকে জানাই।\n",
      "পরদিনই আমার অ্যাকাউন্টে পাঁচ লাখ টাকার সমপরিমাণ পাউন্ড জমা হয়ে যায়।\n",
      "প্রশ্ন : ইংল্যান্ডে যাওয়ার কত দিন পর ইংলিশ চ্যানেল পাড়ি দিলেন?\n",
      "মোশাররফ : আড়াই মাস পর।\n",
      "ওই যে বললাম, কে জেড ইসলাম সাহেবের কাছ থেকে পাওয়া পাঁচ লাখ এবং সরকারের দেড় লাখ - মোট সাড়ে ছয় লাখ টাকা লাগবে কেন?\n",
      "কারণ ওখানে গিয়ে আমাকে তিন মাসের জন্য বাড়ি ভাড়া করতে হবে।\n",
      "একজন ম্যানেজার লাগবে।\n",
      "একজন ট্রেনার লাগবে, যে কিনা আমাকে সাড়ে ২২ মাইলের এই সাঁতারের প্রশিক্ষণ দেবে।\n",
      "প্রতিদিন আট ঘণ্টা করে অনুশীলন করতাম; সাঁতরাতাম অন্তত ২৪ কিলোমিটার অর্থাৎ পুরো চ্যানেলের অর্ধেকের মতো।\n",
      "এর মধ্যে প্রথম দিনটি আমি কখনো ভুলব না।\n",
      "প্রশ্ন : কেন?\n",
      "মোশাররফ : পানি যে কী ঠাণ্ডা ছিল রে ভাই!\n",
      "আমি নেমেই উঠে আসতে চেয়েছিলাম।\n",
      "ট্রেনার কোরি আমাকে নিয়ে গেল।\n",
      "ওখানে অর্চনা পাতিল নামে এক ভারতীয় মেয়ের সঙ্গে পরিচয় হলো।\n",
      "ওর সঙ্গেই নামি পানিতে।\n",
      "কিন্তু হাঁটু পর্যন্ত পানি নেমেই উঠে আসতে চাই ঠাণ্ডার কারণে।\n",
      "মনে হচ্ছিল আমার পা জমে বরফ হয়ে গেছে; এখন তা কেটে ফেললেও টের পাব না।\n",
      "আমার স্ত্রী ছিল ওখানে, ও বলে, 'ভারতের ওই ছোট মেয়েটা পারছে, তুমি পারবে না কেন?'\n",
      "ট্রেনার কোরিও উত্সাহ দিল।\n",
      "আমি পানিতে নেমে অর্চনার সঙ্গে ৪৫ মিনিট সাঁতার কাটলাম।\n",
      "এরপর উঠে আসার সময় দেখি ওপরে থাকা সবাই হাততালি দিচ্ছে।\n",
      "কোরি এসে আমাকে জড়িয়ে ধরে বলে, 'তুমি অসাধারণ করেছ।\n",
      "কারণ প্রথম দিন কেউ এখানে পাঁচ-ছয় মিনিটের বেশি পানিতে থাকতে পারে না।'\n",
      "আমি বলি, 'তাহলে অর্চনা কিভাবে করল?'\n",
      "কোরি বলে, 'ও দুই মাস ধরে অনুশীলন করছে।\n",
      "তবু ৪৫ মিনিটের বেশি পানিতে থাকতে পারে না।\n",
      "আর তুমি কিনা প্রথম দিনই ৪৫ মিনিট থাকলে!'\n",
      "প্রশ্ন : আড়াই মাস পর আপনি ইংলিশ চ্যানেল পাড়ি দিলেন?\n",
      "মোশাররফ : আমি আরো আগেই প্রস্তুত হয়ে গিয়েছিলাম।\n",
      "কিন্তু কর্তৃপক্ষ অনুমতি দিচ্ছিল না।\n",
      "কারণ আমার ওজন কম।\n",
      "এত সময় পানিতে থাকতে হলে ওজন আরো বাড়াতে হবে, শরীরে চর্বি বাড়াতে হবে, যা ঠাণ্ডা থেকে রক্ষা করবে।\n",
      "পরে আমার রুটিনটা ছিল খুব মজার।\n",
      "দিনের ২৪ ঘণ্টার মধ্যে আট ঘণ্টা সাঁতার, আট ঘণ্টা ঘুম আর বাকি আট ঘণ্টা খাওয়া।\n",
      "আর সত্যি সত্যি যেদিন ১০ ঘণ্টা ১৬ মিনিটে ইংলিশ চ্যানেল পাড়ি দিলাম, কী যে ভালো লেগেছিল!\n",
      "এখন থেকে তাহলে ওই চ্যানেল পাড়ি দেওয়াদের মধ্যে বাংলাদেশের নামও থাকবে।\n",
      "এখন পর্যন্ত একমাত্র আমার সৌজন্যেই ওখানে বাংলাদেশের নাম আছে।\n",
      "প্রশ্ন : বাংলাদেশের ইতিহাসে আপনার চেয়ে ভালো সাঁতারু কাউকে দেখেছেন?\n",
      "মোশাররফ : আসবে; সামনে নিশ্চয়ই আসবে।\n",
      "কিন্তু রেকর্ডবই দেখলে বুঝবেন, আমার চেয়ে ভালো সাঁতারু এখন পর্যন্ত কেউ আসেনি।\n",
      "এখানে একটি মজার কথা বলি।\n",
      "এখন দেশের খুবই বড় পর্যায়ে থাকা একজন ব্যক্তির সঙ্গে এক অনুষ্ঠানে দেখা হয়েছিল।\n",
      "এটি অনেক আগের কথা; উনি তখন সম্ভবত জাতীয় সংসদের ডেপুটি স্পিকার।\n",
      "উনি আমাকে মজা করে বলেন, 'অ্যাই মোশাররফ, তুমি কি মনে করো, তুমি বাংলাদেশের সেরা সাঁতারু?\n",
      "তোমার চেয়ে বড় সাঁতারু কেউ নেই?\n",
      "পাকিস্তান আমলে ১৪ আগস্টে স্বাধীনতা দিবস উপলক্ষে নিকলীতে হাওরে সাঁতার হতো।\n",
      "সেখানে আমি বহু বছর প্রথম হয়েছি।\n",
      "তোমার সামনে যে দাঁড়িয়ে আছে, সে-ও কিন্তু তাই তোমার চেয়ে বড় সাঁতারু।'\n",
      "প্রশ্ন : বর্তমান রাষ্ট্রপতি আবদুল হামিদ?\n",
      "মোশাররফ : (হাসি) হ্যাঁ।\n",
      "খুব মজা করে কথা বলেন উনি। ভীষণ রসিক।\n",
      "প্রশ্ন : বাংলাদেশের সাঁতারে আপনি অবিসংবাদিত রাজা ছিলেন।\n",
      "এক সাফে পাঁচটি স্বর্ণপদক পেয়েছেন।\n",
      "যদি বিশ্বমানের প্রশিক্ষণ পেতেন, তাহলে আরো অনেক দূর যেতে পারতেন বলে মনে করেন কি না?\n",
      "ধরুন অলিম্পিক পর্যায়ে?\n",
      "মোশাররফ : সাফে আমার সাফল্য বুড়ো বয়সে।\n",
      "৩২-৩৩-৩৪ বছর বয়স সাঁতারের জন্য অনেক বেশি।\n",
      "১৯৭৯ সালে প্রথম এশিয়ান সাঁতারে তিনটি রৌপ্য পাই আমি।\n",
      "আমার মান বেশ ভালো ছিল।\n",
      "আসলে আমার তরুণ বয়সে যদি ভালো প্রশিক্ষণ পেতাম, তাহলে যেকোনো কিছু সম্ভব ছিল।\n",
      "১৯৭২-৭৩ সালের দিকে রাশিয়া থেকে এক সাঁতার কোচ এসেছিলেন।\n",
      "উনি আমার স্ট্রোক দেখে বলেন, 'তুমি আমার সঙ্গে রাশিয়া চলো।\n",
      "তোমাকে আমি অলিম্পিকে পদক পাইয়ে দেব।'\n",
      "তা কি আর যাওয়া যায়!\n",
      "তবে ওই তরুণ বয়সে বিশ্বমানের প্রশিক্ষণ পেলে আমার অলিম্পিক পদক পাওয়া অসম্ভব ছিল বলে মনে করি না।\n",
      "তবে এখানে আমার ট্রেনার আবদুল কাদির ভাইয়ের কাছে কৃতজ্ঞতা স্বীকার করি।\n",
      "উনি ১৯৭২ থেকে ১৯৮৮ পর্যন্ত পুরোটা সময় আমার ট্রেনার ছিলেন।\n",
      "সাঁতারে যা কিছু অর্জন, তাতে তাঁর অনেক বড় অবদান।\n",
      "আমার মা-বাবার পর তাঁকেই আমি সবচেয়ে বেশি মানি।\n",
      "প্রশ্ন : এত এত পদক জিতছেন, দেশে আপনার ভক্তও ছিল নিশ্চয়ই অনেক?\n",
      "মোশাররফ : বাংলাদেশের ক্রীড়াঙ্গনে তখন দুজনই তারকা।\n",
      "ফুটবলের সালাউদ্দিন আর সাঁতারের মোশাররফ।\n",
      "প্রশ্ন : ভক্তদের কোনো মজার ঘটনা মনে আছে?\n",
      "মোশাররফ : অনেক।\n",
      "তখন তো আর ই-মেইল ছিল না; ভক্তরা চিঠি লিখত।\n",
      "বিভিন্ন পত্রিকা অফিসে, ক্লাবে, সাঁতার ফেডারেশনে এবং যাঁরা আমার বাড়ির ঠিকানা জানত সেখানেও আমার নামে চিঠি আসত।\n",
      "সপ্তাহে এক শ, দুই শ, পাঁচ শ চিঠিও আসত।\n",
      "বেশির ভাগ চিঠি তো মেয়েরাই পাঠাত।\n",
      "ভারতে একবার সাঁতার প্রতিযোগিতায় যাই; ফিরে এসে এসব জায়গার চিঠি একসঙ্গে করে বসি তিন বন্ধু।\n",
      "তখন বন্ধুরা মিলে ঠিক করি, আমরা তিনজন তিনটি চিঠি ওঠাব।\n",
      "আমি যাঁর নাম ওঠাব, তাঁর সঙ্গে দেখা করব।\n",
      "সেটি দেশের যে জায়গাতেই হোক অথবা যদি কলকাতাতেও হয়।\n",
      "কলকাতা থেকেও চিঠি আসত।\n",
      "আর অন্য দুই বন্ধুও এরপর চিঠি তুলবে।\n",
      "যাঁর নাম উঠবে, চিঠির মাধ্যমে ওদের সঙ্গে বন্ধুত্ব করবে আমার নাম দিয়ে।\n",
      "মানে সাঁতারু মোশাররফ সেজে।\n",
      "প্রশ্ন : তাই করেছিলেন?\n",
      "মোশাররফ : হ্যাঁ।\n",
      "দুই বন্ধুর একজন তো অনেক দিন চিঠি চালাচালি করেছিল, রীতিমতো প্রেম হয়ে যায়।\n",
      "মিনি-রূপা-গোপা তিন বোনের নাম থাকলেও চিঠিটি লিখেছিল রূপা; কলকাতা থেকে।\n",
      "আরেক বন্ধুর চিঠি ওঠে ব্রাক্ষণবাড়িয়ার এক মেয়ের।\n",
      "আর আমার তোলা চিঠি ছিল কুমিল্লার এক বড় সরকারি অফিসারের মেয়ের।\n",
      "এর কিছুদিন পরই কুমিল্লায় আমার সংবর্ধনা ছিল।\n",
      "ওখানে গিয়ে ওই মেয়ের ঠিকানা ধরে খুঁজি সাঁতারু অরুণ নন্দীর কাছে।\n",
      "উনি বলেন, 'এই ঠিকানা তো সরকারের বড় অফিসারের।\n",
      "পদমর্যাদায় ডিসির পর যিনি।'\n",
      "ওই বাসায় যাই।\n",
      "ওই অফিসার ভদ্রলোক তখন বাসায় ছিলেন না।\n",
      "তাঁর স্ত্রীকে পরিচয় দিতেই খুব অভ্যর্থনা জানান।\n",
      "তিন মেয়েকে চিত্কার করে ডাকেন।\n",
      "ওই মহিলা নিজের মেজ মেয়েকে দেখিয়ে বলেন, 'দেখুন, ও আপনার কেমন ভক্ত।\n",
      "নিজের রুমের চারদিকে সব তোমার পেপার কাটিং।'\n",
      "সত্যি দেখি তাই; দেয়ালে একটুও ফাঁকা নেই।\n",
      "সব আমার ছবি; আমার পেপার কাটিং।\n",
      "বুঝতে পারছেন তো, কেমন জনপ্রিয় ছিলাম আমি।\n",
      "প্রশ্ন : আপনি বিয়ে করেন কবে?\n",
      "মোশাররফ : আমি বিয়ে করেছি ১৯৮১ সালে।\n",
      "আমার স্ত্রীর নাম আনিলা মোশাররফ।\n",
      "ও ব্যাডমিন্টন খেলত, হার্ডলস করত, রিলে করত।\n",
      "প্রেম করেই বিয়ে।\n",
      "তখন আমাদের নিয়ে পত্রপত্রিকায় অনেক লেখা হয়েছে।\n",
      "আমি তখন এয়ারফোর্সে; ফ্লাইং অফিসার ছিলাম।\n",
      "আমাদের দুজনের ছবি নিয়ে পত্রিকায় হেডিং হয়েছে, 'পাইলট কি সাঁতার সামলাবে নাকি ষোড়শী সামলাবে?' বোঝেন অবস্থা!\n",
      "পরে তো আমাদের বিয়ে হয়।\n",
      "আমার দুটো মেয়ে - মুশায়রা মোশাররফ কেকা, শায়রা মোশাররফ কুহু।\n",
      "ওদের বিয়ে আমেরিকায় হয়েছে; ওখানেই সেটলড।\n",
      "প্রশ্ন : সাঁতার ছাড়ার পর তো সাঁতার ফেডারেশনের সাধারণ সম্পাদকও ছিলেন?\n",
      "মোশাররফ : হ্যাঁ, ১২ বছর সে দায়িত্বে ছিলাম।\n",
      "এরপর ২০০২ সালে আমেরিকা চলে যাই।\n",
      "চলে যেতে একরকম বাধ্য হই।\n",
      "প্রশ্ন : কেন?\n",
      "মোশাররফ : ঢাকা স্টেডিয়ামের সুইমিংপুল আমি টেন্ডার করে লিজ নিয়েছিলাম।\n",
      "ওখানে বাচ্চাদের সাঁতার শেখাচ্ছিলাম।\n",
      "আর ওপরে বানালাম 'সাঁতার জাদুঘর'।\n",
      "ওখানে আমার সব পদক; সব সার্টিফিকেট রাখিয়ে রাখি।\n",
      "শত শত বাচ্চারা প্রতিদিন আসত সেখানে।\n",
      "ওরা দেখত; উজ্জীবিত হতো।\n",
      "'দেখা হয় নাই চক্ষু মেলিয়া' অনুষ্ঠানের ডক্টর এনামুল হক সাহেব ছিলেন জাতীয় জাদুঘরের পরিচালক।\n",
      "উনি আমাকে চিঠি দিলেন যে, আমার সাঁতার জাদুঘরের সব কিছু জাতীয় জাদুঘরে নিতে চান।\n",
      "ওখানে একটি কর্নার করবেন।\n",
      "আমি তখন রাজি হইনি।\n",
      "বলেছি, 'স্যার, আমি সাঁতার জাদুঘর করলাম এত শখ করে।\n",
      "আপনার জাতীয় জাদুঘরে দিব সব; তবে আরো কিছুদিন পর।'\n",
      "এখন মনে হয়, কী ভুলটাই না করেছি!\n",
      "প্রশ্ন : ভুল কেন?\n",
      "মোশাররফ : কারণ ২০০১ সালে আওয়ামী লীগ ক্ষমতা থেকে চলে যাওয়ার পর বিএনপি-জামায়াতের লোকেরা এসে আমার সাঁতার জাদুঘর লুট করে।\n",
      "সব জ্বালিয়ে-পুড়িয়ে দেয়।\n",
      "আমার কোনো পদক আর নেই; কোনো সার্টিফিকেট নেই।\n",
      "আমার কয়েকটি ট্রাংক বোঝাই পেপারকাটিং ছিল; সব পুড়িয়ে দেয়।\n",
      "ভারতের এক প্রতিযোগিতার পুরস্কারে সোনার হরিণ পাই; পাকিস্তানে এক সংবর্ধনায় মুন্নু গ্রুপ দেয় সোনার প্লেট।\n",
      "সব কিছু ওখানে ছিল।\n",
      "যা লুট করে নিয়েছে।\n",
      "এত কিছু করার কারণ আমি আওয়ামী লীগ সমর্থন করি।\n",
      "এটাই আমার অপরাধ!\n",
      "১৯৯৬ সালে আমি 'জনতার মঞ্চে' যোগ দিয়েছিলাম।\n",
      "সেবার নির্বাচনে আওয়ামী লীগ জেতার পর ভালোই ছিলাম।\n",
      "কিন্তু ২০০১ সালে ক্ষমতার পরিবর্তনে আমি টিকতে পারিনি।\n",
      "এমনকি আমাকে গ্রেপ্তার করার সিদ্ধান্তও হয়।\n",
      "বাংলাদেশ নেভির প্রধান নুরুল ইসলাম সাহেবের সঙ্গে খুব ভালো সম্পর্ক ছিল।\n",
      "উনি সব শুনে আমার পাসপোর্ট নিয়ে নিজে গাড়ি চালিয়ে মতিঝিল গিয়ে টিকিট কাটলেন।\n",
      "আমাকে প্লেনের ভেতর বসিয়ে পরে বিদায় নিলেন।\n",
      "আমেরিকায় আমার সব ভাইরা থাকে; রক্তের সম্পর্কের আত্মীয় রয়েছেন অন্তত দেড় শ।\n",
      "তবু আমি দেশে ছিলাম।\n",
      "কিন্তু তখন দেশে থাকলে জেলে যেতে হতো বলে এক দিনের নোটিশে আমেরিকা চলে যাই।\n",
      "স্ত্রী-সন্তানরা ওখানে আগে থেকে থাকায় ভিসা তো আগে থেকেই নেওয়া ছিল।\n",
      "এত বছর আমেরিকায় আছি, কিন্তু এখনো দেশের কথা খুব মনে হয়।\n",
      "প্রশ্ন : শেষ প্রশ্ন।\n",
      "জীবন নিয়ে, ক্যারিয়ার নিয়ে আপনার সন্তুষ্টি কতটা?\n",
      "মোশাররফ : আমি খুব তৃপ্ত।\n",
      "সাঁতারে কত কত পদক পেয়েছি, তা আপনাকে বলেছিই।\n",
      "১৯৭৭ সালে পাই জাতীয় ক্রীড়া পুরস্কার, ১৯৮৬ সালে স্বাধীনতা পদক।\n",
      "বাংলাদেশ ক্রীড়া লেখক সমিতির পুরস্কার পাই ১৯৭৪ ও ১৯৮৫।\n",
      "১৯৮৫ সাফ গেমসের পর 'বেস্ট স্পোর্টসম্যান অব সাউথ এশিয়া' পুরস্কার পাই সাফের সব দেশের সাংবাদিকদের বিবেচনায়।\n",
      "এসব তৃপ্তির তুলনা নেই।\n",
      "সাঁতারের জন্য পৃথিবীর ৭১টি দেশে গিয়েছি।\n",
      "এ কম কথা?\n",
      "আমার বাবাও ছিলেন খুব তৃপ্ত মানুষ।\n",
      "উনি আমাকে বলতেন, 'বাবা, আমার সন্তানরা এত ভালো আছে সবাই, আমি কি স্বপ্ন দেখছি নাকি?'\n",
      "আমার জীবন নিয়ে, ক্যারিয়ার নিয়েও বাবার মতোই মনে হয় - আমি কি স্বপ্ন দেখছি নাকি?\n",
      "\n",
      "The naive person believes every word.- Prov. 14:15. যে অবোধ, সে সকল কথায় বিশ্বাস করে।- হিতো. ১৪:১৫.\n",
      "\n",
      "ঢাকা উত্তরে বিজয়ী আতিক\n",
      "ঢাকা উত্তর সিটি করপোরেশন নির্বাচনে মেয়র পদে পৌনে দুই লাখের বেশি ব্যবধানে জয়ী হয়েছেন আওয়ামী লীগের প্রার্থী আতিকুল ইসলাম।\n",
      "শনিবার দিনভর ভোটগ্রহণ শেষে বিভিন্ন কেন্দ্রের ফল সন্নিবেশিত করে ঢাকার শেরে বাংলা কৃষি বিশ্ববিদ্যালয়ে স্থাপিত সংগ্রহ ও পরিবেশন কেন্দ্র থেকে রাত পৌনে ৩টায় চূড়ান্ত ফল ঘোষণা করেন এই নির্বাচনের রিটার্নিং কর্মকর্তা আবুল কাসেম।\n",
      "নৌকা প্রতীকে আতিকুল ইসলাম পেয়েছেন ৪ লাখ ৪৭ হাজার ২১১ ভোট, তার নিকটতম প্রতিদ্বন্দ্বী বিএনপির তাবিথ আউয়াল ধানের শীষ প্রতীকে পেয়েছেন ২ লাখ ৬৪ হাজার ১৬১ ভোট।\n",
      "ইভিএমে ভোট নেওয়ার পরেও ফল ঘোষণায় এত দেরি হল কেন- সাংবাদিকরা বার বার এই প্রশ্ন করলেও তা এড়িয়ে যান রিটার্নিং কর্মকর্তা কাসেম।\n",
      "ঢাকা উত্তরে মোট ভোটার ছিল ৩০ লাখ ১০ হাজার ২৭৩।\n",
      "এর মধ্যে ২৫ দশমিক ৩০ শতাংশ ভোট দেন বলে রিটার্নিং কর্মকর্তা জানিয়েছেন।\n",
      "মোট ভোট দিয়েছেন ৭ লাখ ৬০ হাজার ৬৫৮ ভোট।\n",
      "মেয়র পদে ভোট দেননি ১৫০৩ জন।\n",
      "ঢাকা উত্তরে মোট ২ হাজার ১২টি ভোটকেন্দ্রের ৭ হাজার ৮৪৬টি ভোট কক্ষে শনিবার সকাল ৮টা থেকে বিকাল ৪টা পর্যন্ত একযোগে ইভিএমে ভোটগ্রহণ চলে।\n",
      "উত্তর সিটি করপোরেশনে মেয়র পদে অন্য প্রার্থীদের মধ্যে কাস্তে প্রতীকে সিপিবির আহম্মেদ সাজেদুল হক রুবেল ১৫ হাজার ১২২ ভোট, হাতপাখা প্রতীকে ইসলামী আন্দোলনের শেখ ফজলে বারী মাসউদ ২৮ হাজার ২০০ ভোট, আম প্রতীকে এনপিপির আনিসুর রহমান দেওয়ান ৩ হাজার ৮৫৩ ভোট এবং বাঘ প্রতীকে পিডিপির শাহীন খান ২ হাজার ১১১ ভোট পেয়েছেন।\n",
      "\n",
      "(Isaiah 8:19, 20; Romans 15:4) It is wrong to add to God's Word or to take anything away from it. - Deuteronomy 4:2; Revelation 22:18, 19. (যিশাইয় ৮:১৯, ২০; রোমীয় ১৫:৪) ঈশ্বরের বাক্যের সঙ্গে কিছু যোগ করা অথবা এর থেকে কিছু বাদ দেওয়া অন্যায়। - দ্বিতীয় বিবরণ ৪:২; প্রকাশিত বাক্য ২২:১৮, ১৯.\n",
      "\n",
      "What an epic choke\n",
      "\n",
      "এইচএসসি ও সমমানের ফল প্রকাশ ১৭ জুলাই\n",
      "আগামী ১৭ জুলাই উচ্চমাধ্যমিক সার্টিফিকেট (এইচএসসি) ও সমমান পরীক্ষার ফল প্রকাশ করা হবে বলে শিক্ষা মন্ত্রণালয় সূত্রে জানা গেছে।\n",
      "ওইদিন সকাল ১০টার দিকে গণভবনে প্রধানমন্ত্রী শেখ হাসিনার হাতে ফলাফলের অনুলিপি তুলে দেওয়া হবে।\n",
      "পরে সংবাদ সম্মেলন করে বিস্তারিত জানাবেন শিক্ষামন্ত্রী ডা. দীপু মনি।\n",
      "গত ১ এপ্রিল এইচএসসি ও সমমান পরীক্ষা শুরু হয়।\n",
      "আটটি সাধারণ শিক্ষা বোর্ড, মাদরাসা ও কারিগরি শিক্ষাবোর্ড মিলিয়ে মোট পরীক্ষার্থী ছিল ১৩ লাখ ৫১ হাজার ৫০৫ জন।\n",
      "এরমধ্যে সাধারণ শিক্ষা বোর্ডের অধীনে শুধু এইসএসসি পরীক্ষার্থী ছিল ১১ লাখ ৩৮ হাজার ৭৪৭ জন।\n",
      "তত্ত্বীয় পরীক্ষা শেষ হয় ১১ মে আর ১২ থেকে ২১ মের মধ্যে শেষ হয় ব্যবহারিক পরীক্ষা।\n",
      "গত বছর এইচএসসি পরীক্ষায় পাসের হার ছিল ৬৬ দশমিক ৬৪ শতাংশ।\n",
      "আর মোট জিপিএ-৫ পেয়েছিল ২৯ হাজার ২৬২ জন। পি\n",
      "\n",
      "সমাজ সংস্কার আন্দোলনের প্রগতিশীল নেতা রাজা রামমোহন রায় (১৭৭৪-১৮৩৩), ঈশ্বরচন্দ্র বিদ্যাসাগর (১৮২০-১৮৯১), ব্রিটিশ পন্ডিত ডিরোজিও, ব্রাহ্মসমাজ নেতৃবৃন্দ এবং অন্যান্য উল্লেখযোগ্য ব্যক্তি নারীদের সামাজিক ও ধর্মীয় নির্যাতন থেকে রক্ষার জন্য ব্যাপক আন্দোলন শুরু করেন। The progressive leaders of social reforms movement like raja rammohun roy (1774-1833), ishwar chandra idyasagar (1820-1891), British scholar d' rozario, leaders of Brahma Samaj and other noted personalities initiated an extensive movement in order to protect the women from social and religious oppression.\n",
      "\n",
      "I erased him out of my mind. আমি তাকে মন থেকে মুছে ফেলেছিলাম\n",
      "\n",
      "উলাট সিদ্দিকিয়া ফাজিল মাদ্রাসা\n",
      "উলাট সিদ্দিকিয়া ফাজিল (ডিগ্রি) মাদ্রাসা পাবনা জেলার সুজানগর উপজেলার অন্যতম প্রাচীন আলিয়া মাদ্রাসা ইসলামী বিদ্যাপীঠ।\n",
      "মাদ্রাসাটি স্থানীয়ভাবে সংক্ষেপে উলাট মাদ্রাসা নামেও পরিচিত।\n",
      "১৯১৫ সালে প্রতিষ্ঠিত এই মাদ্রাসা পাবনা জেলার অন্যতম প্রাচীন শিক্ষা প্রতিষ্ঠান।\n",
      "আলিয়া মাদ্রাসাসমূহের মধ্যে ফলাফলের দিক থেকে মাদ্রাসাটি সুজানগর উপজেলার শীর্ষে থাকে।\n",
      "মাদ্রাসার বর্তমানে অধ্যক্ষের নাম মো. মতিউর রহমান। অবস্থান.\n",
      "মাদ্রাসাটি পাবনা জেলার সুজানগর উপজেলার মানিকহাট ইউনিয়নের উলাট গ্রামে অবস্থিত।\n",
      "এটি ঢাকা পাবনা মহাসড়কের চিনাখড়া বাস স্টেশন হতে ৮ কিলোমিটার দক্ষিণে এবং সুজানগর উপজেলা সদর হতে ১২ কিলোমিটার পূর্বে গাজনার বিলের দক্ষিণ-পশ্চিমে অবস্থিত।\n",
      "মাদ্রাসাটি বর্তমানে ইসলামি আরবি বিশ্ববিদ্যালয়ের অধীনে পরিচালিত একটি ফাজিল মাদ্রাসা। ইতিহাস.\n",
      "উলাট মাদ্রাসা পাবনা জেলার মধ্যে ২য় তম প্রতিষ্ঠিত আলিয়া মাদ্রাসা।\n",
      "১৯১২ সালে প্রথমে মক্তব আকারে চালু করা হয় মাদ্রাসাটি।\n",
      "উলাট গ্রামের হাজী তমিজউদদীন ও বদিউজ্জামান প্রথমে মাদ্রাসার জন্য জমি দান করেন।\n",
      "পরে স্থানীয়দের সহযোগিতায় ছনের একটি ঘর বানানো হয়।\n",
      "১৯১৫ সালে মাদ্রাসাটির ভিত্তিপ্রস্তর স্থাপন করেন ভারতের ফুরফুরা শরীফের পীর মোহাম্মদ আবু বকর সিদ্দিকি, মাদ্রাসার নামকরণ করা হয়, \"উলাট সিদ্দিকিয়া এবতেদায়ী মাদ্রাসা\"।\n",
      "মাদ্রাসার প্রতিষ্ঠাতা প্রধান শিক্ষক ছিলেন যশোর জেলার মাওলানা আব্দুল মালেক।\n",
      "প্রাথমিক এই মাদ্রাসাকে পরিচালনা করার জন্য আশে পাশের গ্রামের সবাইকে নিয়ে মাদ্রাসার পরিচালনা ও উন্নয়ন কমিটি তৈরি করা হয়।\n",
      "এই কমিটির তালিকা দেখুন:\n",
      "মাদ্রাসার সকল অধ্যক্ষের মধ্যে মাওলানা ইসহাক মিয়ার সবচেয়ে দীর্ঘ সময় অধ্যক্ষের দায়িত্ব (১৯৬৭-২০০৪) পালন করেছেন।\n",
      "এবং তার সময়েই মাদ্রাসার সবচেয়ে অবকাঠামো ও শিক্ষাসূচক উন্নয়ন ঘটেছে। বোর্ডের স্বীকৃতি.\n",
      "১৯১৫ সালে ইবতেদায়ী শাখা চালু হবার পরে ১৯৪২ সালে কলকাতা আলিয়া মাদ্রাসা (বর্তমানে আলিয়া বিশ্ববিদ্যালয়) কতৃক দাখিল শ্রেণী অনুমোদন লাভ করে।\n",
      "তখন মাদ্রাসাটির নাম হয় উলাট সিদ্দিকিয়া সিনিয়র মাদ্রাসা, এবং প্রতিষ্ঠানে সুপারিন্টেন্ডেন্ট বা সুপার পদটি চালু হয়।\n",
      "এরপরে ১৯৭৬ সালে আলিম শ্রেণী অনুমোদন লাভ করে এবিং ১৯৯২ সালে ফাজিল অনুমোদন পায়।\n",
      "ফাজিল অনুমোদনের পরে মাদ্রাসার নামকরন হয়, উলাট সিদ্দিকীয়া ফাজিল (ডিগ্রী) মাদ্রাসা।\n",
      "২০০৬ সালে ফাজিল মাদ্রাসাগুলোকে সাধারণ শিক্ষার সমমান দেওয়ার জন্য ইসলামী বিশ্ববিদ্যালয় বাংলাদেশের অধিভুক্ত করা হয়, এর প্রেক্ষিতে এই মাদ্রাসাও ইবির অধিভুক্তি লাভ করে।\n",
      "মাদ্রাসাটি বর্তমানে আলিম পর্যন্ত বাংলাদেশ মাদ্রাসা শিক্ষা বোর্ড ঢাকা এবং ফাযিল শ্রেণি ইসলামি আরবি বিশ্ববিদ্যালয় (২০১৬ সাল থেকে) অধীনে পরিচালিত।\n",
      "অত্র মাদ্রাসার অধীনে একট কারিগরি শিক্ষাপ্রতিষ্ঠান কম্পিউটার ভকেশনাল ইনস্টিটিউট ২০০৪ সাল থেকে পরিচালিত হচ্ছে। শিক্ষা কার্যক্রম.\n",
      "মাদ্রাসাটি দাখিল ও আলিম ফলাফলের দিক থেকে সুজানগর উপেজলার আলিয়া মাদ্রাসাসমূহের মধ্যে প্রথম স্থান দখল করে থাকে।\n",
      "মাদ্রাসার দাখিল ও আলিম শ্রেণীতে বিজ্ঞান শাখা চালু রয়েছে।\n",
      "এছাড়াও ফাজিল শ্রেণীতে কুরআন, হাদিস, ইসলামের ইতিহাস প্রভৃতি বিভাগ চালু রয়েছে। অধ্যক্ষগনের তালিকা.\n",
      "নিম্নে মাদ্রাসার অধ্যক্ষগণের তালিকা দেওয়া হলো: অবকাঠামো.\n",
      "উলাট গ্রামের প্রাণকেন্দ্রে অবস্থিত এ মাদ্রাসাটি প্রায় তিন একর জমির উপর নিজস্ব ক্যাম্পাস রয়েছে।\n",
      "তিনটি দ্বিতলা ও একটি একতলা বিশিষ্ট ভবন ও একটি চারতলা ভবন নির্মাণাধীন।\n",
      "বর্তমানে শিক্ষক কর্মচারীর পদের সংখ্যা ৩৪ জন। সুযোগ-সুবিধা. বাৎসরিক উৎসব.\n",
      "এই মাদ্রাসা প্রাঙ্গনে প্রতিষ্ঠার সময় থেকেই প্রতি বছর ইসলামী জালছার আয়োজন করা হয়ে থাকে।\n",
      "যা সুজানগর উপজেলার মধ্যে সবচেয়ে বড় ইসলামি মাহফিল হিসাবে পরিচিত।\n",
      "আর ১৯৩০-৪০ সালের দিকে এই জালসায় বিভিন্ন জেলার মানুষের সমাগম ঘটতো।\n",
      "এছাড়াও মাদ্রাসার মাঠটি বিভিন্ন রাজনৈতিক প্রোগ্রামের স্থান হিসাবে ব্যবহৃত হয়। সহশিক্ষা কর্মসুচি. বাংলাদেশ স্কাউট\n",
      "বাৎসরিক শিক্ষা সফর\n",
      "বাৎসরিক সাংস্কৃতিক ক্রীড়া প্রতিযোগিতা পোশাক.\n",
      "ছাত্রঃ সাদা পাজামা, গাড় সবুজ (পেস্ট কালার) পাঞ্জাবী, সাদা টুপি এবং সাদা জুতা-মোজা।\n",
      "ছাত্রীঃ সাদা পাজামা, গাড় সবুজ (পেস্ট কালার) ফ্রক, সাদা স্কার্ফ এবং সাদা জুতা-মোজা। প্রাক্তন শিক্ষার্থী.\n",
      "আব্দুস সুবহান - প্রাক্তন শিক্ষার্থী ও মাদ্রাসার চতুর্থ অধ্যক্ষ।\n",
      "আবদুল্লাহ - মাদ্রাসার প্রাক্তন শিক্ষার্থী ও আরবি শিক্ষক। বহিঃসংযোগ.\n",
      "মাদ্রাসার অফিশয়াল ফেসবুক পাতা\n",
      "সহপাঠি ওয়েবসাইটে মাদ্রাসা\n",
      "\n",
      "জুন ৫ বাইবেল পাঠ: গীতসংহিতা ৩৪-৩৭ গান ১৬৭ June 5 Bible reading: Psalms 34-37 Song 167\n",
      "\n",
      "এ বার ধনখড় ক্ষুব্ধ হেলিকপ্টার না-পেয়ে\n",
      "রাজ্যপাল জগদীপ ধনখড় ফের ক্ষুব্ধ।\n",
      "এবার কারণ হেলিকপ্টার না পাওয়া।\n",
      "আজ মঙ্গলবার রাস উৎসবে যোগ দিতে রাজ্যপালের শান্তিপুর সফরের জন্য সরকারের কাছে হেলিকপ্টার চেয়েছিল রাজভবন।\n",
      "প্রশাসনিক কারণে সরকার তা দিতে পারবে না বলায় রাজ্যপালের অসন্তোষ জানিয়ে নবান্নকে পাল্টা চিঠি দেওয়া হয়েছে।\n",
      "ধনখড়ের আজ সড়কপথে শান্তিপুর যাওয়ার কথা।\n",
      "রাজ্যপালের কথায়, ''কতটা অসৌজন্য!\n",
      "একদিনের জন্য হেলিকপ্টার চাইলেও তা দেওয়া যাবে না জানিয়ে দুঃখ প্রকাশ করে চিঠি পাঠানো হয়।'\n",
      "' সূত্রের খবর, গত ৪ নভেম্বর নবান্নের তরফে রাজভবনের চিঠির জবাব পাঠানো হয়েছে।\n",
      "নবান্ন সূত্রের খবর, যে হেলিকপ্টারটি রাজ্য সরকার ভাড়া নিয়ে রেখেছে সেটি প্রয়োজনে যাত্রীদের নিয়ে মালদহ, বালুরঘাট, গঙ্গাসাগর যাতায়াত করে।\n",
      "তা ছাড়া মুখ্যমন্ত্রী, রাজ্যের অন্য মন্ত্রীর সফর ও প্রশাসনিক কাজে ব্যবহার করা হয়।\n",
      "সোমবার সেই কপ্টারেই মুখ্যমন্ত্রী মমতা বন্দ্যোপাধ্যায় দক্ষিণ ২৪ পরগনার ঝড়ে বিধ্বস্ত বিভিন্ন এলাকা আকাশপথে পর্যবেক্ষণ করেন।\n",
      "বুধবার ওই হেলিকপ্টারেই তিনি উত্তর ২৪ পরগনার দুর্গত অঞ্চল দেখবেন বলে ঠিক আছে।\n",
      "সরকারের একটি সূত্র জানাচ্ছে, প্রাকৃতিক দুর্যোগের পরিপ্রেক্ষিতে যে কোনও আপতকালীন প্রয়োজনে সরকারি হেলিকপ্টারটি এখন লাগতে পারে।\n",
      "আরও পড়ুন: সমাবর্তনে রীতি ভাঙা নিয়ে প্রশ্ন\n",
      "সোমবারই বিশ্বভারতীর সমবর্তন সেরে শান্তিনিকেতন থেকে ফেরার পথে দুর্গাপুরে রাজ্যপাল বলেন, ''মুখ্যমন্ত্রী আকাশপথে দুর্গত এলাকা পরিদর্শন করেছেন।\n",
      "তাঁর সঙ্গে যোগাযোগ রয়েছে।\n",
      "অন্য এজেন্সির কাছেও খবর নিচ্ছি।\n",
      "যদি প্রয়োজন মনে করি, নিশ্চয় যাব।\n",
      "ক্ষতিগ্রস্তদের কেউ যাতে বিপাকে না পড়েন তা নিশ্চিত করতে হবে।\n",
      "স্বেচ্ছাসেবী সংস্থাগুলির কাছে আবেদন, ক্ষতিগ্রস্ত মানুষের পাশে থাকুন।''\n",
      "কলকাতায় ফেরার পথে এদিনই সিঙ্গুরের বিডিও অফিসে রাজ্যপালের উপস্থিতি ও স্থানীয় লোকজনের সঙ্গে বিডিওর ঘরে বসে কথাবার্তা বলা নিয়েও বিতর্ক শুরু হয়েছে।\n",
      "হুগলি জেলা সূত্রে খবর, রাজ্যপাল ফেরার পথে সিঙ্গুর বিডিও অফিসের 'ওয়াশরুম' ব্যবহার করতে পারেন বলে তাঁর দফতর থেকে জানানো হয়।\n",
      "আরও পড়ুন: বুলবুলের দাপটে ক্ষতিগ্রস্ত এলাকা পুনর্গঠনে জোড়া টাস্ক ফোর্স মমতার\n",
      "কিন্তু তিনি সেখানে বসে কথাবার্তা বলবেন, নানা লোকের মতামত শুনবেন - তার প্রস্তুতি বা খবর প্রশাসনের কাছে ছিল না।\n",
      "তাই সেখানে সরকারি পদাধিকারীও ছিলেন না।\n",
      "তবু ধনখড় যে-ভাবে ফের প্রকাশ্যে 'দরবার' খুলে বসলেন, তা নিয়ে নবান্নে উষ্মা তৈরি হয়েছে।\n",
      "সিঙ্গুরে রাজ্যপালের কাছে কিছু লোক জানান, এখানে (কারখানার জমিতে) কোনওরকম কৃষিকাজ শুরু হয়নি।\n",
      "কেউ কেউ এ নিয়ে তাঁকে হস্তক্ষেপ করতেও বলেন।\n",
      "ধনখড় তাঁদের 'আশ্বস্ত' করেন, ''রাজভবনের দরজা প্রত্যেকের রাজ্যবাসীর জন্য খোলা।\n",
      "রাজভবনের টুইটার হ্যান্ডলে যে কেউ কিছু জানাতে পারেন।\n",
      "আমি রাজ্যের সাংবিধানিক প্রধান হিসেবে আনুষ্ঠানিক ভাবে সিঙ্গুরে আসব।\n",
      "সংশ্লিষ্ট জমি ঘুরে-দেখে মতামত জানাব।''\n",
      "\n",
      "অভিমান (পর্ব - ২)\n",
      "#অভিমান_(পর্ব_২) #সুমাইয়া_আক্তার\n",
      "সুজয় বারবার ক্ষমা চেয়েও যেন আরিফের মন গলছিল না।\n",
      "নীলা জানলে রাগ করবে ভেবে নিজেকে একটু শান্ত করল আরিফ।\n",
      "সুজয় পার্টিতে আর থাকতে চাইল না।\n",
      "কারণ ওর মুখে যত ঘুষির দাগ, সবাই জেনে ফেলবে।\n",
      "তাই সুজয়কে পেছন দরজা খুলে দিল ও।\n",
      "সুজয় চলে গেল।\n",
      "আরিফের শুধু ঠোঁটে একটু আঘাত লেগেছে।\n",
      "রক্ত পড়ছে একটু করে।\n",
      "ফাস্ট এইড বক্সটা খুঁজতে লাগল ও।\n",
      "কোথায় যে রেখেছে নিজেই খুঁজে পাচ্ছে না।\n",
      "নীলা আরিফকে না দেখতে পেয়ে অণু বেগমের কথায় কিচেন ছেড়ে ওর ঘরের দিকে এলো।\n",
      "এমনিতেও আরিফের ঘরটা ঠিকমতো দেখা হয়নি।\n",
      "বেশ অগোছালো ছেলেটা।\n",
      "সবকিছু গোছানো থাকলেও ওর ঘরটা ছিল অগোছালো।\n",
      "বোঝাই যাচ্ছিল গোছানো ঘর অগোছালো করেছে এটা ওটা খুঁজতে গিয়ে।\n",
      "নীলা দরজায় দাঁড়িয়ে দেখল আরিফ একটা বক্স নিয়ে দাঁড়িয়ে আছে।\n",
      "একটু কেশে নীলা বলল, \"আসব?\"\n",
      "আরিফ পেছনে তাকিয়ে দেখল নীলা।\n",
      "নিজের ঠোঁটের পাশটা ঢেকে বলল,\n",
      "\"আমার ঘরে ঢুকতে পার্মিশন কিসের? আয়।\"\n",
      "নীলা দেখল আরিফের ঠোঁটের কোণ বেয়ে রক্ত পড়ছে।\n",
      "নীলা দ্রুত ওর কাছে গিয়ে ঢাকানো হাতটা সরিয়ে বলল,\n",
      "\"এটা কীভাবে হলো?\"\n",
      "আরিফ হেসে বলল,\n",
      "\"ও কিছু না। একটু লেগেছে।\"\n",
      "নীলা ফাস্ট এইড বক্স খুলে তুলায় ডেটল ভিজিয়ে রক্ত মুছে দিল।\n",
      "তেমন কিছু হয়নি, একটু কেটে গেছে।\n",
      "নীলা সামান্য ভয় পেয়ে গেছিল।\n",
      "ওর চোখে হালকা ভয় আরিফের বুঝতে বাকি থাকেনি।\n",
      "আরিফ নীলার অজান্তে মৃদু একটা হাসি দিল।\n",
      "নীলা আরিফের দিকে কিছুক্ষণ তাকিয়ে থেকে বলল,\n",
      "\"তুই এমন কেন?\n",
      "নিজেকে তো বেশ ঠিক রেখেছিস কিন্তু ঘর এমন কেন?\"\n",
      "আরিফ দুষ্টুমি করে বলল,\n",
      "\"ব্যাচেলর ছেলে থাকলে এমন'ই হয়।\" \"জ্বী না।\n",
      "পঁচা ছেলে ঘরে থাকলে ঘরের অবস্থা এমন'ই হয়।\"\n",
      "আরিফ নীলার গাল টেনে বলল,\n",
      "\"এই পঁচা কাকে বলছিস?\n",
      "গাল টেনে খাল করে দেব।\"\n",
      "\"তোকে দুধ চা খেয়ে গুলি করে দেব!\"\n",
      "দু'জনেই হেসে উঠল।\n",
      "তারপর আরিফ বলল,\n",
      "\"চল বাইরে সবাই হয়তো খুঁজছে।\"\n",
      "ওরা বাইরে বেরিয়ে এলো।\n",
      "জয় আরিফকে দেখে চোখ টিপ দিল।\n",
      "আরিফ ইশারায় বোঝাল তেমন কিছুই না।\n",
      "নীলা সোজা কিচেনে গেল।\n",
      "আরিফ জয়ের পাশে বসল।\n",
      "নীলা অণু বেগমের সাথে কি যেন কথা বলছে আর হাসছে।\n",
      "আরিফ চুপ করে ওদিকে তাকিয়ে আছে।\n",
      "মিউজিক বাজছে বলে কিছুই বোঝা যাচ্ছে না।\n",
      "তবে এতটুকু বোঝা যাচ্ছে একজন মা মেয়ে পেয়ে আর একজন মেয়ে মা পেয়ে খুব খুশি।\n",
      "আরিফের ইচ্ছে করছে সময়টা থেমে দিতে।\n",
      "তখন'ই দুইজন ছেলে আরিফের কাছে এসে দাঁড়াল।\n",
      "তাদের মধ্যে একটি ছিপছিপে ছেলে আরিফকে ডাকল,\n",
      "\"আরিফ...?\"\n",
      "ছেলেটির ডাকে ভাবনার জগৎ থেকে ফিরল না আরিফ।\n",
      "আরো একবার ডেকে যখন কোনো সাড়া পেল না তখন তার পাশে দাঁড়ানো সুঠাম দেহের সুন্দর ছেলেটি সামনে এসে আরিফ যেদিকে তাকিয়ে আছে সেদিকে তাকাল।\n",
      "এক অপুরূপ সুন্দরী কিচেনে কাজ করছে।\n",
      "তাকে যেন কিচেনে থাকা মানাচ্ছে না।\n",
      "পিঠে ছড়িয়ে থাকা কোমর পার লম্বা ঝলমলে ঘনো চুলগুলো সে হাতখোপা করে রেখেছে।\n",
      "তারপরও তার সৌন্দর্য যেন এতটুকু কম হয়নি।\n",
      "ছোট কাটা চুলগুলো সামনে আসছে।\n",
      "মাঝে মাঝে সে হাতের পিঠ দিয়ে সেগুলো সরিয়ে দিচ্ছে।\n",
      "কখনো বা চুলগুলোর প্রতি বিরক্ত হয়ে ফুঁ দিচ্ছে।\n",
      "আরিফ আর ছেলেটি একইভাবে তাকিয়ে আছে নীলার দিকে।\n",
      "পেছন থেকে ছিপছিপে ছেলেটি এসে বলে উঠল,\n",
      "\"সায়ান নিজেকে সামলা ভাই।\"\n",
      "তবুও সায়ানের কোনো হেলদোল নেই।\n",
      "পেছনে থাকা ছেলেটি জোরে ধাক্কা দিল আরিফ আর সায়ানকে।\n",
      "দু'জনেই চমকে উঠল।\n",
      "সায়ান একটু গলা ঝেড়ে বলল, \"কি ব্যাপার?\n",
      "হঠাৎ করে কেউ এভাবে ধাক্কা দেয়?\"\n",
      "আরিফ চেয়ার ছেড়ে বলল,\n",
      "\"আরে ঈশান এ সময় তুই?\"\n",
      "ঈশান হেসে বলল,\n",
      "\"আমরা হয়তো ভুল সময়ে এসে পড়েছি। চল সায়ান।\"\n",
      "আরিফ ঈশানের হাত ধরে বলল,\n",
      "\"তুই কি পাগল হয়ে গেছিস?\n",
      "আমি তো এমনিই বলছিলাম।\n",
      "তুই বলেছিলি রাত আটটার আগে আসতে পারবি না।\n",
      "আর এখন তো মাত্র ছয়টা।\n",
      "তাই আর কি।\"\n",
      "\"এত আগে এলাম বলে তোর কোনো সমস্যা হলো না তো?\"\n",
      "\"কি যে বলিস।\n",
      "তুই আমার এত ভালো বন্ধু।\n",
      "আর তোকে নিয়ে সমস্যা হবে?\"\n",
      "কথাটা বলে আরিফ সায়ানের দিকে তাকাল।\n",
      "ঈশান হেসে বলল,\n",
      "\"ও আমার ফুফতুতো ভাই সায়ান।\n",
      "দেশের বাইরে থাকে বেশি। ব্যবসা আছে।\n",
      "সেই ব্যবসার কাজেই দেশে আসা।\n",
      "আমাদের বাড়িতে ঘুরতে এসেছিল।\n",
      "তাই ভাবলাম ওকে রেখে কীভাবে আসি।\"\n",
      "আরিফ হেসে বলল,\n",
      "\"তোকে সাফাই দিতে হবে?\n",
      "তুই না...\"\n",
      "আরিফ সায়ানের সাথে হাত মিলিয়ে পরিচিত হয়ে নিল।\n",
      "জয়ের ডাকে আরিফ বাইরে গেল ওদের বসে রেখে।\n",
      "সায়ান আবারও নীলার দিকে তাকাল।\n",
      "মনে হচ্ছে যেন আকাশ থেকে এক ডানা কাটা লাল পরী নেমে এসেছে।\n",
      "একটু পর নিজে নীলার দিকে ড্যাবড্যাব করে তাকিয়ে থাকায় নিজেই নিজেকে কয়েকটা গালি দিয়ে দিল।\n",
      "মিনমিন করে বলল,\n",
      "\"এভাবে কেউ ড্যাবড্যাব করে তাকিয়ে থাকে?\n",
      "চক্ষুলজ্জা মনে হয় সায়ান তোর কমে গেছে।\"\n",
      "সায়ান যতটা সম্ভব নীলার দিকে না তাকানোর চেষ্টা করছে।\n",
      "কিন্তু চোখ আর মন দু'টোই ওর সাথ দিচ্ছে না।\n",
      "\"এই প্রথম তোকে কোনো মেয়ের দিকে এভাবে তাকিয়ে থাকতে দেখছি।\n",
      "কি ব্যাপার বল তো?\"\n",
      "ঈশানের কথায় সায়ান নিজেকে সামলে মুখ স্বাভাবিক করে বলল,\n",
      "\"তেমন কিছুই না।\"\n",
      "\"কিচেনে আরিফের মা আর ওর ফ্রেন্ড নীলান্তিকা নীলা।\"\n",
      "সায়ান মনে মনে নীলার নামটা কয়েকবার আওড়ে নিল।\n",
      "কত সুন্দর নাম।\n",
      "যাক তবে নামটা জানা হলো। মুখে বলল,\n",
      "\"তো আমাকে বলছিস কেন?\"\n",
      "ঈশান হা হা করে হেসে বলল,\n",
      "\"আমি জানি ওর নাম শুনতে তোর খুব ইচ্ছা করছিল।\"\n",
      "\"কে বলল তোকে শুনি?\"\n",
      "\"আমিও কাউকে পছন্দ করি হ্যাঁ?\n",
      "প্রথম দেখাতে আমারও মনটা চেয়েছিল প্রথমে তার নাম জানতে।\"\n",
      "সায়ান আর কিছু বলল না।\n",
      "টেবিলে থাকা পত্রিকাটি নিজের হাতে নিল।\n",
      "ভান করল যেন ও পত্রিকা পড়তে মহা ব্যস্ত।\n",
      "পত্রিকায় কিছুক্ষণ মুখ ডুবিয়ে থাকল ও।\n",
      "যদিও সকালে খেলার পৃষ্ঠাটা একটু দেখেছিল।\n",
      "তেমন কিছুই নেই পত্রিকায়।\n",
      "ওই নুন চালের হিসাব।\n",
      "বিরক্ত হয়ে পত্রিকাটি নামিয়ে রাখল সায়ান।\n",
      "কিচেনে দেখল নীলা নেই।\n",
      "আরিফের মা একাই কাজ করছে।\n",
      "চারিদিকে তাকিয়েও নীলাকে দেখতে পেল না ও।\n",
      "মিউজিকের আওয়াজটা ভালো লাগছে না সায়ানের।\n",
      "সফট মিউজিক ওর পছন্দ।\n",
      "তাই নিজের পাশে থাকা গিটার হাতে নিয়ে বাইরে বেরিয়ে এলো সায়ান।\n",
      "বাড়ির ডানদিকে একটা সুইমিংপুল।\n",
      "ও প্যান্ট হাঁটুর কাছে গুটিয়ে এনে পুলে নিজের পা ডুবিয়ে দিল।\n",
      "গিটারের দিকে কিছুক্ষণ তাকিয়ে থেকে বাজাতে লাগল।\n",
      "সায়ান খুব সুন্দর গিটার বাজিয়ে গান করতে পারে।\n",
      "বাড়িতে ঝলমলে আলো থাকলেও এদিকটায় একটু আলো কম।\n",
      "এতে ভালোই হয়েছে।\n",
      "চাঁদ নিজের আলোয় কিছুটা ছুঁয়ে দিচ্ছে।\n",
      "গিটারের আওয়াজে প্রকৃতি যেন ঘুম থেকে উঠে গেল।\n",
      "হালকা বাতাস বইতে শুরু করল।\n",
      "চাঁদের কাছে থাকা মেঘটাও যেন সেই সুরে সরে গেল।\n",
      "রোমান্টিকতা বিরাজ করছে পুরো প্রকৃতি জুড়ে।\n",
      "এমন জায়গা খুব কম মেলে।\n",
      "বাড়িতে মিউজিক বাজছে।\n",
      "তবে খুব কম আওয়াজ আসছে বলে সায়ানেরও ভালো লাগছে।\n",
      "নীলাকে দেখার পর যেন ওর গিটারের সুর আরো মিষ্টি হয়ে গেছে।\n",
      "সায়ান পুলের স্বচ্ছ পানির দিকে তাকিয়ে একমনে নীলার মুখটা ভেবে গিটার বাজিয়ে চলেছে।\n",
      "নীলা জয় আর আরিফের সাথে কিছুক্ষণ আড্ডা দিয়ে ঘরের বাইরে এলো।\n",
      "সেখানে ঈশানও উপস্থিত ছিল।\n",
      "নীলা ঈশানকে খুব ভালোভাবে চেনে না বলে তেমন কথা বলল না।\n",
      "শুধু ভালো মন্দ এই আর কি।\n",
      "মিউজিক ভালো লাগছে না নীলার।\n",
      "কানটা ঝালাপালা হয়ে গেছে।\n",
      "ও পেছনের দরজা খুলে বাইরে বেরিয়ে এলো।\n",
      "বাড়ির সামনে বাগান লাগানো হয়নি।\n",
      "ছোট ছোট ঘাস রাখা হয়েছে।\n",
      "তবে কয়েকটা বড় বড় ফুলের গাছ আছে বাইরের দরজায় ঢোকার পরে।\n",
      "বাড়ির পেছনটাতে বাগান লাগানো হয়েছে।\n",
      "গোলাপ থেকে শুরু করে গাঁদা ফুল কোনোটাই বাদ যায়নি।\n"
     ]
    }
   ],
   "source": [
    "!head -n 1000 $bangla_2b_data_path/\"5860926.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset directory\n",
    "dataset_path = \"/workspace/data/Bangla2B+/shards/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# List all text files in the directory\n",
    "text_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/data/Bangla2B+/shards/9768210.txt',\n",
       " '/workspace/data/Bangla2B+/shards/9116996.txt',\n",
       " '/workspace/data/Bangla2B+/shards/8465782.txt',\n",
       " '/workspace/data/Bangla2B+/shards/7814568.txt',\n",
       " '/workspace/data/Bangla2B+/shards/7163354.txt',\n",
       " '/workspace/data/Bangla2B+/shards/6512140.txt',\n",
       " '/workspace/data/Bangla2B+/shards/651214.txt',\n",
       " '/workspace/data/Bangla2B+/shards/5860926.txt',\n",
       " '/workspace/data/Bangla2B+/shards/5209712.txt',\n",
       " '/workspace/data/Bangla2B+/shards/4558498.txt',\n",
       " '/workspace/data/Bangla2B+/shards/3907284.txt',\n",
       " '/workspace/data/Bangla2B+/shards/3256070.txt',\n",
       " '/workspace/data/Bangla2B+/shards/2604856.txt',\n",
       " '/workspace/data/Bangla2B+/shards/1953642.txt',\n",
       " '/workspace/data/Bangla2B+/shards/1302428.txt',\n",
       " '/workspace/data/Bangla2B+/shards/0.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files_short = text_files[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/data/Bangla2B+/shards/9768210.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_files_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "বাঘাইছড়ির সূর্যোদয় লো\n",
      "হার গেটের কাছে দাঁড়াতেই ভেতর থেকে ভেসে এলো এক বয়স্ক মানুষের ভৎর্সনা।\n",
      "কি যেন বলে ধমকালেন!\n",
      "শিকগুলোর ফাক দিয়ে যতদূর চোখ গেলো বিস্তর ফাকা মাঠ।\n",
      "শেষ প্রান্তে লম্বা দ্বিতল ভবন, যার ডান পাশে প্যাগোডা এবং বাম পাশে ছোট ছোট খুপড়ি ঘর।\n",
      "উঁচু এবং ভারি দেয়াল দ্বারা বেষ্টিত চারপাশ, যেন একটা দূর্গ দাঁড়িয়ে!\n",
      "মেইন গেট থেকে কংক্রিটের রাস্তা সোজা চলে গেছে শেষ মাথায়।\n",
      "আধো আলো আধো আধারে গোটা এলাকা জুড়ে ছড়িয়ে রয়েছে গা ছম ছম করা এক ভয় লাগা পরিবেশ।\n",
      "ছেঁড়া মেঘ সরে সরে যেতেই চাঁদের আলোয় ফুটে উঠছে ভেতরটা।\n",
      "কাউকে দেখা গেলো না, কিন্তু একটু আগেও একজন বয়স্ক মানুষের কন্ঠস্বর বেজেছে টমাসের কানে।\n",
      "ছোট্ট করে তিনি ধাক্কা দিলেন লোহার গেটে, তবে তাতে কোন কাজ হলো না।\n",
      "ফের জোর লাগিয়ে ধাক্কা দিতেই কিছুটা খুললো বটে, তবে এমন এক ভীতিকর শব্দ করে উঠলো যা রাতের নীরবতা ভেঙে আচমকা খুলে যাবে ঘুমন্ত মানুষের চোখ।\n",
      "বাতাসে সেই শব্দের তরঙ্গিত রুপ শিশুর গোঙানি হয়ে ফিরে এলো পরক্ষণে।\n",
      "কেমন জানি করে উঠলো তার বুকের ভেতর!\n",
      "নিজেকে প্রশ্ন করেন টমাস; এখানে কেনো এলাম, আসার কি কোন কথা ছিল!\n",
      "সূর্যোদয় দেখবো বলেই না শেষ রাতে বের হলাম।\n",
      "পাহাড়ের পিচঢালা পথ ধরে হাঁটতে শুরু করলাম।\n",
      "ঢালু জায়গার কাছাকাছি আসতে কোন এক অদৃশ্য মায়ার টানে নেমে গেলাম একফালি সলিং রাস্তায়, যার দু পাশের ছোট গাছগুলো মৃদুমন্দ বাতাসে নতজানু হয়ে পড়লো কূর্ণিশ করার ভঙ্গিমায়।\n",
      "যেন আমার আগমনে বুনোগাছ, লতাপাতার দল জানালো সাদর সম্ভাষণ।\n",
      "গেটের সামনে দাঁড়িয়ে এসব ভাবছিলেন টমাস, টের পাননি কখন চুপিসারে উঠে গেছে সূর্য।\n",
      "পূব আকাশে ছড়িয়ে পড়া কিরণ চোখে লাগতে ঘোর কাটলো তার।\n",
      "সূর্যের আলোয় পরিস্কার দেখতে পেলেন ভবনের গায়ে লেখা; অনাথ আশ্রম, বাঘাইছড়ি, রাঙামাটি।\n",
      "তারমানে যেস্থানটি দূর্গ ভেবে তিনি ভুল করছিলেন, সেটি আসলে আশ্রম।\n",
      "লোহার গেটের অল্প একটু ফাকা হওয়া জায়গা দিয়ে তিনি কোন রকম গলিয়ে দিলেন নিজের শরীর।\n",
      "ঠিক তখনই আবার চিৎকার করে উঠলেন সেই বয়স্ক মানুষটি।\n",
      "ছোট্ট খুপড়ি ঘরের জানালা দিয়ে এক ঝলক উঁকি মেরে বাইরে চলে এলেন।\n",
      "হাতে তাঁর লম্বা লাঠি।\n",
      "কংক্রিটের রাস্তায় সেটি দিয়ে আঘাত করে চলেছেন আর বিড়বিড় করে কী যেন বলছেন!\n",
      "মাথাভরা উস্কোখুস্কো চুল ও মুখ ভরা দাড়িতে ঢেকে যাওয়া চেহারা দেখে টমাসের মনে হলো মাথায় কোন গন্ডগোল আছে তাঁর।\n",
      "কিন্তু কি কারণে সাত সকালে ক্ষেপেছেন!\n",
      "এই অপরিচিত জায়গায় হুট করে কোন দিকে দৌড় দেওয়া যায়!\n",
      "ভাবলেন টমাস, আবার পুলোকিত হলেন এই ভেবে যে পাগলা টাইপের লোকটি হয়ে যেতে পারেন তার কোন গল্পের আলোড়িত চরিত্র। টমাস হালদার।\n",
      "একজন জনপ্রিয় জার্মান লেখক।\n",
      "গল্পের নেশাই চষে বেড়ান গোটা দুনিয়া।\n",
      "বিশ্বের বিভিন্ন দেশে অনুদিত হয়েছে তার সাহিত্যকর্ম, পেয়েছেন বহু আন্তর্জাতিক পুরস্কার।\n",
      "মূলত বাস্তব চরিত্রগুলো তুলে ধরতে নিজেই মিশে যান সেইসব চরিত্রের সাথে, ক্লান্তিহীন ভাবে লেগে থাকেন মানুষের ভেতরের মানুষটি খুঁজে পাওয়ার তাড়নায়।\n",
      "গল্পের সন্ধানে তিনি প্রথমবারের মত এসেছেন বাংলাদেশ।\n",
      "যদিও ইতিমধ্যে নানা লোকালয় ঘুরে কাটিয়ে ফেলেছেন এক মাস।\n",
      "যদিও গল্প লেখার মত চরিত্রের খোঁজ মেলেনি, তবুও আশা ছাড়েননি এই ভেবে যে বাঙালির জীবনযাত্রায় গল্পের কোন সীমারেখা নেই।\n",
      "এখানকার মানুষের জীবনে রয়েছে হরেক রকমের গল্প।\n",
      "কোথাও হাসি-আনন্দে ভরপুর, কোথাও আবার সীমাহীন বেদনামাখা, কোথাও আবার সবকিছু থাকার পরও একফালি অপূর্ণতা গ্রাস করেছে সংসার জীবনের সব সুখ।\n",
      "কোথাও আবার হাজারও অনটনের মধ্যে জীবন হয়েছে প্রশান্তিময়, সুখের স্বর্গ।\n",
      "টমাস আরও কয়েক কদম হেটে গেলেন।\n",
      "সবেমাত্র সকাল হতে শুরু করেছে বাঘাইছড়ির আশ্রমে।\n",
      "ঘুম থেকে সদ্য জেগে ওঠা কিশোররা আড়ামোড়া ভাঙছে দাঁড়িয়ে বারান্দায়।\n",
      "সূর্যের সোনালি আভায় চিকচিক করছে ঘাসের ডগায় জমে থাকা শিশির।\n",
      "সেখানে পা বুলিয়ে চলেছে কেউ কেউ।\n",
      "একটু আগে গেটের কাছে দাঁড়িয়ে যে ভয় ভয় ব্যাপারটা কাজ করছিল, তা কাটতে শুরু করেছে।\n",
      "টমাস খেয়াল করলেন হঠাৎ করে বেড়ে গেছে তার হাঁটার গতি।\n",
      "অর্থাৎ দৃষ্টিশক্তি থেকে পাওয়া সংকেতে মস্তিস্কের সেরেব্রল কর্টেক্স কাজের সূত্রপাত ঘটালে পায়ের পেশীতে আদেশমূলক সাড়া পৌছে দিয়েছে মোটর নিউরন।\n",
      "পজেটিভ সিগন্যাল পেয়ে দ্রুত চলতে শুরু করেছে দুই পা।\n",
      "এর আগে অ্যাডরেনালিন নামক হরমোন গায়ের লোম খাড়া করে দিয়েছিল, যা এখন নিসৃত হচ্ছে না।\n",
      "মানবদেহে ক্রিয়াশীল এমন নানা বিষয়ে যৎ সামান্য লেখাপড়া তার করতে হয়েছে কেবলই গল্প লেখার স্বার্থে।\n",
      "লেখালেখির পূর্বশর্ত হলো পড়াশোনা।\n",
      "টমাস দেখলেন হাটা থামিয়ে দিয়ে দাঁড়িয়ে গেছেন সেই বয়স্ক মানুষটি।\n",
      "তবে দাঁড়িয়ে আছেন পথ আগলে।\n",
      "কাছাকাছি পৌঁছে তিনি অনুমান করলেন তাঁর বয়স সত্তর বা পঁচাত্তর।\n",
      "কিছুটা অযতেœর ছাপ রয়েছে শরীরে।\n",
      "শুনতে পেলেন বৃদ্ধ বলছেন, 'বাচ্চা লইতে আইছিস বাচ্চা, বাচ্চা লইতে আইছিস বাচ্চা।'\n",
      "এমন কথা কেনো বলছেন!\n",
      "অবিরাম জিজ্ঞাসা, বাচ্চা লইতে আইছিস, বাচ্চা।\n",
      "কিছুটা বিরক্ত হয়ে টমাস বললেন, বাচ্চা নিতে মানে!\n",
      "আপনার কথা বুঝতে পারলাম না।\n",
      "তখন ব্যাঙ্গাত্মক হাসি দিয়ে নিজের চেহারা বিকৃত করে বয়স্ক মানুষটি যা বললেন, তা বোঝা গেলো না বা ওই শব্দের অর্থ জানা নেই তার।\n",
      "যদিও জার্মান ভাষা ছাড়া দ্বিতীয় যে ভাষায় তিনি বেশি পারদর্শী, তা হলো 'বাংলা।'\n",
      "টমাস স্মিত হেসে সতর্কতার সাথে বৃদ্ধকে অতিক্রম করলেন।\n",
      "এগিয়ে যেতে যেতে তিনি পেছন ফিরে দেখলেন বৃদ্ধ মানুষটি ভাবলেশহীন।\n",
      "তবে ইতিমধ্যে একদল কিশোর তার পেছন পেছন হাঁটতে শুরু করেছে, তারা খিল খিল করে হাসছে।\n",
      "টমাস জানতে চাইলেন তোমরা হাসছো কেনো?\n",
      "কোন উত্তর না দিয়ে ওদের মধ্য থেকে কেউ একজন পাল্টা প্রশ্ন করে; আপনি কি অফিস রুম খুঁজছেন?\n",
      "টমাস উত্তর দেন, আমি কিছু খুঁজছি না।\n",
      "তখন অপর একজন জানতে চাই, তাহলে কোথায় যাবেন?\n",
      "কোথাও না, ঘুরতে এসেছি।\n",
      "তোমাদের এখানে কেউ ঘুরতে আসে না?\n",
      "তখন গম্ভীর ভাবে একটি ছেলে না সূচক জবাব দেয়।\n",
      "'এখানে বিদেশী লোকজন আসে....'\n",
      "এটুকু বলে সে থেমে যায়।\n",
      "টমাস প্রশ্ন করেন, থামলে কেনো?\n",
      "কোন উত্তর দেয় না ছেলেটি।\n",
      "পরে ওদের ভেতর থেকে কেউ একজন বললো, আপনি কি কাউকে নিতে এসেছেন?\n",
      "টমাস চুপ থাকে।\n",
      "হঠাৎ এক কিশোর বলে উঠলো নিতে চাইলে নিতে পারেন, আমি যাবো।\n",
      "তখন অপর এক কিশোর ভিড় ঠেলে সামনে এসে বললো, \"এই তুই না, আমি যাবো।\n",
      "আপনি আমাকে নেন, আমার অনেক বুদ্ধি, শরীরে শক্তিও আছে অনেক।\n",
      "আমি আপনার ছেলে হবো, চাকর হবো, যা বলবেন তাই।\"\n",
      "সেখানে আরও কয়েকটা ছোট বাচ্চা ছুটে এসে তাকে ঘিরে ধরলো।\n",
      "সবাই ভীষণ উৎসুক।\n",
      "ওদের আচরণ, কথাবার্তা এবং কিছুক্ষণ আগে বৃদ্ধের মুখে উচ্চারিত 'বাচ্চা লইতে আইছিস, বাচ্চা' বাক্যটির সাথে টমাস যেন একটা যোগসুত্র খুঁজে পেলেন।\n",
      "তিনি মনে মনে বললেন সম্ভাবত এখান থেকে শিশু সন্তান দত্তক নিয়ে যান নিঃসন্তান দম্পতিরা।\n",
      "এতে করে হয়তো কারো কারো ভাগ্য ফেরে, উদিত হয় নতুন সূর্য, বদলে যায় জীবন।\n",
      "আবার উল্টোটাও ঘটে।\n",
      "দত্তক নেয়ার অন্তরালে পাচার হয়ে যায় অবুঝ শিশুরা।\n",
      "এরাই হয়তো মরুভূমির দেশগুলোতে জনপ্রিয় উটের দৌড় প্রতিযোগিতায় 'জকি' হিসাবে ব্যবহৃত হয়।\n",
      "চরম নির্মমতার শিকার হয়ে করে পঙ্গুত্ববরণ, নিভে যায় জীবন প্রদীপ।\n",
      "কংক্রিটের রাস্তা যেখানে গিয়ে শেষ হয়েছে, সেই প্রাচীন ভবনটির সামনে পৌঁছানোর পর বাচ্চারা টমাসকে নিয়ে ভেতরে প্রবেশ করলো।\n",
      "লম্বা বারান্দা দিয়ে তারা হইচই করতে করতে ঢুকলো ক্লাসরুমে।\n",
      "গম গম করে উঠলো গোটা রুম।\n",
      "সবাই কথা বলে চলেছে, কেউ কারোর কথা শুনছে না।\n",
      "কিন্তু হঠাৎ থেমে গেলো। পিনপতন নীরবতা।\n",
      "কোন নড়াচড়া নেই, কোন শব্দ নেই।\n",
      "সবার দৃষ্টি দরজায়, সেখানে দন্ডায়মান একজন মানুষ, পরনে তার কমলা রঙের ফতুয়া ও কালো প্যান্ট।\n",
      "মাথা ভরা সাদা চুল।\n",
      "তিনি কোন কথা বললেন না।\n",
      "তবে স্পষ্ট তার চাহনির ভাষা।\n",
      "টমাসের মনে হলো রাগ দমিয়ে রাখতে গিয়ে স্ফীত হয়ে গেছে লোকটির চিবুকের দুপাশ।\n",
      "চোখে মুখে ফুটে উঠেছে এক ধরণের কাঠিন্য।\n",
      "ছেলেগুলো মাথা নিচু করে বেরিয়ে যেতে লাগলো।\n",
      "এক সময় শব্দহীন হয়ে পড়লো গোটা রুম।\n",
      "এগিয়ে এসে তিনি বললেন, \"আই এ্যাম উসিরি ভান্তে।\n",
      "সুপারেনটেনডেন্ট অব দ্যা অরফানেজ। অ্যান্ড ইউ?\" টমাস হালদার। ফ্রম জার্মানি।\n",
      "\"বাট ওয়াচিং ইউর ফেস ইট সিম টু মি ইউ আর বেঙ্গলি ম্যান।\"\n",
      "ইয়েস, আমি বাঙালি। বাংলা জানি।\n",
      "আমার বাবা মা দুজনে ভারতীয় বংশোদ্ভূত বাঙালি।\n",
      "মা বৌদ্ধ, বাবা ক্যাথলিক এবং আমি ধর্মহীন।\n",
      "তবে নাস্তিক নই, এক ঈশ্বরে বিশ্বাসী।\n",
      "কি করা হয় জার্মানিতে?\n",
      "সুপারেনটেনডেন্ট সাহেব প্রশ্ন করলেন। লেখালেখি।\n",
      "ও বুঝেছি, জার্নালিস্ট! না, লেখক।\n",
      "কিন্তু এই অনাথ আশ্রমে কি মনে করে? ঘুরতে।\n",
      "তাই বলে সক্কাল বেলা!\n",
      "টমাস বললেন সূর্য ওঠা দেখবো বলে শেষ রাতে কটেজ থেকে বের হয়েছিলাম, তারপর ঘুরতে ঘুরতে চলে এলাম।\n",
      "আচ্ছা একটি বিষয় জানতে খুব কৌতুহল হচ্ছে।\n",
      "ঢোকার সময় প্রবীণ একজন মানুষের সাথে আমার দেখা হয়েছিল।\n",
      "একটি লাঠিও ছিল তাঁর হাতে। কে উনি?\n",
      "ও বুঝেছি, বুড়োর সাথে আপনার দেখা হয়েছে।\n",
      "উনি আপনার ওপর চড়াও হয়নি তো!\n",
      "না, কিছু বলেননি।\n",
      "বিদেশী লোক দেখলে বুড়োর মাথা ঠিক থাকে না, যা সব কান্ড করে বসে!\n",
      "একবার তো এক চাইনিজ ভদ্রলোকের মাথা ফাটিয়ে দিয়েছিলেন।\n",
      "থানা পুলিশ হলো, পরে আমরা গিয়ে ছাড়িয়ে নিয়ে আসি।\n",
      "টমাস জানতে চাইলেন তাহলে এমন মাথা খারাপ লোককে এখানে রেখেছেন কেনো।\n",
      "কি করবো বলুন, তাড়িয়ে দিলেও যেতে চায় না।\n",
      "তাছাড়া ওর প্রতি সবার একটা মায়া জন্মে গেছে।\n",
      "একসময় তিনি আমাদের আশ্রমের গল্পের শিক্ষক ছিলেন।\n",
      "মাথায় সমস্যা দেখা দেয়ায় তাকে বাদ দেয়া হয়েছে।\n",
      "এখানেই থেকে গেছেন, ঘুরেফিরে বেড়ান, বাচ্চাদের সাথে খেলা করেন, বাগান পরিষ্কার করেন, এইসব আর কি!\n",
      "আশ্রমের প্রতি ওর অনেক দরদ।\n",
      "বাচ্চাদের এত যে আদর করে আপনি না দেখলে বুঝবেন না লেখক সাহেব।\n",
      "কি নাম ওর? অনিমেষ ব্যানার্জি।\n",
      "গল্পের ক্লাসে উনি বাচ্চাদের মুগ্ধ করে রাখতেন।\n",
      "নিজেও গল্প লেখেন খুব ভাল।\n",
      "তাঁর লেখা একটি গল্পের পান্ডুলিপি ও ডায়েরি রাখা আছে আমার অফিসের আলমারিতে।\n",
      "আপনি লেখক মানুষ, ওর একটা বই ছাপিয়ে দিতে পারেন।\n",
      "বেচারার মনে অনেক দুঃখ।\n",
      "চলুন সামনের দিকে যাই, সাত সকালে যখন চলেই এসেছেন আশ্রমের রুটিন মাফিক কাজগুলো দেখে নিতে পারেন।\n",
      "আপনার গল্প লেখার কাজে লাগলেও লাগতে পারে।\n",
      "টমাস জানতে চাইলেন এখানে সবাই কি বুদ্ধিষ্ট। না।\n",
      "শিক্ষকদের মধ্যে হিন্দু ও খ্রিস্টান আছে।\n",
      "আর এখানে আশ্রিত অনাথরা!\n",
      "টমাসের প্রশ্ন শুনে একগাল হেসে সুপারেনটেনডেন্ট বললেন \"অনাথের আবার ধর্ম কী, আশ্রমের এসেই না ওরা 'ধর্ম' পায়।\n",
      "এরপর তিনি \"বুদ্ধাং শরণং গচ্ছামি, ধন্মং শরণং গচ্ছামি, সঙ্ঘং শরণং গচ্ছামি\" বলতে বলতে এগোতে থাকলেন।\n",
      "টমাসও তার তালে তালে 'শরণং গচ্ছামি, শরণং গচ্ছামি' বলতে শুরু করলেন এবং গোটা আশ্রম ঘুরে দেখলেন।\n",
      "কিন্তু একটি জিনিস কিছুতেই মেলাতে পারলেন না, ঘুরে ফিরে বারবার একই প্রশ্ন তার মনের ভেতরে ঘুরপাক খেতে থাকলো, কেনো জানি অফিস রুমের পাশের ওই কৃষ্ণচূড়া গাছ, ভবনের পেছনের দোলনা, টুকরো টুকরো অনেক কিছু চেনা মনে হতে লাগলো তার।\n",
      "রাতে ঘুমাতে যাওয়ার আগে অভ্যাস মত সবকিছু নোটবুকে টুকলেন টমাস।\n",
      "পরের দিন ভোরে বাঘাইছড়ি এলাকায় বসবাসরত ক্ষুদ্র নৃ গোষ্ঠির জীবনযাপন কাছ থেকে দেখবেন বলে বের হলেন।\n",
      "বিশেষ করে মুরং উপজাতির বিষয়ে গবেষণাধর্মী কিছু লেখার ইচ্ছে থেকে তিনি চলে গেলেন মুরং পাড়ায়।\n",
      "সেখানে যাওয়ার আগে সেনা ক্যাম্প থেকে অনুমতি নিতে হলো।\n",
      "বরাবরের মত তিনি নিজের লেখক পরিচয়টা খুব সাধারণ ভাবে উল্লেখ করলেন।\n",
      "আট কিলোমিটার আঁকাবাঁকা পথ পাড়ি দিয়ে টমাস পৌঁছালেন পাহাড়ের খুব উঁচুতে, যেখানে পনেরো থেকে বিশটি পরিবারের বসবাস।\n",
      "পর্যটকদের খুব বেশি যাতায়াত নেই সেখানে।\n",
      "তাকে দেখে ওরা উৎসুক হয়ে পড়লো।\n",
      "মুরং সর্দারের সাথে তার পরিচয় হলো, কিন্তু রাতে থাকার ব্যাপারে তিনি আপত্তি করলেন।\n",
      "পরে অবশ্য টমাসের বিশেষ অনুরোধে তিনি বয়স্ক একজন মহিলাকে ডেকে পাঠালেন এবং সেই মহিলা টমাসকে তার বাড়িতে নিয়ে গেলেন।\n",
      "মাটি থেকে দশ বারো ফুট উঁচুতে গাছ, বাঁশ ও শন দিয়ে বানানো একটি মাচাং ঘরে তার থাকার ব্যবস্থা করে দিলেন।\n",
      "বাইরের চারপাশে যখন নিগুঢ় অন্ধকার, অল্প বয়সী দুটো মেয়ে খাবার নিয়ে তার ঘরে প্রবেশ করলো।\n",
      "মিনমিন করে জ্বলছিল একটি লম্ফ।\n",
      "আবছা আলোয় দেখা গেলো তাদের স্বল্প বসন।\n",
      "এক দেড় ফুট চওড়া একটি কাপড় দ্বারা নাভীর নিচ থেকে হাঁটুর উপরিভাগ পর্যন্ত কোন রকম আবৃত্ত।\n",
      "অপর একটি টুকরো কাপড়ে প্যাচিয়ে রাখা স্তনদ্বয়।\n",
      "দেহের বাকি অংশ রয়েছে অনাবৃত্ত।\n",
      "ওদের গলায় বিভিন্ন ধরণের পুঁতির মালা, বাহুতে রুপার বালা এবং মাথার খোপায় কাঠের চিরুনি গুজে রাখা।\n",
      "ডিনার হিসাবে তারা টমাসকে খেতে দিলো 'ন্যাপি' নামে এক ধরণের খাবার।\n",
      "খাওয়া শেষ না হওয়া পর্যন্ত তারা বসে থাকলো।\n",
      "হঠাৎ টমাস দেখলেন সর্দার লোকটা দরজার কাছে দাঁড়িয়ে আছেন এবং তিনি ভেতরে ঢুকতেই মেয়ে দুটো ইতস্ত বোধ করে একপাশে সরে গেলো।\n",
      "তিনি টমাসের কাছে কিছু টাকা চাইলেন এবং চাহিদা মত টাকা পেয়ে চলে গেলেন।\n",
      "এরপর অনেকটা সময় কেউ কোন কথা বললো না, মেয়ে দুটো নিজেদের মধ্যে কোন আলোচনা করলো না।\n",
      "একসময় তাদের একজন নিঃশব্দে একটি মগ এগিয়ে দিয়ে দিলো।\n",
      "তারা আকার ইঙ্গিতে বোঝালো যে পাত্রের পানীয় হচ্ছে তাদের ঘরে বানানো এক ধরণের মদ, যার নাম 'দোচুয়ানি।'\n",
      "টমাস মদের পাত্রটি নিজের ঠোঁটের কাছে ধরলেন এবং তা পান না করে শুধু ঘ্রাণ নিতে থাকলেন।\n",
      "এক সময় ঘরের বাতি নিভে গেলো, ওদের শরীরের বুনো গন্ধে হারিয়ে গেলো মদের ঝাঝালো ঘ্রাণ।\n",
      "সকালে মানুষের কথা বলার শব্দে ঘুম ভাঙলো টমাসের।\n",
      "তখন অনেক বেলা হয়ে গেছে।\n",
      "তিনি মাচাং ঘর থেকে বেরিয়ে নিচে নামতে নামতে দেখলেন দুজন পুলিশ দাঁড়িয়ে আছে।\n",
      "তাদের পেছনে সেই মুরং সর্দার, বয়স্ক মহিলা ও মেয়ে দুটো।\n",
      "ওদেরকে ঘিরে আরও কয়েকজন উপজাতি লোক দাঁড়িয়ে।\n",
      "কেউ কেউ ফিসফিস করে কথা বলছে, কেউ আবার তার দিকে তাকিয়ে যেন চোখ রাঙাচ্ছে।\n",
      "ঘাবড়ে গেলেন টমাস এবং মনে করতে শুরু করলেন গত রাতে ঘুমিয়ে না পড়া পর্যন্ত কি কি ঘটেছিল মাচাং ঘরের নির্জনতায়, একজন পাত্রে মদ ঢালছিল, অপরজন তুলে দিচ্ছিল তার মুখে.....।\n",
      "একটু অন্যমনস্ক হয়ে গিয়েছিলেন টমাস, একজন পুলিশ \"গুড মর্নিং স্যার\" বলতেই তার ভাবনায় ছেদ ঘটলো এবং সামনে তাকালেন।\n",
      "একজনকে বলতে শুনলেন 'এমন জনশূন্য নির্জন পাহাড়ে বিদেশী মানুষের থাকা মোটেও নিরাপদ নয়।'\n",
      "টমাস বললেন, আমি তো সেনা ক্যাম্পের অনুমতি নিয়ে এসেছি।\n",
      "কেউ তো আমাকে বিষয়টি সেভাবে বলেনি।\n",
      "তাছাড়া আমার আরও দু'চারদিন থাকার প্রয়োজন রয়েছে।\n",
      "পুলিশ তখন বিনয়ের সাথে বললো, আপনার নিজের নিরাপত্তার কথা বিবেচনা করে চলে যাওয়া উচিৎ।\n",
      "কেননা এখানে মিয়ানমারের একটি উগ্রপন্থী সন্ত্রাসী গোষ্ঠি মাঝে মধ্যে উৎপাত চালায়।\n",
      "যদিও পরিস্থিতি আগের মত অতটা খারাপ নেই, তবুও আপনি বিদেশী মানুষ, মুক্তিপণের দাবিতে আপনাকে অপহরণ করলে দেশ জুড়ে তোলপাড় শুরু হয়ে যাবে।\n",
      "আমাদের চাকরি যাবে।\n",
      "টমাস আর কথা বাড়ালেন না, গোছগাছ করে তখনই রওনা হয়ে গেলেন।\n",
      "কটেজে ফিরে টমাস দেখলেন আশ্রমের সুপারেনটেনডেন্ট উসিরি ভান্তে লবিতে হাঁটাহাটি করছেন।\n",
      "তার হাতে একটা ডায়েরি এবং বান্ডিল করা কিছু কাগজ রয়েছে।\n",
      "কুশলাদি বিনিময়ের পর সুপারেনটেনডেন্ট বললেন, 'আশ্রমের কাজে আগামীকাল রাতে আমার ঢাকা যাওয়ার কথা রয়েছে।\n",
      "ভাবলাম কবে আপনি আবার চলে যান, যদি দেখা না হয়, তাই অনিমেষের গল্পের পান্ডুলিপি এবং ডায়েরিটা সাথে করে এনেছি, যদি দয়া করে এগুলো আপনি রেখে দিতেন।\n",
      "টমাস একগাল হেসে সেগুলো হাতে নিয়ে পাতা উল্টাতে উল্টাতে ঘরে ঢুকলেন।\n",
      "সুপারেনটেনডেন্ট সাহেব তার কাছ থেকে বিদায় নিয়ে ব্যস্ততার সাথে বেরিয়ে গেলেন।\n",
      "রাতে ডিনার শেষ করে টমাস পড়তে শুরু করলেনঃ\n",
      "শান্তি-অশান্তির মাঝে দোদুল্যমান মানুষগুলো অসুখের যন্ত্রণা প্রশমিত করতে কখনও কখনও মন থেকে, কখনও বা অনিচ্ছা সত্ত্বেও হয়ে ওঠে নিষ্ঠুর।\n",
      "আমি তেমনি একজন নিষ্ঠুর মানুষ হলাম।\n",
      "একজন নিষ্ঠুর পিতা।\n",
      "অথচ এই আমি আমার একমাত্র সন্তানের জন্মের পর বলেছিলাম, সব অপূর্ণতায় পূর্ণতা এনে দেয় সন্তানের মুখ।\n",
      "নির্ভেজাল এই সত্য উপলব্ধির পরও আমি বদলে গেলাম।\n",
      "অকৃতজ্ঞের মত বদলে গেলাম।\n",
      "স্ত্রী অবনিতার সহজ-সরল মুখ কিংবা শিশু অনিরুদ্ধর হাসিমাখা চাহনি আমাকে আবেগী করে তুলতো না আর।\n",
      "কোন কিছু পাওয়ার বিনিময়ে নয়, শ্রেণী উত্থানও নয় বরং শ্রেণী পতনের এক অজানা আশঙ্কায় ভয়ার্ত চিন্তাগুলো দলবদ্ধ হতে থাকলো মনের গহিনে।\n",
      "দায়-দেনা ও সংসারে টানাপোড়নের কারণে ঘটে যাওয়া ছন্দপতন আতœবিশ্বাসে ধরিয়েছিল চিড়!\n",
      "হতে পারে এটাই অন্যতম কারণ, কেননা শুরুর দিকে অল্প বেতনে ব্যাংকের চাকরিতে যুৎসুই হয়ে উঠছিল না জীবন-যাপন।\n",
      "বিয়ের পর সংসারের প্রয়োজনে প্রশস্ত হতে থাকে খরচের হাত।\n",
      "কিন্তু আয়-ব্যয়ের সাথে সঙ্গতিহীন জীবনে ধার-কর্য করে বিলাসিতা অবশ্যই নির্বুদ্ধিতা।\n",
      "এই সব বোকামি আমার নির্ভরতার জায়গাগুলোর সাথে তৈরি করেছিল এক ধরণের দূরত্ব।\n",
      "ভালবাসার মানুষগুলো অচেনা হতে থাকলো এবং নিজের অজান্তে একজন ভিতু মানুষে রুপান্তরিত হলাম।\n",
      "আর এসবের পেছনে স্ত্রী অবনিতাকে দায়ি করে রাগ ক্ষোভে ফেটে পড়তাম।\n",
      "মনে হতো ওর সাথে নিজের কপাল জুড়ে যাওয়াটা ছিল জীবনের সকল দুর্ভাগ্যের সূচনা।\n",
      "কিন্তু আমার এই ভাবনা কতটুকু সঠিক বা কতটুকু ভুল ছিল, সেই উপলব্দি যখন হলো তখন আমি একজন সর্বহারা নিঃস্ব মানুষ।\n",
      "১১ মে ছিল অনিরুদ্ধর জন্মদিন।\n",
      "১৯৮০ সালের এই দিনে তার বয়স যখন পাঁচ বছর, তার সাথে চরম এক নিষ্ঠুর প্রতারণার আশ্রয় নিলাম।\n",
      "একজন পিতা হিসাবে পুত্রের সাথে করা এটাই মনে হয় পৃথিবীর সবচেয়ে জঘন্যতম প্রতারণা।\n",
      "কী ভাবে পারলাম আমি, একজন জন্মদাতা পিতা হয়ে!\n",
      "ছেলেকে বলেছিলাম তোমাকে ভাল একটা স্কুলে ভর্তি করিয়ে দেবো।\n",
      "সেখানে তুমি থাকবে, অনেক রকমের খাবার খাবে, খেলার জন্য বড় মাঠ পাবে, সাইকেল, ফুটবল আরও কত কি যে পাবে, তা সবগুলো না দেখে তোমাকে বলতেও পারছি না।\n",
      "সেখানে তুমি অনেক সঙ্গি পাবে, আমিও থাকবো তোমার সাথে।\n",
      "মনে পড়ে অনিরুদ্ধ প্রশ্ন করেছিল, \"বাবা!\n",
      "মা থাকবে না?\"\n",
      "আমি বলেছিলাম, কাল সকালে তোমার মা চলে আসবেন।\n",
      "সে আর কিছু বলেনি, বোঝেওনি।\n",
      "সম্ভাবত পৃথিবীর কোন শিশু তার জন্মদাতা পিতাকে অবিশ্বাস করতে পারে না।\n",
      "সে হয়তো আমাকে খুঁজেছে, তার মায়ের অপেক্ষা করে গেছে, ফুপিয়ে ফুপিয়ে কেঁদেছে কত দিন কে জানে!\n",
      "মানুষ যখন \"ভাল\"কে \"মন্দ\" ভাবে, ভাল কিছুর মর্যাদা বুঝতে অক্ষম হয়, ভগবান তখন হাতে কলমে শিক্ষা দিয়ে তাকে 'ভাল মন্দ'র পার্থক্য বুঝিয়ে দেন।\n",
      "কতটা অমানুষ আমি, পিতৃহারা একটি মেয়ে, অনিরুদ্ধর মা, যে আমার বিয়ে করা বউ, তাকে জোর করে বাড়ি থেকে তাড়িয়ে দিলাম।\n",
      "সে আমার পা দুটো ধরে মিনতি করে বলেছিল আমাকে তোমার দাসী করে রেখে দাও, কপালের সিঁদুর মুছে দিও না।\n",
      "আমি তার কোন কথা শুনিনি।\n",
      "আমি আমার পরিকল্পনা মাফিক কাকলি নামের পূর্ব পরিচিত একটি মেয়েকে বিয়ে করি।\n",
      "যতদূর জেনেছিলাম, অবনিতা ও তার মা ভারতে চলে যায়।\n",
      "আমিও ঠিকানা বদলে ফেলি।\n",
      "ওই ঘটনার মাসখানেক পর আকস্মিক আমার চাকরি চলে যায় এবং টানা চার মাস বেকার জীবন কাটিয়ে খুবই ছোট্ট একটি প্রতিষ্ঠানে নামমাত্র বেতনে ক্লার্কের চাকরি পাই, যা কাকলিকে প্রচন্ডভাবে হতাশ করে।\n",
      "এবং দিনে দিনে সে আমার প্রতি বীতশ্রদ্ধ হয়ে পড়ে।\n",
      "আমাদের দাম্পত্য জীবনের বছর পুরলো না, তার আগেই সে আমার সঞ্চিত টাকা পয়শা এবং অলংকার যা যা ছিল সবকিছু নিয়ে চলে গেলো অন্য এক পুরুষের হাত ধরে।\n",
      "টমাস হালদার হাসলেন।\n",
      "প্রথম পাতা থেকে আবারও চোখ বুলিয়ে আসলেন এবং যেখানে থেমে গিয়ে ছিলেন সেখানে ফিরে এসে তিনি মনে মনে বললেন অনিমেষ বুড়োর কাহিনী পরিস্কার বোঝা গেলো।\n",
      "নিজের জীবনের ভুল ভ্রান্তিগুলো তিনি লিখে গেছেন গল্পের ঢংয়ে।\n",
      "তাঁর লেখার হাত খারাপ না।\n",
      "টমাস পরের পাতাগুলো আর উল্টালেন না, মাথার কাছে রেখে ঘুমিয়ে পড়লেন।\n",
      "সকালে তিনি ডায়েরি ও পান্ডুলিপি সাথে করে আশ্রমে নিয়ে গেলেন সুপারেনটেনডেন্টকে ফেরত দেবেন বলে।\n",
      "কেননা ডায়রিটা তার কাছে এক ধরণের অভিশপ্ত জঞ্জাল বলে মনে হলো।\n",
      "কিন্তু আশ্রমে গিয়ে তিনি জানতে পারলেন গতকাল রাতে বৃদ্ধ অনিমেষ ব্যানার্জি হঠাৎ অসুস্থ হয়ে পড়লে তাকে স্থানীয় একটি হাসপাতালে ভর্তি করা হয়েছে।\n",
      "মস্তিস্কে রক্তক্ষরণ হওয়ায় তাঁর অবস্থা বেশ খারাপ।\n",
      "সুপারেনটেনডেন্ট সাহেব তাঁর সাথেই আছেন।\n",
      "টমান ভাবলেন হাসপাতালে গিয়ে দেখে আসবেন, আবার মনে মনে বললেন নিষ্ঠুর মানুষের মুখ যত কম দেখা যাই ততই ভাল।\n",
      "দুপুরে খাওয়া দাওয়া শেষ করে কটেজের লবিতে দাঁড়িয়ে তিনি একটা দীর্ঘ নিঃশ্বাস ছাড়লেন এবং মনে মনে বললেন আমাকে হতাশ করলো বাংলাদেশ।\n",
      "লেখার মত কোন কিছুই পেলাম না।\n",
      "মুরং উপজাতির বিষয়ে কিছুই জানা হলো না, আশ্রমের বাচ্চাদের জীবন নিয়ে লেখা যেতো, কিন্তু সেক্ষেত্রে সময়ের প্রয়োজন।\n",
      "সেটি সম্ভব হচ্ছে না, কেননা তাড়াতাড়ি দেশে ফেরার তাগিদ দিয়েছেন বাবা সাইমন হালদার।\n",
      "টমাসের মা অসুস্থ, ছেলেকে দেখার জন্য তিনি অস্থির হয়ে উঠেছেন।\n",
      "যদিও আশ্রমের প্রবীণ ব্যক্তি অনিমেষকে নিয়ে গল্প লেখার একটা পরিকল্পনা মনে মনে করেছিলেন টমাস, প্রথম সাক্ষাতে কৌতুহলী হয়ে উঠেছিলেন, উৎফুল্ল ছিলেন।\n",
      "কিন্তু গোটা জীবন পর্যালোচনা করে তার মনে হলো গল্প হিসাবে খুব একটা জমবে না 'অনিমেষ।'\n",
      "অনার্থক বিক্ষিপ্ত করে তোলা হবে পাঠকের মন।\n",
      "তাঁর প্রায়শ্চিত্তটুকু পড়ে করুণার বদলে বরং ঘৃণাই প্রাধান্য পাবে।\n",
      "তাছাড়া গোটা জীবন কাহিনীতে এমন কোন চমক খুঁজে পাওয়া যাইনি যা গভীরভাবে নাড়া দিতে পারে।\n",
      "তারপরও মনটা বেশ খারাপ হয়ে আছে তার এ কারণে যে, গত এক সপ্তাহ ধরে যাকে নিয়ে কল্পলোকে গল্প বুনছিলেন, সেই মানুষটি এখন মৃত্যুর পথযাত্রী।\n",
      "আর প্রয়োজন না থাকায় স্বার্থপরের মত বিদায় নিচ্ছি আমি, একজন লেখক।\n",
      "কী এক অদ্ভুত ব্যাপার!\n",
      "গল্পের বাস্তবিক বা কাল্পনিক চরিত্রের সাথে লেখকের সম্পর্ক এক ধরণের স্বার্থের বলয়ে আবদ্ধ।\n",
      "যেখানে কেবল একাই লাভবান হন লেখক।\n",
      "রুমে ফিরে খোলা জানালা দিয়ে পাহাড়ের কোল ঘেষে একেবেকে যাওয়া রাস্তা, পাহাড়ী রাস্তায় ব্যবহৃত চাঁদের গাড়িগুলোর সতর্ক ছুটে চলা, টিলার ভাঁজে ভাঁজে ঝুলে থাকা জুমঘর, এমন অনেক কিছু চুপচাপ টমাস দেখছিলেন খন্ড খন্ড দৃষ্টিতে।\n",
      "হঠাৎ কোথা থেকে আসা দমকা হাওয়া ঝড়ের মত নাড়িয়ে দিলো ভেতরের সবকিছু।\n",
      "যেন কাছাকাছি কোথাও আঘাত করে ছুটে এসেছে ভারি বাতাস।\n",
      "টেবিলের ওপর ফেলে রাখা পান্ডুলিপির পাতাগুলো ছড়িয়ে গেলো।\n",
      "সেই বাতাসে অনিমেষের পাতলা থিনথিনে ডায়েরির পাতাগুলো উল্টে যেতে যেতে যেখানে গিয়ে থামলো, সেই পাতায় আটকে গেলো টমাসের চোখ।\n",
      "ফের পড়তে শুরু করলেনঃ\n",
      "মানুষের নিয়তি বড় অদ্ভুত!\n",
      "যে আশ্রমে নিজের সন্তানকে অনাথ বানিয়ে রেখে গেলাম, এক বছরের মধ্যে ভাগ্য আমাকে সেখানে ফিরিয়ে আনলো।\n",
      "সেই বাঘাইছড়ির আশ্রম, যেখানের দেয়ালে দেয়ালে শুকিয়ে গেছে আমার পাঁচ বছরের শিশু সন্তান অনিরুদ্ধর চোখের জল।\n",
      "একদিন আশ্রমের গোডাউনে টাল বেধে ফেলে রাখা রেজিষ্ট্রার খাতাগুলো ঘাটতে শুরু করলাম এবং এখানে আসার পর থেকে চুপিচুপি যা আমি খুঁজে যাচ্ছিলাম, তা পেয়ে গেলাম।\n",
      "আমি অনিরুদ্ধর হদিস পেয়ে গেলাম।\n",
      "পিতৃ পরিচয় গোপন করে তাকে আশ্রমে রেখে যাওয়ার পনেরো দিন পর অর্থাৎ ১৯৮০ সালের ২৬ মে একজন বিদেশী ভদ্রলোক অনিরুদ্ধকে দত্তক হিসাবে নিয়ে চলে যান।\n",
      "ওই খাতা থেকে আমি সেই ভদ্র লোকের নাম ঠিকানা সংগ্রহ করি এবং চিঠি লিখি।\n",
      "নিজের সন্তানকে ফিরে পাওয়ার আকুতি জানাই।\n",
      "একাধিক চিঠি দিয়েও তার কাছ থেকে একটি চিঠিরও উত্তর আসেনি।\n",
      "তবে ঠিকানা রেখে দিয়েছি।\n",
      "এরপর আর কিছু লেখা নেই ডায়েরিতে।\n",
      "মনটা তার বিক্ষিপ্ত হয়ে উঠেছে।\n",
      "তার বুকের ভেতরে কেমন এক শুন্যতা অনুভব হচ্ছে।\n",
      "সত্যি সত্যি এটা কি তাঁর জীবনের গল্প!\n",
      "ফাকা পাতাগুলো নিঃশব্দে টমাস উল্টে যেতে লাগলেন।\n",
      "ডায়েরির শেষের পাতায় পৌঁছে তিনি থমকে গেলেন।\n",
      "তাকিয়ে রইলেন অবাক বিস্ময়ে।\n",
      "সেখানে লেখা রয়েছেঃ SAIMON HALDAR\n",
      "বৃদ্ধ অনিমেষের ডায়েরিতে বাবার নাম লেখা কেনো?\n",
      "বাড়ির ঠিকানা হুবহু লেখা কেনো?\n",
      "আশ্চর্যান্বিত হয়ে নিজেকে প্রশ্ন করেন টমাস, এসবের মানে কি?\n",
      "উনি কেনো আমার বাবার কাছে চিঠি লিখতেন!\n",
      "তারমানে আমি কি সেই অনিরুদ্ধ?\n",
      "আর কিছু ভাবতে ইচ্ছে করে না টমাসের।\n",
      "তিনি বিছানায় বসে পড়লেন, চোখ বন্ধ করলেন, আবার খুলে ফেললেন।\n",
      "তার মনের গহীনে এখন যে অশান্তি কু-লী পাকাতে শুরু করেছে তা থেকে পরিত্রাণ পেতে অস্থির হয়ে উঠলেন।\n",
      "বাবা সাইমন হালদারকে ফোন করলেন এবং চরম উত্তেজনার সাথে জানতে চাইলেন তিনি কি তাঁর পোষ্য সন্তান!\n",
      "ওপাশ থেকে কোন জবাব এলো না এবং আবারও জিজ্ঞাসা করতেই বাবা সাইমন হালদার প্রসঙ্গ ঘুরিয়ে বললেন, 'তুমি নিশ্চয় রাত জেগে গল্প লিখেছো, সকালে ঘুমাওনি।\n",
      "তোমার বিশ্রাম দরকার, ঘুমিয়ে পড়ো'।\n",
      "এরপর লাইনটা কেটে গেলে টমাস আবারও ফোন দিলেন এবং বললেন, \"বাবা বিষয়টা জানা জরুরি।\n",
      "আমি কি তোমাদের দত্তক নেয়া ছেলে?\"\n",
      "কয়েক সেকেন্ড পর ওপাশ থেকে কেবল একটি বাক্য উচ্চারিত হলো \"হ্যা তুমি তাই।\"\n",
      "কটেজের ঝুল বারান্দায় চুপচাপ দাঁড়িয়ে আকাশের দিকে চেয়ে হাত জোড় করে আছেন টমাস।\n",
      "গল্পের খোঁজে বাংলাদেশে এসে তিনি নিজেই এখন গল্প হয়ে গেছেন।\n",
      "মনে মনে সৃষ্টিকর্তার অনুগ্রহ প্রার্থনা করে বললেন, \"হে ঈশ্বর তুমি আমার দুঃখিনী মা'কে খুঁজে পেতে সাহায্য কর, আমার জন্মদাতা পিতাকে বাঁচিয়ে রাখো।\"\n",
      "বাঘাইছড়ির সেই হাসপাতালের পথে উর্ধ্বশ্বাসে ছুটতে শুরু করলেন টমাস।\n",
      "\n",
      "চিরচরিত নিয়ম ভেঙে বরের বাড়িতে এলো কনেপক্ষ, নিয়ে গেল বর\n",
      "জুমবাংলা ডেস্ক : চিরচরিত নিয়মানুযায়ী মেহমানসহ বর কনের বাড়িতে গিয়ে বিয়ে করেন।\n",
      "এবার ব্যতিক্রমী এক ঘটনা ঘটেছে মেহেরপুরের গাংনী উপজেলার চৌগাছা গ্রামে।\n",
      "আনুষ্ঠানিকতা সেরে বউ নিয়ে বাড়ি ফেরেন সকলে।\n",
      "শনিবার দুপুরে বিয়ের কনে যাত্রীদের নিয়ে স্বয়ং বরের বাড়িতে হাজির হয়ে বিয়ের পিঁড়িতে বসেন।\n",
      "যৌতুকমুক্ত বিয়ে ও নারী অধিকার নিশ্চিত করতে উভয় পরিবারের আয়োজনে এ বিয়ে বলে জানিয়েছেন তারা।\n",
      "আড়ম্বরপূর্ণ বিয়ের আয়োজন।\n",
      "বরের বিয়ে বাড়ির আশেপাশে আয়োজনের কমতি নেই।\n",
      "রান্না চলছে আর দাওয়াতী মেহমানদের অভ্যর্থনা চলছে সকাল থেকেই।\n",
      "দৃষ্টিনন্দন বিয়ের গেটের দুই পাশে লাইনে দাঁড়ানো অসংখ্য মানুষ।\n",
      "বিয়ের বহর গেটের কাছে আসতেই এক অন্যরকম উত্তেজনাকর আনন্দ।\n",
      "মাইক্রোবাস থেকে নামলেন লাল বেনারসি শাড়ি পরা বধূবেশে কনে।\n",
      "চুয়াডাঙ্গার হাজরাহাটি গ্রামের কামরুজ্জামানের মেয়ে খাদিজা আক্তার খুশি কুষ্টিয়া ইসলামীয়া কলেজে অনার্স পড়ুয়া মেয়ে তার পরিবার ও সহযাত্রীদের নিয়ে বিয়ে করতে আসেন বর মেহেরপুরের গাংনীর চৌগাছার কমরেড আব্দুল মাবুদের ছেলে তরিকুল ইসলাম জয়ের বাড়িতে।\n",
      "ভিন্নধর্মী এ বিয়ের আয়োজন ঘিরে এলাকার মানুষেরও উৎসাহ উদ্দীপনার কমতি ছিল না।\n",
      "উৎসুক দৃষ্টিতে সবাই তাকিয়ে ছিলেন কখন আসবে কনে ও কনেযাত্রীরা।\n",
      "অবশেষে দুপুরে সাতটি মাইক্রোবাস ও ৩০টি মোটরসাইকেলের বহর নিয়ে কনে এসে নামলেন বরের বাড়ির গেটের সামনে।\n",
      "প্রথানুযায়ী ফুল ও মিষ্টি মুখ করিয়ে কনেকে বরণ করেন বর পক্ষ।\n",
      "এরপর শুরু হয় বিয়ের আনুষ্ঠানিকতা।\n",
      "বিয়ের পর বর পক্ষের দাওয়াতী আত্মীয়-স্বজন ও কনে যাত্রীদের ভুড়িভোজ করানো হয়।\n",
      "বিকেলে বরকে নিয়ে কনে চলে যান তার বাবার বাড়িতে।\n",
      "সেখানে কয়েকদিন কাটানোর পর কনে সঙ্গে নিয়ে বর ফিরে আসবেন আপনালয়ে।\n",
      "প্রতিক্রিয়ায় কনে খাদিজা আক্তার খুশি বলেন, নারী-পুরুষের সমান অধিকার হিসেবে একজন মেয়ে একজন ছেলেকে বিয়ে করতে তার বাড়িতে যেতে পারেন তা কখনও বাস্তবায়ন হয়নি।\n",
      "সেই বাধার বৃত্ত ভেঙে আমি শুরু করেছি আশা করছি অনেকেই এখন এটি করবেন।\n",
      "বর তরিকুল ইসলাম জয় বলেন, বিয়েতে সবাই কনের বাড়িতে যায় আমার বিয়েতে কনে এসেছে বিয়ে করতে।\n",
      "বেশ ভালোই লাগছে।\n",
      "বরের বাবা কমরেড মাবুদ বলেন, নারী অধিকার প্রতিষ্ঠার জন্য আমাদের অনেক কিছুই করার রয়েছে।\n",
      "মুখে আমরা বললেও তা বাস্তবায়ন করছি কতটুকু?\n",
      "তাই আমি এ আয়োজনের মধ্য দিয়ে নারী-পুরুষের সমতার বিষয়টি সামনে আনতে চেয়েছি।\n",
      "নারী নেত্রী পারভীন বলেন, বরপক্ষ কনেপক্ষের বাড়িতে যাবে এটি প্রচলিত প্রথা।\n",
      "এ প্রথা ভেঙে কনেপক্ষ বরের বাড়িতে বিয়ে করতে আসছে তা অবশ্যই আনন্দদায়ক। মেয়েরাও পারে।\n",
      "মেয়েরা সব দিকেই আজ এগিয়ে যাচ্ছে।\n",
      "এ বিয়েতে সেটাই প্রমাণিত।\n",
      "\n",
      "ধর্ষিতাদের মধ্যে দুইজন ১৯৯৯ সালের জানুয়ারিতে আক্রমণকারীদের বিরুদ্ধে সাক্ষ্য দেয়ার জন্য গুয়েতেমালায় আসেন। Two of the rape victims returned to Guatemala in January 1999 to testify against their attackers.\n",
      "\n",
      "যেহেতু এই ধরনের পিপি এবং ই দান গ্রহীতা সত্তার নেট খরচ বা নেট অবস্থানকে প্রভাবিত করে না, তাই এটি রাজস্ব, লাভ বা অন্য কোন অর্থায়ন উৎস নয়। এই দান নেট খরচ বা নেট অবস্থানকে প্রভাবিত করে না, ঠিক এখন থেকে কিন্তু হয়ত ভবিষ্যতে। Also , to provide us greater ability to attract and retain technical talent , the legislation would provide authority comparable to that of the executive branch to compensate selected scientific and technical staff at seniorexecutive pay levels . There 's a great demand for qualified technical and scientific staff , and they must be properly compensated .\n",
      "\n",
      "র‍্যান্ডম থট ৯\n",
      "বৃষ্টি মানেই এক নিদারুণ অনুভূতি, বৃষ্টি মানেই সৃষ্টিকে নতুন করে আবিষ্কার করবার এক সাহসী আশামতী।\n",
      "তাই এই ছোট উপলব্ধি আমার....\n",
      "নিঝুম রাতের অবহেলিত বৃষ্টির ধারালো করাত;\n",
      "সহস্র অশ্রুহাতেও দুঃখের মাঝে হাসিকে দেয় বরাত;\n",
      "তর্জনী উঁচিয়ে ভয় দেখায় সমস্ত পাপের প্রতিবিম্বকে;\n",
      "ভেঙেচুরে গুড়িয়ে দিয়ে নতুন করে গড়িয়ে নেবে সমস্ত অসমর্পিত জাতপাতের বেড়াজালকে। -স্বপনীল ❤\n",
      "\n",
      "নিজেকে যাচাই করুন ............ 1\n",
      "ইংরেজি মাসের হিসেব অনুযায়ী বাংলা নববর্ষ আমরা প্রতি বছর এপ্রিল মাসের 14 তারিখ পালন করে থাকি।\n",
      "শুরু থেকে এই নিয়মই চলে আসছে।\n",
      "চলুন একটু পিছনের দিকে ফিরে তাকাই।\n",
      "আপনাদেরকে বলতে হবে_ বাংলা 1লা বৈশাখ, 0001 তারিখে ইংরেজী কত তারিখ ছিল।\n",
      "আর, ইংরেজি নববর্ষের দিন অর্থাৎ 1লা জানুয়ারী বাংলায় কত তারিখ হয় সেটাও চেষ্ট করুন।\n",
      "\n",
      "What does he do? কি করে?\n",
      "\n",
      "রহিম ইয়ার খান\n",
      "রহিম ইয়ার খান  পাকিস্তানের পাঞ্জাব প্রদেশের একটি শহর।\n",
      "Rahim Yar Khan's Shaikh Zayed International Airport on worldatlas.com website সংগ্রহের তারিখ ১১ মার্চ ২০১৮\n",
      "এটি রহিম ইয়ার খান জেলা এবং রহিম ইয়ার খান তেহসিলের রাজধানী।\n",
      "এই শহরের প্রশাসন নয়টি ইউনিয়ন পরিষদে উপবিভক্ত।\n",
      "১৮৮১ সালে, ভাওয়ালপুরের নবাব এই শহরের বর্তমান নাম তার প্রথম সন্তান এবং যুবরাজ রহিম ইয়ার খানের নামে দিয়েছিলেন।\n",
      "Profile of the city of Rahim Yar Khan on world66.com website সংগ্রহের তারিখ ১১ মার্চ ২০১৮Rahim Yar Khan to become municipal corporation Samaa TV News website, Published 13 December 2017, সংগ্রহের তারিখ ১১ মার্চ ২০১৮ জনসংখ্যা.\n",
      "১৯৯৮ সালে শহরের জনসংখ্যা ছিল ২৩৩,৫৩৭, তবে পাকিস্তানের ২০১১ সালের আদমশুমারি অনুসারে, জনসংখ্যা ১৯ বছরে প্রায় ৮০.০২% বৃদ্ধি পেয়ে বেড়ে দাঁড়িয়েছে ৪২০,৪১৯।\n",
      "http://www.citypopulation.de/Pakistan-100T.html শিক্ষা.\n",
      "জেলার স্বাক্ষরতার হার দশম শ্রেণি স্তর বিদ্যালয়ের মোট ৩৮%, যা স্থানীয়ভাবে 'ম্যাট্রিক পরীক্ষায় স্নাতক' নামে পরিচিত।\n",
      "Pakistan Social and Living Standards Measurement Survey (2014-2015) Pakistan Bureau of Statistics, Government of Pakistan website, Published March 2016, Retrieved 12 March 2018 শিক্ষা প্রতিষ্ঠান.\n",
      "শহরে কয়েকটি উল্লেখযোগ্য শিক্ষাপ্রতিষ্ঠান রয়েছে, এর কয়েকটি নিম্নরূপ:\n",
      "খাজা ফরিদ প্রকৌশল ও তথ্য প্রযুক্তি বিশ্ববিদ্যালয়\n",
      "শেখ জায়েদ মেডিকেল কলেজ ও হাসপাতাল\n",
      "ভাওয়ালপুর ইসলামিয়া বিশ্ববিদ্যালয়, আরওয়াইকে ক্যাম্পাস\n",
      "সরকারি কলোনী উচ্চ বিদ্যালয়\n",
      "বীকনহাউস স্কুল সিস্টেম\n",
      "শেখ জায়েদ পাবলিক স্কুল অ্যান্ড কলেজ\n",
      "পাঞ্জাব গ্রুপ অফ কলেজ আর্মি পাবলিক স্কুল অ্যান্ড কলেজ\n",
      "আল-হুদা গ্রামার স্কুল\n",
      "জাতীয় ব্যবসায়িক প্রশাসন ও অর্থনীতি কলেজ\n",
      "জাতীয় গ্যারিসন মাধ্যমিক বিদ্যালয়\n",
      "নিমস (NIMS) স্কুল সিস্টেম\n",
      "\n",
      "That looks like a bad accident. Yeah, should we get out and help? No, there’ s a police car behind us. He’ ll stop. Looks like the one guy lost control in all this rain, and the other one hit him. Yeah. It’ s pretty bad, that car looks like a coke can. These accidents always cause traffic jams on rainy days. Yeah, it looks like we’ re in for a long drive. Ah, well. Put on the news. I got up late and missed it. All right. এটা একটা খারাপ দুর্ঘটনা মনে হচ্ছে। হ্যাঁ, আমাদের কি বের হয়ে সাহায্য করা উচিত? না, আমাদের পিছনে একটা পুলিশ গাড়ি আছে। সে থামবে। মনে হচ্ছে এক লোক বৃষ্টিতে নিয়ন্ত্রণ হারিয়ে ফেলেছে, আর অন্যজন তাকে মেরেছে. হ্যাঁ। এটা খুব খারাপ, গাড়িটা দেখতে কোক ক্যানের মত। এই দুর্ঘটনাগুলো সব সময় বর্ষাকালে যানজটের সৃষ্টি করে। হ্যাঁ, মনে হচ্ছে আমরা অনেক দূরে গাড়ি চালিয়ে যাচ্ছি। আহ, ভাল. খবর দাও। আমি দেরি করে ঘুম থেকে উঠি এবং তা মিস করি। ঠিক আছে।\n",
      "\n",
      "ইসলামি ধর্মীয় নেতা\n",
      "ইসলামী ধর্মীয় নেতাগণ ঐতিহ্যগতভাবে এমন মানুষ ছিলেন, যারা জীবিকা, মসজিদ বা সরকারেরের অংশ হিসাবে, তাদের সম্প্রদায় বা জাতির মধ্যে একটি গুরুত্বপূর্ণ ভূমিকা পালন করে।\n",
      "তবে, অমুসলিম দেশে মুসলমানদের সংখ্যালঘুদের পাশাপাশি তুরস্ক, ইন্দোনেশিয়া ও বাংলাদেশের মত ধর্মনিরপেক্ষ মুসলমান রাষ্ট্রের আধুনিক প্রসঙ্গে ধর্মীয় নেতৃত্ব বিভিন্ন ধরনের আন-আনুষ্ঠানিক আকার নিতে পারে।\n",
      "খতিব মসজিদে ইমামতি করে মসজিদে ইমামতি করে খুতবা দেন(শুক্রবার মধ্যরাত পর্যন্ত নামাজ পড়ার পূর্বে)। আলীম.\n",
      "('Alim) উলামা (আরবি : علماء), একবচন عالم' আলিম, 'পণ্ডিত', আক্ষরিকভাবে \"শিখেছি\", উল্লিখিত শব্দও; স্ত্রীবাচক অর্থে আলিমাহ (একবচন) এবং উলুম (বহুবচন)), \" ইসলামী ধর্মীয় বিজ্ঞান \"ধর্মীয় আধিপত্য\" মধ্যে \"পণ্ডিত বা কর্তৃপক্ষ\"। আল্লামা.\n",
      "আল্লাহ সুবহানাহু ওয়া তায়ালার মর্যাদাসম্পন্ন এবং মর্যাদাপূর্ণ খ্যাতি যা কেবল ইসলামী চিন্তাধারার সর্বোচ্চ শাসক, আইনশাস্ত্র ও দর্শন দ্বারা পরিচালিত হয়।\n",
      "এটি সুন্নি ইসলামের পাশাপাশি শিয়া ইসলামে সম্মানিত হিসাবে ব্যবহার করা হয়। আলমামি.\n",
      "\"আলমামি\" পশ্চিম আফ্রিকার মুসলিম শাখার একটি শিরোনাম, বিশেষ করে ১৯শতকের রাজত্ব রাজ্যে ব্যবহৃত। খলিফা.\n",
      "খলিফা মুলত মুহাম্মাদ এর মৃত্যুর পর মুসলিম সম্প্রদায়ের প্রধান নির্বাচিত ব্যক্তির জন্য ব্যবহৃত হয়। ইমাম.\n",
      "ইমাম একটি আরবি শব্দ অর্থ \"নেতা\"।\n",
      "উদাহরণস্বরূপ একটি দেশের শাসক ইমাম বলা যেতে পারে।\n",
      "তবে শব্দটি ইসলামিক ঐতিহ্য, বিশেষত শিয়া বিশ্বাসে গুরুত্বপূর্ণ উল্লেখ রয়েছে।\n",
      "সুন্নি বিশ্বাসে, শব্দটির ব্যবহার চারটি সুন্নি মাযহাবের প্রতিষ্ঠাতা পণ্ডিতদের জন্য বা ধর্মীয় আইনশাস্ত্রের স্কুলগুলির (ফিক্হ) জন্য ব্যবহৃত হয়। প্রধান ইমাম.\n",
      "আল-আজহার মসজিদ এবং আল-আজহার বিশ্ববিদ্যালয়ের \"প্রধান ইমাম\" বা \"ইমামের ইমাম\" (আরবী: الإمام الأكبر) একটি মর্যাদাকর সুন্নি ইসলাম শিরোনাম এবং মিশরের একটি বিখ্যাত সরকারি শিরোনাম।\n",
      "এটি কিছু মুসলিম দ্বারা ইসলামী আইনশাস্ত্রের জন্য সুন্নি ইসলামের সর্বোচ্চ কর্তৃত্বকে নির্দেশ করে, বিশ্বব্যাপী আধ্যাত্মিক আশআরী ও মাতুরিদি ঐতিহ্যের অনুসারীদের উপর ইমামের প্রভাব একটি বড় প্রভাব রাখে, অথচ আতিয়ার ও সালাফিদের রক্ষাকর্মীরা তাদের নেতাদের খুঁজে বের করে।\n",
      "আরব উপদ্বীপে ইমাম এর ধারণা কোরআন থেকে উদ্ভূত।\n",
      "হজরত ইবরাহীমকে সফল আত্মত্যাগের পর ইমাম হিসাবে উন্নীত করা হয়।\n",
      "বিচারের দিনে প্রত্যেক ব্যক্তি তার ইমামকে ডাকবে।\n",
      "এবং একটি ইমামই মুবীন যিনি পবিত্র গ্রন্থের শিক্ষা অনুযায়ী সমগ্র মহাবিশ্বের অন্তর্ভুক্ত।\n",
      "আল হাক্কি মিজান মিজানি সুফি আদেশের গ্র্যান্ড ইমামের জন্য শিরোনাম হল নোবল ইমপেরিয়াল শেখ। প্রধান মুফতি.\n",
      "\"গ্র্যান্ড মুফতি\" শিরোনাম (আরবি: ) সুন্নি মুসলিম সম্প্রদায়ের ধর্মীয় আইনের সর্বোচ্চ কর্মকর্তা। মুয়াযযিন.\n",
      "মুয়াযযিন হলেন সেই ব্যক্তি যিনি নামাজের সময় হলে জামাতে অংশগ্রহণের আহবান জানিয়ে মসজিদ থেকে উচ্চস্বরে আযান দিয়ে থাকেন।\n",
      "এছাড়া জামাতে নামাজ আদায়ের ক্ষেত্রে তিনিই ইক্বামাহ্‌ দিয়ে থাকেন। মুজতাহিদ.\n",
      "কুরআন ও হাদীস এর দোভাষী, ইসলামিক শাস্ত্র।\n",
      "এই ঐতিহ্যগতভাবে মুফতিরা যারা ইসলামী আইন ব্যাখ্যা করার জন্য ব্যাখ্যা (ইজতেহাদ) ব্যবহার করতেন, কিন্তু অনেক আধুনিক ধর্মনিরপেক্ষ প্রেক্ষাপটে ইসলামি আইন আর স্থলবিহীন আইন নয়।\n",
      "এই ক্ষেত্রে, ঐতিহ্যবাহী মুফতিকে বিশ্ববিদ্যালয়ের বা মাদ্রাসার অধ্যাপকের পদে স্থানান্তর করা হতে পারে, যিনি ঐতিহাসিকভাবে স্থানীয় মুসলমান সম্প্রদায়ের উত্তরাধিকার, তালাক ইত্যাদি বিষয়ক উপদেষ্টা হিসেবে কাজ করেন। কিয়াই.\n",
      "কিয়াই বা কিয়ি মূলত জাভানিজ সংস্কৃতিতে ব্যবহৃত একটি শিরোনাম।\n",
      "শুধুমাত্র একটি পুরুষ ব্যক্তি এই নাম্বার সঙ্গে বলা হয়।\n",
      "তার স্ত্রীকে 'ন্যাই' বলা হয়।\n",
      "প্রাথমিক আধুনিক সময়ে এটি মূলত একটি পন্ডেক পেসেন্টেনের প্রধান শিক্ষকের জন্য ব্যবহৃত হয়।\n",
      "যাইহোক, আজকাল এটি ইন্দোনেশিয়াতে এই শিরোনাম সহ কোন সাংস্কৃতিক ঘটনা থেকে কোন বয়স্ক প্রচারককে ডাকার জন্য সাধারণ।\n",
      "প্রাচীন জাভানিবাসীদের আধ্যাত্মিক বিশ্বাসের কারণে, শিরোনাম 'কিয়াই' প্রায় সব ব্যক্তি ও জিনিসকে কলুষিত করার জন্য ব্যবহার করা হয়।\n",
      "অতএব, ক্রিস, অস্ত্র, গামেলান, বৃক্ষ এবং নির্দিষ্ট পূজা করা প্রাণীদের জন্যও এটি প্রচলিত।\n",
      "শুধুমাত্র শিয়া মুসলমানদের দ্বারা ব্যবহৃত শিরোনাম. আয়াতুল্লাহ.\n",
      "আয়াতুল্লাহ্ (আরবী: آية الله; ফার্সি: آیت‌الله) প্রধান শিয়া ধর্মগ্রন্থের একটি মর্যাদাপূর্ণ শিরোনাম।\n",
      "আয়াতুল্লাহ্ অর্থ \"ঈশ্বরের চিহ্ন\"; যারা এটি বহন করে ইসলামী গবেষণায় বিশেষজ্ঞ বলে মনে করা হয়। গ্র্যান্ড আয়াতুল্লাহ.\n",
      "কেবলমাত্র কয়েকজন গুরুত্বপূর্ণ আয়েতুল্লাহকে গ্র্যান্ড আয়াতুল্লাহ (আয়াতেল্লা উজমা, \"ঈশ্বরের মহান চিহ্ন\") পদে দেওয়া হয়েছে।\n",
      "এটি সাধারণত ঘটে যখন আয়তুল্লাহ এক অনুগামীরা তাকে অনেক পরিস্থিতিতে বলে এবং তার জুরিস্টিক বই প্রকাশ করার জন্য তাকে জিজ্ঞেস করে, যেখানে তিনি দৈনিক মুসলমান বিষয়গুলির অধিকাংশের উত্তর দেন।\n",
      "এই বইটিকে রেসালাহ বলা হয়, যা সাধারণত আল-উরওয়াতু-উল-কিতাব গ্রন্থের একটি পুনর্বিবেচনার কারণ, তাদের সর্বাধিক প্রামাণিক ইসলামিক সূত্রের জ্ঞান এবং বর্তমান জীবনে তাদের আবেদন অনুযায়ী।\n",
      "\n",
      "নতুন কোচের খোঁজ পেয়েছে বিসিবি?\n",
      "বাংলাদেশের কোচ হবেন কে - এ জল্পনা-কল্পনা চলছে অনেক দিন ধরেই।\n",
      "কদিন পর পর শোনা যায় একেকজনের নাম।\n",
      "এখন যেমন শোনা যাচ্ছে সাবেক ইংলিশ উইকেটকিপার ব্যাটসম্যান স্টিভ রোডসের নাম।\n",
      "দু-এক দিনের মধ্যে তাঁর ঢাকায় আসার কথা সাক্ষাৎকার দিতে।\n",
      "চন্ডিকা হাথুরুসিংহের বিদায়ের পর বাংলাদেশের প্রধান কোচ হওয়ার আগ্রহ প্রকাশ করেছেন বেশ কয়েকজন।\n",
      "এঁদের মধ্যে দুজন সশরীরে সাক্ষাৎকারও দিয়ে গেছেন।\n",
      "কারও কারও সঙ্গে কথাবার্তা অনেক দূর এগিয়েও শেষ পর্যন্ত চূড়ান্ত হয়নি।\n",
      "শোনা যাচ্ছে, কোচ সমস্যা সমাধানে বিসিবি নাকি মোটামুটি শেষ ধাপেই পৌঁছে গেছে।\n",
      "নতুন হেড কোচ হিসেবে খুব জোরের সঙ্গেই শোনা যাচ্ছে ৫৩ বছর বয়সী ইংলিশ কোচ স্টিভ রোডসের নাম।\n",
      "অবশ্য বিসিবি ও রোডস দুই পক্ষই বলছে আলোচনা প্রাথমিক পর্যায়েই আছে।\n",
      "বিসিবির প্রধান নির্বাহী নিজামউদ্দিন চৌধুরী তাঁকে সংক্ষিপ্ত তালিকায় রাখার কথাই বলেছেন, 'স্টিভ রোডস সংক্ষিপ্ত তালিকায় আছেন।\n",
      "আশা করছি, আগামী দুদিনের মধ্যে বোর্ডের সঙ্গে তিনি দেখা করবেন।\n",
      "আপনারা অতীতে দেখেছেন রিচার্ড পাইবাস-ফিল সিমন্স এসেছিলেন।\n",
      "একই ধারাবাহিকতায় রোডসও নিয়োগপ্রক্রিয়ার সঙ্গে সংশ্লিষ্ট ব্যক্তিদের সামনে নিজের কর্মপরিকল্পনা উপস্থাপন করবেন।\n",
      "ক্রিকইনফোকে রোডসও বলেছেন প্রায় একই কথা, 'এটা নিশ্চিত করতে পারি, বাংলাদেশের সঙ্গে আমার কথা হয়েছে।\n",
      "এটাও বলতে পারি, আমি আগ্রহী।\n",
      "কাজটা অনেক সম্মানজনক।\n",
      "তবে কোনো কিছুই চূড়ান্ত হয়নি।\n",
      "সবকিছু নিশ্চিত হয়ে গেছে এটা ভেবে নেওয়া উচিত হবে না।'\n",
      "হাথুরু চলে যাওয়ার পর সাকিবদের হেড কোচের সন্ধানে বিসিবি অনেক বড় নামের পেছনেই ছুটেছে।\n",
      "টম মুডি, মাহেলা জয়াবর্ধনে, কুমার সাঙ্গাকারা, অ্যান্ডি ফ্লাওয়ার, জিওফ মার্শ - কে ছিলেন না এই তালিকায়!\n",
      "পল ফারব্রেসের সঙ্গে তো কথাবার্তা অনেক দূর এগিয়েও হলো না।\n",
      "আরেক বড় নাম গ্যারি কারস্টেনকে বিসিবি চেয়েছিল 'ডিরেক্টর অব কোচিং' হিসেবে।\n",
      "পরে প্রোটিয়া কোচের ভূমিকা বদলে হলো 'হেড অব কোচ অ্যান্ড টিম ম্যানেজমেন্ট সিলেকশন'।\n",
      "আপাতত কোচের সন্ধানে বিসিবিকে সহায়তা করাই হবে তাঁর কাজ।\n",
      "সেই কাজের অংশ হিসেবে কদিন আগে ঢাকায় এসেছিলেন কারস্টেন।\n",
      "জানা গেছে, তিনি যে সংক্ষিপ্ত তালিকা করেছেন, সেটিতে রোডসের নাম ওপরেই আছে।\n",
      "তবে বিসিবির একটি সূত্র জানিয়েছে, রোডসের বিষয়টি এখনো ৫০-৫০।\n",
      "তাঁর উপস্থাপনা যদি মুগ্ধতা জাগানিয়া হয়, তবে চূড়ান্ত পর্যায়ে যাবে।\n",
      "১১ টেস্ট ও ৯ ওয়ানডে খেলা রোডস কাউন্টি দল উস্টারশায়ারের হয়ে খেলেছেন ১৯৮৫ থেকে ২০০৪ পর্যন্ত।\n",
      "এই উইকেটকিপার ব্যাটসম্যান খেলা ছাড়ার পর কাউন্টি দলটির ডিরেক্টর অব ক্রিকেটও হয়েছিলেন।\n",
      "বাংলাদেশের খেলোয়াড়দের মধ্যে একমাত্র সাকিব আল হাসানের অভিজ্ঞতা আছে তাঁর সঙ্গে কাজ করার।\n",
      "২০১০ সালে সাকিব যখন কাউন্টি দল উস্টারশায়ারের হয়ে খেলতে গিয়েছিলেন, তখন তাঁদের প্রধান ছিলেন রোডসই।\n",
      "ফ্র্যাঞ্চাইজিভিত্তিক টি-টোয়েন্টি ক্রিকেটের দাপটে লম্বা সময়ের জন্য এখন 'হাই প্রোফাইল' কোচ পাওয়াই কঠিন হয়ে গেছে বিসিবির।\n",
      "বিসিবি এখন 'হাইপ্রোফাইল' কোচের পথে না হেঁটে মাঝারি খ্যাতির কোচের দিকে হাত বাড়িয়েছে।\n",
      "বিসিবি চাচ্ছে এমন কোচ, যাঁর ক্রিকেট মস্তিষ্ক ক্ষুরধার, দলের সঙ্গেও থাকবেন অনেক দিন।\n",
      "চন্ডিকা হাথুরুসিংহের মতোই নিজের প্রোফাইল সমৃদ্ধ করবেন বাংলাদেশে ভালো কাজ করে।\n",
      "রোডস যে অন্য প্রার্থীর তুলনায় বেশ এগিয়ে আছেন, সেটি বোঝা গেল বিসিবির প্রধান নির্বাহীর কথায়, 'এই মুহূর্তে যে কজন কোচকে পাওয়া গেছে, তাঁদের মধ্যে তিনি একজন।\n",
      "তাঁর অভিজ্ঞতাকে গুরুত্ব দিচ্ছি।\n",
      "আগামী বিশ্বকাপ ইংল্যান্ডে হবে।\n",
      "এটাও বিবেচ্য বিষয়।\n",
      "ইংল্যান্ডের কন্ডিশন বা ওই ধরনের কন্ডিশনের কাউকে যদি দলের সঙ্গে সম্পৃক্ত করা যায়, তাহলে বাড়তি সুবিধা পাওয়া যেতে পারে।'\n",
      "\n",
      "আজ সারাদিন থিয়েটার ইনস্টিটিউট , চট্টগ্রাম\n",
      "বাদল সাঁঝের নাটক।\n",
      "আজ সুখেন্দু স্মৃতি নাট্যপদক প্রদান ও তির্যক নাট্যদলের নাটক রক্তকরবী।\n",
      "নির্দেশনা দিয়েছেন আহমেদ ইকবাল হায়দার। সন্ধ্যা সাতটায়।\n",
      "বাংলাদেশ শিল্পকলা একাডেমী\n",
      "২০তম জাতীয় চারুকলা প্রদর্শনী।\n",
      "জাতীয় চিত্রশালায়, চলবে আগামী ১২ জুলাই পর্যন্ত। বেঙ্গল শিল্পালয় , ধানমন্ডি\n",
      "বিটিভির শিল্পনির্দেশকদের কাজ নিয়ে প্রদর্শনী 'সৃজনের উদ্যানে'।\n",
      "চলবে আগামী ৩ জুলাই পর্যন্ত। চারুকলা অনুষদ , ঢাকা বিশ্ববিদ্যালয়\n",
      "সুমনের ভাস্কর্য 'ব্র্যান্ড কালচার'।\n",
      "চলবে আগামী ১ জুলাই পর্যন্ত। গ্যালারি কায়া , উত্তরা গ্লিম্পসেস।\n",
      "কয়েকজন প্রখ্যাত চিত্রশিল্পীর বাছাই করা চিত্রকর্ম নিয়ে প্রদর্শনী।\n",
      "চলবে আগামী ২০ জুলাই পর্যন্ত।\n",
      "ঢাকা আর্ট সেন্টার , ধানমন্ডি\n",
      "বীরেন সোমের ছাপচিত্র প্রদর্শনী।\n",
      "চলবে ২৪ জুলাই পর্যন্ত। বলাকা সিনেওয়ার্ল্ড\n",
      "পোড়ামন ও টেলিভিশন।\n",
      "দুপুর সাড়ে ১২টা, বেলা সাড়ে তিনটা, সন্ধ্যা সাড়ে ছয়টা ও রাত নয়টায়। স্টার সিনেপ্লেক্স l\n",
      "ম্যান অব স্টিল, সকাল ১০টা ৫০, বেলা একটা ৪০, বিকেল চারটা ৩৫ ও সন্ধ্যা সাড়ে সাতটায়। l জিআইজো :\n",
      "রিটেলিয়েশন থ্রিডি, বেলা ১১টা, দেড়টা, বিকেল চারটা ও সন্ধ্যা সাতটায়। l\n",
      "আয়রনম্যান ৩ থ্রিডি, সকাল ১০টা ৫০, বেলা একটা ২৫ ও বিকেল চারটায়। l\n",
      "পার্কার, বেলা সাড়ে ১১টা, দুইটা, বিকেল সোয়া চারটা ও সন্ধ্যা পৌনে সাতটায়।\n",
      "\n",
      "- Where's the French guy? - ফ্রান্সের লোকটা কোথায়?\n",
      "\n",
      "নোটিশ বোর্ড\n",
      "ইনডিপেনডেন্ট ইউনিভার্সিটি, বাংলাদেশে (আইইউবি) ফিলিপ সি জেসাপ ইন্টারন্যাশনাল ল মুট কোর্টের দ্বিতীয় বাংলাদেশ জাতীয় রাউন্ডের পুরস্কার বিতরণী গত শনিবার অনুষ্ঠিত হয়েছে।\n",
      "এতে প্রধান অতিথি ছিলেন বিশ্ববিদ্যালয়ের উপাচার্য অধ্যাপক এম ওমর রহমান। সংবাদ বিজ্ঞপ্তি।\n",
      "স্টামফোর্ড ইউনিভার্সিটি সাহিত্য ফোরামের আয়োজনে দিনব্যাপী 'শুদ্ধ উচ্চারণ কর্মশালা' সম্প্রতি অনুষ্ঠিত হয়েছে।\n",
      "এতে প্রশিক্ষক ছিলেন স্বাধীন বাংলা বেতার কেন্দ্রের কণ্ঠযোদ্ধা আশরাফুল আলম ও আবৃত্তিকার ভাস্বর বন্দ্যোপাধ্যায়। সংবাদ বিজ্ঞপ্তি।\n",
      "উত্তরা ইউনিভার্সিটিতে 'শেকড়ের টানে আমরা' স্লোগানে গত শনিবার লোক-উৎসব অনুষ্ঠিত হয়েছে।\n",
      "এতে প্রধান অতিথি ছিলেন বাণিজ্য মন্ত্রণালয়ের অতিরিক্ত সচিব এস এম রেজওয়ান হোসেন। সংবাদ বিজ্ঞপ্তি।\n",
      "\n",
      "4 Grieving the spirit can be the first step leading to the total loss of the influence of God's active force in a Christian's life. ৪ আত্মাকে দুঃখিত করাই প্রথম পদক্ষেপ হতে পারে, যা একজন খ্রিস্টানের জীবনে ঈশ্বরের সক্রিয় শক্তির প্রভাব সম্পূর্ণরূপে হারিয়ে ফেলার দিকে পরিচালিত করতে পারে।\n",
      "\n",
      "নারী দিবসে মহিলাদের অনুপ্ররণা স্নেহা-আরিফারা, লড়াইয়ের কাহিনি শেয়ার করলেন প্রধানমন্ত্রীর টুইটার হ্যান্ডেলে\n",
      "আন্তর্জাতিক নারী দিবসে নিজের সোশ্যাল মিডিয়া অ্যাকাউন্ট দেশের মহিলাদের জন্য ছেড়ে দেওয়ার কথা ঘোষণা করেছিলেন প্রধানমন্ত্রী নরেন্দ্র মোদী।\n",
      "জানিয়েছিলেন,যেসব মহিলার জীবন অন্যের কাছে অনুপ্ররণা হয়ে উঠতে পারে তাদের কথা শেয়ার করা যাবে তাঁর সোশ্য়াল মিডিয়া অ্যাকাউন্টে।\n",
      "রাবিবার এমনই সাত মহিলা তাঁদের লড়াইয়ের কথা শেয়ার করলেন প্রধানমন্ত্রীর সোশ্য়াল মিডিয়া অ্যাকাউন্টে।\n",
      "আরও পড়ুন-পরাজিতদের ওপরেই আস্থা, রাজ্যসভায় তৃণমূলের প্রার্থী হচ্ছেন অর্পিতা-মৌসম-দীনেশ\n",
      "চেন্নাইয়ের স্নেহা মহাডোস লিখলেন তাঁর লড়াইয়ের কাহিনী।\n",
      "তাঁর হাত ধরে তৈরি হয়েছে ফুডব্যাঙ্ক ইন্ডিয়া।\n",
      "মায়ের কাছ অনুপ্রাণিত হয়ে গৃহহীনদের জন্য তৈরি করেছিলেন ফুডব্যাঙ্ক অ্যাকাউন্ট।\n",
      "তাঁর কর্মকাণ্ডের পরিচিতি দিয়ে একটি ভিডিয়ো পোস্টে করেছেন স্নেহা।\n",
      "লিখেছেন প্রধানমন্ত্রীর এই অ্যাউন্ট ব্যবহার করছি যাতে দেশে ক্ষুধার দুরীকরণের পক্ষে সচেতনতা গড়ে ওঠে।\n",
      "শারীরিক প্রতিবন্ধীদের নিয়ে কাজ করে চলেছেন ডা মালবিকা আইয়ার।\n",
      "প্রধানমন্ত্রীর টুইটার হ্যান্ডেলে নিজের কর্মকাণ্ডের কথা শেয়ার করেছেন তিনি।\n",
      "জানিয়েছেন, কীভাবে মাত্র ১৩ বছর বয়সে বোমা বিস্ফোরণের শিকার মালবিকা শেষপর্য্ন্ত পিএইডি করেছেন।\n",
      "বিস্ফোরণে হাত উড়ে যায়, পায়েরও মারাত্মক ক্ষতি হয়।\n",
      "তার পরেও লড়াই থামেনি।\n",
      "শ্রীনগরের এক হস্তশিল্পী শেয়ার করেছেন তার লড়াইয়ের কাহিনি।\n",
      "কাশ্মীরের হস্তশিল্প আন্তর্জাতিক বাজারে পৌঁছে দেওয়া ও উপত্যকার শিল্পীদের উত্সাহ দেওয়ার কাজ করে চলেছেন আরিফা।\n",
      "প্রধানমন্ত্রীর টুইটার হ্যান্ডেলে শেয়ার করেছেন সেই কাহিনী।\n",
      "আরও পড়ুন-নারীদিবসেই স্বপ্নভঙ্গ হ্যারিদের; ভারতকে দুরমুশ করে রেকর্ড পঞ্চমবার বিশ্বচ্যাম্পিয়ন অস্ট্রেলিয়া\n",
      "জল সংরক্ষণের জন্য কাজ করছেন হায়দরাবাদের কল্পনা রমেশ।\n",
      "পেশায় আর্কিটেক্ট কল্পনা ভবিষ্যতের জন্য সংরক্ষণ করার সচেতনতা গড়ে তুলছেন।\n",
      "তাঁর দাবি, ছোট্ট একটি প্রচেষ্ঠার বড় প্রভাব হতে পারে।\n",
      "মহারাষ্ট্রের বানজারা সম্প্রদায়ের শিল্পী বিজয়া পাওয়ার শেয়ার করেছেন তাঁর গল্প।\n",
      "বানজারা সম্প্রদায়ের হস্তশিল্প জনপ্রিয় করে তোলার নিরলস চেষ্ট করে চলেছেন বিজয়া।\n",
      "গত দুদশক ধরে কাজ করে চলেছেন।\n",
      "\n",
      "১৫ যিহোবার সাক্ষিরা সামাজিক সমস্যাগুলো, বিশেষ করে তাদের চারপাশে যে নোংরা অভ্যাসগুলো আছে, সেগুলো দেখে চোখ বন্ধ করে থাকেন না। 15 Jehovah's Witnesses do not ignore the social problems - especially the defiling unscriptural practices - around them.\n",
      "\n",
      "Elevated railway উত্তোলিত রেলপথ\n",
      "\n",
      "পাবনায় ট্রাক ভাঙচুর, আটক ৪৯ - banglanews24.com\n",
      "পাবনা-ঢাকা মহাসড়কের ধোপাঘাটা এলাকায় শিবির কর্মীরা ২টি ট্রাক ভাঙচুর করেছে।\n",
      "এছাড়া বিভিন্ন সড়কে টায়ারে আগুন জ্বালিয়ে পিকেটিং করছে তারা।\n",
      "রোববার সকাল ৬টা থেকে শুরু হওয়া দেশব্যাপী ২০ দলের ডাকা ৩৬ ঘণ্টার হরতাল পাবনায় বিক্ষিপ্তভাবে চলছে।\n",
      "এদিকে, নাশকতার আশঙ্কায় জেলার ১১ থাকার বিভিন্ন স্থানে অভিযান চালিয়ে বিএনপি-জামায়াতের নেতাকর্মীসহ ৪৯ জনকে আটক করেছে পুলিশ।\n",
      "হরতালে বন্ধ রয়েছে পাবনা থেকে দুরপাল্লার বাস-ট্রাক চলাচল।\n",
      "তবে সড়ক ও মহাসড়কে সিএনজিচালিত অটোরিকশা, টেম্পু, ইঞ্জিনচালিত নসিমন-করিমন চলাচল স্বাভাবিক রয়েছে।\n",
      "বিভিন্ন সড়কে পুলিশ, র‌্যাব ও ডিবি পুলিশের টহল অব্যাহত রয়েছে।\n",
      "পাবনার সহকারী পুলিশ সুপার (এএসপি) সিদ্দিকুর রহমান বাংলানিউজকে এসব তথ্যের সত্যতা নিশ্চিত করেছেন। বাংলাদেশ সময়\n",
      "মাদারীপুর জেলা পরিষদ চেয়ারম্যানের মৃত্যুতে প্রধানমন্ত্রীর শোক\n",
      "মাদারীপুর জেলা পরিষদ চেয়ারম্যান মিয়াজ উদ্দিন খান আর নেই\n",
      "দোহারে বন্যা দুর্গত ৩০০ জনকে খাদ্য দিল স্বেচ্ছাসেবক লীগ\n",
      "খাগড়াছড়ি জেলা আ'লীগের সাবেক সভাপতির মৃত্যু\n",
      "শপথ নিলেন সাহাদারা ও শাহীন\n",
      "জনরোষের ভয়ে বিএনপি বেপরোয়া আচরণ করছে: কাদের\n",
      "পদত্যাগকারী স্বাস্থ্যের ডিজির বিচার চান ফখরুল\n",
      "\n",
      "বাগমারায় দুই মোটরসাইকেলের সংঘর্ষে নিহত ১\n",
      "রাজশাহীর বাগমারায় দুটি মোটরসাইকেলের মুখোমুখি সংঘর্ষে মিস্টার হোসেন (২৭) নামে এক রাজমিস্ত্রি নিহত হয়েছেন।\n",
      "বৃহস্পতিবার সকাল সাড়ে ৮টার দিকে উপজেলার বাসুপাড়া ইউনিয়নের চিকাবাড়ী এলাকায় এ দুর্ঘটনা ঘটে।\n",
      "নিহত মিস্টার আলী ওই ইউনিয়নের সাইপাড়া এলাকার আলতাফ হোসেনের ছেলে।\n",
      "বাগমারা থানা পুলিশের ভারপ্রাপ্ত কর্মকর্তা (ওসি) সেলিম হোসেন এ তথ্য নিশ্চিত করে বলেন, সকালে ব্যক্তিগত কাজে একডালা এলাকায় যাচ্ছিলেন মিস্টার হোসেন।\n",
      "পথে চিকাবাড়ী পৌঁছালে বিপরীতগামী আরেকটি মোটরসাইকেলের সঙ্গে তার মুখোমুখি সংঘর্ষ হয়।\n",
      "এতে মারাত্মক আহত হন তিনি।\n",
      "স্থানীয়রা প্রথমে তাকে উপজেলা স্বাস্থ্যকেন্দ্রে নেন।\n",
      "পরে রাজশাহী মেডিকেল কলেজ (রামেক) হাসপাতালে নেয়া হলে সেখানে মারা যান তিনি।\n",
      "এ ঘটনায় আইনগত ব্যবস্থা নেয়া হচ্ছে বলে জানান ওসি।\n",
      "ফেরদৌস সিদ্দিকী/আরএআর/জেআইএম\n",
      "\n",
      "পেঙ্গুইন রেড ক্লাসিকস Penguin Essentials\n",
      "\n",
      "মোবাইলে যখন চার্জ থাকে না ঃ সমস্যা সমাধানে করণীয়\n",
      "প্রয়োজনের মুহূর্তে যখন মোবাইল ফোনের চার্জ শেষ হয়ে যায়, তখন খুঁজতে হয় চার্জার।\n",
      "চার্জ দেওয়ার জায়গাও চাই।\n",
      "চার্জ দেওয়ার সুবিধা যখন হাতের নাগালে থাকে না, তখনই বিপদ।\n",
      "এমন বিপদে হয়তো অনেকেই পড়েছেন।\n",
      "আবার এক বছর আগে কেনা মোবাইল ফোনে প্রথম প্রথম যে পরিমাণ চার্জ থাকত, এক বছর পরে তার অর্ধেকও থাকে না বলে অনেকে দুঃখ করেন।\n",
      "এতে ব্যবহারকারীর কতটা ভুল আর ফোনটিরই বা কী সমস্যা, এ প্রশ্নও ওঠে।\n",
      "সমস্যার সমাধান কী?\n",
      "ব্যাটারি সমস্যার সমাধান নিয়ে সম্প্রতি সিএনএন প্রকাশ করেছে একটি প্রতিবেদন।\n",
      "চার্জ দেব কি দেব না?\n",
      "মোবাইল ফোন নিয়ে অনেকেরই জিজ্ঞাসা থাকে, পুরোপুরি চার্জ দেব, নাকি অল্প দেব?\n",
      "চার্জ শেষ হয়ে গেলে আবার চার্জে দেব, নাকি অল্প চার্জ বাকি থাকতেই দেব?\n",
      "বিশেষজ্ঞরা এই প্রশ্নগুলোর উত্তরে বলছেন, একটা সময় রিচার্জেবল ব্যাটারিগুলোতে পূর্ণ চার্জ দিলে এবং সেই চার্জ শেষ করে আবার চার্জে দিলে তা সবচেয়ে ভালো কাজ করত।\n",
      "গত কয়েক বছরে ব্যাটারির উপাদান ও চার্জ দেওয়ার এই নিয়মেও পরিবর্তন এসেছে।\n",
      "বিশেষজ্ঞদের মতে, এখনকার অধিকাংশ মোবাইল ফোনে লিথিয়াম-আয়ন ব্যাটারির ব্যবহার দেখা যায়।\n",
      "এ ধরনের ব্যাটারিতে যখন ২০ থেকে ৮০ শতাংশ চার্জ থাকে, তখন সবচেয়ে ভালো কাজ করতে দেখা যায়।\n",
      "তাই সময়ের সঙ্গে মোবাইল ফোনের ব্যাটারি থেকে সর্বোচ্চ সুবিধা পেতে এ নিয়মটি মেনে চললে ভালো।\n",
      "ব্যাটারির নীরব ঘাতক\n",
      "ব্যাটারি-সংক্রান্ত অধিকাংশ বিষয়ই মোবাইলের প্রসেসরের ওপর নির্ভর করে।\n",
      "তবে ফোনের ব্যাটারির আয়ু দীর্ঘায়ু হবে কি না, তা ফোনের অ্যাপ্লিকেশন ব্যবহারের ওপর নির্ভর করে।\n",
      "বিশেষজ্ঞরা বলেন, মোবাইল ফোনের মেসেজিং অ্যাপ্লিকেশনের ব্যবহার ব্যাটারির আয়ু দ্রুত শেষ করে ফেলতে সক্ষম।\n",
      "এসব অ্যাপ্লিকেশন ব্যবহার না করলেও ব্যাটারি থেকে চার্জ খরচ করতে পারে।\n",
      "বেশি তাপে ব্যাটারি কম টেকে\n",
      "ব্যাটারির দীর্ঘায়ুর সঙ্গে তাপমাত্রার বিশেষ সম্পর্ক রয়েছে।\n",
      "বিশেষজ্ঞরা বলেন, বেশি তাপে ব্যাটারি কম টেকে।\n",
      "মোবাইল ফোনটি যদি সব সময় বেশি গরম হয়, তখন ফোনটির ব্যাটারি দ্রুত শেষ হয়ে যাওয়ার আশঙ্কা থাকে।\n",
      "ট্যাবলেট কম্পিউটার ও মোবাইল ফোনের এ বিষয়টি নিয়ে ব্যবহারকারীর অবশ্য তেমন কিছু করার থাকে না।\n",
      "তবে মোবাইল ফোন কেনার পর যদি দেখেন তা চালানোর পর বেশি গরম হচ্ছে, তখন এর ব্যাটারির আয়ু সম্পর্কে ধারণা করে নিতে পারবেন।\n",
      "সমস্যা সমাধানে করণীয়\n",
      "ব্যাটারি দ্রুত শেষ হয়ে যাচ্ছে, এখন কী করা যায়?\n",
      "বিশেষজ্ঞরা জানান, ব্যাটারি দীর্ঘায়ু করতে ঘন ঘন চার্জ দিন, তবে চার্জ দেওয়াটা যেন আবার অতিরিক্ত পর্যায়ে চলে না যায়।\n",
      "চার্জ কমে গেলে যেমন চার্জ দেবেন কিন্তু মাঝেমধ্যে আবার ব্যাটারির চার্জ সম্পূর্ণ শেষ করে ফেলবেন।\n",
      "আপনার ফোনের ব্রাইটনেস বা ঔজ্জ্বল্য যতটা সম্ভব কমিয়ে রাখবেন।\n",
      "ফোনের ব্যাকগ্রাউন্ডে কোন অ্যাপ্লিকেশনটি বেশি চার্জ খরচ করছে, সেটি খুঁজে বন্ধ করে দিন।\n",
      "বিশেষজ্ঞরা জানিয়েছেন, সাধারণ কিছু অভ্যাসের মাধ্যমে ব্যাটারির চার্জ বেশি সময় ধরে রাখা যায়।\n",
      "ব্যাটারির চার্জ একটু বেশি সময় ধরে রাখতে খুব অল্প সময়ে এ ধরনের পরিবর্তন আনা যেতে পারে।\n",
      "পর্দার ঔজ্জ্বল্য কমিয়ে রাখা\n",
      "স্মার্টফোনের পর্দার ঔজ্জ্বল্য কমিয়ে রাখা ভালো।\n",
      "ফোনের সেটিংস থেকে এটি পরিবর্তন করা যায়, আবার কোনো কোনো মোবাইলে ব্রাইটনেস পরিবর্তনের জন্য শর্টকাট কি-ও থাকে।\n",
      "প্রয়োজন ছাড়া সব বেতার সংযোগ বন্ধ\n",
      "জিপিআরএস/এজ, জিপিএস, ওয়াই-ফাই, ব্লুটুথের মতো বেতার সংযোগগুলো প্রয়োজনের সময় ছাড়া বন্ধ রাখা উচিত।\n",
      "কারণ, এই সংযোগগুলো চালু থাকলে সেগুলো নিকটবর্তী সংযোগের উৎসটি খুঁজে বের করার চেষ্টা করতে থাকে।\n",
      "আর এই সময়ে যে পরিমাণ ব্যাটারি খরচ হয়, তা সেবা ব্যবহারের সময়ের চেয়েও বেশি।\n",
      "পুশ নোটিফিকেশন বন্ধ রাখা\n",
      "ই-মেইল, ফেসবুক, গুগল প্লাস, টুইটারসহ আরও বিভিন্ন ধরনের অ্যাপলিকেশনে 'পুশ নোটিফিকেশন' নামের একটি সুবিধা থাকে।\n",
      "যেটি চালু থাকলে মোবাইল ফোনটি একটি নির্দিষ্ট সময় পর পর সার্ভার থেকে নতুন তথ্য সংগ্রহ করে।\n",
      "ফলে প্রয়োজন না থাকলেও নির্দিষ্ট সময় পর পর ফোনটি নিজের মতো করে কাজ করবে, আর চার্জ খরচ হবে।\n",
      "নির্দিষ্ট ধরনের অ্যাপ্লিকেশন\n",
      "স্মার্টফোনে বিভিন্ন ধরনের অ্যাপ ব্যবহার করা যায়।\n",
      "এগুলোর ব্যবহারের জন্য বিভিন্ন মাত্রার মেমোরি, প্রসেসিং পাওয়ার লাগে।\n",
      "মোবাইল কেনার সময় সতর্ক থাকামোবাইল ফোন কেনার সময় তাতে কী ধরনের ব্যাটারি রয়েছে তা যাচাই করে নিন।\n",
      "ফোন বেশি গরম হয় কি না - পরীক্ষা করে দেখতে পারেন।\n",
      "ওয়ারেন্টি দেখে কিনুন।\n",
      "আর্টিকেল টি কেমন ছিলো?\n",
      "ভালো ছিলো 0\n",
      "এক কথায় অসাধারণ 0\n",
      "একদম বাজে 0\n",
      "ট্যাগ সমূহ: এক্সক্লুসিভ পোস্টটিপস-এন্ড-ট্রিকসমোবাইল\n",
      "\n",
      "(3) For the purpose of an audit under sub-section (2) the Auditor-General or any person authorised by him in this behalf shall have access to all records, books, documents, cash or sums deposited with banks, securities, stores and other property of the Institute and may examine any member, officer or employee of the Institute. (৩) উপ-ধারা (২) মোতাবেক হিসাব নিরীক্ষার উদ্দেশ্যে মহা-হিসাব নিরীক্ষক কিংবা তার নিকট হতে ক্ষমতাপ্রাপ্ত কোন ব্যক্তি ইনস্টিটিউটের সকল রেকর্ড, দলিল-দস্তাবেজ, নগদ বা ব্যাংকে গচ্ছিত অর্থ, জামানত, ভাণ্ডার এবং অন্যবিধ সম্পত্তি পরীক্ষা করে দেখতে পারবেন এবং ইনস্টিটিউটের যে কোন সদস্য, কর্মকর্তা বা কর্মচারীকে জিজ্ঞাসাবাদ করতে পারবেন।\n",
      "\n",
      "এশীয় হাতি\n",
      "এশীয় হাতি বা \"এশীয়াটিক হাতি\" (বৈজ্ঞানিক নাম: \"Elephas maximus\") \"এলিফাস\" গণের অন্তর্গত একমাত্র জীবিত প্রজাতি।\n",
      "এটি দক্ষিণ ও দক্ষিণ-পূর্ব এশিয়া জুড়ে বিস্তৃত।\n",
      "পূর্বে ভারত থেকে পশ্চিমে বোর্নিও পর্যন্ত এদের দেখা মেলে।\n",
      "এশীয় হাতির তিনটি স্বীকৃত উপপ্রজাতি রয়েছে - \"Elephas maximus maximus\" (শ্রীলঙ্কা), \"Elephas maximus sumatranus\" (সুমাত্রা দ্বীপ) ও \"Elephas maximus indicus\"।\n",
      "এশিয়ার ভূচর প্রাণীদের মধ্যে এশীয় হাতি বৃহত্তম।\n",
      "বাংলাদেশের ১৯৭৪ জিয়া উদ্দিন আহমেদ (সম্পা.), \"বাংলাদেশ উদ্ভিদ ও প্রাণী জ্ঞানকোষ: স্তন্যপায়ী\", খণ্ড: ২৭ (ঢাকা: বাংলাদেশ এশিয়াটিক সোসাইটি, ২০০৯), পৃ. ৮-১০।\n",
      "ও ২০১২ সালের বন্যপ্রাণী (সংরক্ষণ ও নিরাপত্তা) আইনে এ প্রজাতিটি সংরক্ষিত।\n",
      "বাংলাদেশ গেজেট, অতিরিক্ত, জুলাই ১০, ২০১২, গণপ্রজাতন্ত্রী বাংলাদেশ সরকার, পৃষ্ঠা-১১৮৪৮৯\n",
      "\n",
      "কি শুরু হইলো আমার সোনার বাংলায়!!\n",
      "সোনার বাংলা ছাইরা এবার আমজনতাকেই পলাইতে হইবো মনে হইতাছে!! আজিব ব্যাপার!!!\n",
      "আজ বানিজ্য মেলা থাইকা ভাইয়ার তিন বন্ধুকে ধইরা নিয়া গেছে পুলিশ!\n",
      "ওরা সবাই ভাইয়ার সাথেই রুয়েটে পড়ে!!\n",
      "ওরা কেউ কেন রাজনৈতিক দলের ধারের কাছেও নাই!!\n",
      "কি শুরু হইলো আমার সোনার বাংলায়!!\n",
      "সোনার বাংলা ছাইরা এবার আমজনতাকেই পলাইতে হইবো মনে হইতাছে!!!\n",
      "সবাই সবার জন্য দোয়া কইরেন....\n",
      "\n",
      "কামরানকে চায় না সিলেটের আ'লীগ নেতারা - banglanews24.com\n",
      "মেয়র হিসেবে আর কামরানকে দেখতে চান না সিলেট আওয়ামী লীগের অধিকাংশ নেতা।\n",
      "নানা কারণেই স্থানীয় আওয়ামী লীগের একটি শক্তিশালী গ্রুপ কামরানের ওপর প্রচণ্ড ক্ষুব্ধ।\n",
      "সিসিক নির্বাচনে কামরানের নির্বাচনী প্রচারণায় অংশ নিলেও মনে প্রাণে তার‍া কামরানের পক্ষে কাজ করবেন না, সরেজমিনে খোঁজ নিয়ে এমন চিত্রই পাওয়া গেল।\n",
      "সিলেট আওয়ামী লীগ এবং দলটির সহযোগী সংগঠনগুলোর বিভিন্ন পর্যায়ের নেতাকর্মীরাও বাংলানিউজের সঙ্গে আলাপকালে এমন মনোভাবই ব্যক্ত করেন।\n",
      "কামরানের বিরুদ্ধে নানা অভিযোগ উত্থাপন করে আওয়ামী লীগ নেতাকর্মীরা বলেন, কামরান গত ২০ বছর ধরে মেয়র পদে আছেন।\n",
      "কিন্তু নগরীর বাহ্যিক কোনো উন্নয়ন করতে পারেননি তিনি।\n",
      "সরকারি অর্থ লুটপাট করেছেন।\n",
      "লুটপাটের টাকায় ‍তিনি আঙ্গুল ফুলে কলাগাছ হয়েছেন।\n",
      "পক্ষান্তরে দলীয় নেতাকর্মীদের দিকে তিনি ফিরেও তাকাননি।\n",
      "পাশাপাশি নেতৃত্ব কুক্ষিগত করে রাখার প্রবণতায় মহানগর আওয়ামী লীগকে তিনি কার্যত বিকল করে রেখেছেন।\n",
      "নতুন নেতৃত্বও সৃষ্টি করেননি।\n",
      "দলীয় নেতাকর্মীদের কোন পৃষ্ঠপোষকতা না করে নগরীর উন্নয়নকাজ তার পছন্দের লোক দিয়ে করিয়েছেন।\n",
      "দল ক্ষমতায় থাকলেও আওয়ামী লীগের নেতাকর্মীদের কোন লাভ হয়নি।\n",
      "তার কারণেই মহানগর আওয়ামী লীগের সাংগঠনিক কার্যক্রম অচল হয়ে পড়েছে বলে অভিযোগ করেন তারা।\n",
      "মূলত এসব কারণেই সিলেট আওয়ামী লীগের নেতারা কামরানের ওপর দারুণ ক্ষুব্ধ।\n",
      "এই অবস্থায় বিরোধী দল বিএনপি একক প্রার্থী ঘোষণা করায় মহাবিপদে পড়েছেন কামরান।\n",
      "তার অবস্থা হয়েছে এখন 'শ্যাম রাখি না কূল রাখি' এমন।\n",
      "নির্বাচনী বৈতরণী পার হতে তাকে এবার কঠিন চ্যালেঞ্জের সামনে পড়তে হবে বলে জানালেন সিলেটের রাজনীতি সচেতন মহল।\n",
      "আওয়ামী লীগের ভেতরের এক সূত্র জানায়, জেলা ও মহানগর কমিটির একটি শক্তিশালী গ্রুপ কামরানের বিরুদ্ধে অবস্থান নিয়েছে।\n",
      "প্রকাশ্যে বিরোধিতা না করলেও নেপথ্যে তার বিরুদ্ধে কাজ করছেন তারা।\n",
      "যে কোনো মূল্যেই কামরানকে তারা আর সিলেটের মেয়র হিসেবে দেখতে চান না।\n",
      "জানা গেছে, সিলেট আওয়ামী লীগের প্রভাবশালী নেতা বদর উদ্দিন আহমদ কামরান আওয়ামী লীগের প্রথম মেয়াদের ক্ষমতায় সাবেক পররাষ্ট্রমন্ত্রী প্রয়াত আবদুস সামাদ আজাদের গ্রুপ করতেন।\n",
      "পরবর্তীতে সামাদ আজাদের মৃত্যুর পর এ গ্রুপের হাল ধরেন জেলা আওয়ামী লীগের সাবেক সাধারণ সম্পাদক প্রয়াত ইফতেখার হোসেন শামীম।\n",
      "তার মৃত্যুর পর এ গ্রুপের হাল ধরেন নগর আওয়ামী লীগের সভাপতি ও মেয়র বদর উদ্দিন কামরান।\n",
      "পাশাপাশি নগর আওয়ামী লীগ নেতা ও বিদায়ী সিটি কাউন্সিলর আজাদুর রহমান আজাদ ও সাবেক ছাত্রনেতা রঞ্জিত সরকারের গ্রুপকে তিনি পৃষ্ঠপোষকতা দিয়ে আসছেন।\n",
      "আজাদ ও রঞ্জিত গ্রুপের বহিরাগত ছাত্রলীগ নেতাকর্মীরাই বারবার এমসি কলেজে সন্ত্রাসী কার্যক্রম চাল‍ায় বলে অভিযোগ আছে।\n",
      "অস্ত্রের মহড়ায় প্রায়ই রণক্ষেত্রে পরিণত হয় এমসি কলেজ।\n",
      "এই গ্রুপটিই এমসি কলেজের ঐতিহ্যবাহী ছাত্রাবাসটি পুড়িয়েছে বলে জানা গেছে।\n",
      "আর তাদের এসব সন্ত্রাসী কর্মক‍াণ্ডে পরোক্ষভাবে মদদ দেন মেয়র কামরান।\n",
      "অথচ সিলেট আওয়ামী লীগের বেশিরভাগ নেতাকর্মীই এসব সন্ত্রাসী কর্মকাণ্ড কোনোভাবেই সমর্থন করে না।\n",
      "তাই আজাদ-রণজিৎ গ্রুপকে প্রশ্রয় দিয়ে কামরান বারবার সমালোচিত হচ্ছেন।\n",
      "সূত্র জানায়, মেয়র কামরান আওয়ামী লীগের ত্যাগী নেতাকর্মীদের কৌশলে দূরে সরিয়ে রেখেছেন।\n",
      "তার উদ্দেশ্য সিলেট আওয়ামী লীগে নিজের একচ্ছত্র আধিপত্য বজায় রাখ‍া।\n",
      "তার সমান্তরালে কোন নেতা সৃষ্টি হোক, তা তিনি চান না।\n",
      "এ কারণেই আওয়ামী লীগকে সাংগঠনিকভাবে অনেকটাই বিকল করে রেখেছেন তিনি।\n",
      "দলকে সুসংগঠিত করতে কখনই তিনি কোন উদ্যোগ নেননি।\n",
      "গত কয়েক বছর ধরে কেন্দ্রীয় কর্মসূচি ছাড়া সিলেটে আওয়ামী লীগের নিজস্ব কোনো কর্মসূচি পালিত হয়নি।\n",
      "এদিকে কামরানের ঘনিষ্ঠ একটি সূত্র জানায়, দলীয় নেতাদের সঙ্গে কামরানের দূরত্ব তৈরি হওয়ার অন্তরালের কারণগুলোর মধ্যে অন্যতম প্রধান কারণ হচ্ছে বিগত জোট সরকার ও পরবর্তী তত্ত্বাবধায়ক সরকারের আমলে দলীয় নেতাকর্মীর বিরুদ্ধে দায়ের হওয়া মামলার সমাধান বা মীমাংসার কোন উদ্যোগ নেননি তিনি।\n",
      "এ পরিস্থিতিতে আগামীতে সরকার পরিবর্তন হলে আওয়ামী লীগের নেতাকর্মীরা এলাকায় থাকতে পারবেন না মনে করে অনেক নেতাকর্মীই আতঙ্কে রয়েছেন।\n",
      "আওয়ামী লীগ নেতারা মামলাগুলো নিষ্পত্তির ব্যাপারে দফায় দফায় কামরানকে অনুরোধ জানালেও তিনি এ ব্যাপারে কোন পদক্ষেপ নেননি।\n",
      "এ কারণে জেলা ও মহানগর কমিটির নেতারা তার ওপর ভীষণ ক্ষুব্ধ।\n",
      "সূত্র জানায়, সাবেক মেয়র কামরান একটি বিশেষ গোষ্ঠীর সঙ্গে চলাফেরা করেন।\n",
      "আওয়ামী লীগের নেতাকর্মীরা এটি স্বাভাবিক ভাবে নিতে পারেননি।\n",
      "মুক্তিযুদ্ধের নেতৃত্বদানকারী রাজনৈতিক দলের নেতা হওয়া সত্ত্বেও জামায়াতের নেতাকর্মীদের সঙ্গে রয়েছে কামরানের নিবিড় সম্পর্ক।\n",
      "তাদের সঙ্গে কামরানের ব্যবসা-বাণিজ্যও রয়েছে বলে জানা গেছে।\n",
      "অভিযোগ রয়েছে তিনি এমনকি হেফাজতে ইসলামের সমাবেশেও কলা ও পাউরুটি পাঠিয়েছেন।\n",
      "তার এসব কর্মকাণ্ড আওয়ামী লীগের নেতাকর্মীরা ভালোভাবে গ্রহণ করেনি।\n",
      "এদিকে সিলেট নগরবাসীরা আড়ালে বদর উদ্দিন আহমেদ কামরানকে 'মিস্টার থ্রি পার্সেন্ট' বলে অভিহিত করে।\n",
      "যে কোনও কাজ পাওয়ার আগেই কামরানকে পুরো কাজের শতকরা তিনভাগ টাকা অগ্রিম দিতে হয় বলেই তার এমন নামকরণ বলে জানা গেছে।\n",
      "তবে এভাবেই বিপুল অর্থ-বিত্তের মালিক বনে গেছেন তিনি।\n",
      "এ নিয়ে জনগণের প্রশ্নের মুখে পড়ে দলীয় নেতাকর্মীদের প্রতিনিয়তই বিব্রত হতে হয়।\n",
      "আওয়ামী লীগের অপর এক নেতা জানান, ৮০'র দশকের প্রারম্ভে ভাগ্যান্বেষণে মধ্যপ্রাচ্য যান বদর উদ্দিন কামরান।\n",
      "মাত্র দু'লাখ টাকা নিয়ে দেশে ফেরেন তিনি।\n",
      "এরপর পৌরসভার ওয়ার্ড কমিশনার পদে নির্বাচন করেন তিনি।\n",
      "ধাপে ধাপে তিনি সিলেট পৌরসভার চেয়ারম্যানও নির্বাচিত হন।\n",
      "পরবর্তীতে সিলেট সিটি করপোরেশনের ঘোষণা হলে ২০০৩ সালে মেয়র নির্বাচিত হন তিনি।\n",
      "২০০৮ সালের মেয়র নির্বাচনেও তিনি কারাবন্দী অবস্থায় বিপুল ভোটে পুনরায় মেয়র নির্বাচিত হন।\n",
      "জানা গেছে, পৌরসভার কমিশনার নির্বাচিত হওয়ার পর থেকেই কামরানের জীবন পাল্টে যেতে থাকে।\n",
      "অর্থ বিত্ত আর প্রাচুর্যের নাগাল পান।\n",
      "এভাবেই বদর উদ্দিন আহমেদ কামরান এখন সিলেটের অন্যতম শীর্ষ ধনী।\n",
      "তার কি পরিমাণ সহায়-সম্পত্তি ও অর্থ আছে তা তিনি নিজেও জানেন না বলে দাবি করেছেন আওয়ামী লীগের নেতারা।\n",
      "সম্পত্তির মধ্যে কিছু নিজের নামে থাকলেও বেশিরভাগই রাখা আছে স্ত্রী ও সন্তানের নামে।\n",
      "অথচ গত ২০ বছরে অর্থ ও সম্পদের পাহাড় গড়লেও দলীয় নেতাকর্মীদের দিকে ফিরেও তাকাননি তিনি।\n",
      "একসময় যারা তার জন্যে ত্যাগ স্বীকার করেছে মেয়র হওয়ার পর তিনি তাদেরই প্রথম দূরে সরিয়ে দিয়েছেন।\n",
      "তাই আওয়ামী লীগের সাধারণ নেতাকর্মীরা ধীরে ধীরে তার কাছ থেকে দূরে সরে গেছেন।\n",
      "কিন্তু মেয়র কামরান মনে করেন, দলীয় নেতাকর্মীরা তার সঙ্গেই আছেন।\n",
      "এ প্রসঙ্গে বাংলানিউজকে বদর উদ্দিন আহমেদ কামরান বলেন, \"আওয়ামী লীগে কোন বিভাজন নেই।\n",
      "দলের প্রত্যেক নেতাকর্মীই তাকে জয়ী করার জন্যে মনে প্রাণে কাজ করছেন।\n",
      "\" তিনি আরও দাবি করেন,''বিএনপিতে বিরোধ থাকলেও আওয়ামী লীগে কোন বিরোধ নেই।\n",
      "''কিন্তু সরেজমিনে সিলেট নগরীর ভোটারদের সঙ্গে কথা বলে জানা গেল, আওয়ামী লীগের নেতাকর্মীদের ক্ষোভের কারণে তিনি এবার কঠিন চ্যালেঞ্জের মধ্যে পড়েছেন।\n",
      "বিএনপির মেয়র প্রার্থী এবং নেতাকর্মীদের মধ্যে বিরোধ মিটে গেলে কামরানকে বিজয়ী হওয়ার জন্যে কঠিন লড়াইয়ে অবতীর্ণ হতে হবে।\n",
      "সেই লড়াইয়ে কে জিতবেন তা দেখার জন্যে এখন অধীর আগ্রহে অপেক্ষা করছেন সিলেট নগরবাসী। বাংলাদেশ সময়\n",
      "স্বেচ্ছাসেবক দলের সভাপতি বাবু আইসিইউতে\n",
      "বন্যাদুর্গতদের পাশে দাঁড়াতে নেতাকর্মীদের নির্দেশ\n",
      "'নগর চত্বরে'র নাম বদলে দিল আ'লীগের অঙ্গ সংগঠন\n",
      "মানুষের হাহাকার-ক্রন্দন শোনার কেউ নেই: রব\n",
      "সবাইকে বঙ্গবন্ধুর আদর্শের রাজনীতি করতে হবে: প্রাণিসম্পদ মন্ত্রী\n",
      "সজীব ওয়াজেদ জয় একজন স্বপ্নচারী মানুষ, একজন তারুণ্যের জাদুকর\n",
      "এমপি ইসরাফিলের মৃত্যুতে জাপা চেয়ারম্যানের শোক\n",
      "\n",
      "বিশ্ববিদ্যালয় ছাত্রী রুম্পা হত্যা: জিজ্ঞাসাবাদ শেষে সৈকত গ্রেপ্তার\n",
      "স্টামফোর্ড বিশ্ববিদ্যালয়ের ইংরেজি বিভাগের (স্নাতক) ছাত্রী রুবাইয়াত শারমিন রুম্পার (২০) হত্যার ঘটনায় তার কথিত ছেলেবন্ধু আবদুর রহমান সৈকতকে প্রাথমিক জিজ্ঞাসাবাদের পর গ্রেপ্তার দেখানো হয়েছে।\n",
      "রিমান্ড আবেদনের জন্য সৈকতকে আজ আদালতে পাঠানো হবে।\n",
      "রোববার সৈকতকে গ্রেপ্তারের বিষয়টি নিশ্চিত করেছে ঢাকা মহানগর গোয়েন্দা পুলিশ (ডিবি)।\n",
      "ডিবি দক্ষিণ বিভাগের উপ-কমিশনার (ডিসি) রাজীব আল মাসুদ চ্যানেল আই অনলাইনকে জানান, সন্দেভাজন হিসেবে সৈকতকে প্রাথমিক জিজ্ঞাসাবাদ করা হয়েছে।\n",
      "রুম্পা হত্যা মামলায় তাকে গ্রেপ্তার দেখিয়ে রিমান্ড আবেদন করে আদালতে পাঠানো হবে।\n",
      "শনিবার সন্ধ্যায় সৈকত নামে এক যুবককে জিজ্ঞাসাবাদের জন্য ডিবি হেফাজতে নেওয়া হয়।\n",
      "জানা যায়, রুম্পার সঙ্গে সৈকতের প্রেমের সম্পর্ক ছিল।\n",
      "সৈকত একই বিশ্ববিদ্যালয়ের বিবিএ'র ছাত্র।\n",
      "বিজ্ঞাপন এর আগে বুধবার রাত পৌনে ১১টার দিকে সিদ্ধেশ্বরী সার্কুলার রোডের আয়েশা শপিং কমপ্লেক্সের পেছনের দুই ভবনের মাঝে এক তরুণীর মরদেহ উদ্ধার করে পুলিশ।\n",
      "তাৎক্ষণিকভাবে মৃতদেহ দেখে আশেপাশের লোকজন কেউ চিনতে না পারায়, শনাক্তের জন্য নিহতের ফিঙ্গারপ্রিন্ট সংগ্রহ করা হয়।\n",
      "নিহতর বাবা হবিগঞ্জের একটি পুলিশ ফাঁড়ির পরিদর্শক।\n",
      "বাবা হবিগঞ্জে থাকলেও মা ও পরিবারের অন্য সদস্যদের সঙ্গে ঢাকার শান্তিবাগে থাকতেন রুম্পা।\n",
      "রুম্পার স্বজনরা জানান: দুই ভাইবোনের মধ্যে রুম্পা ছিলেন বড়।\n",
      "তাদের গ্রামের বাড়ি ময়মনসিংহে।\n",
      "\n",
      "'মুকুট ধরে রাখতে সর্বোচ্চ দেবে আবাহনী'\n",
      "নয়বারের ফেডারেশন কাপের চ্যাম্পিয়ন আবাহনীকে হাতছানি দিচ্ছে প্রতিযোগিতার সর্বোচ্চ দশবারের সেরা মোহামেডান স্পোর্টিং ক্লাবের পাশে বসার সুযোগ।\n",
      "সুযোগ কাজে লাগাতে প্রস্তুত অধিনায়ক মামুন মিয়া।\n",
      "একই লক্ষ্য কোচ দ্রাগো মামিচেরও।\n",
      "কিন্তু সূচিতে কম দিনের ব্যবধানে খেলা থাকায় বাংলাদেশে ফুটবল ফেডারেশনের (বাফুফে) ওপর ক্ষোভের কথা জানিয়েছেন ক্রোয়েশিয়ার এই কোচ।\n",
      "আগামী মঙ্গলবার সন্ধ্যা সাড়ে ৭টায় শিরোপা লড়াইয়ে নামবে দুই আবাহনী।\n",
      "চট্টগ্রাম আবাহনীর সামনে সুযোগ প্রথমবারের মতো ফাইনালে এসে বাজিমাত করার। আগের দিনের\n",
      "সংবাদ সম্মেলনে আবাহনী লিমিটেডের অধিনায়ক মামুন জানালেন শিরোপা ধরে রাখার আশাবাদ।\n",
      "\"আপনারা সবাই জানেন, আমরা গত আসরের চ্যাম্পিয়ন এবং আমরা মুকুট ধরে\n",
      "রাখার জন্য মনেপ্রাণে চেষ্টা করব।\n",
      "আগামীকাল ফাইনাল; ভালো পারফর্ম করে আমাদের জিততে\n",
      "হবে এবং লক্ষ্য পূরণ করতে হবে।\"\n",
      "\"আমরা অতিরিক্ত আত্মবিশ্বাসী নই।\n",
      "সেটা হওয়ার কোনো সুযোগ নেই। অন্য\n",
      "সব ম্যাচের মতো এ ম্যাচকেও একইভাবে নিচ্ছি এবং আগের ম্যাচগুলোয় আমরা যেভাবে ছন্দ ধরে\n",
      "রেখে খেলেছি, এ ম্যাচেও সেভাবে খেলার লক্ষ্য আমাদের।\"\n",
      "মামুনরা কতটা ছন্দময় ফুটবলের পসরা মেলতে পারবেন, সেটা নিয়ে যথেষ্ঠ\n",
      "সন্দিহান কোচ মামিচ।\n",
      "কেননা গত ৩১ মে এএফসি কাপের ম্যাচ খেলার পর গত শনিবার ফেডারেশন\n",
      "কাপের সেমি-ফাইনাল খেলেছে তার শিষ্যরা।\n",
      "ফেডারেশন কাপের ফাইনাল একদিন পেছালেও সাত দিনের\n",
      "মধ্যে তিন ম্যাচ খেলার সূচি নিয়ে বাফুফেকে ধুয়ে দিয়েছেন তিনি।\n",
      "\"এটা আমাদের জন্য বড় সমস্যা।\n",
      "আমরা স্রেফ রিকোভারির জন্য লড়াই করছি।\n",
      "আশা করি, আগামীকালের ম্যাচে জন্য ছেলেরা শারীরিক ধকল কাটিয়ে উঠবে।\"\n",
      "\"রিকোভারির জন্য ২৪ ঘণ্টা সময় যথেষ্ঠ নয়।\n",
      "আমি অবশ্যই আমার খেলোয়াড়দের রক্ষা করব।\n",
      "যদি কেউ চোট পায়, তাহলে ওই সূচির কারণে পাবে।\n",
      "যদি কাল আমাদের ফাইনালটা খেলতে\n",
      "হত, তাহলে আমি মনে করি না, আমরা মাঠে সেরা একাদশ নামাতে পারতাম।\n",
      "আমি আশা করি, কেউ না\n",
      "কেউ বিষয়টা ভাববে।\"\n",
      "প্রথমবারের মতো ফেডারেশন কাপের ফাইনালে ওঠা চট্টগ্রামের দলটি সম্পর্কে\n",
      "জানাশোনা না থাকার কথাও অকপটে স্বীকার করলেন মামিচ।\n",
      "তবে ঠিকই জানালেন মুকুট ধরে রাখার প্রত্যয়।\n",
      "\"আমি অনেক দল দেখেছি, কিন্তু দুর্ভাগ্যজনকভাবে চট্টগ্রাম আবাহনীকে দেখা হয়নি।\n",
      "তবে শুনেছি তারাও শক্তিশালী দল এবং সে কারণে এ মুহূর্তে আমরা যেটা করতে\n",
      "পারি, সেটা হচ্ছে তাদের বিপক্ষে সর্বোচ্চ দেওয়ার চেষ্টা করা।\n",
      "কঠিন ম্যাচ হবে। কেননা\n",
      "তারাও জয়ের জন্য সর্বোচ্চটা দেবে।\n",
      "মাঠে আমি কঠিন লড়াইয়ের প্রত্যাশা করছি।\"\n",
      "\"আমি সবসময় আমার দলের উন্নতি নিয়ে ভাবি।\n",
      "আশা করি ছেলেরা আমার নির্দেশনা অনুসরণ করবে।\n",
      "প্রতি ম্যাচে তারা ভালো থেকে আরও ভালো হচ্ছে এবং এ কারণে সাফল্য পাওয়ার\n",
      "প্রশ্নে আমি বাস্তববাদী।\"\n",
      "\n",
      "যিহোবা আমাদের কাছ থেকে যা চান WHAT JEHOVAH REQUIRES OF US\n",
      "\n",
      "কিভাবে ধরা হলো নিউজিল্যান্ডে মসজিদে হামলাকারীকে\n",
      "নিউজিল্যান্ডের সাউথ আইল্যান্ডের ক্রাইস্টচার্চ শহরের আল নূর এবং লিনউড মসজিদে গত শুক্রবারের হামলার ঘটনায় ৫০ জন নিহত এবং ৫০ জন আহত হয়েছেন।\n",
      "দেশটির প্রধানমন্ত্রী জাসিন্ডা আরডার্ন এটিকে নিউজিল্যান্ডের সবচেয়ে কালো দিনগুলোর একটি হিসেবে উল্লেখ করেছেন।\n",
      "ব্রেনটন ট্যারেন্ট নামের এক ব্যক্তি এই হামলা করার পাশাপাশি এটি ফেসবুকে লাইভ স্ট্রিম করেন।\n",
      "সোমবার প্রকাশিত একটি প্রতিবেদনে যুক্তরাজ্যভিত্তিক গণমাধ্যম বিবিসি তুলে ধরেছেন যে কিভাবে তিনি এই হামলা চালিয়েছেন এবং তাকে গ্রেপ্তার করা হয়েছে।\n",
      "ট্যারেন্ট একাধিক সেমি-অটোমেটিক অস্ত্র নিয়ে প্রথমে আল নূর মসজিদের ভেতরে থাকা পুরুষ, নারী ও ছেলেমেয়েদের উদ্দেশ্যে এলোপাথাড়ি গুলি ছোড়েন।\n",
      "এই হামলাকারী যখন আল নূরের পশ্চিম দিকের একটি ইন্ডাস্ট্রিয়াল এস্টেটে পৌঁছান, তখন তার মাথায় থাকা ক্যামেরাটি চালু হয়।\n",
      "তিনি মসজিদটির কাছাকাছি পৌঁছে ডিনস অ্যাভিনিউয়ের সামনে তার গাড়িটি পার্ক করেন।\n",
      "এরপর তিনি গাড়ি থেকে নেমে এটির বুট থেকে একটি অস্ত্র বের করে মসজিদের দিকে হেঁটে যান।\n",
      "স্থানীয় সময় অনুযায়ী দুপুর একটা ৪০ মিনিটের দিকে তিনি নামাজরত মুসলিমদের ওপর গুলি ছোড়া শুরু করেন।\n",
      "ছয় মিনিট পর তিনি ডিনস অ্যাভিনিউ থেকে গাড়ি নিয়ে বোটানিক গার্ডেনস হয়ে বিলে অ্যাভিনিউয়ের দিকে রওনা হয়।\n",
      "এসময় তার মাথার ক্যামেরাটির সংযোগ বিচ্ছিন্ন হয়ে যায়।\n",
      "দ্বিতীয় হামলাটি হয় বেশ কিছুক্ষণ পর শহরটির কেন্দ্র থেকে পাঁচ কিলোমিটার পূর্বে অবস্থিত লিনউড মসজিদে।\n",
      "প্রায় একটা ৫৫ মিনিটের দিকে অস্ট্রেলিয়ান বংশোদ্ভূত ট্যারেন্ট তার গাড়ি থেকে নেমে প্রথমে এক ব্যক্তি এবং তার স্ত্রীকে গুলি করেন।\n",
      "কার পার্কিংয়ের দায়িত্বে থাকা মোহাম্মেদ অখিল উদ্দিন জানান, বন্দুকধারী প্রধান প্রবেশপথ দিয়ে ঢোকেন।\n",
      "কিন্তু কোনও দরজা খুঁজে না পেয়ে জানালার উদ্দেশ্যে গুলি ছোড়েন।--------------------------------------------- আরও পড়ুন : নেদারল্যান্ডসে হামলায় নিহতের সংখ্যা বেড়ে ৩, হামলাকারী তুর্কি ---------------------------------------------নামাজ পড়তে আসা আব্দুল আজিজ গুলির শব্দ শোনার পর একটি ক্রেডিট কার্ড রিডার বন্দুকের মতো করে হাতে ধরে বেরিয়ে আসেন।\n",
      "তিনি এটি হামলাকারী ট্যারেন্টের দিকে ছোড়েন।\n",
      "হামলাকারীও তার উদ্দেশ্যে গুলি ছোড়েন।\n",
      "এরপর হামলাকারী তার গাড়ির দিকে এগিয়ে যেতে থাকেন।\n",
      "৪৮ বছর বয়সী আজিজ জানান, তিনি হামলাকারীকে ভেতরে ঢুকতে দিতে চাননি।\n",
      "হামলাকারীর ফেলে দেয়া একটি খালি বন্দুক হাতে নিয়ে তাকে অনুসরণ আজিজ।\n",
      "তিনি বলেন, যখন হামলাকারী আমার হাতে বন্দুক দেখলেন, তখন তিনি তার হাতের বন্দুকটি ফেলে দিয়ে গাড়িটির দিকে ছুটে যেতে থাকেন।\n",
      "তিনি আরও বলেন, এসময় আমি তাকে বাধা দেয়ার চেষ্টা করি।\n",
      "তিনি তার গাড়িতে গিয়ে বসেন এবং আমি আমার হাতের বন্দুকটি একটি তীরের মতো তার জানালার দিকে ছুড়ি।\n",
      "এরপর দুজন পুলিশ তাকে প্রতিহত এবং গ্রেপ্তার করেন।\n",
      "এই দুই পুলিশের মধ্যে শুধু একজনের হাতে একটি হ্যান্ডগান ছিল।\n",
      "প্রথম হামলার প্রায় ৩৬ মিনিট পর হামলাকারীকে গ্রেপ্তার করা হয়।\n",
      "ট্যারেন্টের গাড়িতে পাওয়া দুটি আইইডি(ইম্প্রোভাইসড এক্সপ্লোসিভ ডিভাইস) পরে নিষ্ক্রিয় করে সেনাবাহিনী।\n",
      "আরও পড়ুন : কে/এসএস\n",
      "\n",
      "মা-বউ-বাচ্চাকে যুক্তরাষ্ট্র পাঠিয়ে দিলেন ব্যারিস্টার সুমন\n",
      "জুমবাংলা ডেস্ক : মা, স্ত্রী ও দুই সন্তানকে যুক্তরাষ্ট্রে রেখে নিজে দেশে ফিরে আসবেন বলে ফেইসবুক লাইভে জানিয়েছেন ব্যারিস্টার সৈয়দ সায়েদুল হক সুমন।\n",
      "বৃহস্পতিবার নতুন বছর উদযাপন উপলক্ষে যুক্তরাষ্ট্রে প্রবাসীদের আয়োজনে একটি অনুষ্ঠানে এ তথ্য জানান সুমন।\n",
      "পরে অনুষ্ঠানে দেওয়া বক্তব্যটির ভিডিও নিজ ফেইসবুক পেজে শেয়ার করেন তিনি।\n",
      "ব্যারিস্টার সুমন বলেন, 'প্রথম আমি সিদ্ধান্ত নিয়েছি, আমার বউ বাচ্চা আমেরিকায় রেখে যাচ্ছি।\n",
      "আমার সম্ভাবনা, আমি যদি বেঁচে না থাকি তাহলে আমি চাই না আমার পরবর্তী প্রজন্ম ওইভাবে সাফার করুক।\n",
      "ওরা যেন আমাকে দোষারোপ করতে না পারে।\n",
      "আমার দুই বাচ্চা, আমার মা এবং আমার বউ আমেরিকায় রেখে যাচ্ছি।\n",
      "আমি যদি কখনও ফিরে আসতে না পারি, পৃথিবীতে বেঁচে না থাকি, তাহলে আপনারা তাদের খেয়াল রাখবেন'।\n",
      "এই আইনজীবী বলেন, 'অনেকে বলেন বাংলাদেশে মাইনোরিটি হচ্ছে হিন্দু।\n",
      "আবার অনেকে বলেন বাংলাদেশে মাইনোরিটি মুসলিম সম্প্রদায়ের লোকেরা।\n",
      "কিন্তু আমি বলি এরা কেউ মাইনোরিটি সম্প্রদায় নয়।\n",
      "মাইনোরিটি হলো সত্যতা।\n",
      "সত্য বলার লোক বাংলাদেশে অনেক কম।\n",
      "বাংলাদেশে সংখ্যালঘু হচ্ছে সৎ মানুষগুলো'।\n",
      "ভিডিওটি দেখতে ক্লিক করুন\n",
      "\n",
      "কাপ্তাইয়ের হিন্দু তীর্থ রাম পাহাড় সীতা পাহাড়\n",
      "কাপ্তাই উপজেলায় অবস্থিত হিন্দু ধর্মাবলম্বীদের তীর্থস্থান রাম পাহাড় ও সীতা পাহাড়।\n",
      "হিন্দু পুরাণ মতে, দেবতা রাম ও সীতার স্মৃতিবিজড়িত রাম পাহাড় ও সীতা পাহাড় কালের সাক্ষী হয়ে পাশাপাশি দাঁড়িয়ে আছে।\n",
      "এই দুই পাহাড়ের মাঝ দিয়ে বয়ে গেছে কর্ণফুলী নদী।\n",
      "দু'পাশে দুই পাহাড়, মাঝখানে খরস্রোতা নদী, এমন দৃষ্টিনন্দন প্রাকৃতিক পরিবেশের ছোঁয়া পেতে বিপুলসংখ্যক দর্শনার্থী ও পর্যটক এখানে আসেন।\n",
      "হিন্দু পুরাণ অনুযায়ী, রাজা দশরথের প্রথম পক্ষের সন্তান রাম।\n",
      "দশরথের দ্বিতীয় স্ত্রী কৈকেয়ী নিজপুত্র ভরতকে সিংহাসনের উত্তরাধিকারী বানাতে ষড়যন্ত্র করে রামকে ১২ বছরের জন্য বনবাসে পাঠান।\n",
      "পতিভক্ত সীতা ও ছোট ভাই লক্ষণও রামের সাথে বনবাসে যান।\n",
      "বনবাসে যাবার পর রাম দৈববাণী পান যে, তারা কোথাও গেলে স্ত্রী সীতাকে নির্দিষ্ট গণ্ডির মধ্যে রাখতে হবে, এর বাইরে গেলে সীতার বিপদ হবে।\n",
      "নির্দেশ অনুযায়ী সীতাকে গণ্ডির মধ্যে রেখে রাম ও লক্ষণ শিকারে যান।\n",
      "লংকার রাজা রাবনের নজর পড়ে সুন্দরী সীতার ওপর।\n",
      "আগে থেকেই সীতার রূপে মুগ্ধ রাবন তাকে কাছে পেতে ব্যাকুল।\n",
      "আর গণ্ডির ভিতর থাকা সীতাকে কাছে পাওয়া কঠিন হবে ভেবে রাবন ঠাকুরের ছদ্মবেশে সীতার কাছে খাবার চান।\n",
      "সীতা সরল মনে খাবার নিয়ে বাইরে গেলে রাবন যাদুবলে সীতাকে তুলে কর্ণফুলী নদীর দক্ষিণ তীরে পাহাড়ঘেরা গভীর বনে নিয়ে যান।\n",
      "সেখানেও রাবনের মনস্কামনা পূর্ণ হয়নি।\n",
      "সতী নারী সীতা দু'পুত্র নিয়ে কাপ্তাইয়ের গভীর অরণ্যে বাস করতে থাকেন।\n",
      "কালক্রমে এটি সীতা পাহাড় নামে পরিচিতি পায়।\n",
      "সীতা কর্ণফুলী নদীর যে ঘাটে স্নান করতেন, সেটি সীতার ঘাট নামে পরিচিত।\n",
      "এই সীতার ঘাটে প্রতিদিন দূর-দূরান্ত থেকে হিন্দু ধর্মাবলম্বীরা পূজা-অর্চনা করতে আসেন।\n",
      "এখানে সীতার নামে একটি মন্দিরও স্থাপিত হয়।\n",
      "হিন্দু পুরাণের কাহিনী অনুযায়ী রাম-লক্ষণ দু'ভাই সীতাকে রাবনের কবল থেকে উদ্ধার করতে কর্ণফুলী নদীর উত্তর পাশে অবস্থিত গভীর অরণ্যঘেরা পাহাড়ে আশ্রয় নেন এবং যুদ্ধ করে সীতাকে উদ্ধার করেন।\n",
      "রাম ও লক্ষণের আশ্রয় নেয়া সেই পাহাড়টি পরে 'রাম পাহাড়' নামে পরিচিতি পায়।\n",
      "\n",
      "তথ্যসূত্র আদমশুমারি রিপোর্ট ২০০১, বাংলাদেশ পরিসংখ্যান ব্যুরো; কলাপাড়া উপজেলা সাংস্কৃতিক সমীক্ষা প্রতিবেদন ২০০৭। References Bangladesh Population Census 2001, Bangladesh Bureau of Statistics; Cultural survey report of Kalapara Upazila 2007.\n",
      "\n",
      "(Psalm 146:3) Those inspired words also warn us against trusting in our own unaided efforts. (গীতসংহিতা ১৪৬:৩) এই অনুপ্রাণিত বাক্যগুলো শুধুমাত্র নিজেদের প্রচেষ্টার ওপর নির্ভর করার বিরুদ্ধেও আমাদের সাবধান করে।\n",
      "\n",
      "ইয়েমেনে পিছিয়ে যেতে পারে প্রেসিডেন্ট নির্বাচন\n",
      "ইয়েমেনে অস্থির পরিস্থিতির কারণে পিছিয়ে যেতে পারে ২১ ফেব্র\"য়ারির নির্ধারিত প্রেসিডেন্ট নির্বাচন।\n",
      "দুবাই, জানুয়ারি ১৭ (বিডিনিউজ টোয়েন্টিফোর ডটকম/রয়টার্স)- ইয়েমেনে অস্থির পরিস্থিতির কারণে পিছিয়ে যেতে পারে ২১ ফেব্র\"য়ারির নির্ধারিত প্রেসিডেন্ট নির্বাচন।\n",
      "ইয়েমেনের পররাষ্ট্রমন্ত্রী আবু বকর আল-কিরবি একথা জানিয়েছেন।\n",
      "মঙ্গলবার আল আরাবিয়া টেলিভিশনে দেওয়া এক সাক্ষাৎকারে ইয়েমেনের পররাষ্ট্রমন্ত্রী বলেন, \"প্রেসিডেন্ট নির্বাচন পরিকল্পিতভাবে হবে বলে আমিও অন্যদের মতো প্রত্যাশা করছি।\"\n",
      "\"তবে দুর্ভাগ্যবশত নিরাপত্তা জনিত কিছু সমস্যা সমাধান না হলে ২১ ফেব্র\"য়ারি নির্বাচন করা কঠিন হবে।\"\n",
      "ইয়েমেন সরকার থেকে এ ঘোষণার ফলে সহিংসতা বন্ধে পরিকল্পিত ক্ষমতা হস্তান্তর প্রক্রিয়া নিয়ে আশঙ্কা দেখা দিয়েছে।\n",
      "সরকারের এ ঘোষণা আন্দোলনকারী ও বিরোধীদের ক্ষুব্ধ করতে পারে বলেও ধারণা করা হচ্ছে।\n",
      "ইয়েমেনি প্রেসিডেন্ট আলি আব্দুল¬াহ সালেহকে ক্ষমতা থেকে অপসারণের লক্ষ্যে উপসাগরীয় সহযোগিতা সংস্থা ও সৌদি আরবের মধ্যস্থতায় ক্ষমতা হস্তান্তর চুক্তির অংশ হিসেবে ফেব্র\"য়ারির ওই প্রেসিডেন্ট নির্বাচন অনুষ্ঠানের কথা ছিল।\n",
      "প্রেসিডেন্ট আলি আব্দুল¬াহ সালেহর বিরুদ্ধে গত বছরের শুরুর দিক থেকে আন্দোলন চলছে।\n",
      "বিডিনিউজ টোয়েন্টিফোর ডটকম/এমডি/এলকিউ/১৬৪৮ঘ.\n",
      "\n",
      "গোপালগঞ্জের জাকিয়া হত্যা মামলার দ্রুত বিচার দাবি - banglanews24.com\n",
      "গোপালগঞ্জের চাঞ্চল্যকর গৃহবধূ জাকিয়া হত্যা মামলা দ্রুত বিচারের দাবি জানিয়েছে তার পরিবার।\n",
      "ঢাকা: গোপালগঞ্জের চাঞ্চল্যকর গৃহবধূ জাকিয়া হত্যা মামলা দ্রুত বিচারের দাবি জানিয়েছে তার পরিবার।\n",
      "বৃহস্পতিবার (৮ আগস্ট) সকালে বাংলাদেশ ক্রাইম রিপোর্টার্স অ্যাসোসিয়েশন মিলনায়তনে এক সংবাদ সম্মেলনে নিহত জাকিয়ার ছোট ভাই আছিম উদ্দিন মল্লিক এ দাবি জানান।\n",
      "তিনি বলেন, ২০০৫ সালে তার বোন জাকিয়া বেগমের সঙ্গে মোর্শেদায়ান নিশানের বিয়ে হয়।\n",
      "বিয়ের পাঁচ বছর পর নিশান তার বোনের কাছে এক কোটি টাকা যৌতুক দাবি করেন।\n",
      "এ নিয়ে সংসারে অশান্তি শুরু হয়।\n",
      "এর ধারবাহিকতায় ২০১৬ সালের ৪ ফেব্রুয়ারি রাতে নিশান, এহসান সুজন, আনিচুর রহমান, হাসান শেখ মিলে ধারালো অস্ত্র দিয়ে তার বোনকে কুপিয়ে হত্যা করে।\n",
      "এ ঘটনায় তার বাবা জালাল উদ্দিন মল্লিক বাদী হয়ে গোপালগঞ্জ সদর থানায় নিশানকে প্রধান আসামি করে নারী ও শিশু নির্যাতন দমন আইনে মামলা দায়ের করেন।\n",
      "আছিম উদ্দিন মল্লিক বলেন, নিশান বর্তমানে জেল হাজতে এবং বাকি তিন আসামি জামিনে রয়েছেন।\n",
      "তবে জেল থেকে নিশান প্রভাব খাটিয়ে মামলার কার্যক্রম দেরি করছেন।\n",
      "তিনি বিভিন্ন কৌশলে এবং অসুস্থতার অজুহাত দেখিয়ে একের পর এক তারিখ নিচ্ছেন।\n",
      "বর্তমানে নিশান ঢাকা কেন্দ্রীয় কারাগার কেরানীগঞ্জে রয়েছেন।\n",
      "এ হত্যা মামলাটির দ্রুত বিচার সম্পন্ন করতে প্রধানমন্ত্রী ও স্বরাষ্ট্রমন্ত্রীর হস্তক্ষেপ কামনা করেন নিহত জাকিয়ার পরিবারের সদস্যরা।\n",
      "বাংলাদেশ সময়: ১৭১০ ঘণ্টা, আগস্ট ০৮, ২০১৯ এজেডএস/আরবি/\n",
      "সিলেটে ৫ চিকিৎসকসহ নতুন শনাক্ত ৭৬\n",
      "রাত ১২টায় উঠে যাচ্ছে ওয়ারীর লকডাউন\n",
      "মাস্ক কেলেঙ্কারি: অপরাজিতার শারমিন জাহান গ্রেফতার\n",
      "রাজশাহীতে নিয়ন্ত্রণ হারিয়ে উল্টে গেলো গরু বোঝাই ট্রাক\n",
      "করোনায় জীবন গেলো আরো এক পুলিশ সদস্যের\n",
      "রাজশাহীতে বজ্রপাতে কৃষকের মৃত্যু\n",
      "কুমারখালীতে তরুণীকে গণধর্ষণের অভিযোগে মামলা, আটক ১\n",
      "\n",
      "আমির-সলমনকে নিয়ে কোথায় যাচ্ছিলেন অমিতাভ বচ্চন?\n",
      "বুধবার ইনস্টাগ্রামে নিজের নস্ট্যালজিয়া ট্রিপের ছবি শেয়ার করেছেন বিগ বি।\n",
      "ছবিতে দেখা যাচ্ছে অমিতাভের সঙ্গে রয়েছেন শ্রীদেবী, আমির খান ও সলমন খান।\n",
      "অমিতাভ বচ্চনের ফ্যান পোস্ট করেন এই ছবিটি।\n",
      "টুইটার থেকেই স্মৃতি বিজরিত এক ছবি উপহার পেলেন অমিতাভ বচ্চন।\n",
      "বুধবার ইনস্টাগ্রামে নিজের নস্ট্যালজিয়া ট্রিপের সেই ছবি শেয়ার করেছেন বিগ বি।\n",
      "ছবিতে দেখা যাচ্ছে অমিতাভের সঙ্গে রয়েছেন শ্রীদেবী, আমির খান ও সলমন খান।\n",
      "ছবিতে এও দেখা যাচ্ছে ক্যামেরার দিকে তাকিয়ে পোজ দিচ্ছেন সিনিয়র বচ্চন।\n",
      "সলমন এবং আমিরের তখন অনেক কম বয়স, এমনকি মেগাস্টারের সঙ্গে বেশ খুশি মনেই দেখা যাচ্ছে তাদের।\n",
      "আর শ্রীদেবী, সেই একইরকম অনবদ্য সুন্দর।\n",
      "বলিউড শাহেনশাহর এক ভক্ত টুইটারে শেয়ার করেছিলেন এই ছবি।\n",
      "ক্যাপশনে লিখেছিলেন- \"বিরল ছবি।\n",
      "লন্ডনের ওয়েম্বলি স্টেডিয়ামে ঝুম্মা চুম্মা কনর্সাটের মহড়ার সময় অমিতাভ বচ্চন, শ্রীদেবী জি, আমির খান ও সলমন খানের ছবি\"।\n",
      "পরে এই ছবিই নিজের ইনস্টাগ্রামে শেয়ার করেন সিনিয়র বচ্চন।\n",
      "আরও পড়ুন, অনুষ্কা শর্মার যমজ জুলিয়া মাইকেলস? কীভাবে সম্ভব?\n",
      "গতবছর প্রথমবার অমিতাভ ও আমির বড়পর্দায় একসঙ্গে অভিনয় করেছেন 'ঠাগস অফ হিন্দুস্থানে'র দৌলতে।\n",
      "তবে সলমন খানের সঙ্গে 'বাবুল', 'বাগবান', 'গড তুস্সি গ্রেট হো'-এর মতো ছবি করেছেন বিগ বি।\n",
      "আলিয়া ভাট ও রণবীর কাপুরের সঙ্গে অয়ন মুখোপাধ্যায়ের ছবি ব্রহ্মাস্ত্র-র শুটিংয়ে ব্যস্ত তিনি।\n",
      "এর মধ্যেও সোশাল মিডিয়ায় ছবি শেয়ার করে ও ব্লগে তার বর্ণনা লেখার সময় ঠিক বার করে ফেলেছেন অমিতাভ বচ্চন।\n",
      "তাই তো তিনি মেগাস্টার।\n",
      "\n",
      ".বিটি\n",
      ".বিটি ভুটানের (འབྲུག་ཡུལ) কাউন্ট্রি কোড টপ লেভেল ডোমেইন, ইন্টারনেট প্রদত্ত রাষ্ট্রীয় সংকেত ও ডোমেইন সাফিক্স।\n",
      "ভুটানের যোগাযোগ মন্ত্রণালয় এটি নিয়ন্ত্রণ করে থাকে।\n",
      "নভেম্বর ৭, ২০০৫ সালের তথ্যমতে, এই ডোমেইন নামে মোট ৮৪ টি নাম নিবন্ধিত হয়েছে। বহিঃসংযোগ.\n",
      ".BT domain name registry\n",
      "IANA .bt whois information\n",
      "\n",
      "All of this required focus at a time when we were fatigued mentally, physically, and emotionally.\" আর এই সব কিছু এমন এক সময়ে করতে হয়েছে, যখন আমরা মানসিকভাবে, শারীরিকভাবে ও আবেগগতভাবে অবসন্ন ছিলাম।\"\n",
      "\n",
      "চোখে মলম দিয়ে অপহরণ চেষ্টা, আটক চার\n",
      "এক কিশোরীর চোখে মলম মেখে অপহরণ চেষ্টার অভিযোগে নোয়াখালী শহরে শনিবার রাতে চার মহিলাকে ধরে পুলিশে দিয়েছে এলাকাবাসী।\n",
      "নোয়াখালী, অগাস্ট ২৭ (বিডিনিউজ টোয়েন্টিফোর ডটকম)- এক কিশোরীর চোখে মলম মেখে অপহরণ চেষ্টার অভিযোগে নোয়াখালী শহরে শনিবার রাতে চার মহিলাকে ধরে পুলিশে দিয়েছে এলাকাবাসী।\n",
      "কাজী কলোনি থেকে আটক চারজন হলেন চট্টগ্রাম নগরীর পাহাড়তলীর রাজিয়া খানম প্রিয়া (২৭) ও পাঁচলাইশের নূর নাহার (৩০), নোয়াখালীর সেনবাগের শিল্পী (২৮) এবং কুমিল্লার চান্দিনা উপজেলার সুমি (২১)।\n",
      "এদের কাছ থেকে মলমের কৌটা, দু'টি ছোরা, অজ্ঞান করার কাজে ব্যবহৃত পাউডার ও একটি মাইক্রোবাস উদ্ধার করা হয়।\n",
      "এরা মলম পার্টির সদস্য বলে পুলিশে জানিয়েছে।\n",
      "চোখে মলম দেওয়ায় অসুস্থ হয়ে পড়া নাহিদা আফরোজ টুইঙ্কেলকে (১৩) নোয়াখালী মেডিক্যাল কলেজ হাসপাতালে ভর্তি করা হয়েছে।\n",
      "সুধারাম থানার ওসি মোমাশাররফ হোসেন তরফদার বিডিনিউজ টোয়েন্টিফোর ডটকমকে বলেন, রাত সাড়ে ৯টার দিকে মলম পার্টির ছয় সদস্য কাজী কলোনির মাইন উদ্দিন শাহীনের বাসায় ঢুকে তার মেয়ে নাহিদা আফরোজ টুইঙ্কেলের চোখে মলম লাগিয়ে টানা হেঁচড়া শুরু করে।\n",
      "পরে অস্ত্রের ভয় দেখিয়ে নিয়ে যাওয়ার চেষ্টা চালায়।\n",
      "এ সময় স্থানীয় লোকজন চারজনকে আটক করে থানায় খবর দেয়।\n",
      "বাকি দুইজন পালিয়ে যায়।\n",
      "এ ব্যপারে সুধারাম থানায় মামলার প্রস্তুতি চলছে।\n",
      "বিডিনিউজ টোয়েন্টিফোর ডটকম/প্রতিনিধি/ডিডি/২৩২৪ ঘ.\n",
      "\n",
      "\"এনডিএল\" - জাতীয় ডায়েট গ্রন্থাগার \"NDL\" - National Diet Library\n",
      "\n",
      "দশম জাতীয় সংসদ এর নির্বাচনী আচরণ বিঁধি , রাস্টপতি সংসদ ভেঙ্গে দেবে , এবং তিন মাসের মাধ্যমে নির্বাচন কমিশন বৈধ সব দলকে নির্বাচনের আওতায় সভা সমাবেশ এর যে কোন নিষিদ্ধতা উটিয়ে স্থিতিশীল পরিবেশে তার দায়িত্ব পালনে ব্রতী হবে ।। য়ে\n",
      "নবম জাতীয় সংসদের মেয়াদ আজ শেষ ।।\n",
      "এবার রাস্ট পতি যে কোন সময় সংসদ ভেঙ্গে দেবে ।\n",
      "এবং ৯০ দিনের মধ্য দশম জাতীয় সংসদের রূপরেখা প্রস্তুত করবে । তার\n",
      "মধ্যস্ততায় নির্বাচন কমিশন স্বাধীন ভাবে নতুন সংসদীয় সমন্বয়\n",
      "কমিটির একাধিক লোক নিয়োগ করে ( ইচ্ছামত সব দল থেকে\n",
      "২ এক জন পরিচালনা পরযদ বা সহযোগী লোক নিয়োগের মাধ্যমে\n",
      "তত্ত্বাবধায়ক বা অন্তর্বর্তী সরকারের লিস্ট রাস্টপতির নিকট\n",
      "প্রেরন করবে ।।\n",
      "রাস্ট পতি যাচাই বাচাই এর মাধ্যমে নিরঙ্কুশ জাতীয় নির্বাচনের\n",
      "লক্ষ্য নির্বাচন কমিশন এর অবকা্টামো কে শক্তিশালি করার মাধ্যমে\n",
      "নির্দলীয় নিরপ্রেক্ষ উপদেষ্টা কমিটির দিক নির্দেশ প্রধানে দশম\n",
      "জাতীয় সংসদ কে নিরপ্রেক্ষ তার এক বলিস্ট মুক্ষম পরিবেশে\n",
      "সব দল কে তাদের নির্বাচনী আচরন বিধি পালনের ঘোষণায়\n",
      "সচেষ্ট এবং নিরপ্রেক্ষ ভুমিকা পালন করবে ।।\n",
      "রাস্টপতি দেশের শান্তি শ্রিংখলা অটুট রাখার জন্য প্রয়োজনে\n",
      "সেনাবাহিনী , পুলিশ, আনসার ভিডিপি সহ সকল আইন শ্রিংখলা বাহিনিকে গুরুত্ব সহকারে মাটে নামাতে পারবে ।।\n",
      "নির্বাচন কমিশন শুস্থ নির্বাচনের লক্ষ্য , সকল সরকারী আধা সরকারি চাকুরি জীবীদের বিশেষ প্রশিক্ষনের মাধ্যমে সারা দেশে\n",
      "দশম জাতীয় নির্বাচন কে শক্তিশালি করার প্রয়োজনীয় পদক্ষেপ নেবে ।।\n",
      "রাস্টপতি এবং নির্বাচন কমিশন যে কোন মুহূর্তে দেশ ও জাতীর\n",
      "কল্যানে তাদের বলিষ্ঠ ভুমিকা হবে আগামী জাতীয় সংসদ এবং\n",
      "বাংলাদেশের জনগনের নিশ্চিত শান্তি ও সুরক্ষা ।।\n",
      "\n",
      "বাজারে এবার দৃষ্টি নিয়ন্ত্রিত কম্পিউটার\n",
      "এক প্রতিবেদনে ব্রিটিশ দৈনিক টেলিগ্রাফ জানিয়েছে, বার্লিনের টেকক্রানসে নতুন প্রযুক্তির ট্যাবলেট কম্পিউটারটি উন্মুক্ত করা হয়।\n",
      "কোপেনহেগেন ভিত্তিক কোম্পানি দি আই ট্রিবিউট এ ডিভাইসটি তৈরি করে।\n",
      "ধারণা করা হচ্ছে ট্যাবলেট কম্পিউটারটি কম সময়ের মধ্যে টাচস্ক্রিন ডিভাইসের জায়গাটি দখল করে নেবে আই ট্রিবিউট।\n",
      "ইউএসবি থ্রি সংযোগের মাধ্যমে আই ট্রিবিউট গ্যাজেট ট্যাবলেট কম্পিউটার ও স্মার্টফোনে লাগালে ডিভাইসটি হাতের আঙ্গুলের স্পর্শের পরিবর্তে চোখের ইশারায় কাজ করবে।\n",
      "তখন স্মার্টফোন ও ট্যাবলেট কম্পিউটারে হাতের স্পর্শ ছাড়াই বিভিন্ন রকম মজার গেইম খেলা যাবে।\n",
      "বার্লিনে টেক ক্রান্স টেকনোলজি এক্সপোতে নির্মাতা প্রতিষ্ঠানের মুখপাত্র সানি অ্যালসট্রাপ জোহানসেন বলেন, \"আমরা এ প্রযুক্তিকে এমনভাবে ট্যাবলেট কম্পিউটারে স্থাপন করেছি, খুব দ্রুত ও সূক্ষভাবে এটি কাজ করে।\"\n",
      "আই ট্র্যাকিং ফিচারের কিছু ডিভাইস বাজারে পাওয়া গেলেও এখনও সহজলভ্য নয়।\n",
      "তবে, আগামী পাঁচ বছরের মধ্যে এ প্রযুক্তি হাতে হাতে পৌঁছে যাবে বলে আশা করছেন নির্মাতারা।\n",
      "আর্টিকেল টি কেমন ছিলো?\n",
      "ভালো ছিলো 0\n",
      "এক কথায় অসাধারণ 0\n",
      "একদম বাজে 0\n",
      "ট্যাগ সমূহ: টেকনোলজিতথ্য প্রযুক্তিবাজারে এবার দৃষ্টি নিয়ন্ত্রিত কম্পিউটারবিজ্ঞানসাধারন জ্ঞ্যান\n",
      "\n",
      "1942.03.10 | যুগান্তর ১০ মার্চ ১৯৪২ তারিখের মূল পত্রিকা\n",
      "1942.03.10 | যুগান্তর ১০ মার্চ ১৯৪২ তারিখের মূল পত্রিকা Loading...\n",
      "\n",
      "চতুর্ভুজের হরিচরিত কাব্যে দেখা যায় যে, বরেন্দ্র ব্রাহ্মণগণ শ্রুতি, স্মৃতি, পুরাণ, ব্যাকরণ ও কাব্যে বিচক্ষণ ছিলেন। Haricharita refers to Varendra Brahmins as masters of Shruti, Smrti, Purana, grammar and kavya.\n",
      "\n",
      "Continuing east, the highway crosses over Plymouth Road before intersecting an eastbound exit for Vivglenn Road, which connects to Route 555. আরও পূর্বে গিয়ে মহাসড়কটি ভিভগ্লেন সড়কের পূর্বগামী নিষ্ক্রমন পথকে ছেদ করার আগে প্লেমাছ সড়ক অতিক্রম করে রুট ৫৫৫ এর সাথে সংযুক্ত হয়।\n",
      "\n",
      "এক্সকিউজ মি , দ্যা ইয়াং লেডি ইজ ওয়েটিং ফর ইউ\n",
      "একটা ছোটখাটো ইন্টারভিউ নিতে হবে।\n"
     ]
    }
   ],
   "source": [
    "!head -n 1000 '/workspace/data/Bangla2B+/shards/6512140.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for idx, file_path in enumerate(text_files_short):\n",
    "    print(file_path)\n",
    "\n",
    "    f = open(file_path, 'r', encoding='utf-8')\n",
    "\n",
    "    text = \"\"\n",
    "    idx = 0\n",
    "    for l in f:\n",
    "        line = l.strip()\n",
    "        \n",
    "        if line == \"\":\n",
    "            idx += 1\n",
    "            data = {\"id\": idx, \"text\": text}\n",
    "            dataset.append(data)\n",
    "\n",
    "            text = \"\"\n",
    "        else:\n",
    "            text += \" \" + line\n",
    "\n",
    "        if idx == 30:\n",
    "            break\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and yield text data from files\n",
    "def generate_examples(files):\n",
    "    for idx, file_path in enumerate(files):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            print(file_path)\n",
    "            text = \"\"\n",
    "            line_id = 0\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == \"\":\n",
    "                    line_id += 1\n",
    "                    yield {\"id\": line_id, \"text\": text.strip()}\n",
    "                    text = \"\"\n",
    "                else:\n",
    "                    text += \" \" + line\n",
    "            if text:  # yield the last chunk if it exists\n",
    "                line_id += 1\n",
    "                yield {\"id\": line_id, \"text\": text.strip()}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "def process_file(file_path):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        print(file_path)\n",
    "        text = \"\"\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                idx += 1\n",
    "                dataset.append({\"id\": idx, \"text\": text.strip()})\n",
    "                text = \"\"\n",
    "            else:\n",
    "                text += \" \" + line\n",
    "        if text:  # yield the last chunk if it exists\n",
    "            idx += 1\n",
    "            dataset.append({\"id\": idx, \"text\": text.strip()})\n",
    "    return dataset\n",
    "\n",
    "def generate_examples_concurrent(files):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_file, files))\n",
    "        for result in results:\n",
    "            for item in result:\n",
    "                yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process_file(file_path):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = \"\"\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                idx += 1\n",
    "                dataset.append({\"id\": idx, \"text\": text.strip()})\n",
    "                text = \"\"\n",
    "            else:\n",
    "                text += \" \" + line\n",
    "        if text:  # yield the last chunk if it exists\n",
    "            idx += 1\n",
    "            dataset.append({\"id\": idx, \"text\": text.strip()})\n",
    "    return dataset\n",
    "\n",
    "def generate_examples_pool(files):\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_file, files)\n",
    "        for result in results:\n",
    "            for item in result:\n",
    "                yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6938da065f542e0ade142b27d5cfbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/data/Bangla2B+/shards/9768210.txt/workspace/data/Bangla2B+/shards/9116996.txt/workspace/data/Bangla2B+/shards/8465782.txt/workspace/data/Bangla2B+/shards/7163354.txt/workspace/data/Bangla2B+/shards/7814568.txt/workspace/data/Bangla2B+/shards/6512140.txt/workspace/data/Bangla2B+/shards/651214.txt/workspace/data/Bangla2B+/shards/5860926.txt/workspace/data/Bangla2B+/shards/5209712.txt/workspace/data/Bangla2B+/shards/3907284.txt/workspace/data/Bangla2B+/shards/4558498.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/workspace/data/Bangla2B+/shards/3256070.txt\n",
      "/workspace/data/Bangla2B+/shards/2604856.txt\n",
      "/workspace/data/Bangla2B+/shards/1953642.txt/workspace/data/Bangla2B+/shards/1302428.txt\n",
      "/workspace/data/Bangla2B+/shards/0.txt\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c2317aa2a24a2a86ef694258026dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dataset from the text files\n",
    "dataset = Dataset.from_generator(generate_examples, gen_kwargs={\"files\": text_files}, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/\"\n",
    "data_cache_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/data/Bangla2B+/shards/parquet_files/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'text'],\n",
      "    num_rows: 9951012\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "parquet_files = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) if f.endswith(\".parquet\")]\n",
    "# Load the dataset\n",
    "dataset = load_dataset('parquet', data_files=parquet_files, split='train', keep_in_memory=False)\n",
    "\n",
    "# Display the dataset structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/\"\n",
    "\n",
    "# # Convert the Hugging Face dataset to a pandas DataFrame\n",
    "# df = pd.DataFrame(dataset)\n",
    "\n",
    "# # Set the number of chunks equal to the number of files\n",
    "# num_chunks = len(text_files)\n",
    "\n",
    "# # Calculate the chunk size\n",
    "# chunk_size = len(df) // num_chunks\n",
    "\n",
    "# # Save the DataFrame as multiple Parquet files\n",
    "# output_dir = dataset_dir + \"parquet_files\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# for i in range(num_chunks):\n",
    "#     print(f\"processing text_files {text_files[0]}\")\n",
    "#     start_idx = i * chunk_size\n",
    "#     end_idx = (i + 1) * chunk_size if i < num_chunks - 1 else len(df)\n",
    "#     chunk_df = df[start_idx:end_idx]\n",
    "#     output_parquet_path = os.path.join(output_dir, f\"output_dataset_part_{i + 1}.parquet\")\n",
    "#     chunk_df.to_parquet(output_parquet_path, index=False)\n",
    "#     print(f\"Saved chunk {i + 1} as {output_parquet_path}\")\n",
    "\n",
    "# print(\"Dataset saved as multiple Parquet files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/\"\n",
    "data_cache_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:55:01] Energy consumed for RAM : 33.430527 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:01] Energy consumed for all GPUs : 2.978734 kWh. Total GPU Power : 66.53664718906617 W\n",
      "[codecarbon INFO @ 23:55:01] Energy consumed for all CPUs : 6.232465 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:01] 42.641725 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:01] Energy consumed for RAM : 23.852370 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:01] Energy consumed for all GPUs : 2.124792 kWh. Total GPU Power : 66.51263187339158 W\n",
      "[codecarbon INFO @ 23:55:01] Energy consumed for all CPUs : 4.447263 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:01] 30.424425 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:02] Energy consumed for RAM : 33.446868 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:02] Energy consumed for all GPUs : 2.980236 kWh. Total GPU Power : 66.51641349009809 W\n",
      "[codecarbon INFO @ 23:55:02] Energy consumed for all CPUs : 6.235392 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:02] 42.662496 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:03] Energy consumed for RAM : 23.842817 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:03] Energy consumed for all GPUs : 2.123869 kWh. Total GPU Power : 66.53702525111787 W\n",
      "[codecarbon INFO @ 23:55:03] Energy consumed for all CPUs : 4.445497 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:03] 30.412183 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $data_cache_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "if block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        logger.warning(\n",
    "            \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n",
    "            \" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n",
    "            \" override this default with `--block_size xxx`.\"\n",
    "        )\n",
    "        block_size = 1024\n",
    "else:\n",
    "    if block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(block_size, tokenizer.model_max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "# since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "tok_logger = transformers.utils.logging.get_logger(\n",
    "    \"transformers.tokenization_utils_base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[\"text\"])\n",
    "    # clm input could be much much longer than block_size\n",
    "    # if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "    #     tok_logger.warning(\n",
    "    #         \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "    #         \" before being passed to the model.\"\n",
    "    #     )\n",
    "    return output\n",
    "\n",
    "# # Example tokenizer function that does not truncate\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True, max_length=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'॥ অচেনা ছন্দ ॥ থাকব তোমার মনের মাঝেই, অপেক্ষাটাই ভ্রান্ত, জ্বলছে প্রদীপ নিশীথ মনে, আলোর শিখায় ক্লান্ত। উঠবে জ্বলে শিশির রাতে, আকাঙ্ক্ষা যে বৃষ্টি ছড়ায়, হাত বাড়িয়ে বৃষ্টি ফোঁটা, সুখগুলো যে ভাসছে হাওয়ায়। তোমার হাতে হাতটি রেখে, রাখব মেঘে কান, ভালোবাসার অথই জলে, ভাঙছে অভিমান। ভোররাতে উঠলেই যে, থমকে দাঁড়ায় সাঁঝ, উড়ছে হাওয়ায় শব্দগুচ্ছ, আর অল্প সর্বনাশ। মিলিয়ে যাচ্ছি তোমার মাঝে, রূপকথারা গল্প ভাঁজে, প্রেমের পরশ মাথায় নিয়ে, প্রেমের প্রতীক্ষায়। ছোঁয়াও তোমার স্পর্শখানি, মুখ ফেরাক অভিমান, এক আকাশ তারা ছোঁয়া, দারুণ ঐক্যতান ।'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:55:16] Energy consumed for RAM : 33.433654 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:16] Energy consumed for all GPUs : 2.979013 kWh. Total GPU Power : 67.00016561244053 W\n",
      "[codecarbon INFO @ 23:55:16] Energy consumed for all CPUs : 6.233048 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:16] 42.645715 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:16] Energy consumed for RAM : 23.855500 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:16] Energy consumed for all GPUs : 2.125069 kWh. Total GPU Power : 66.50578090735848 W\n",
      "[codecarbon INFO @ 23:55:16] Energy consumed for all CPUs : 4.447846 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:16] 30.428415 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:17] Energy consumed for RAM : 33.449997 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:17] Energy consumed for all GPUs : 2.980515 kWh. Total GPU Power : 66.94725414710838 W\n",
      "[codecarbon INFO @ 23:55:17] Energy consumed for all CPUs : 6.235975 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:17] 42.666488 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:55:18] Energy consumed for RAM : 23.845946 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:55:18] Energy consumed for all GPUs : 2.124146 kWh. Total GPU Power : 66.491941295469 W\n",
      "[codecarbon INFO @ 23:55:18] Energy consumed for all CPUs : 4.446080 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:55:18] 30.416172 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "t = dataset[1310000]['text']\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 12906, 98, 36278, 227, 11372, 248, 60008, 87648, 42412, 36278, 249, 87648, 53906, 99, 117261, 98, 36278, 98, 50228, 243, 11372, 105, 36278, 97, 28025, 233, 11372, 106, 50228, 108, 36278, 106, 87648, 60008, 73358, 36278, 106, 50228, 251, 60008, 11372, 229, 11, 36278, 227, 11372, 103, 60008, 11372, 243, 53906, 115, 50228, 253, 50228, 229, 36278, 255, 53906, 108, 50228, 101, 53906, 97, 11, 36278, 250, 53906, 105, 11372, 110, 11372, 249, 60008, 36278, 103, 53906, 108, 11372, 99, 28025, 222, 11372, 103, 36278, 101, 81278, 114, 28025, 222, 11372, 98, 36278, 106, 87648, 60008, 11, 36278, 228, 11372, 110, 28025, 233, 73358, 36278, 114, 81278, 244, 50228, 107, 11372, 120, 36278, 243, 53906, 110, 50228, 101, 53906, 97, 100278, 36278, 231, 11372, 254, 11372, 105, 60008, 36278, 250, 53906, 105, 11372, 110, 60008, 36278, 114, 81278, 114, 62456, 73358, 36278, 108, 50228, 97, 60008, 11, 36278, 228, 11372, 243, 50228, 247, 53906, 243, 53906, 115, 42412, 36278, 107, 60008, 36278, 105, 28025, 225, 11372, 115, 53906, 253, 62456, 36278, 249, 11372, 94, 11372, 120, 42412, 11372, 107, 11372, 120, 11, 36278, 117, 50228, 97, 36278, 105, 50228, 94, 11372, 120, 62456, 11372, 107, 11372, 120, 60008, 36278, 105, 28025, 225, 11372, 115, 53906, 253, 62456, 36278, 104, 28025, 233, 11372, 223, 11372, 253, 42412, 11, 36278, 116, 28025, 223, 11372, 244, 11372, 245, 28025, 223, 11372, 110, 28025, 233, 36278, 107, 60008, 36278, 255, 50228, 116, 11372, 249, 60008, 36278, 117, 50228, 241, 11372, 107, 11372, 120, 42412, 11372, 107, 11372, 120, 100278, 36278, 97, 28025, 233, 11372, 106, 50228, 108, 36278, 117, 50228, 97, 60008, 36278, 117, 50228, 97, 11372, 253, 62456, 36278, 108, 60008, 11372, 244, 60008, 11, 36278, 108, 50228, 244, 11372, 105, 36278, 106, 60008, 11372, 246, 60008, 36278, 243, 50228, 101, 11, 36278, 255, 50228, 110, 28025, 233, 11372, 105, 50228, 116, 50228, 108, 36278, 227, 11372, 98, 11372, 229, 36278, 250, 11372, 110, 60008, 11, 36278, 255, 50228, 247, 11372, 249, 60008, 36278, 227, 11372, 255, 81278, 106, 50228, 101, 100278, 36278, 255, 28025, 233, 73358, 73358, 50228, 97, 60008, 36278, 231, 11372, 254, 11372, 110, 60008, 11372, 229, 36278, 107, 60008, 11, 36278, 98, 11372, 106, 11372, 243, 60008, 36278, 99, 50228, 223, 11372, 94, 11372, 120, 42412, 11372, 107, 11372, 120, 36278, 116, 50228, 223, 11372, 251, 11, 36278, 231, 11372, 94, 11372, 120, 11372, 249, 60008, 36278, 117, 50228, 241, 11372, 107, 11372, 120, 42412, 11372, 107, 11372, 120, 36278, 114, 11372, 105, 53906, 99, 11372, 245, 28025, 223, 11372, 248, 53906, 249, 11, 36278, 228, 73358, 36278, 227, 11372, 110, 53906, 103, 36278, 116, 73358, 53906, 105, 87648, 50228, 114, 100278, 36278, 106, 81278, 110, 81278, 107, 11372, 120, 60008, 36278, 107, 50228, 248, 53906, 249, 62456, 36278, 97, 28025, 233, 11372, 106, 50228, 108, 36278, 106, 50228, 251, 60008, 11, 36278, 108, 28025, 224, 11372, 103, 11372, 243, 11372, 98, 50228, 108, 42412, 36278, 245, 11372, 110, 53906, 103, 36278, 255, 50228, 223, 11372, 250, 60008, 11, 36278, 103, 53906, 108, 60008, 11372, 106, 60008, 73358, 36278, 103, 73358, 11372, 114, 36278, 106, 50228, 98, 50228, 107, 11372, 120, 36278, 101, 81278, 107, 11372, 120, 60008, 11, 36278, 103, 53906, 108, 60008, 11372, 106, 60008, 73358, 36278, 103, 53906, 108, 11372, 97, 28025, 222, 11372, 243, 53906, 115, 50228, 107, 11372, 120, 100278, 36278, 249, 28025, 233, 11372, 223, 11372, 107, 11372, 120, 42412, 11372, 241, 36278, 97, 28025, 233, 11372, 106, 50228, 108, 36278, 116, 53906, 103, 73358, 53906, 114, 11372, 244, 50228, 101, 62456, 11, 36278, 106, 28025, 223, 11372, 244, 36278, 104, 60008, 73358, 50228, 243, 36278, 227, 11372, 255, 81278, 106, 50228, 101, 11, 36278, 237, 11372, 243, 36278, 228, 11372, 243, 50228, 114, 36278, 97, 50228, 108, 42412, 36278, 249, 28025, 233, 11372, 223, 11372, 107, 11372, 120, 42412, 11, 36278, 99, 50228, 108, 28025, 223, 11372, 96, 36278, 238, 11372, 243, 53906, 107, 11372, 97, 50228, 101, 102414], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 22:08:03] Energy consumed for RAM : 32.096589 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:03] Energy consumed for all GPUs : 2.860012 kWh. Total GPU Power : 66.91085458762382 W\n",
      "[codecarbon INFO @ 22:08:03] Energy consumed for all CPUs : 5.982964 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:03] 40.939565 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:05] Energy consumed for RAM : 22.518178 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:05] Energy consumed for all GPUs : 2.006100 kWh. Total GPU Power : 66.47329304191439 W\n",
      "[codecarbon INFO @ 22:08:05] Energy consumed for all CPUs : 4.197870 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:05] 28.722149 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:06] Energy consumed for RAM : 32.112661 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:06] Energy consumed for all GPUs : 2.861552 kWh. Total GPU Power : 66.92025673787462 W\n",
      "[codecarbon INFO @ 22:08:06] Energy consumed for all CPUs : 5.985991 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:06] 40.960203 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:08] Energy consumed for RAM : 22.509340 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:08] Energy consumed for all GPUs : 2.005209 kWh. Total GPU Power : 66.92731153904987 W\n",
      "[codecarbon INFO @ 22:08:08] Energy consumed for all CPUs : 4.196129 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:08] 28.710678 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:18] Energy consumed for RAM : 32.099718 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:18] Energy consumed for all GPUs : 2.860289 kWh. Total GPU Power : 66.48072113752616 W\n",
      "[codecarbon INFO @ 22:08:18] Energy consumed for all CPUs : 5.983547 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:18] 40.943555 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:20] Energy consumed for RAM : 22.521308 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:20] Energy consumed for all GPUs : 2.006379 kWh. Total GPU Power : 66.92838319595698 W\n",
      "[codecarbon INFO @ 22:08:20] Energy consumed for all CPUs : 4.198454 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:20] 28.726140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:21] Energy consumed for RAM : 32.115790 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:21] Energy consumed for all GPUs : 2.861829 kWh. Total GPU Power : 66.47771980224734 W\n",
      "[codecarbon INFO @ 22:08:21] Energy consumed for all CPUs : 5.986574 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:21] 40.964193 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:23] Energy consumed for RAM : 22.512470 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:23] Energy consumed for all GPUs : 2.005485 kWh. Total GPU Power : 66.4779405177663 W\n",
      "[codecarbon INFO @ 22:08:23] Energy consumed for all CPUs : 4.196712 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:23] 28.714667 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:33] Energy consumed for RAM : 32.102848 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:33] Energy consumed for all GPUs : 2.860566 kWh. Total GPU Power : 66.45028024708715 W\n",
      "[codecarbon INFO @ 22:08:33] Energy consumed for all CPUs : 5.984130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:33] 40.947544 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:35] Energy consumed for RAM : 22.524437 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:35] Energy consumed for all GPUs : 2.006655 kWh. Total GPU Power : 66.45466000011575 W\n",
      "[codecarbon INFO @ 22:08:35] Energy consumed for all CPUs : 4.199037 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:35] 28.730129 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:36] Energy consumed for RAM : 32.118919 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:36] Energy consumed for all GPUs : 2.862106 kWh. Total GPU Power : 66.46036403770117 W\n",
      "[codecarbon INFO @ 22:08:36] Energy consumed for all CPUs : 5.987157 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:36] 40.968182 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:38] Energy consumed for RAM : 22.515599 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:38] Energy consumed for all GPUs : 2.005762 kWh. Total GPU Power : 66.4556992915991 W\n",
      "[codecarbon INFO @ 22:08:38] Energy consumed for all CPUs : 4.197296 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:38] 28.718657 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:48] Energy consumed for RAM : 32.105977 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:48] Energy consumed for all GPUs : 2.860845 kWh. Total GPU Power : 66.90971089573917 W\n",
      "[codecarbon INFO @ 22:08:48] Energy consumed for all CPUs : 5.984714 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:48] 40.951535 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:50] Energy consumed for RAM : 22.527567 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:50] Energy consumed for all GPUs : 2.006932 kWh. Total GPU Power : 66.45246920474524 W\n",
      "[codecarbon INFO @ 22:08:50] Energy consumed for all CPUs : 4.199620 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:50] 28.734119 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:51] Energy consumed for RAM : 32.122049 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:51] Energy consumed for all GPUs : 2.862384 kWh. Total GPU Power : 66.88780249824624 W\n",
      "[codecarbon INFO @ 22:08:51] Energy consumed for all CPUs : 5.987741 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:51] 40.972174 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:08:53] Energy consumed for RAM : 22.518728 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:08:53] Energy consumed for all GPUs : 2.006041 kWh. Total GPU Power : 66.89893672881405 W\n",
      "[codecarbon INFO @ 22:08:53] Energy consumed for all CPUs : 4.197879 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:08:53] 28.722648 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:03] Energy consumed for RAM : 32.109106 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:03] Energy consumed for all GPUs : 2.861121 kWh. Total GPU Power : 66.4299587355825 W\n",
      "[codecarbon INFO @ 22:09:03] Energy consumed for all CPUs : 5.985297 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:03] 40.955525 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:05] Energy consumed for RAM : 22.530696 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:05] Energy consumed for all GPUs : 2.007211 kWh. Total GPU Power : 66.88077758549811 W\n",
      "[codecarbon INFO @ 22:09:05] Energy consumed for all CPUs : 4.200204 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:05] 28.738111 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:06] Energy consumed for RAM : 32.125179 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:06] Energy consumed for all GPUs : 2.862661 kWh. Total GPU Power : 66.44313314009716 W\n",
      "[codecarbon INFO @ 22:09:06] Energy consumed for all CPUs : 5.988324 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:06] 40.976164 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:08] Energy consumed for RAM : 22.521858 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:08] Energy consumed for all GPUs : 2.006318 kWh. Total GPU Power : 66.43723687465322 W\n",
      "[codecarbon INFO @ 22:09:08] Energy consumed for all CPUs : 4.198462 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:08] 28.726638 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:18] Energy consumed for RAM : 32.112236 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:18] Energy consumed for all GPUs : 2.861398 kWh. Total GPU Power : 66.46979754887685 W\n",
      "[codecarbon INFO @ 22:09:18] Energy consumed for all CPUs : 5.985880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:18] 40.959514 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:20] Energy consumed for RAM : 22.533826 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:20] Energy consumed for all GPUs : 2.007488 kWh. Total GPU Power : 66.47228482786453 W\n",
      "[codecarbon INFO @ 22:09:20] Energy consumed for all CPUs : 4.200787 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:20] 28.742100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:21] Energy consumed for RAM : 32.128308 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:21] Energy consumed for all GPUs : 2.862938 kWh. Total GPU Power : 66.47503623200208 W\n",
      "[codecarbon INFO @ 22:09:21] Energy consumed for all CPUs : 5.988907 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:21] 40.980153 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:23] Energy consumed for RAM : 22.524987 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:23] Energy consumed for all GPUs : 2.006594 kWh. Total GPU Power : 66.47703780246565 W\n",
      "[codecarbon INFO @ 22:09:23] Energy consumed for all CPUs : 4.199046 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:23] 28.730627 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:33] Energy consumed for RAM : 32.115365 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:33] Energy consumed for all GPUs : 2.861677 kWh. Total GPU Power : 66.91541295955265 W\n",
      "[codecarbon INFO @ 22:09:33] Energy consumed for all CPUs : 5.986463 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:33] 40.963506 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:35] Energy consumed for RAM : 22.536955 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:35] Energy consumed for all GPUs : 2.007765 kWh. Total GPU Power : 66.47392205386839 W\n",
      "[codecarbon INFO @ 22:09:35] Energy consumed for all CPUs : 4.201370 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:35] 28.746090 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:36] Energy consumed for RAM : 32.131437 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:36] Energy consumed for all GPUs : 2.863215 kWh. Total GPU Power : 66.48783707164392 W\n",
      "[codecarbon INFO @ 22:09:36] Energy consumed for all CPUs : 5.989490 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:36] 40.984142 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:38] Energy consumed for RAM : 22.528117 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:38] Energy consumed for all GPUs : 2.006873 kWh. Total GPU Power : 66.91355924784233 W\n",
      "[codecarbon INFO @ 22:09:38] Energy consumed for all CPUs : 4.199629 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:38] 28.734619 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:48] Energy consumed for RAM : 32.118495 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:48] Energy consumed for all GPUs : 2.861954 kWh. Total GPU Power : 66.47665435607408 W\n",
      "[codecarbon INFO @ 22:09:48] Energy consumed for all CPUs : 5.987047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:48] 40.967495 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:50] Energy consumed for RAM : 22.540085 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:50] Energy consumed for all GPUs : 2.008043 kWh. Total GPU Power : 66.91694950972398 W\n",
      "[codecarbon INFO @ 22:09:50] Energy consumed for all CPUs : 4.201953 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:50] 28.750081 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:51] Energy consumed for RAM : 32.134566 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:51] Energy consumed for all GPUs : 2.863494 kWh. Total GPU Power : 66.91505864566751 W\n",
      "[codecarbon INFO @ 22:09:51] Energy consumed for all CPUs : 5.990074 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:51] 40.988134 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:09:53] Energy consumed for RAM : 22.531246 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:09:53] Energy consumed for all GPUs : 2.007150 kWh. Total GPU Power : 66.4763712467608 W\n",
      "[codecarbon INFO @ 22:09:53] Energy consumed for all CPUs : 4.200212 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:09:53] 28.738608 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:03] Energy consumed for RAM : 32.121624 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:03] Energy consumed for all GPUs : 2.862231 kWh. Total GPU Power : 66.46919545104393 W\n",
      "[codecarbon INFO @ 22:10:03] Energy consumed for all CPUs : 5.987630 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:03] 40.971485 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:05] Energy consumed for RAM : 22.543214 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:05] Energy consumed for all GPUs : 2.008320 kWh. Total GPU Power : 66.47172096776357 W\n",
      "[codecarbon INFO @ 22:10:05] Energy consumed for all CPUs : 4.202537 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:05] 28.754071 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:06] Energy consumed for RAM : 32.137696 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:06] Energy consumed for all GPUs : 2.863770 kWh. Total GPU Power : 66.47623176156817 W\n",
      "[codecarbon INFO @ 22:10:06] Energy consumed for all CPUs : 5.990657 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:06] 40.992123 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:08] Energy consumed for RAM : 22.534376 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:08] Energy consumed for all GPUs : 2.007427 kWh. Total GPU Power : 66.47815876835098 W\n",
      "[codecarbon INFO @ 22:10:08] Energy consumed for all CPUs : 4.200795 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:08] 28.742598 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:18] Energy consumed for RAM : 32.124754 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:18] Energy consumed for all GPUs : 2.862507 kWh. Total GPU Power : 66.48045542730709 W\n",
      "[codecarbon INFO @ 22:10:18] Energy consumed for all CPUs : 5.988213 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:18] 40.975474 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:20] Energy consumed for RAM : 22.546344 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:20] Energy consumed for all GPUs : 2.008597 kWh. Total GPU Power : 66.47710785844154 W\n",
      "[codecarbon INFO @ 22:10:20] Energy consumed for all CPUs : 4.203120 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:20] 28.758061 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:21] Energy consumed for RAM : 32.140826 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:21] Energy consumed for all GPUs : 2.864047 kWh. Total GPU Power : 66.47325587307918 W\n",
      "[codecarbon INFO @ 22:10:21] Energy consumed for all CPUs : 5.991240 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:21] 40.996113 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:23] Energy consumed for RAM : 22.537505 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:23] Energy consumed for all GPUs : 2.007706 kWh. Total GPU Power : 66.92120954607185 W\n",
      "[codecarbon INFO @ 22:10:23] Energy consumed for all CPUs : 4.201379 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:23] 28.746589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:33] Energy consumed for RAM : 32.127883 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:33] Energy consumed for all GPUs : 2.862786 kWh. Total GPU Power : 66.92434917457162 W\n",
      "[codecarbon INFO @ 22:10:33] Energy consumed for all CPUs : 5.988797 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:33] 40.979466 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:35] Energy consumed for RAM : 22.549473 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:35] Energy consumed for all GPUs : 2.008876 kWh. Total GPU Power : 66.9222722872729 W\n",
      "[codecarbon INFO @ 22:10:35] Energy consumed for all CPUs : 4.203703 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:35] 28.762052 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:36] Energy consumed for RAM : 32.143955 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:36] Energy consumed for all GPUs : 2.864326 kWh. Total GPU Power : 66.92123903665129 W\n",
      "[codecarbon INFO @ 22:10:36] Energy consumed for all CPUs : 5.991824 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:36] 41.000105 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:38] Energy consumed for RAM : 22.540635 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:38] Energy consumed for all GPUs : 2.007983 kWh. Total GPU Power : 66.47373657699045 W\n",
      "[codecarbon INFO @ 22:10:38] Energy consumed for all CPUs : 4.201962 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:38] 28.750579 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:48] Energy consumed for RAM : 32.131012 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:48] Energy consumed for all GPUs : 2.863063 kWh. Total GPU Power : 66.46505974568949 W\n",
      "[codecarbon INFO @ 22:10:48] Energy consumed for all CPUs : 5.989380 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:48] 40.983455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:50] Energy consumed for RAM : 22.552603 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:50] Energy consumed for all GPUs : 2.009153 kWh. Total GPU Power : 66.46175683968313 W\n",
      "[codecarbon INFO @ 22:10:50] Energy consumed for all CPUs : 4.204286 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:50] 28.766042 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:51] Energy consumed for RAM : 32.147085 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:51] Energy consumed for all GPUs : 2.864603 kWh. Total GPU Power : 66.45965245069331 W\n",
      "[codecarbon INFO @ 22:10:51] Energy consumed for all CPUs : 5.992407 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:51] 41.004095 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:53] Energy consumed for RAM : 22.543764 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:10:53] Energy consumed for all GPUs : 2.008259 kWh. Total GPU Power : 66.45990371122186 W\n",
      "[codecarbon INFO @ 22:10:53] Energy consumed for all CPUs : 4.202545 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:10:53] 28.754569 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:03] Energy consumed for RAM : 32.134142 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:03] Energy consumed for all GPUs : 2.863340 kWh. Total GPU Power : 66.45312161856658 W\n",
      "[codecarbon INFO @ 22:11:03] Energy consumed for all CPUs : 5.989963 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:03] 40.987445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:05] Energy consumed for RAM : 22.555732 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:05] Energy consumed for all GPUs : 2.009429 kWh. Total GPU Power : 66.45541802772401 W\n",
      "[codecarbon INFO @ 22:11:05] Energy consumed for all CPUs : 4.204870 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:05] 28.770031 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:06] Energy consumed for RAM : 32.150215 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:06] Energy consumed for all GPUs : 2.864880 kWh. Total GPU Power : 66.45485630155282 W\n",
      "[codecarbon INFO @ 22:11:06] Energy consumed for all CPUs : 5.992990 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:06] 41.008084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:08] Energy consumed for RAM : 22.546894 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:08] Energy consumed for all GPUs : 2.008538 kWh. Total GPU Power : 66.90056062378397 W\n",
      "[codecarbon INFO @ 22:11:08] Energy consumed for all CPUs : 4.203129 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:08] 28.758560 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:18] Energy consumed for RAM : 32.137271 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:18] Energy consumed for all GPUs : 2.863618 kWh. Total GPU Power : 66.89265473050638 W\n",
      "[codecarbon INFO @ 22:11:18] Energy consumed for all CPUs : 5.990546 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:18] 40.991436 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:20] Energy consumed for RAM : 22.558862 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:20] Energy consumed for all GPUs : 2.009708 kWh. Total GPU Power : 66.88816321867039 W\n",
      "[codecarbon INFO @ 22:11:20] Energy consumed for all CPUs : 4.205453 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:20] 28.774023 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:21] Energy consumed for RAM : 32.153344 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:21] Energy consumed for all GPUs : 2.865158 kWh. Total GPU Power : 66.8857612330828 W\n",
      "[codecarbon INFO @ 22:11:21] Energy consumed for all CPUs : 5.993573 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:21] 41.012076 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:23] Energy consumed for RAM : 22.550023 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:23] Energy consumed for all GPUs : 2.008815 kWh. Total GPU Power : 66.44238002216791 W\n",
      "[codecarbon INFO @ 22:11:23] Energy consumed for all CPUs : 4.203712 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:23] 28.762550 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:33] Energy consumed for RAM : 32.140401 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:33] Energy consumed for all GPUs : 2.863895 kWh. Total GPU Power : 66.46132967981099 W\n",
      "[codecarbon INFO @ 22:11:33] Energy consumed for all CPUs : 5.991130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:33] 40.995426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:35] Energy consumed for RAM : 22.561991 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:35] Energy consumed for all GPUs : 2.009985 kWh. Total GPU Power : 66.4611343080519 W\n",
      "[codecarbon INFO @ 22:11:35] Energy consumed for all CPUs : 4.206036 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:35] 28.778012 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:36] Energy consumed for RAM : 32.156474 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:36] Energy consumed for all GPUs : 2.865435 kWh. Total GPU Power : 66.46285140183642 W\n",
      "[codecarbon INFO @ 22:11:36] Energy consumed for all CPUs : 5.994157 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:36] 41.016065 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:38] Energy consumed for RAM : 22.553153 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:38] Energy consumed for all GPUs : 2.009092 kWh. Total GPU Power : 66.45947802535284 W\n",
      "[codecarbon INFO @ 22:11:38] Energy consumed for all CPUs : 4.204295 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:38] 28.766540 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:48] Energy consumed for RAM : 32.143530 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:48] Energy consumed for all GPUs : 2.864172 kWh. Total GPU Power : 66.49183541592133 W\n",
      "[codecarbon INFO @ 22:11:48] Energy consumed for all CPUs : 5.991713 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:48] 40.999415 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:50] Energy consumed for RAM : 22.565121 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:50] Energy consumed for all GPUs : 2.010262 kWh. Total GPU Power : 66.47737116774807 W\n",
      "[codecarbon INFO @ 22:11:50] Energy consumed for all CPUs : 4.206620 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:50] 28.782002 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:51] Energy consumed for RAM : 32.159603 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:51] Energy consumed for all GPUs : 2.865712 kWh. Total GPU Power : 66.47214970818028 W\n",
      "[codecarbon INFO @ 22:11:51] Energy consumed for all CPUs : 5.994740 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:51] 41.020055 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:11:53] Energy consumed for RAM : 22.556283 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:11:53] Energy consumed for all GPUs : 2.009369 kWh. Total GPU Power : 66.47211371826418 W\n",
      "[codecarbon INFO @ 22:11:53] Energy consumed for all CPUs : 4.204878 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:11:53] 28.770530 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:03] Energy consumed for RAM : 32.146659 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:03] Energy consumed for all GPUs : 2.864451 kWh. Total GPU Power : 66.89390084756923 W\n",
      "[codecarbon INFO @ 22:12:03] Energy consumed for all CPUs : 5.992296 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:03] 41.003406 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:05] Energy consumed for RAM : 22.568250 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:05] Energy consumed for all GPUs : 2.010538 kWh. Total GPU Power : 66.44988734228208 W\n",
      "[codecarbon INFO @ 22:12:05] Energy consumed for all CPUs : 4.207203 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:05] 28.785991 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:06] Energy consumed for RAM : 32.162733 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:06] Energy consumed for all GPUs : 2.865991 kWh. Total GPU Power : 66.90512186870876 W\n",
      "[codecarbon INFO @ 22:12:06] Energy consumed for all CPUs : 5.995323 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:06] 41.024046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:08] Energy consumed for RAM : 22.559412 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:08] Energy consumed for all GPUs : 2.009647 kWh. Total GPU Power : 66.89737861453138 W\n",
      "[codecarbon INFO @ 22:12:08] Energy consumed for all CPUs : 4.205462 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:08] 28.774521 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:18] Energy consumed for RAM : 32.149789 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:18] Energy consumed for all GPUs : 2.864728 kWh. Total GPU Power : 66.45766470138578 W\n",
      "[codecarbon INFO @ 22:12:18] Energy consumed for all CPUs : 5.992880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:18] 41.007396 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:20] Energy consumed for RAM : 22.571379 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:20] Energy consumed for all GPUs : 2.010817 kWh. Total GPU Power : 66.91574930842975 W\n",
      "[codecarbon INFO @ 22:12:20] Energy consumed for all CPUs : 4.207786 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:20] 28.789982 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:21] Energy consumed for RAM : 32.165862 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:21] Energy consumed for all GPUs : 2.866267 kWh. Total GPU Power : 66.4555786999163 W\n",
      "[codecarbon INFO @ 22:12:21] Energy consumed for all CPUs : 5.995907 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:21] 41.028036 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:23] Energy consumed for RAM : 22.562542 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:23] Energy consumed for all GPUs : 2.009924 kWh. Total GPU Power : 66.4617308936933 W\n",
      "[codecarbon INFO @ 22:12:23] Energy consumed for all CPUs : 4.206045 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:23] 28.778511 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:33] Energy consumed for RAM : 32.152918 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:33] Energy consumed for all GPUs : 2.865004 kWh. Total GPU Power : 66.46822925699783 W\n",
      "[codecarbon INFO @ 22:12:33] Energy consumed for all CPUs : 5.993463 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:33] 41.011385 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:35] Energy consumed for RAM : 22.574509 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:35] Energy consumed for all GPUs : 2.011094 kWh. Total GPU Power : 66.45436841537766 W\n",
      "[codecarbon INFO @ 22:12:35] Energy consumed for all CPUs : 4.208369 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:35] 28.793972 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:36] Energy consumed for RAM : 32.168992 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:12:36] Energy consumed for all GPUs : 2.866544 kWh. Total GPU Power : 66.45688303768132 W\n",
      "[codecarbon INFO @ 22:12:36] Energy consumed for all CPUs : 5.996490 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:12:36] 41.032026 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "tokenizer(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:57:08] Energy consumed for RAM : 0.084637 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:08] Energy consumed for all GPUs : 0.016837 kWh. Total GPU Power : 66.50390158295059 W\n",
      "[codecarbon INFO @ 00:57:08] Energy consumed for all CPUs : 0.015778 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:08] 0.117251 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:57:12] Energy consumed for RAM : 34.200218 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:12] Energy consumed for all GPUs : 3.057189 kWh. Total GPU Power : 66.47566328287594 W\n",
      "[codecarbon INFO @ 00:57:12] Energy consumed for all CPUs : 6.377327 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:12] 43.634735 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:57:14] Energy consumed for RAM : 24.621848 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:14] Energy consumed for all GPUs : 2.203277 kWh. Total GPU Power : 66.9189327834632 W\n",
      "[codecarbon INFO @ 00:57:14] Energy consumed for all CPUs : 4.592174 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:14] 31.417299 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# dynamic block size in grouping texts\n",
    "def group_texts_old(examples):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Dynamic block size based on token length\n",
    "    if total_length < 1000:\n",
    "        block_size = 512\n",
    "    elif total_length < 5000:\n",
    "        block_size = 1024\n",
    "    else:\n",
    "        block_size = 2048\n",
    "    \n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "    result = {\n",
    "        k: [t[i * block_size : (i + 1) * block_size] for i in range(num_blocks)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:57:16] Energy consumed for RAM : 24.613869 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:16] Energy consumed for RAM : 34.218084 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:16] Energy consumed for all GPUs : 2.202370 kWh. Total GPU Power : 66.4691496863541 W\n",
      "[codecarbon INFO @ 00:57:16] Energy consumed for all CPUs : 4.590478 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:16] 31.406718 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:57:16] Energy consumed for all GPUs : 3.058751 kWh. Total GPU Power : 66.4767250772533 W\n",
      "[codecarbon INFO @ 00:57:16] Energy consumed for all CPUs : 6.380386 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:16] 43.657221 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "def combine_short_sequences(examples, max_length=256, absolute_max_length=2048):\n",
    "    combined = {k: [] for k in examples.keys()}\n",
    "    current_sequence = {k: [] for k in examples.keys()}\n",
    "    current_length = 0\n",
    "    newline_token_ids = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "    \n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        sequence_length = len(examples['input_ids'][i])\n",
    "        if current_length + sequence_length > max_length or current_length + sequence_length > absolute_max_length:\n",
    "            # Add the current combined sequence and start a new one\n",
    "            for k in combined.keys():\n",
    "                combined[k].append(list(chain(*current_sequence[k])))\n",
    "                current_sequence[k] = []\n",
    "            current_length = 0\n",
    "        \n",
    "        # Add the current sequence\n",
    "        for k in examples.keys():\n",
    "            if k == 'input_ids':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append(newline_token_ids)\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            elif k == 'attention_mask':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append([1] * len(newline_token_ids))\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            else:\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "        \n",
    "        current_length += sequence_length\n",
    "        if current_sequence['input_ids']:\n",
    "            current_length += len(newline_token_ids)\n",
    "    \n",
    "    # Add the last combined sequence\n",
    "    for k in combined.keys():\n",
    "        if current_sequence[k]:\n",
    "            combined[k].append(list(chain(*current_sequence[k])))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Use this before group_texts\n",
    "    examples = combine_short_sequences(examples)\n",
    "\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    block_size = 512\n",
    "    # More granular block size selection\n",
    "    if total_length < 512:\n",
    "        block_size = 512\n",
    "    if total_length < 1024:\n",
    "        block_size = 1024\n",
    "    else: # elif total_length < 2048:\n",
    "        block_size = 2048\n",
    "    # elif total_length < 4096:\n",
    "    #     block_size = 4096\n",
    "    # else:\n",
    "    #     block_size = 8192  # Increase maximum block size, ensure your model can handle this\n",
    "    \n",
    "    # Adaptive padding\n",
    "    padding_unit = 128  # or another suitable value\n",
    "    if total_length < block_size:\n",
    "        block_size = ((total_length + padding_unit - 1) // padding_unit) * padding_unit\n",
    "    \n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "    result = {k: [] for k in concatenated_examples.keys()}\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        block_start = i * block_size\n",
    "        block_end = min((i + 1) * block_size, total_length)\n",
    "        for k, t in concatenated_examples.items():\n",
    "            block = t[block_start:block_end]\n",
    "            if len(block) < block_size:\n",
    "                padding_length = block_size - len(block)\n",
    "                if k == 'input_ids':\n",
    "                    block = block + [tokenizer.pad_token_id] * padding_length\n",
    "                elif k == 'attention_mask':\n",
    "                    block = block + [0] * padding_length\n",
    "            result[k].append(block)\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 9951012\n",
       "})"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:57:23] Energy consumed for RAM : 0.087766 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:23] Energy consumed for all GPUs : 0.017114 kWh. Total GPU Power : 66.46623037631711 W\n",
      "[codecarbon INFO @ 00:57:23] Energy consumed for all CPUs : 0.016361 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:23] 0.121241 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:57:27] Energy consumed for RAM : 34.203348 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:27] Energy consumed for all GPUs : 3.057468 kWh. Total GPU Power : 66.91138937204263 W\n",
      "[codecarbon INFO @ 00:57:27] Energy consumed for all CPUs : 6.377910 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:27] 43.638726 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:57:29] Energy consumed for RAM : 24.624978 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:29] Energy consumed for all GPUs : 2.203553 kWh. Total GPU Power : 66.45734062831096 W\n",
      "[codecarbon INFO @ 00:57:29] Energy consumed for all CPUs : 4.592757 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:29] 31.421289 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/\"\n",
    "data_cache_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $data_cache_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 9951012\n",
       "})"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:57:31] Energy consumed for RAM : 24.616998 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:31] Energy consumed for RAM : 34.221213 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:31] Energy consumed for all GPUs : 2.202649 kWh. Total GPU Power : 66.91778727230283 W\n",
      "[codecarbon INFO @ 00:57:31] Energy consumed for all CPUs : 4.591062 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:31] 31.410709 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:57:31] Energy consumed for all GPUs : 3.059030 kWh. Total GPU Power : 66.91545151869394 W\n",
      "[codecarbon INFO @ 00:57:31] Energy consumed for all CPUs : 6.380969 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:31] 43.661212 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 20:54:47] Energy consumed for RAM : 31.180022 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:54:47] Energy consumed for all GPUs : 2.778560 kWh. Total GPU Power : 66.8408264009772 W\n",
      "[codecarbon INFO @ 20:54:47] Energy consumed for all CPUs : 5.812006 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:54:47] 39.770588 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:54:50] Energy consumed for RAM : 21.601152 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:54:50] Energy consumed for all GPUs : 1.924663 kWh. Total GPU Power : 66.8182510097272 W\n",
      "[codecarbon INFO @ 20:54:50] Energy consumed for all CPUs : 4.026942 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:54:50] 27.552757 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:54:50] Energy consumed for RAM : 31.195707 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:54:50] Energy consumed for all GPUs : 2.780112 kWh. Total GPU Power : 66.8355754558007 W\n",
      "[codecarbon INFO @ 20:54:50] Energy consumed for all CPUs : 5.815062 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:54:50] 39.790880 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:54:51] Energy consumed for RAM : 21.592277 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:54:51] Energy consumed for all GPUs : 1.923735 kWh. Total GPU Power : 66.82898201583005 W\n",
      "[codecarbon INFO @ 20:54:51] Energy consumed for all CPUs : 4.025131 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:54:51] 27.541143 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:02] Energy consumed for RAM : 31.183151 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:02] Energy consumed for all GPUs : 2.778839 kWh. Total GPU Power : 67.03324725512233 W\n",
      "[codecarbon INFO @ 20:55:02] Energy consumed for all CPUs : 5.812590 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:02] 39.774580 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:05] Energy consumed for RAM : 21.604282 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:05] Energy consumed for all GPUs : 1.924942 kWh. Total GPU Power : 66.97303344880224 W\n",
      "[codecarbon INFO @ 20:55:05] Energy consumed for all CPUs : 4.027525 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:05] 27.556749 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:05] Energy consumed for RAM : 31.198836 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:05] Energy consumed for all GPUs : 2.780389 kWh. Total GPU Power : 66.50421635595708 W\n",
      "[codecarbon INFO @ 20:55:05] Energy consumed for all CPUs : 5.815645 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:05] 39.794870 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:06] Energy consumed for RAM : 21.595407 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:06] Energy consumed for all GPUs : 1.924012 kWh. Total GPU Power : 66.49601855402246 W\n",
      "[codecarbon INFO @ 20:55:06] Energy consumed for all CPUs : 4.025714 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:06] 27.545133 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:17] Energy consumed for RAM : 31.186280 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:17] Energy consumed for all GPUs : 2.779116 kWh. Total GPU Power : 66.46153650780823 W\n",
      "[codecarbon INFO @ 20:55:17] Energy consumed for all CPUs : 5.813173 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:17] 39.778570 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:20] Energy consumed for RAM : 21.607411 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:20] Energy consumed for all GPUs : 1.925219 kWh. Total GPU Power : 66.45944975662177 W\n",
      "[codecarbon INFO @ 20:55:20] Energy consumed for all CPUs : 4.028108 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:20] 27.560739 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:20] Energy consumed for RAM : 31.201966 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:20] Energy consumed for all GPUs : 2.780665 kWh. Total GPU Power : 66.45838906166925 W\n",
      "[codecarbon INFO @ 20:55:20] Energy consumed for all CPUs : 5.816229 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:20] 39.798860 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:21] Energy consumed for RAM : 21.598536 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:21] Energy consumed for all GPUs : 1.924288 kWh. Total GPU Power : 66.46081674175961 W\n",
      "[codecarbon INFO @ 20:55:21] Energy consumed for all CPUs : 4.026297 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:21] 27.549122 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:32] Energy consumed for RAM : 31.189410 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:32] Energy consumed for all GPUs : 2.779393 kWh. Total GPU Power : 66.47881829237662 W\n",
      "[codecarbon INFO @ 20:55:32] Energy consumed for all CPUs : 5.813756 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:32] 39.782560 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:35] Energy consumed for RAM : 21.610541 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:35] Energy consumed for all GPUs : 1.925496 kWh. Total GPU Power : 66.54760869388689 W\n",
      "[codecarbon INFO @ 20:55:35] Energy consumed for all CPUs : 4.028692 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:35] 27.564729 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:35] Energy consumed for RAM : 31.205096 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:35] Energy consumed for all GPUs : 2.780944 kWh. Total GPU Power : 66.99065498295255 W\n",
      "[codecarbon INFO @ 20:55:35] Energy consumed for all CPUs : 5.816812 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:35] 39.802852 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:36] Energy consumed for RAM : 21.601666 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:36] Energy consumed for all GPUs : 1.924568 kWh. Total GPU Power : 66.99926444824989 W\n",
      "[codecarbon INFO @ 20:55:36] Energy consumed for all CPUs : 4.026881 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:36] 27.553114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:47] Energy consumed for RAM : 31.192539 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:47] Energy consumed for all GPUs : 2.779672 kWh. Total GPU Power : 67.05150178659 W\n",
      "[codecarbon INFO @ 20:55:47] Energy consumed for all CPUs : 5.814339 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:47] 39.786551 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:50] Energy consumed for RAM : 21.613671 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:50] Energy consumed for all GPUs : 1.925775 kWh. Total GPU Power : 66.99961636123423 W\n",
      "[codecarbon INFO @ 20:55:50] Energy consumed for all CPUs : 4.029275 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:50] 27.568721 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:50] Energy consumed for RAM : 31.208225 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:50] Energy consumed for all GPUs : 2.781222 kWh. Total GPU Power : 66.58074174236306 W\n",
      "[codecarbon INFO @ 20:55:50] Energy consumed for all CPUs : 5.817395 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:50] 39.806842 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:55:51] Energy consumed for RAM : 21.604795 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:55:51] Energy consumed for all GPUs : 1.924845 kWh. Total GPU Power : 66.57776585186541 W\n",
      "[codecarbon INFO @ 20:55:51] Energy consumed for all CPUs : 4.027464 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:55:51] 27.557104 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:02] Energy consumed for RAM : 31.195669 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:02] Energy consumed for all GPUs : 2.779950 kWh. Total GPU Power : 66.5560132299859 W\n",
      "[codecarbon INFO @ 20:56:02] Energy consumed for all CPUs : 5.814923 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:02] 39.790541 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:05] Energy consumed for RAM : 21.616800 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:05] Energy consumed for all GPUs : 1.926052 kWh. Total GPU Power : 66.55697881048891 W\n",
      "[codecarbon INFO @ 20:56:05] Energy consumed for all CPUs : 4.029858 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:05] 27.572711 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:05] Energy consumed for RAM : 31.211354 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:05] Energy consumed for all GPUs : 2.781499 kWh. Total GPU Power : 66.53251116923293 W\n",
      "[codecarbon INFO @ 20:56:05] Energy consumed for all CPUs : 5.817979 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:05] 39.810832 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:06] Energy consumed for RAM : 21.607925 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:06] Energy consumed for all GPUs : 1.925122 kWh. Total GPU Power : 66.53725629819333 W\n",
      "[codecarbon INFO @ 20:56:06] Energy consumed for all CPUs : 4.028047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:06] 27.561094 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:17] Energy consumed for RAM : 31.198798 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:17] Energy consumed for all GPUs : 2.780227 kWh. Total GPU Power : 66.48483646984272 W\n",
      "[codecarbon INFO @ 20:56:17] Energy consumed for all CPUs : 5.815506 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:17] 39.794531 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:20] Energy consumed for RAM : 21.619929 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:20] Energy consumed for all GPUs : 1.926329 kWh. Total GPU Power : 66.46595240017862 W\n",
      "[codecarbon INFO @ 20:56:20] Energy consumed for all CPUs : 4.030442 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:20] 27.576700 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:20] Energy consumed for RAM : 31.214484 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:20] Energy consumed for all GPUs : 2.781778 kWh. Total GPU Power : 66.91437344678323 W\n",
      "[codecarbon INFO @ 20:56:20] Energy consumed for all CPUs : 5.818562 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:20] 39.814823 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:21] Energy consumed for RAM : 21.611054 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:21] Energy consumed for all GPUs : 1.925401 kWh. Total GPU Power : 66.91277587807224 W\n",
      "[codecarbon INFO @ 20:56:21] Energy consumed for all CPUs : 4.028631 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:21] 27.565085 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:32] Energy consumed for RAM : 31.201928 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:32] Energy consumed for all GPUs : 2.780503 kWh. Total GPU Power : 66.46726576141329 W\n",
      "[codecarbon INFO @ 20:56:32] Energy consumed for all CPUs : 5.816089 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:32] 39.798521 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:35] Energy consumed for RAM : 21.623059 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:35] Energy consumed for all GPUs : 1.926606 kWh. Total GPU Power : 66.46562457563128 W\n",
      "[codecarbon INFO @ 20:56:35] Energy consumed for all CPUs : 4.031025 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:35] 27.580690 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:35] Energy consumed for RAM : 31.217613 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:35] Energy consumed for all GPUs : 2.782054 kWh. Total GPU Power : 66.46976839823266 W\n",
      "[codecarbon INFO @ 20:56:35] Energy consumed for all CPUs : 5.819145 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:35] 39.818813 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:36] Energy consumed for RAM : 21.614183 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:36] Energy consumed for all GPUs : 1.925677 kWh. Total GPU Power : 66.47246864678067 W\n",
      "[codecarbon INFO @ 20:56:36] Energy consumed for all CPUs : 4.029214 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:36] 27.569075 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:47] Energy consumed for RAM : 31.205057 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:47] Energy consumed for all GPUs : 2.780782 kWh. Total GPU Power : 66.90689438893276 W\n",
      "[codecarbon INFO @ 20:56:47] Energy consumed for all CPUs : 5.816673 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:47] 39.802512 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:50] Energy consumed for RAM : 21.626188 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:50] Energy consumed for all GPUs : 1.926885 kWh. Total GPU Power : 66.91229083834529 W\n",
      "[codecarbon INFO @ 20:56:50] Energy consumed for all CPUs : 4.031608 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:50] 27.584681 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:50] Energy consumed for RAM : 31.220743 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:50] Energy consumed for all GPUs : 2.782331 kWh. Total GPU Power : 66.46657503075406 W\n",
      "[codecarbon INFO @ 20:56:50] Energy consumed for all CPUs : 5.819728 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:50] 39.822802 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:56:51] Energy consumed for RAM : 21.617313 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:56:51] Energy consumed for all GPUs : 1.925954 kWh. Total GPU Power : 66.4573653665544 W\n",
      "[codecarbon INFO @ 20:56:51] Energy consumed for all CPUs : 4.029797 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:56:51] 27.573065 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:02] Energy consumed for RAM : 31.208187 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:02] Energy consumed for all GPUs : 2.781059 kWh. Total GPU Power : 66.4590368016871 W\n",
      "[codecarbon INFO @ 20:57:02] Energy consumed for all CPUs : 5.817256 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:02] 39.806502 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:05] Energy consumed for RAM : 21.629318 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:05] Energy consumed for all GPUs : 1.927162 kWh. Total GPU Power : 66.46125448847427 W\n",
      "[codecarbon INFO @ 20:57:05] Energy consumed for all CPUs : 4.032191 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:05] 27.588671 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:05] Energy consumed for RAM : 31.223872 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:05] Energy consumed for all GPUs : 2.782608 kWh. Total GPU Power : 66.45861063855209 W\n",
      "[codecarbon INFO @ 20:57:05] Energy consumed for all CPUs : 5.820312 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:05] 39.826792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:06] Energy consumed for RAM : 21.620443 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:06] Energy consumed for all GPUs : 1.926231 kWh. Total GPU Power : 66.46365454216195 W\n",
      "[codecarbon INFO @ 20:57:06] Energy consumed for all CPUs : 4.030380 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:06] 27.577054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:17] Energy consumed for RAM : 31.211316 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:17] Energy consumed for all GPUs : 2.781336 kWh. Total GPU Power : 66.4644908747414 W\n",
      "[codecarbon INFO @ 20:57:17] Energy consumed for all CPUs : 5.817839 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:17] 39.810491 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:20] Energy consumed for RAM : 21.632446 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:20] Energy consumed for all GPUs : 1.927438 kWh. Total GPU Power : 66.49273063480845 W\n",
      "[codecarbon INFO @ 20:57:20] Energy consumed for all CPUs : 4.032775 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:20] 27.592659 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:20] Energy consumed for RAM : 31.227002 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:20] Energy consumed for all GPUs : 2.782887 kWh. Total GPU Power : 66.91189225550993 W\n",
      "[codecarbon INFO @ 20:57:20] Energy consumed for all CPUs : 5.820895 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:20] 39.830783 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:21] Energy consumed for RAM : 21.623572 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:21] Energy consumed for all GPUs : 1.926510 kWh. Total GPU Power : 66.9118132587728 W\n",
      "[codecarbon INFO @ 20:57:21] Energy consumed for all CPUs : 4.030964 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:21] 27.581046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:32] Energy consumed for RAM : 31.214446 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:32] Energy consumed for all GPUs : 2.781615 kWh. Total GPU Power : 66.90335806492352 W\n",
      "[codecarbon INFO @ 20:57:32] Energy consumed for all CPUs : 5.818422 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:32] 39.814483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:35] Energy consumed for RAM : 21.635575 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:35] Energy consumed for all GPUs : 1.927717 kWh. Total GPU Power : 66.89975192928674 W\n",
      "[codecarbon INFO @ 20:57:35] Energy consumed for all CPUs : 4.033358 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:35] 27.596650 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:35] Energy consumed for RAM : 31.230131 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:35] Energy consumed for all GPUs : 2.783164 kWh. Total GPU Power : 66.45527385403541 W\n",
      "[codecarbon INFO @ 20:57:35] Energy consumed for all CPUs : 5.821478 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:35] 39.834773 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:36] Energy consumed for RAM : 21.626702 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:36] Energy consumed for all GPUs : 1.926787 kWh. Total GPU Power : 66.45501099981166 W\n",
      "[codecarbon INFO @ 20:57:36] Energy consumed for all CPUs : 4.031547 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:36] 27.585035 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:47] Energy consumed for RAM : 31.217575 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:47] Energy consumed for all GPUs : 2.781891 kWh. Total GPU Power : 66.46849666487566 W\n",
      "[codecarbon INFO @ 20:57:47] Energy consumed for all CPUs : 5.819006 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:47] 39.818472 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:50] Energy consumed for RAM : 21.638705 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:50] Energy consumed for all GPUs : 1.927994 kWh. Total GPU Power : 66.47598514476945 W\n",
      "[codecarbon INFO @ 20:57:50] Energy consumed for all CPUs : 4.033941 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:50] 27.600640 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:50] Energy consumed for RAM : 31.233260 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:50] Energy consumed for all GPUs : 2.783440 kWh. Total GPU Power : 66.47969413666651 W\n",
      "[codecarbon INFO @ 20:57:50] Energy consumed for all CPUs : 5.822061 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:50] 39.838762 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:57:51] Energy consumed for RAM : 21.629831 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:57:51] Energy consumed for all GPUs : 1.927064 kWh. Total GPU Power : 66.4742646185218 W\n",
      "[codecarbon INFO @ 20:57:51] Energy consumed for all CPUs : 4.032130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:57:51] 27.589025 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:58:02] Energy consumed for RAM : 31.220704 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:58:02] Energy consumed for all GPUs : 2.782168 kWh. Total GPU Power : 66.47803829011966 W\n",
      "[codecarbon INFO @ 20:58:02] Energy consumed for all CPUs : 5.819589 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:58:02] 39.822462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:58:05] Energy consumed for RAM : 21.641834 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:58:05] Energy consumed for all GPUs : 1.928271 kWh. Total GPU Power : 66.46836150073159 W\n",
      "[codecarbon INFO @ 20:58:05] Energy consumed for all CPUs : 4.034524 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:58:05] 27.604629 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:58:05] Energy consumed for RAM : 31.236390 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:58:05] Energy consumed for all GPUs : 2.783719 kWh. Total GPU Power : 66.91720335954183 W\n",
      "[codecarbon INFO @ 20:58:05] Energy consumed for all CPUs : 5.822645 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:58:05] 39.842754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 20:58:06] Energy consumed for RAM : 21.632961 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 20:58:06] Energy consumed for all GPUs : 1.927342 kWh. Total GPU Power : 66.92270227237249 W\n",
      "[codecarbon INFO @ 20:58:06] Energy consumed for all CPUs : 4.032713 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 20:58:06] 27.593016 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "# since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "tok_logger = transformers.utils.logging.get_logger(\n",
    "    \"transformers.tokenization_utils_base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='meta-llama/Meta-Llama-3-8B', vocab_size=128000, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>', 'pad_token': '<|reserved_special_token_250|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128005: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128010: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128011: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128012: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128013: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128014: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128015: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<|reserved_special_token_248|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<|reserved_special_token_249|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<|reserved_special_token_250|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.testing_utils import CaptureLogger\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[\"text\"])\n",
    "    # clm input could be much much longer than block_size\n",
    "    # if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "    #     tok_logger.warning(\n",
    "    #         \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "    #         \" before being passed to the model.\"\n",
    "    #     )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/\"\n",
    "data_cache_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_dataset_part_16.parquet',\n",
       " 'output_dataset_part_15.parquet',\n",
       " 'output_dataset_part_14.parquet',\n",
       " 'output_dataset_part_13.parquet',\n",
       " 'output_dataset_part_12.parquet',\n",
       " 'output_dataset_part_11.parquet',\n",
       " 'output_dataset_part_10.parquet',\n",
       " 'output_dataset_part_9.parquet',\n",
       " 'output_dataset_part_8.parquet',\n",
       " 'output_dataset_part_7.parquet',\n",
       " 'output_dataset_part_6.parquet',\n",
       " 'output_dataset_part_5.parquet',\n",
       " 'output_dataset_part_4.parquet',\n",
       " 'output_dataset_part_3.parquet',\n",
       " 'output_dataset_part_2.parquet',\n",
       " 'output_dataset_part_1.parquet']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "path = Path(dataset_dir)\n",
    "files = [file.name for file in path.glob(\"*.parquet\")]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:57:44] Energy consumed for RAM : 24.628108 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:44] Energy consumed for all GPUs : 2.203830 kWh. Total GPU Power : 66.46734825569662 W\n",
      "[codecarbon INFO @ 00:57:44] Energy consumed for all CPUs : 4.593340 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:44] 31.425279 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:57:46] Energy consumed for RAM : 24.620128 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:46] Energy consumed for RAM : 34.224343 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:57:46] Energy consumed for all GPUs : 2.202926 kWh. Total GPU Power : 66.47249076188298 W\n",
      "[codecarbon INFO @ 00:57:46] Energy consumed for all CPUs : 4.591645 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:46] 31.414698 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:57:46] Energy consumed for all GPUs : 3.059307 kWh. Total GPU Power : 66.47018658463261 W\n",
      "[codecarbon INFO @ 00:57:46] Energy consumed for all CPUs : 6.381552 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:57:46] 43.665201 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "def transform_record(record):\n",
    "    record['timestamp'] = [record['timestamp']] if record['timestamp'] is not None else []\n",
    "    record['url'] = [record['url']] if record['url'] is not None else []\n",
    "    record['source'] = [record['source']] if reucord['source'] is not None else []\n",
    "    record['labels'] = record.get('labels', [])\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1842,\n",
       " 'text': 'পরিনিতা আজ দিশার বিয়ে তাই সকাল থেকে আনন্দ হই হুল্লোড়ে মেতে উঠেছে দিশাদের বাড়িটা।। সকাল থেকে অথিতি রা সব আসতে শুরু করে ছে সানাইএর মিষ্টি সুর খুশি আর আনন্দে ঝলকে উঠছে বাড়ির প্রতিটি কোন। আজ দিশা ভীষন খুশি বহুদিনের প্রতীক্ষার পর ওর মনের মানুষের সাথে বিবাহ বন্ধেনে আবদ্ধ হবে।। আজ ও মনে পড়ে অরুনের সাথে প্রথম যেদিন দেখা হয়েছিল। অনেক খুশি দু চোখ ভরা স্বপ্ন সাজিয়ে দিশাও একটু একটু করে ডুবে যাচ্ছে আগামী দিনের রঙিন স্বপ্নে।। আজ দিশা কে একবারও কারুর ডাকতে হয়নি দশটার মানুষ আজ ভোর চারটে উঠেছে দধি মঙ্গল গায়ে হলুদ সব নিয়ম কানুন সুন্দর করে পালন করা হচ্চে।। বিকেলে লাস্ট বার গায়ে হলুদ ছুঁইয়ে স্নান করিয়ে দিশা বসেছে এখন সাজতে।। পার্লারের দুটো মেয়ে খুব সুন্দর করে সাজিয়ে তুলছে কনের সাজে লাল বেনারসি গায়ে সোনার গহনা কপালে চন্দন লালটিপ হাতে শাখা পলা মাথায় লাল চেলি খোঁপায় জুঁইয়ের মালা কি মিষ্টি যে লাগছে বলার ভাষা নেই।। দিদি দেখ আজ তোমার বর তোমাকে এই রূপে দেখে ভিরমি খাবে।। পার্লারের মেয়েগুলোর মুখে এই কথা শুনে দিশা লজ্জা পেলেও মনে মনে বেশ খুশি হয়েছে সে। সেই কোন সকাল থেকে অপেক্ষা করছে দিশা অরুণ কখন আসবে কখন ওকে দেখতে পাবে দেখতে দেখতে সময় যে ঘনিয়ে এসেছে আর কিছুক্ষণের অপেক্ষা তারপরই তো অরুনের পরিনিতা।। ভাবতে ভাবতে কখন যে স্বপ্নের জগতে হারিয়ে গেছে খেয়াল নেই।। তারপর ওকে নিয়ে বসানো হয় বউ আসরে সমস্ত অতিথি রা আসছেন উপহার দিচ্ছেন ও এখন এখানেই ব্যস্ত প্রায়।। হঠাৎ কিছু একটা গোলমাল আর সাথে মায়ের বুক ফাটা কান্না শুনে দৌড়ে যায় দিশা।। কি হয়েছে এত চিৎকার কিসের??? আর মা তুমি এইভাবে কাঁদছো কেন আমি জানি তুমি আমাকে ছাড়া থাকতে পারোনা কিন্তু মন খারাপ করোনা আমি তো আসবো মাঝে মধ্যে তোমার কাছে।। দিশার মা দিশা কে জড়িয়ে ধরে আরো কাঁদতে থাকে।। মা কেঁদোনা প্লিজ কেঁদোনা তুমি কাঁদলে আমার ও তো কষ্ট হয় বল আর আমি কাঁদলে সাজটাই নষ্ট হয়ে যাবে তখন অরুণ এসে এই পাগলী দিশা কে বিয়ে না করেই চলে যাবে।। দিশা অরুণ এর............ মা কি হয়েছে ওর??বলো না কোনো বিপদ হয়নি তো ও ঠিক আছে তো??এই দাদাভাই কি হলো চুপ করে আছিস আমার কিন্ত হাত পা ঠান্ডা হয়েছে বল না বল না রে ও ঠিক আছে তো?? বোন অরুণ সিঁড়ি দিয়ে স্লিপ করে পড়ে গিয়েছে মাথায় গুরুতর আঘাত পেয়েছে কন্ডিশন খুব খারাপ আমাদের এখনই যেতে হবে।। কি থেকে কি হয়ে গেল দিশা ভেবেছিল আর কয়েক ঘন্টা পর হয়তো ও অরুনের হয়ে যাবে কিন্তু সব হিসেব যে গুলিয়ে যাচ্ছে ও কে আর পাওয়া হবে তো ??নাকি সব সময় থামিয়ে দিয়ে ও দিশা কে ছেড়ে না ও আর ভাবতে পারছেনা পাগলের মতো চিৎকার করে সাধের সাজ গোজ প্রিয় মানুষকে দেখানোর অনুভতি সব দুমড়ে মুচড়ে ফেলে ছুটে গেল অরুনের কাছে।। ডক্টর বলেই দিয়েছে 24 ঘন্টা না কাটলে কিছুই বলা যাবে না। দিশার কাছে আরো একটা দিনের অপেক্ষা। অপেক্ষা করে থাকতে ওর ক্লান্তি নেই কিন্তু যার জন্য এই অপেক্ষা সে থাকবে তো 24 ঘন্টা সময় দেবে তো??নাকি....... হসপিটালের ঠাকুরের কাছে হত্যে দিয়ে পরে আছে দিশা তখনও তার পরনে লাল বেনারসি হাতে সাকা পলা। কাঁদতে কাঁদতে কখন যে ঘুমিয়ে পড়েছে খেয়াল নেই ভোরের আলো ফুটতে দৌড়ে গেল ওর কেবিনের দিকে। অপারেশন সাকসেস ফুল কিন্তু দুঃখের বিষয় দুটো চোখই নষ্ট হয়ে গেছে।। দিন পনের থাকার পর এখন একটু সুস্থ তাই আজ তাকে বাড়িতে নিয়ে যাওয়া হচ্চে । হাঁটতে পারেনা হুইল চেয়ারে বসিয়ে দিশা নিয়ে যাচ্ছে ঘরে।। হাতের স্পর্শে ও বোঝার চেষ্টা করছে কে ওর সাথে আছে গলার স্বর শুনে বুঝতে পারে দিশা কে তুমি এখানে ??? আমি তোমার স্ত্রী তোমার কাছে থাকবো না তো কোথায় থাকবো। কিন্তু আমি তো তোমাকে বিয়ে করিনি তুমি করোনি তো কি হয়েছে আমি করেছি তোমাকে অপারেশন টেবিলে তোলার আগে তোমার বুড়ো আঙুলের ছোঁয়ায় ভরিয়েছি সিঁথি এ যে আমার এ জন্মের সাধ যদি তোমাকে আর না পাই তাই তোমার পরিনিতা হওয়ার স্বপ্ন টাকে মুছতে পারলাম না গো। এমন পাগলামি কেন করলে দিশা একজন অন্ধের সাথে তোমার সুন্দর জীবনকে অন্ধকারে ডুবিয়ে দিলে। তুমি তো বোকা নও তবে কেন এমন পাগলামি করলে ওহ দয়া করছো আমায় করুনা করছো আমাকে ?? ভুল বুঝলে তো তুমি অরুণ আমি তোমাকে ভালোবাসি দয়া কেন করবো আর আজ যদি আমার সাথে এমন ঘটত তুমি তাহলে আমার হাত ছেড়ে চলে যেতে তো?? এসব কেন বলছো আমি তোমাকে ছেড়ে বাঁচার কথা ভাবতেই পারিনা যে। তবে আমি কিকরে পারি শুনি আমিও যে তোমায় বড্ড ভালোবাসি আর সারাজীবন ভালোবাসবো।। আর অন্ধ কাকে বলছো শুনি আমার চোখ দিয়ে তুমি দেখবে এখন তো বড় অপারেশন হয়েছে বছর খানেক বাদে আমার একটি চোখ তোমাকে দেব তার পর আমাদের দুজনের ভালোবাসায় আমরা একসাথে পথ চলবো। আফসোস তো একটাই যে যার জন্য সেদিন অত সুন্দর সাজলাম তার দেখা হলোনা। কিন্তু সেই সাধ অপূর্ন রাখবো না যেদিন তুমি দৃষ্টি পাবে সেদিন আবারো সেদিনের মত নব্য পরিনিতার বেশে তোমার চোখে ধরা দেবো। আজ দু বছর অন্ধত্বের অভিশাপ কাটিয়ে দুজনে দুজনের দৃষ্টি ভাগ করে নিয়েছে। সেই সকাল থেকে দুজন দুজনের একান্ত অপেক্ষা আজ যে ওদের বৌভাত । সমস্ত অনুষ্ঠান মিটিয়ে ঘরে এসে একান্তে পাওয়ায় অপেক্ষা । আর অন্ধের দিবা রাত্রি নয় দু চোখে দৃষ্টি নিয়ে হাজারো স্বপ্ন নিয়ে প্রতিটা দিন প্রতিটা মহূর্ত কাটবে অনেক অনেক ভালোবাসায়। ওদের বিবাহিত জীবন সুন্দর প্রেমময় হয়ে উঠুক এই কামনা করি।। rama middya@@@'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 19:13:46] Energy consumed for RAM : 29.915701 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:13:46] Energy consumed for all GPUs : 2.666263 kWh. Total GPU Power : 66.51286656412894 W\n",
      "[codecarbon INFO @ 19:13:46] Energy consumed for all CPUs : 5.576338 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:13:46] 38.158302 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:13:48] Energy consumed for RAM : 20.337187 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:13:48] Energy consumed for all GPUs : 1.812341 kWh. Total GPU Power : 66.50362653606635 W\n",
      "[codecarbon INFO @ 19:13:48] Energy consumed for all CPUs : 3.791230 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:13:48] 25.940758 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:13:49] Energy consumed for RAM : 29.931425 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:13:49] Energy consumed for all GPUs : 2.667792 kWh. Total GPU Power : 66.48978793955844 W\n",
      "[codecarbon INFO @ 19:13:49] Energy consumed for all CPUs : 5.579353 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:13:49] 38.178569 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:13:49] Energy consumed for RAM : 20.328001 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:13:49] Energy consumed for all GPUs : 1.811413 kWh. Total GPU Power : 66.49174099852603 W\n",
      "[codecarbon INFO @ 19:13:49] Energy consumed for all CPUs : 3.789417 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:13:49] 25.928831 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:01] Energy consumed for RAM : 29.918831 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:01] Energy consumed for all GPUs : 2.666540 kWh. Total GPU Power : 66.47380384219403 W\n",
      "[codecarbon INFO @ 19:14:01] Energy consumed for all CPUs : 5.576921 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:01] 38.162292 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:03] Energy consumed for RAM : 20.340316 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:03] Energy consumed for all GPUs : 1.812618 kWh. Total GPU Power : 66.48549806970456 W\n",
      "[codecarbon INFO @ 19:14:03] Energy consumed for all CPUs : 3.791813 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:03] 25.944747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:04] Energy consumed for RAM : 29.934554 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:04] Energy consumed for all GPUs : 2.668071 kWh. Total GPU Power : 66.93101406004976 W\n",
      "[codecarbon INFO @ 19:14:04] Energy consumed for all CPUs : 5.579936 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:04] 38.182561 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:04] Energy consumed for RAM : 20.331131 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:04] Energy consumed for all GPUs : 1.811690 kWh. Total GPU Power : 66.48960098583403 W\n",
      "[codecarbon INFO @ 19:14:04] Energy consumed for all CPUs : 3.790000 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:04] 25.932821 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:16] Energy consumed for RAM : 29.921960 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:16] Energy consumed for all GPUs : 2.666817 kWh. Total GPU Power : 66.46998588650702 W\n",
      "[codecarbon INFO @ 19:14:16] Energy consumed for all CPUs : 5.577505 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:16] 38.166282 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:18] Energy consumed for RAM : 20.343445 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:18] Energy consumed for all GPUs : 1.812897 kWh. Total GPU Power : 66.91353747668023 W\n",
      "[codecarbon INFO @ 19:14:18] Energy consumed for all CPUs : 3.792396 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:18] 25.948739 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:19] Energy consumed for RAM : 29.937683 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:19] Energy consumed for all GPUs : 2.668347 kWh. Total GPU Power : 66.4755450813976 W\n",
      "[codecarbon INFO @ 19:14:19] Energy consumed for all CPUs : 5.580519 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:19] 38.186550 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:19] Energy consumed for RAM : 20.334260 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:19] Energy consumed for all GPUs : 1.811969 kWh. Total GPU Power : 66.91306256321545 W\n",
      "[codecarbon INFO @ 19:14:19] Energy consumed for all CPUs : 3.790584 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:19] 25.936812 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:31] Energy consumed for RAM : 29.925089 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:31] Energy consumed for all GPUs : 2.667096 kWh. Total GPU Power : 66.927727639775 W\n",
      "[codecarbon INFO @ 19:14:31] Energy consumed for all CPUs : 5.578088 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:31] 38.170273 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:33] Energy consumed for RAM : 20.346575 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:33] Energy consumed for all GPUs : 1.813174 kWh. Total GPU Power : 66.4738951880406 W\n",
      "[codecarbon INFO @ 19:14:33] Energy consumed for all CPUs : 3.792980 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:33] 25.952728 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:34] Energy consumed for RAM : 29.940813 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:34] Energy consumed for all GPUs : 2.668624 kWh. Total GPU Power : 66.46900687610099 W\n",
      "[codecarbon INFO @ 19:14:34] Energy consumed for all CPUs : 5.581103 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:34] 38.190540 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:34] Energy consumed for RAM : 20.337390 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:34] Energy consumed for all GPUs : 1.812245 kWh. Total GPU Power : 66.47125078166663 W\n",
      "[codecarbon INFO @ 19:14:34] Energy consumed for all CPUs : 3.791167 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:34] 25.940802 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:46] Energy consumed for RAM : 29.928219 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:46] Energy consumed for all GPUs : 2.667373 kWh. Total GPU Power : 66.49272525841856 W\n",
      "[codecarbon INFO @ 19:14:46] Energy consumed for all CPUs : 5.578671 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:46] 38.174263 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:48] Energy consumed for RAM : 20.349704 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:48] Energy consumed for all GPUs : 1.813451 kWh. Total GPU Power : 66.49227813707847 W\n",
      "[codecarbon INFO @ 19:14:48] Energy consumed for all CPUs : 3.793563 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:48] 25.956718 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:49] Energy consumed for RAM : 29.943943 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:49] Energy consumed for all GPUs : 2.668903 kWh. Total GPU Power : 66.94162476619685 W\n",
      "[codecarbon INFO @ 19:14:49] Energy consumed for all CPUs : 5.581686 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:49] 38.194532 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:49] Energy consumed for RAM : 20.340519 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:14:49] Energy consumed for all GPUs : 1.812522 kWh. Total GPU Power : 66.49712855511302 W\n",
      "[codecarbon INFO @ 19:14:49] Energy consumed for all CPUs : 3.791750 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:14:49] 25.944792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:15:01] Energy consumed for RAM : 29.931349 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:15:01] Energy consumed for all GPUs : 2.667650 kWh. Total GPU Power : 66.49533130479567 W\n",
      "[codecarbon INFO @ 19:15:01] Energy consumed for all CPUs : 5.579254 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:15:01] 38.178253 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:15:03] Energy consumed for RAM : 20.352834 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:15:03] Energy consumed for all GPUs : 1.813730 kWh. Total GPU Power : 66.95173446753702 W\n",
      "[codecarbon INFO @ 19:15:03] Energy consumed for all CPUs : 3.794146 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:15:03] 25.960710 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:15:04] Energy consumed for RAM : 29.947072 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:15:04] Energy consumed for all GPUs : 2.669180 kWh. Total GPU Power : 66.51121809468145 W\n",
      "[codecarbon INFO @ 19:15:04] Energy consumed for all CPUs : 5.582269 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:15:04] 38.198521 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:15:04] Energy consumed for RAM : 20.343649 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:15:04] Energy consumed for all GPUs : 1.812799 kWh. Total GPU Power : 66.50747340276206 W\n",
      "[codecarbon INFO @ 19:15:04] Energy consumed for all CPUs : 3.792333 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:15:04] 25.948782 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "dataset[1679]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 9951012\n",
       "})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_check(examples):\n",
    "    result = tokenize_function(examples)\n",
    "    print(f\"Average tokens per text: {sum(len(ids) for ids in result['input_ids']) / len(result['input_ids'])}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 01:04:43] Energy consumed for RAM : 0.178768 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:04:43] Energy consumed for all GPUs : 0.025266 kWh. Total GPU Power : 66.45217954854103 W\n",
      "[codecarbon INFO @ 01:04:43] Energy consumed for all CPUs : 0.033471 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:04:43] 0.237504 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: data_cache_dir/*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 01:04:43] Energy consumed for RAM : 34.294039 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:04:43] Energy consumed for all GPUs : 3.065549 kWh. Total GPU Power : 66.46761154810146 W\n",
      "[codecarbon INFO @ 01:04:43] Energy consumed for all CPUs : 6.394875 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:04:43] 43.754462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:04:45] Energy consumed for RAM : 24.715597 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:04:45] Energy consumed for all GPUs : 2.211636 kWh. Total GPU Power : 66.46741567342838 W\n",
      "[codecarbon INFO @ 01:04:45] Energy consumed for all CPUs : 4.609726 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:04:45] 31.536959 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:04:48] Energy consumed for RAM : 34.311758 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:04:48] Energy consumed for all GPUs : 3.067127 kWh. Total GPU Power : 66.46899709844739 W\n",
      "[codecarbon INFO @ 01:04:48] Energy consumed for all CPUs : 6.397969 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:04:48] 43.776854 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:04:49] Energy consumed for RAM : 24.707905 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:04:49] Energy consumed for all GPUs : 2.210758 kWh. Total GPU Power : 66.86162323964247 W\n",
      "[codecarbon INFO @ 01:04:49] Energy consumed for all CPUs : 4.608080 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:04:49] 31.526744 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:04:58] Energy consumed for RAM : 0.181897 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:04:58] Energy consumed for all GPUs : 0.025542 kWh. Total GPU Power : 66.4752583974527 W\n",
      "[codecarbon INFO @ 01:04:58] Energy consumed for all CPUs : 0.034054 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:04:58] 0.241493 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:04:58] Energy consumed for RAM : 34.297168 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:04:58] Energy consumed for all GPUs : 3.065825 kWh. Total GPU Power : 66.46386038704607 W\n",
      "[codecarbon INFO @ 01:04:58] Energy consumed for all CPUs : 6.395458 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:04:58] 43.758452 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:00] Energy consumed for RAM : 24.718726 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:00] Energy consumed for all GPUs : 2.211915 kWh. Total GPU Power : 66.89739691852802 W\n",
      "[codecarbon INFO @ 01:05:00] Energy consumed for all CPUs : 4.610310 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:00] 31.540951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:03] Energy consumed for RAM : 34.314887 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:03] Energy consumed for all GPUs : 3.067406 kWh. Total GPU Power : 66.9120770060736 W\n",
      "[codecarbon INFO @ 01:05:03] Energy consumed for all CPUs : 6.398552 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:03] 43.780845 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:04] Energy consumed for RAM : 24.711035 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:04] Energy consumed for all GPUs : 2.211035 kWh. Total GPU Power : 66.46771943254383 W\n",
      "[codecarbon INFO @ 01:05:04] Energy consumed for all CPUs : 4.608664 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:04] 31.530733 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:13] Energy consumed for RAM : 0.185026 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:13] Energy consumed for all GPUs : 0.025819 kWh. Total GPU Power : 66.51306083157054 W\n",
      "[codecarbon INFO @ 01:05:13] Energy consumed for all CPUs : 0.034637 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:13] 0.245483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:13] Energy consumed for RAM : 34.300298 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:13] Energy consumed for all GPUs : 3.066102 kWh. Total GPU Power : 66.51216873965026 W\n",
      "[codecarbon INFO @ 01:05:13] Energy consumed for all CPUs : 6.396041 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:13] 43.762441 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:15] Energy consumed for RAM : 24.721856 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:15] Energy consumed for all GPUs : 2.212192 kWh. Total GPU Power : 66.50889343770908 W\n",
      "[codecarbon INFO @ 01:05:15] Energy consumed for all CPUs : 4.610893 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:15] 31.544941 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:18] Energy consumed for RAM : 34.318016 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:18] Energy consumed for all GPUs : 3.067683 kWh. Total GPU Power : 66.50715044180116 W\n",
      "[codecarbon INFO @ 01:05:18] Energy consumed for all CPUs : 6.399136 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:18] 43.784835 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:19] Energy consumed for RAM : 24.714164 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:19] Energy consumed for all GPUs : 2.211312 kWh. Total GPU Power : 66.5109342849027 W\n",
      "[codecarbon INFO @ 01:05:19] Energy consumed for all CPUs : 4.609247 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:19] 31.534723 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:28] Energy consumed for RAM : 0.188156 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:28] Energy consumed for all GPUs : 0.026098 kWh. Total GPU Power : 66.91967102318804 W\n",
      "[codecarbon INFO @ 01:05:28] Energy consumed for all CPUs : 0.035220 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:28] 0.249475 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:28] Energy consumed for RAM : 34.303427 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:28] Energy consumed for all GPUs : 3.066381 kWh. Total GPU Power : 66.91969209357569 W\n",
      "[codecarbon INFO @ 01:05:28] Energy consumed for all CPUs : 6.396625 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:28] 43.766433 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:30] Energy consumed for RAM : 24.724985 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:30] Energy consumed for all GPUs : 2.212469 kWh. Total GPU Power : 66.48074335145122 W\n",
      "[codecarbon INFO @ 01:05:30] Energy consumed for all CPUs : 4.611476 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:30] 31.548930 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:33] Energy consumed for RAM : 34.321146 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:33] Energy consumed for all GPUs : 3.067960 kWh. Total GPU Power : 66.47343699427377 W\n",
      "[codecarbon INFO @ 01:05:33] Energy consumed for all CPUs : 6.399719 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:33] 43.788825 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:34] Energy consumed for RAM : 24.717294 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:34] Energy consumed for all GPUs : 2.211590 kWh. Total GPU Power : 66.91667133731073 W\n",
      "[codecarbon INFO @ 01:05:34] Energy consumed for all CPUs : 4.609830 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:34] 31.538714 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:43] Energy consumed for RAM : 0.191286 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:43] Energy consumed for all GPUs : 0.026375 kWh. Total GPU Power : 66.45883158972606 W\n",
      "[codecarbon INFO @ 01:05:43] Energy consumed for all CPUs : 0.035804 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:43] 0.253464 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:43] Energy consumed for RAM : 34.306557 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:43] Energy consumed for all GPUs : 3.066658 kWh. Total GPU Power : 66.46224845174868 W\n",
      "[codecarbon INFO @ 01:05:43] Energy consumed for all CPUs : 6.397208 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:43] 43.770423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:45] Energy consumed for RAM : 24.728115 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:45] Energy consumed for all GPUs : 2.212748 kWh. Total GPU Power : 66.90688949297926 W\n",
      "[codecarbon INFO @ 01:05:45] Energy consumed for all CPUs : 4.612060 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:45] 31.552922 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:48] Energy consumed for RAM : 34.324276 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:48] Energy consumed for all GPUs : 3.068237 kWh. Total GPU Power : 66.46989451763194 W\n",
      "[codecarbon INFO @ 01:05:48] Energy consumed for all CPUs : 6.400302 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:48] 43.792814 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:49] Energy consumed for RAM : 24.720423 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:49] Energy consumed for all GPUs : 2.211867 kWh. Total GPU Power : 66.46602462540672 W\n",
      "[codecarbon INFO @ 01:05:49] Energy consumed for all CPUs : 4.610413 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:49] 31.542704 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:58] Energy consumed for RAM : 0.194415 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:58] Energy consumed for all GPUs : 0.026652 kWh. Total GPU Power : 66.47165457021534 W\n",
      "[codecarbon INFO @ 01:05:58] Energy consumed for all CPUs : 0.036387 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:58] 0.257454 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:05:58] Energy consumed for RAM : 34.309686 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:05:58] Energy consumed for all GPUs : 3.066935 kWh. Total GPU Power : 66.4726138561391 W\n",
      "[codecarbon INFO @ 01:05:58] Energy consumed for all CPUs : 6.397791 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:05:58] 43.774412 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:06:00] Energy consumed for RAM : 24.731244 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:06:00] Energy consumed for all GPUs : 2.213024 kWh. Total GPU Power : 66.4659821727038 W\n",
      "[codecarbon INFO @ 01:06:00] Energy consumed for all CPUs : 4.612643 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:06:00] 31.556912 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:06:03] Energy consumed for RAM : 34.327405 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:06:03] Energy consumed for all GPUs : 3.068516 kWh. Total GPU Power : 66.90951894881606 W\n",
      "[codecarbon INFO @ 01:06:03] Energy consumed for all CPUs : 6.400885 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:06:03] 43.796806 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:06:04] Energy consumed for RAM : 24.723553 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 01:06:04] Energy consumed for all GPUs : 2.212144 kWh. Total GPU Power : 66.46980303316194 W\n",
      "[codecarbon INFO @ 01:06:04] Energy consumed for all CPUs : 4.610997 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 01:06:04] 31.546693 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf data_cache_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm_datasets = []\n",
    "block_size=512\n",
    "path = Path(dataset_dir)\n",
    "files = [file.name for file in path.glob(\"*.parquet\")]\n",
    "# \\\n",
    "for idx, file in enumerate(files):\n",
    "    data_file = os.path.join(path, file)\n",
    "    filename = \"\".join(file.split(\".\")[:-1])\n",
    "    print(filename)\n",
    "    cache_path = os.path.join(\n",
    "        data_cache_dir, filename + f\"_{block_size}\"\n",
    "    )\n",
    "    print(cache_path)\n",
    "    os.makedirs(cache_path, exist_ok=True)\n",
    "    try:\n",
    "        processed_dataset = datasets.load_from_disk(\n",
    "            cache_path, keep_in_memory=False\n",
    "        )\n",
    "        logger.info(f\"training datasets-{filename} has been loaded from disk\")\n",
    "    except Exception:\n",
    "        cache_dir = os.path.join(\n",
    "            data_cache_dir, filename + f\"_parquet_{block_size}\"\n",
    "        )\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        raw_dataset = load_dataset(\n",
    "            \"parquet\",\n",
    "            data_files=data_file,\n",
    "            cache_dir=cache_dir,\n",
    "            keep_in_memory=False,\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"{file} has been loaded\")\n",
    "\n",
    "        chunk_size = 76000 # derived from uonlp/culturaX dataset\n",
    "        \n",
    "        for i in range(0, len(raw_dataset['train']), chunk_size):\n",
    "            logger.info(f\" Processing {i}-th chunk with chunk_size = {chunk_size} for file = {file}\")\n",
    "            # Create a chunk that's a DatasetDict, just like raw_dataset\n",
    "            chunk = DatasetDict({\n",
    "                'train': Dataset.from_dict(\n",
    "                    raw_dataset['train'].select(range(i, min(i+chunk_size, len(raw_dataset['train'])))).to_dict()\n",
    "                )\n",
    "            })\n",
    "            tokenized_chunk = chunk.map(\n",
    "                tokenize_and_check,\n",
    "                batched=True,\n",
    "                num_proc=16, #data_args.preprocessing_num_workers,\n",
    "                remove_columns=[\"id\", \"text\"],\n",
    "                load_from_cache_file=True,\n",
    "                keep_in_memory=False,\n",
    "                cache_file_names={\n",
    "                    k: os.path.join(cache_dir, \"tokenized.arrow\")\n",
    "                    for k in chunk\n",
    "                },\n",
    "                desc=\"Running tokenizer on chunked dataset\",\n",
    "            )\n",
    "            grouped_chunk = tokenized_chunk.map(\n",
    "                group_texts,\n",
    "                batched=True,\n",
    "                #batch_size=1000,\n",
    "                num_proc=16, #data_args.preprocessing_num_workers,\n",
    "                load_from_cache_file=True,\n",
    "                keep_in_memory=False,\n",
    "                cache_file_names={\n",
    "                    k: os.path.join(cache_dir, \"grouped.arrow\")\n",
    "                    for k in tokenized_chunk\n",
    "                },\n",
    "                desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                processed_dataset = grouped_chunk\n",
    "            else:\n",
    "                processed_dataset = concatenate_datasets([processed_dataset, grouped_chunk])\n",
    "        processed_dataset.save_to_disk(cache_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if idx == 0:\n",
    "        lm_datasets = processed_dataset[\"train\"]\n",
    "    else:\n",
    "        if lm_datasets.features.type != processed_dataset[\"train\"].features.type:\n",
    "          print(lm_datasets.features.type)\n",
    "          print(processed_dataset[\"train\"].features.type)\n",
    "\n",
    "          print(\"Converting to match types\")\n",
    "\n",
    "\n",
    "          print(\"Before transformation:\", processed_dataset[\"train\"].features)\n",
    "          processed_dataset['train'] = processed_dataset['train'].map(transform_record)\n",
    "          print(\"After transformation:\", processed_dataset[\"train\"].features)\n",
    "\n",
    "          continue\n",
    "          # assert (\n",
    "          #     lm_datasets.features.type\n",
    "          #     == processed_dataset[\"train\"].features.type\n",
    "          # )\n",
    "        lm_datasets = concatenate_datasets(\n",
    "            [lm_datasets, processed_dataset[\"train\"]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_16\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_16_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95159b9dec64c4e9d81d88f927b7a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_15\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_15_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2f99fdb8f44b89a97f7616d355affd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_14\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_14_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245216c2e4b941ad97da64eb72636420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_13\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_13_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab334155e414e1faec5b44ea01267af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_12\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_12_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759bae1075204229b797cf6d981b3c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_11\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_11_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1dbb8f9e2b4a11bcec7c8e73e6d03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_10\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_10_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb47b0272a0d47ebb5f9a651fab7bfef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_9\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_9_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26a1d9d5f94491bac9341c4fec693e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_8\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_8_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a8da2ab1f946cf9f772ff715e905ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_7\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_7_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b34cd49e564f1fb5d3e2b79db14829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_6\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_6_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e65bb0ddb04748b8c8c27400e6b16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_5\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_5_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3a4c89f60f4fa19f50a56d278d4298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_4\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_4_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f11ef2dcb84f4a9e924920ce0c8838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_3\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_3_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115f62b7f3b042dab5365480986432b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_2\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_2_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f0210ab7aa4a4bbef21ee69d459ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dataset_part_1\n",
      "/workspace/data/Bangla2B+/shards/parquet_files/cache/output_dataset_part_1_512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97509572af74c438b0ce149390ef9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size: 6486911 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "lm_datasets = []\n",
    "block_size = 512\n",
    "path = Path(dataset_dir)\n",
    "files = [file.name for file in path.glob(\"*.parquet\")]\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    data_file = os.path.join(path, file)\n",
    "    filename = \"\".join(file.split(\".\")[:-1])\n",
    "    print(filename)\n",
    "    cache_path = os.path.join(data_cache_dir, filename + f\"_{block_size}\")\n",
    "    print(cache_path)\n",
    "    os.makedirs(cache_path, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        processed_dataset = datasets.load_from_disk(cache_path)\n",
    "        logger.info(f\"training datasets-{filename} has been loaded from disk\")\n",
    "    except FileNotFoundError:\n",
    "        cache_dir = os.path.join(data_cache_dir, filename + f\"_parquet_{block_size}\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        raw_dataset = load_dataset(\"parquet\", data_files=data_file, cache_dir=cache_dir)\n",
    "        \n",
    "        logger.info(f\"{file} has been loaded\")\n",
    "        chunk_size = 76000  # derived from uonlp/culturaX dataset\n",
    "        \n",
    "        all_processed_chunks = []\n",
    "        \n",
    "        for i in range(0, len(raw_dataset['train']), chunk_size):\n",
    "            logger.info(f\" Processing {i}-th chunk with chunk_size = {chunk_size} for file = {file}\")\n",
    "            chunk = DatasetDict({\n",
    "                'train': Dataset.from_dict(\n",
    "                    raw_dataset['train'].select(range(i, min(i+chunk_size, len(raw_dataset['train'])))).to_dict()\n",
    "                )\n",
    "            })\n",
    "            tokenized_chunk = chunk.map(\n",
    "                tokenize_and_check,\n",
    "                batched=True,\n",
    "                num_proc=16,\n",
    "                remove_columns=[\"id\", \"text\"],\n",
    "                load_from_cache_file=True,\n",
    "                keep_in_memory=False,\n",
    "                cache_file_names={k: os.path.join(cache_dir, f\"tokenized_{i}.arrow\") for k in chunk},\n",
    "                desc=\"Running tokenizer on chunked dataset\",\n",
    "            )\n",
    "            grouped_chunk = tokenized_chunk.map(\n",
    "                group_texts,\n",
    "                batched=True,\n",
    "                num_proc=16,\n",
    "                load_from_cache_file=True,\n",
    "                keep_in_memory=False,\n",
    "                cache_file_names={k: os.path.join(cache_dir, f\"grouped_{i}.arrow\") for k in tokenized_chunk},\n",
    "                desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "            )\n",
    "            \n",
    "            all_processed_chunks.append(grouped_chunk['train'])\n",
    "        \n",
    "        processed_dataset = concatenate_datasets(all_processed_chunks)\n",
    "        processed_dataset.save_to_disk(cache_path)\n",
    "    \n",
    "    if idx == 0:\n",
    "        lm_datasets = processed_dataset\n",
    "    else:\n",
    "        if lm_datasets.features.type != processed_dataset.features.type:\n",
    "            print(lm_datasets.features.type)\n",
    "            print(processed_dataset.features.type)\n",
    "            print(\"Converting to match types\")\n",
    "            print(\"Before transformation:\", processed_dataset.features)\n",
    "            processed_dataset = processed_dataset.map(transform_record)\n",
    "            print(\"After transformation:\", processed_dataset.features)\n",
    "        \n",
    "        lm_datasets = concatenate_datasets([lm_datasets, processed_dataset])\n",
    "\n",
    "print(f\"Final dataset size: {len(lm_datasets)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 6486911\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data tokenization/grouping Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_data_cache_dir = '/workspace/data/Bangla2B+/shards/parquet_files/cache/tmp_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:03:18] Energy consumed for RAM : 23.946173 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:03:18] Energy consumed for all GPUs : 2.133053 kWh. Total GPU Power : 66.77463415377888 W\n",
      "[codecarbon INFO @ 00:03:18] Energy consumed for all CPUs : 4.464769 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:03:18] 30.543995 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p tmp_data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:01:16] Energy consumed for RAM : 33.508764 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:16] Energy consumed for all GPUs : 2.985688 kWh. Total GPU Power : 66.54217577868891 W\n",
      "[codecarbon INFO @ 00:01:16] Energy consumed for all CPUs : 6.247047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:16] 42.741500 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:16] Energy consumed for RAM : 23.930609 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:16] Energy consumed for all GPUs : 2.131746 kWh. Total GPU Power : 66.99472204144618 W\n",
      "[codecarbon INFO @ 00:01:16] Energy consumed for all CPUs : 4.461845 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:16] 30.524201 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:01:17] Energy consumed for RAM : 33.525053 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:01:17] Energy consumed for all GPUs : 2.987191 kWh. Total GPU Power : 66.85065739579665 W\n",
      "[codecarbon INFO @ 00:01:17] Energy consumed for all CPUs : 6.249969 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:01:17] 42.762213 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "def group_texts_old(examples):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Dynamic block size based on token length\n",
    "    if total_length < 1000:\n",
    "        block_size = 512\n",
    "    elif total_length < 5000:\n",
    "        block_size = 1024\n",
    "    else:\n",
    "        block_size = 2048\n",
    "    \n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "    result = {\n",
    "        k: [t[i * block_size : (i + 1) * block_size] for i in range(num_blocks)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def combine_short_sequences(examples, max_length=256, absolute_max_length=4096):\n",
    "    combined = {k: [] for k in examples.keys()}\n",
    "    current_sequence = {k: [] for k in examples.keys()}\n",
    "    current_length = 0\n",
    "    newline_token_ids = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)\n",
    "    \n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        sequence_length = len(examples['input_ids'][i])\n",
    "        if current_length + sequence_length > max_length: # or current_length + sequence_length > absolute_max_length:\n",
    "            # Add the current combined sequence and start a new one\n",
    "            for k in combined.keys():\n",
    "                combined[k].append(list(chain(*current_sequence[k])))\n",
    "                current_sequence[k] = []\n",
    "            current_length = 0\n",
    "        \n",
    "        # Add the current sequence\n",
    "        for k in examples.keys():\n",
    "            if k == 'input_ids':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append(newline_token_ids)\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            elif k == 'attention_mask':\n",
    "                if current_sequence[k]:  # If it's not the first sequence in this combination\n",
    "                    current_sequence[k].append([1] * len(newline_token_ids))\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "            else:\n",
    "                current_sequence[k].append(examples[k][i])\n",
    "        \n",
    "        current_length += sequence_length\n",
    "        if current_sequence['input_ids']:\n",
    "            current_length += len(newline_token_ids)\n",
    "    \n",
    "    # Add the last combined sequence\n",
    "    for k in combined.keys():\n",
    "        if current_sequence[k]:\n",
    "            combined[k].append(list(chain(*current_sequence[k])))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Use this before group_texts\n",
    "    examples = combine_short_sequences(examples)\n",
    "\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    block_size = 512\n",
    "    # More granular block size selection\n",
    "    if total_length < 512:\n",
    "        block_size = 512\n",
    "    if total_length < 1024:\n",
    "        block_size = 1024\n",
    "    elif total_length < 2048:\n",
    "        block_size = 2048\n",
    "    elif total_length < 4096:\n",
    "        block_size = 4096\n",
    "    else:\n",
    "        block_size = 8192  # Increase maximum block size, ensure your model can handle this\n",
    "    \n",
    "    # Adaptive padding\n",
    "    padding_unit = 128  # or another suitable value\n",
    "    if total_length < block_size:\n",
    "        block_size = ((total_length + padding_unit - 1) // padding_unit) * padding_unit\n",
    "    \n",
    "    num_blocks = (total_length + block_size - 1) // block_size\n",
    "    result = {k: [] for k in concatenated_examples.keys()}\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        block_start = i * block_size\n",
    "        block_end = min((i + 1) * block_size, total_length)\n",
    "        for k, t in concatenated_examples.items():\n",
    "            block = t[block_start:block_end]\n",
    "            if len(block) < block_size:\n",
    "                padding_length = block_size - len(block)\n",
    "                if k == 'input_ids':\n",
    "                    block = block + [tokenizer.pad_token_id] * padding_length\n",
    "                elif k == 'attention_mask':\n",
    "                    block = block + [0] * padding_length\n",
    "            result[k].append(block)\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_dataset_part_16.parquet',\n",
       " 'output_dataset_part_15.parquet',\n",
       " 'output_dataset_part_14.parquet',\n",
       " 'output_dataset_part_13.parquet',\n",
       " 'output_dataset_part_12.parquet',\n",
       " 'output_dataset_part_11.parquet',\n",
       " 'output_dataset_part_10.parquet',\n",
       " 'output_dataset_part_9.parquet',\n",
       " 'output_dataset_part_8.parquet',\n",
       " 'output_dataset_part_7.parquet',\n",
       " 'output_dataset_part_6.parquet',\n",
       " 'output_dataset_part_5.parquet',\n",
       " 'output_dataset_part_4.parquet',\n",
       " 'output_dataset_part_3.parquet',\n",
       " 'output_dataset_part_2.parquet',\n",
       " 'output_dataset_part_1.parquet']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:04:32] Energy consumed for RAM : 23.971375 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:32] Energy consumed for all GPUs : 2.135362 kWh. Total GPU Power : 66.94436090580334 W\n",
      "[codecarbon INFO @ 00:04:32] Energy consumed for all CPUs : 4.469443 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:32] 30.576180 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:32] Energy consumed for RAM : 33.565685 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:32] Energy consumed for all GPUs : 2.990801 kWh. Total GPU Power : 66.88714493723276 W\n",
      "[codecarbon INFO @ 00:04:32] Energy consumed for all CPUs : 6.257552 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:32] 42.814038 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:04:33] Energy consumed for RAM : 23.961781 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 00:04:33] Energy consumed for all GPUs : 2.134443 kWh. Total GPU Power : 66.88251132678994 W\n",
      "[codecarbon INFO @ 00:04:33] Energy consumed for all CPUs : 4.467685 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 00:04:33] 30.563908 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p $tmp_data_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $tmp_data_cache_dir/*maxblocksize*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace99d18aa91435f8762e8f8127fff67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:20:08] Energy consumed for RAM : 0.034289 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:20:08] Energy consumed for all GPUs : 0.003130 kWh. Total GPU Power : 67.0725282206584 W\n",
      "[codecarbon INFO @ 16:20:08] Energy consumed for all CPUs : 0.006417 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:20:08] 0.043836 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc906e27e8e3450cad87de46f9e67ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=16):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:20:24] Energy consumed for RAM : 0.037499 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:20:24] Energy consumed for all GPUs : 0.003426 kWh. Total GPU Power : 67.15667321014074 W\n",
      "[codecarbon INFO @ 16:20:24] Energy consumed for all CPUs : 0.007034 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:20:24] 0.047959 kWh of electricity used since the beginning.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8218 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1371.464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10318 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1132.768\n",
      "Average tokens per text: 1472.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26897 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1203.758\n",
      "Average tokens per text: 1379.857\n",
      "Average tokens per text: 1319.489\n",
      "Average tokens per text: 1410.032\n",
      "Average tokens per text: 1240.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11578 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1229.115\n",
      "Average tokens per text: 1349.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12640 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1283.995\n",
      "Average tokens per text: 1336.694\n",
      "Average tokens per text: 1325.56\n",
      "Average tokens per text: 1284.224\n",
      "Average tokens per text: 1601.246\n",
      "Average tokens per text: 1397.8266666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11911 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1397.785\n",
      "Average tokens per text: 1517.976\n",
      "Average tokens per text: 1201.956\n",
      "Average tokens per text: 1267.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10405 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1257.456\n",
      "Average tokens per text: 1266.808\n",
      "Average tokens per text: 1326.7373333333333\n",
      "Average tokens per text: 1210.83\n",
      "Average tokens per text: 1391.31\n",
      "Average tokens per text: 1174.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9639 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1380.387\n",
      "Average tokens per text: 1375.6106666666667\n",
      "Average tokens per text: 1386.561\n",
      "Average tokens per text: 1822.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12069 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1365.71"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:20:39] Energy consumed for RAM : 0.040594 kWh. RAM Power : 751.453685760498 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average tokens per text: 1167.582\n",
      "Average tokens per text: 1344.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:20:39] Energy consumed for all GPUs : 0.003704 kWh. Total GPU Power : 67.29936961047042 W\n",
      "[codecarbon INFO @ 16:20:39] Energy consumed for all CPUs : 0.007611 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:20:39] 0.051908 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1243.9066666666668\n",
      "Average tokens per text: 1186.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13760 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1299.245\n",
      "Average tokens per text: 1212.307\n",
      "Average tokens per text: 1285.033\n",
      "Average tokens per text: 1320.661\n",
      "Average tokens per text: 1439.3653333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9597 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1227.558\n",
      "Average tokens per text: 1369.43\n",
      "Average tokens per text: 1313.2853333333333\n",
      "Average tokens per text: 1256.993\n",
      "Average tokens per text: 1429.294\n",
      "Average tokens per text: 1288.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22938 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1299.86\n",
      "Average tokens per text: 1258.385\n",
      "Average tokens per text: 1469.899\n",
      "Average tokens per text: 1322.808\n",
      "Average tokens per text: 1270.572\n",
      "Average tokens per text: 1424.35\n",
      "Average tokens per text: 1379.562\n",
      "Average tokens per text: 1441.046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12367 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average tokens per text: 1556.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22921 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1341.49\n",
      "Average tokens per text: 1313.424\n",
      "Average tokens per text: 1562.556\n",
      "Average tokens per text: 1254.254\n",
      "Average tokens per text: 1369.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8227 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1310.939\n",
      "Average tokens per text: 1223.2093333333332\n",
      "Average tokens per text: 1412.86\n",
      "Average tokens per text: 1171.4\n",
      "Average tokens per text: 1462.717\n",
      "Average tokens per text: 1246.211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11177 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1485.151\n",
      "Average tokens per text: 1252.068\n",
      "Average tokens per text: 1348.867\n",
      "Average tokens per text: 1156.744\n",
      "Average tokens per text: 1290.679\n",
      "Average tokens per text: 1372.831\n",
      "Average tokens per text: 1424.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:20:53] Energy consumed for RAM : 0.043665 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:20:53] Energy consumed for all GPUs : 0.003977 kWh. Total GPU Power : 66.8292553308757 W\n",
      "[codecarbon INFO @ 16:20:53] Energy consumed for all CPUs : 0.008183 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:20:53] 0.055825 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per text: 1547.448\n",
      "Average tokens per text: 1282.599\n",
      "Average tokens per text: 1433.8973333333333\n",
      "Average tokens per text: 1324.077\n",
      "Average tokens per text: 1132.7133333333334\n",
      "Average tokens per text: 1484.138\n",
      "Average tokens per text: 1422.0173333333332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f7776d0ad44abe860e17d4ec2f82c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of dynamic block_size (num_proc=16):   0%|          | 0/76000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:21:08] Energy consumed for RAM : 0.046794 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:21:09] Energy consumed for all GPUs : 0.004261 kWh. Total GPU Power : 68.15703057131094 W\n",
      "[codecarbon INFO @ 16:21:09] Energy consumed for all CPUs : 0.008774 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:21:09] 0.059829 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "lm_datasets = []\n",
    "block_size=4096\n",
    "\n",
    "\n",
    "path = Path(dataset_dir)\n",
    "files = [file.name for file in path.glob(\"*.parquet\")]\n",
    "\n",
    "file = files[0]\n",
    "\n",
    "data_file = os.path.join(path, file)\n",
    "filename = \"\".join(file.split(\".\")[:-1])\n",
    "\n",
    "cache_dir = os.path.join(\n",
    "    tmp_data_cache_dir, filename + f\"_parquet_maxblocksize_{block_size}\"\n",
    ")\n",
    "\n",
    "raw_dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=data_file,\n",
    "    cache_dir=cache_dir,\n",
    "    keep_in_memory=False,\n",
    ")\n",
    "\n",
    "# Process in chunks similar to Dataset1 file size\n",
    "chunk_size = 76000\n",
    "for i in range(0, len(raw_dataset['train']), chunk_size):\n",
    "    cur_i = 0\n",
    "    i = cur_i\n",
    "    logger.info(f\"{file} has been loaded\")\n",
    "    \n",
    "    # Create a chunk that's a DatasetDict, just like raw_dataset\n",
    "    chunk = DatasetDict({\n",
    "        'train': Dataset.from_dict(\n",
    "            raw_dataset['train'].select(range(i, min(i+chunk_size, len(raw_dataset['train'])))).to_dict()\n",
    "        )\n",
    "    })\n",
    "    tokenized_chunk = chunk.map(\n",
    "        tokenize_and_check,\n",
    "        batched=True,\n",
    "        num_proc=16, #data_args.preprocessing_num_workers,\n",
    "        remove_columns=[\"id\", \"text\"],\n",
    "        load_from_cache_file=True,\n",
    "        keep_in_memory=False,\n",
    "        cache_file_names={\n",
    "            k: os.path.join(cache_dir, \"tokenized.arrow\")\n",
    "            for k in chunk\n",
    "        },\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    grouped_chunk = tokenized_chunk.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        #batch_size=1000,\n",
    "        num_proc=16, #data_args.preprocessing_num_workers,\n",
    "        load_from_cache_file=True,\n",
    "        keep_in_memory=False,\n",
    "        cache_file_names={\n",
    "            k: os.path.join(cache_dir, \"grouped.arrow\")\n",
    "            for k in tokenized_chunk\n",
    "        },\n",
    "        desc=f\"Grouping texts in chunks of dynamic block_size\",\n",
    "    )\n",
    "    \n",
    "    if i == cur_i:\n",
    "        processed_dataset = grouped_chunk\n",
    "    else:\n",
    "        processed_dataset = concatenate_datasets([processed_dataset, grouped_chunk])\n",
    "    \n",
    "    break\n",
    "\n",
    "idx = 0\n",
    "if idx == 0:\n",
    "    lm_datasets = processed_dataset\n",
    "else:\n",
    "    lm_datasets = concatenate_datasets([lm_datasets, processed_dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text'],\n",
       "        num_rows: 76000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12464\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12466\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:21:23] Energy consumed for RAM : 0.049881 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:21:23] Energy consumed for all GPUs : 0.004535 kWh. Total GPU Power : 66.68287320723222 W\n",
      "[codecarbon INFO @ 16:21:23] Energy consumed for all CPUs : 0.009350 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:21:23] 0.063765 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: /workspace/data/Bangla2B+/shards/parquet_files/cache/*_2048*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 19:17:16] Energy consumed for RAM : 29.959513 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:16] Energy consumed for all GPUs : 2.670150 kWh. Total GPU Power : 66.60798925581423 W\n",
      "[codecarbon INFO @ 19:17:16] Energy consumed for all CPUs : 5.584504 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:16] 38.214168 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:18] Energy consumed for RAM : 20.380998 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:18] Energy consumed for all GPUs : 1.816228 kWh. Total GPU Power : 66.56412289409282 W\n",
      "[codecarbon INFO @ 19:17:18] Energy consumed for all CPUs : 3.799396 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:18] 25.996622 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:19] Energy consumed for RAM : 29.975244 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:19] Energy consumed for all GPUs : 2.671681 kWh. Total GPU Power : 66.53960412594786 W\n",
      "[codecarbon INFO @ 19:17:19] Energy consumed for all CPUs : 5.587520 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:19] 38.234445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:19] Energy consumed for RAM : 20.371816 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:19] Energy consumed for all GPUs : 1.815300 kWh. Total GPU Power : 66.54110528486237 W\n",
      "[codecarbon INFO @ 19:17:19] Energy consumed for all CPUs : 3.797583 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:19] 25.984699 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:31] Energy consumed for RAM : 29.962643 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:31] Energy consumed for all GPUs : 2.670427 kWh. Total GPU Power : 66.60834912635606 W\n",
      "[codecarbon INFO @ 19:17:31] Energy consumed for all CPUs : 5.585087 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:31] 38.218157 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:33] Energy consumed for RAM : 20.384127 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:33] Energy consumed for all GPUs : 1.816508 kWh. Total GPU Power : 67.05003019956938 W\n",
      "[codecarbon INFO @ 19:17:33] Energy consumed for all CPUs : 3.799979 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:33] 26.000614 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:34] Energy consumed for RAM : 29.978374 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:34] Energy consumed for all GPUs : 2.671958 kWh. Total GPU Power : 66.59948419452284 W\n",
      "[codecarbon INFO @ 19:17:34] Energy consumed for all CPUs : 5.588103 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:34] 38.238435 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:34] Energy consumed for RAM : 20.374946 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:34] Energy consumed for all GPUs : 1.815577 kWh. Total GPU Power : 66.60179589607391 W\n",
      "[codecarbon INFO @ 19:17:34] Energy consumed for all CPUs : 3.798166 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:34] 25.988690 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:46] Energy consumed for RAM : 29.965772 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:46] Energy consumed for all GPUs : 2.670707 kWh. Total GPU Power : 67.095026400144 W\n",
      "[codecarbon INFO @ 19:17:46] Energy consumed for all CPUs : 5.585671 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:46] 38.222150 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:48] Energy consumed for RAM : 20.387257 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:48] Energy consumed for all GPUs : 1.816785 kWh. Total GPU Power : 66.66114497672719 W\n",
      "[codecarbon INFO @ 19:17:48] Energy consumed for all CPUs : 3.800562 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:48] 26.004604 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:49] Energy consumed for RAM : 29.981504 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:49] Energy consumed for all GPUs : 2.672236 kWh. Total GPU Power : 66.66560146836655 W\n",
      "[codecarbon INFO @ 19:17:49] Energy consumed for all CPUs : 5.588687 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:49] 38.242426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:17:49] Energy consumed for RAM : 20.378076 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:17:49] Energy consumed for all GPUs : 1.815857 kWh. Total GPU Power : 67.11191424273039 W\n",
      "[codecarbon INFO @ 19:17:49] Energy consumed for all CPUs : 3.798750 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:17:49] 25.992682 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:01] Energy consumed for RAM : 29.968902 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:01] Energy consumed for all GPUs : 2.670984 kWh. Total GPU Power : 66.50239894975 W\n",
      "[codecarbon INFO @ 19:18:01] Energy consumed for all CPUs : 5.586254 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:01] 38.226140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:03] Energy consumed for RAM : 20.390386 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:03] Energy consumed for all GPUs : 1.817062 kWh. Total GPU Power : 66.48318885329668 W\n",
      "[codecarbon INFO @ 19:18:03] Energy consumed for all CPUs : 3.801146 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:03] 26.008594 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:04] Energy consumed for RAM : 29.984633 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:04] Energy consumed for all GPUs : 2.672514 kWh. Total GPU Power : 66.93417760599343 W\n",
      "[codecarbon INFO @ 19:18:04] Energy consumed for all CPUs : 5.589270 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:04] 38.246418 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:04] Energy consumed for RAM : 20.381205 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:04] Energy consumed for all GPUs : 1.816134 kWh. Total GPU Power : 66.48968794977596 W\n",
      "[codecarbon INFO @ 19:18:04] Energy consumed for all CPUs : 3.799333 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:04] 25.996672 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:16] Energy consumed for RAM : 29.972032 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:16] Energy consumed for all GPUs : 2.671261 kWh. Total GPU Power : 66.48037221170718 W\n",
      "[codecarbon INFO @ 19:18:16] Energy consumed for all CPUs : 5.586837 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:16] 38.230130 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:18] Energy consumed for RAM : 20.393516 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:18] Energy consumed for all GPUs : 1.817339 kWh. Total GPU Power : 66.48781833032625 W\n",
      "[codecarbon INFO @ 19:18:18] Energy consumed for all CPUs : 3.801729 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:18] 26.012584 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:19] Energy consumed for RAM : 29.987763 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:19] Energy consumed for all GPUs : 2.672791 kWh. Total GPU Power : 66.48146464359765 W\n",
      "[codecarbon INFO @ 19:18:19] Energy consumed for all CPUs : 5.589853 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:19] 38.250407 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:19] Energy consumed for RAM : 20.384335 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:19] Energy consumed for all GPUs : 1.816411 kWh. Total GPU Power : 66.47683387347917 W\n",
      "[codecarbon INFO @ 19:18:19] Energy consumed for all CPUs : 3.799916 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:19] 26.000662 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:31] Energy consumed for RAM : 29.975162 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:31] Energy consumed for all GPUs : 2.671540 kWh. Total GPU Power : 66.92898224469899 W\n",
      "[codecarbon INFO @ 19:18:31] Energy consumed for all CPUs : 5.587421 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:31] 38.234122 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:33] Energy consumed for RAM : 20.396645 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:33] Energy consumed for all GPUs : 1.817618 kWh. Total GPU Power : 66.93314310703667 W\n",
      "[codecarbon INFO @ 19:18:33] Energy consumed for all CPUs : 3.802312 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:33] 26.016575 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:34] Energy consumed for RAM : 29.990893 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:34] Energy consumed for all GPUs : 2.673068 kWh. Total GPU Power : 66.47908479895885 W\n",
      "[codecarbon INFO @ 19:18:34] Energy consumed for all CPUs : 5.590437 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:34] 38.254397 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:34] Energy consumed for RAM : 20.387465 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:34] Energy consumed for all GPUs : 1.816689 kWh. Total GPU Power : 66.93107325418967 W\n",
      "[codecarbon INFO @ 19:18:34] Energy consumed for all CPUs : 3.800500 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:34] 26.004654 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:46] Energy consumed for RAM : 29.978291 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:46] Energy consumed for all GPUs : 2.671817 kWh. Total GPU Power : 66.5096603343115 W\n",
      "[codecarbon INFO @ 19:18:46] Energy consumed for all CPUs : 5.588004 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:46] 38.238111 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:48] Energy consumed for RAM : 20.399775 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:48] Energy consumed for all GPUs : 1.817895 kWh. Total GPU Power : 66.50555186146636 W\n",
      "[codecarbon INFO @ 19:18:48] Energy consumed for all CPUs : 3.802895 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:48] 26.020565 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:49] Energy consumed for RAM : 29.994022 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:49] Energy consumed for all GPUs : 2.673347 kWh. Total GPU Power : 66.95995899418158 W\n",
      "[codecarbon INFO @ 19:18:49] Energy consumed for all CPUs : 5.591020 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:49] 38.258389 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:18:49] Energy consumed for RAM : 20.390595 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:18:49] Energy consumed for all GPUs : 1.816967 kWh. Total GPU Power : 66.50851534370646 W\n",
      "[codecarbon INFO @ 19:18:49] Energy consumed for all CPUs : 3.801083 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:18:49] 26.008644 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:19:01] Energy consumed for RAM : 29.981420 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:19:01] Energy consumed for all GPUs : 2.672094 kWh. Total GPU Power : 66.51511312560757 W\n",
      "[codecarbon INFO @ 19:19:01] Energy consumed for all CPUs : 5.588587 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:19:01] 38.242101 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:19:03] Energy consumed for RAM : 20.402904 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:19:03] Energy consumed for all GPUs : 1.818172 kWh. Total GPU Power : 66.51180145960119 W\n",
      "[codecarbon INFO @ 19:19:03] Energy consumed for all CPUs : 3.803479 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:19:03] 26.024555 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:19:04] Energy consumed for RAM : 29.997151 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:19:04] Energy consumed for all GPUs : 2.673624 kWh. Total GPU Power : 66.51970413754324 W\n",
      "[codecarbon INFO @ 19:19:04] Energy consumed for all CPUs : 5.591603 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:19:04] 38.262379 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:19:04] Energy consumed for RAM : 20.393724 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 19:19:04] Energy consumed for all GPUs : 1.817244 kWh. Total GPU Power : 66.50966507340307 W\n",
      "[codecarbon INFO @ 19:19:04] Energy consumed for all CPUs : 3.801666 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 19:19:04] 26.012634 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /workspace/data/Bangla2B+/shards/parquet_files/cache/*_2048*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12957285\n",
       "})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:56:16] Energy consumed for RAM : 29.696693 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 18:56:16] Energy consumed for all GPUs : 2.646775 kWh. Total GPU Power : 66.46680862266813 W\n",
      "[codecarbon INFO @ 18:56:16] Energy consumed for all CPUs : 5.535509 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 18:56:16] 37.878976 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:56:17] Energy consumed for RAM : 20.118143 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 18:56:17] Energy consumed for all GPUs : 1.792853 kWh. Total GPU Power : 66.45684881849293 W\n",
      "[codecarbon INFO @ 18:56:17] Energy consumed for all CPUs : 3.750396 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 18:56:17] 25.661392 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:56:18] Energy consumed for RAM : 20.108937 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 18:56:18] Energy consumed for RAM : 29.712382 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 18:56:18] Energy consumed for all GPUs : 1.791926 kWh. Total GPU Power : 66.89637662758402 W\n",
      "[codecarbon INFO @ 18:56:18] Energy consumed for all CPUs : 3.748584 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 18:56:18] 25.649447 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:56:18] Energy consumed for all GPUs : 2.648307 kWh. Total GPU Power : 66.90597227934752 W\n",
      "[codecarbon INFO @ 18:56:18] Energy consumed for all CPUs : 5.538523 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 18:56:18] 37.899211 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 9951012\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:37:23] Energy consumed for RAM : 0.358403 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all GPUs : 0.032155 kWh. Total GPU Power : 67.06957136951658 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all CPUs : 0.066870 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:23] 0.457428 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for RAM : 9.952299 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for RAM : 0.348776 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for RAM : 9.936665 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all GPUs : 0.887587 kWh. Total GPU Power : 67.08748801699643 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all CPUs : 1.855008 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all GPUs : 0.031206 kWh. Total GPU Power : 67.07833542091556 W\n",
      "[codecarbon INFO @ 16:37:23] 12.694894 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all CPUs : 0.065054 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all GPUs : 0.886104 kWh. Total GPU Power : 67.07366673461813 W\n",
      "[codecarbon INFO @ 16:37:23] 0.445037 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all CPUs : 1.852047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:23] 12.674816 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for RAM : 0.361532 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for all GPUs : 0.032434 kWh. Total GPU Power : 67.06312044861888 W\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for all CPUs : 0.067453 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:38] 0.461420 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for RAM : 9.955427 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for RAM : 0.351904 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for all GPUs : 0.887868 kWh. Total GPU Power : 67.53701147464933 W\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for all CPUs : 1.855591 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:38] 12.698886 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for all GPUs : 0.031487 kWh. Total GPU Power : 67.53975229428268 W\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for all CPUs : 0.065637 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for RAM : 9.939793 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:38] 0.449029 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for all GPUs : 0.886386 kWh. Total GPU Power : 67.51011683389247 W\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for all CPUs : 1.852630 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:38] 12.678809 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for RAM : 0.364662 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all GPUs : 0.032715 kWh. Total GPU Power : 67.49317251314427 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all CPUs : 0.068036 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:53] 0.465414 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for RAM : 9.958557 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for RAM : 0.355034 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all GPUs : 0.888147 kWh. Total GPU Power : 67.04063031997846 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all CPUs : 1.856175 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:53] 12.702879 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all GPUs : 0.031766 kWh. Total GPU Power : 67.05496273456964 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for RAM : 9.942923 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all CPUs : 0.066221 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:53] 0.453021 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all GPUs : 0.886665 kWh. Total GPU Power : 67.04023708038572 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all CPUs : 1.853214 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:53] 12.682802 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for RAM : 0.367791 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all GPUs : 0.032995 kWh. Total GPU Power : 67.05171253791671 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all CPUs : 0.068620 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:08] 0.469406 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for RAM : 9.961686 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for RAM : 0.358162 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all GPUs : 0.888426 kWh. Total GPU Power : 67.05229575507781 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all CPUs : 1.856758 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:08] 12.706870 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all GPUs : 0.032046 kWh. Total GPU Power : 67.06794511607039 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for RAM : 9.946052 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all CPUs : 0.066804 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:08] 0.457012 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all GPUs : 0.886944 kWh. Total GPU Power : 67.03784093128327 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all CPUs : 1.853797 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:08] 12.686793 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for RAM : 0.370920 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all GPUs : 0.033273 kWh. Total GPU Power : 66.8966488284396 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all CPUs : 0.069203 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:23] 0.473396 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for RAM : 9.964815 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for RAM : 0.361291 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all GPUs : 0.888707 kWh. Total GPU Power : 67.35344129510516 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all CPUs : 1.857341 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:23] 12.710863 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all GPUs : 0.032326 kWh. Total GPU Power : 67.35145329784908 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all CPUs : 0.067387 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for RAM : 9.949181 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:23] 0.461004 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all GPUs : 0.887225 kWh. Total GPU Power : 67.34142726848393 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all CPUs : 1.854380 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:23] 12.690786 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for RAM : 0.374050 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all GPUs : 0.033554 kWh. Total GPU Power : 67.31823120391884 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all CPUs : 0.069786 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:38] 0.477389 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for RAM : 9.967944 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for RAM : 0.364420 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all GPUs : 0.888985 kWh. Total GPU Power : 66.86954908365333 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all CPUs : 1.857924 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:38] 12.714854 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all GPUs : 0.032605 kWh. Total GPU Power : 66.88498429010423 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all CPUs : 0.067970 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:38] 0.464995 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for RAM : 9.952311 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all GPUs : 0.887503 kWh. Total GPU Power : 66.86528582411339 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all CPUs : 1.854964 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:38] 12.694777 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for RAM : 0.377179 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all GPUs : 0.033832 kWh. Total GPU Power : 66.92679006205671 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all CPUs : 0.070369 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:53] 0.481381 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for RAM : 9.971074 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for RAM : 0.367549 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all GPUs : 0.889264 kWh. Total GPU Power : 66.92505016120126 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all CPUs : 1.858508 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:53] 12.718845 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all GPUs : 0.032883 kWh. Total GPU Power : 66.92488734941978 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all CPUs : 0.068554 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:53] 0.468986 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for RAM : 9.955441 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all GPUs : 0.887782 kWh. Total GPU Power : 66.91451904126264 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all CPUs : 1.855547 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:53] 12.698769 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for RAM : 0.380308 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for all GPUs : 0.034111 kWh. Total GPU Power : 66.93288391049411 W\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for all CPUs : 0.070953 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:08] 0.485371 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for RAM : 9.974203 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for RAM : 0.370678 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for all GPUs : 0.889543 kWh. Total GPU Power : 66.92597972164388 W\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for all CPUs : 1.859091 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:08] 12.722837 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for all GPUs : 0.033164 kWh. Total GPU Power : 67.37840407682081 W\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for all CPUs : 0.069137 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:08] 0.472979 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for RAM : 9.958570 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for all GPUs : 0.888062 kWh. Total GPU Power : 67.37072061673986 W\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for all CPUs : 1.856130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:08] 12.702763 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for RAM : 0.383437 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all GPUs : 0.034392 kWh. Total GPU Power : 67.47821223919848 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all CPUs : 0.071536 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:23] 0.489365 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for RAM : 9.977332 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for RAM : 0.373807 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all GPUs : 0.889824 kWh. Total GPU Power : 67.47840752072649 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all CPUs : 1.859674 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:23] 12.726830 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all GPUs : 0.033443 kWh. Total GPU Power : 67.02581396583112 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all CPUs : 0.069720 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:23] 0.476970 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for RAM : 9.961700 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all GPUs : 0.888342 kWh. Total GPU Power : 67.0210792207795 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all CPUs : 1.856713 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:23] 12.706755 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for RAM : 0.386566 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all GPUs : 0.034671 kWh. Total GPU Power : 67.03181906537311 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all CPUs : 0.072119 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:38] 0.493357 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for RAM : 9.980462 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for RAM : 0.376936 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all GPUs : 0.890103 kWh. Total GPU Power : 67.03688296702654 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all CPUs : 1.860257 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:38] 12.730822 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all GPUs : 0.033722 kWh. Total GPU Power : 67.0409657685013 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all CPUs : 0.070304 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:38] 0.480962 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for RAM : 9.964830 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all GPUs : 0.888621 kWh. Total GPU Power : 67.03213240605581 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all CPUs : 1.857297 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:38] 12.710747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for RAM : 0.389695 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for all GPUs : 0.034950 kWh. Total GPU Power : 66.96103431120527 W\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for all CPUs : 0.072702 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:53] 0.497348 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for RAM : 9.983591 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for RAM : 0.380065 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for all GPUs : 0.890382 kWh. Total GPU Power : 66.94966944343325 W\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for all CPUs : 1.860841 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:53] 12.734814 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for all GPUs : 0.034001 kWh. Total GPU Power : 66.94753403699026 W\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for all CPUs : 0.070887 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:53] 0.484954 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for RAM : 9.967960 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for all GPUs : 0.888902 kWh. Total GPU Power : 67.38991708554344 W\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for all CPUs : 1.857880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:53] 12.714741 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for RAM : 0.392824 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all GPUs : 0.035230 kWh. Total GPU Power : 67.25787921044625 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all CPUs : 0.073286 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:08] 0.501340 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for RAM : 9.986720 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for RAM : 0.383195 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all GPUs : 0.890662 kWh. Total GPU Power : 67.26663623503175 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all CPUs : 1.861424 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:08] 12.738806 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all GPUs : 0.034281 kWh. Total GPU Power : 67.2659527457286 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all CPUs : 0.071470 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:08] 0.488946 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for RAM : 9.971089 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all GPUs : 0.889180 kWh. Total GPU Power : 66.80783706024292 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all CPUs : 1.858463 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:08] 12.718733 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for RAM : 0.395954 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for all GPUs : 0.035509 kWh. Total GPU Power : 66.95284710083696 W\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for all CPUs : 0.073869 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:23] 0.505332 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for RAM : 9.989850 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for RAM : 0.386324 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for all GPUs : 0.890941 kWh. Total GPU Power : 66.9535763388247 W\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for all CPUs : 1.862007 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:23] 12.742798 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for all GPUs : 0.034560 kWh. Total GPU Power : 66.95582270457454 W\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for all CPUs : 0.072053 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:23] 0.492938 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for RAM : 9.974219 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for all GPUs : 0.889459 kWh. Total GPU Power : 66.94896058354495 W\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for all CPUs : 1.859047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:23] 12.722724 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for RAM : 0.399083 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all GPUs : 0.035788 kWh. Total GPU Power : 67.04198929188433 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all CPUs : 0.074452 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:38] 0.509324 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for RAM : 9.992979 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for RAM : 0.389453 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all GPUs : 0.891220 kWh. Total GPU Power : 67.04213959944722 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all CPUs : 1.862590 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:38] 12.746790 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all GPUs : 0.034840 kWh. Total GPU Power : 67.0425819979584 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all CPUs : 0.072637 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:38] 0.496930 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for RAM : 9.977349 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all GPUs : 0.889738 kWh. Total GPU Power : 67.03608092643731 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all CPUs : 1.859630 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:38] 12.726717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for RAM : 0.402212 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for all GPUs : 0.036067 kWh. Total GPU Power : 66.99271242220293 W\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for all CPUs : 0.075035 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:53] 0.513315 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for RAM : 9.996108 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for RAM : 0.392583 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for all GPUs : 0.891501 kWh. Total GPU Power : 67.43631212031102 W\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for all CPUs : 1.863174 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:53] 12.750783 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for all GPUs : 0.035120 kWh. Total GPU Power : 67.42896036818073 W\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for all CPUs : 0.073220 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:53] 0.500923 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for RAM : 9.980478 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for all GPUs : 0.890019 kWh. Total GPU Power : 67.43231815204504 W\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for all CPUs : 1.860213 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:53] 12.730711 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for RAM : 0.405342 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for all GPUs : 0.036348 kWh. Total GPU Power : 67.36673033454012 W\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for all CPUs : 0.075619 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:08] 0.517308 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for RAM : 9.999238 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for RAM : 0.395712 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for all GPUs : 0.891780 kWh. Total GPU Power : 66.92613974871375 W\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for all CPUs : 1.863757 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:08] 12.754774 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for all GPUs : 0.035399 kWh. Total GPU Power : 66.9251011165643 W\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for all CPUs : 0.073803 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:08] 0.504915 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for RAM : 9.983608 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for all GPUs : 0.890298 kWh. Total GPU Power : 66.92602055978047 W\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for all CPUs : 1.860797 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:08] 12.734702 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for RAM : 0.408471 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all GPUs : 0.036627 kWh. Total GPU Power : 66.86292450950346 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all CPUs : 0.076202 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:23] 0.521299 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for RAM : 10.002367 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for RAM : 0.398842 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all GPUs : 0.892058 kWh. Total GPU Power : 66.85555796229237 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all CPUs : 1.864340 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:23] 12.758766 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all GPUs : 0.035678 kWh. Total GPU Power : 66.8518014249823 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all CPUs : 0.074387 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:23] 0.508906 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for RAM : 9.986738 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all GPUs : 0.890576 kWh. Total GPU Power : 66.84913893984803 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all CPUs : 1.861380 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:23] 12.738694 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for RAM : 0.411600 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for all GPUs : 0.036905 kWh. Total GPU Power : 66.76985657218904 W\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for all CPUs : 0.076785 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:38] 0.525290 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for RAM : 10.005496 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for RAM : 0.401971 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for all GPUs : 0.892336 kWh. Total GPU Power : 66.78090207538193 W\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for all CPUs : 1.864923 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:38] 12.762756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for all GPUs : 0.035956 kWh. Total GPU Power : 66.78838425962782 W\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for all CPUs : 0.074970 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:38] 0.512896 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for RAM : 9.989867 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for all GPUs : 0.890856 kWh. Total GPU Power : 67.22096671280433 W\n",
      "[codecarbon INFO @ 16:41:38] Energy consumed for all CPUs : 1.861963 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:38] 12.742687 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for RAM : 0.414730 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for all GPUs : 0.037185 kWh. Total GPU Power : 67.28160732696985 W\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for all CPUs : 0.077369 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:53] 0.529283 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for RAM : 10.008626 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for RAM : 0.405101 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for all GPUs : 0.892617 kWh. Total GPU Power : 67.27567454762669 W\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for all CPUs : 1.865507 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:53] 12.766750 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for all GPUs : 0.036236 kWh. Total GPU Power : 67.27025615789425 W\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for all CPUs : 0.075553 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:53] 0.516890 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for RAM : 9.992997 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for all GPUs : 0.891134 kWh. Total GPU Power : 66.82540453591545 W\n",
      "[codecarbon INFO @ 16:41:53] Energy consumed for all CPUs : 1.862546 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:53] 12.746678 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for RAM : 0.417858 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for all GPUs : 0.037463 kWh. Total GPU Power : 66.81344945976362 W\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for all CPUs : 0.077952 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:08] 0.533273 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for RAM : 10.011756 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for RAM : 0.408230 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for all GPUs : 0.892895 kWh. Total GPU Power : 66.79425324408246 W\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for all CPUs : 1.866090 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:08] 12.770741 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for all GPUs : 0.036514 kWh. Total GPU Power : 66.80038523150391 W\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for all CPUs : 0.076136 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:08] 0.520881 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for RAM : 9.996127 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for all GPUs : 0.891413 kWh. Total GPU Power : 66.79728936108033 W\n",
      "[codecarbon INFO @ 16:42:08] Energy consumed for all CPUs : 1.863130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:08] 12.750670 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for RAM : 0.420988 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for all GPUs : 0.037742 kWh. Total GPU Power : 66.90715635544132 W\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for all CPUs : 0.078535 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:23] 0.537265 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for RAM : 10.014886 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for RAM : 0.411360 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for all GPUs : 0.893174 kWh. Total GPU Power : 66.90224727201841 W\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for all CPUs : 1.866673 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:23] 12.774733 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for all GPUs : 0.036793 kWh. Total GPU Power : 66.90448738410461 W\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for all CPUs : 0.076720 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:23] 0.524873 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for RAM : 9.999257 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for all GPUs : 0.891691 kWh. Total GPU Power : 66.90533304946531 W\n",
      "[codecarbon INFO @ 16:42:23] Energy consumed for all CPUs : 1.863713 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:23] 12.754661 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for RAM : 0.424117 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for all GPUs : 0.038020 kWh. Total GPU Power : 66.81288177770915 W\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for all CPUs : 0.079118 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:38] 0.541256 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for RAM : 10.018015 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for RAM : 0.414490 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for all GPUs : 0.893454 kWh. Total GPU Power : 67.26095264949016 W\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for all CPUs : 1.867257 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:38] 12.778726 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for all GPUs : 0.037073 kWh. Total GPU Power : 67.25793588954053 W\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for all CPUs : 0.077303 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:38] 0.528866 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for RAM : 10.002387 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for all GPUs : 0.891972 kWh. Total GPU Power : 67.26010887051076 W\n",
      "[codecarbon INFO @ 16:42:38] Energy consumed for all CPUs : 1.864296 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:38] 12.758655 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for RAM : 0.427246 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for all GPUs : 0.038301 kWh. Total GPU Power : 67.35763789688579 W\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for all CPUs : 0.079702 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:53] 0.545249 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for RAM : 10.021145 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for RAM : 0.417619 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for all GPUs : 0.893732 kWh. Total GPU Power : 66.90945661000634 W\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for all CPUs : 1.867840 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:53] 12.782717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for all GPUs : 0.037352 kWh. Total GPU Power : 66.90263983993964 W\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for all CPUs : 0.077886 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:53] 0.532857 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for RAM : 10.005516 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for all GPUs : 0.892250 kWh. Total GPU Power : 66.90337378871281 W\n",
      "[codecarbon INFO @ 16:42:53] Energy consumed for all CPUs : 1.864880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:42:53] 12.762646 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for RAM : 0.430376 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for all GPUs : 0.038580 kWh. Total GPU Power : 66.96727749534162 W\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for all CPUs : 0.080285 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:08] 0.549240 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for RAM : 10.024274 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for RAM : 0.420748 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for all GPUs : 0.894011 kWh. Total GPU Power : 66.96553545967762 W\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for all CPUs : 1.868423 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:08] 12.786709 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for all GPUs : 0.037631 kWh. Total GPU Power : 66.96692396977164 W\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for all CPUs : 0.078470 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:08] 0.536849 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for RAM : 10.008646 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for all GPUs : 0.892529 kWh. Total GPU Power : 66.96386591642145 W\n",
      "[codecarbon INFO @ 16:43:08] Energy consumed for all CPUs : 1.865463 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:08] 12.766638 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for RAM : 0.433505 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for all GPUs : 0.038858 kWh. Total GPU Power : 66.87507024095522 W\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for all CPUs : 0.080868 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:23] 0.553232 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for RAM : 10.027405 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for RAM : 0.423878 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for all GPUs : 0.894290 kWh. Total GPU Power : 66.87043367948283 W\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for all CPUs : 1.869007 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:23] 12.790701 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for all GPUs : 0.037909 kWh. Total GPU Power : 66.88513747028028 W\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for all CPUs : 0.079053 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for RAM : 10.011776 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:23] 0.540841 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for all GPUs : 0.892810 kWh. Total GPU Power : 67.33753274975903 W\n",
      "[codecarbon INFO @ 16:43:23] Energy consumed for all CPUs : 1.866046 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:23] 12.770632 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for RAM : 0.436634 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for all GPUs : 0.039139 kWh. Total GPU Power : 67.51531790451014 W\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for all CPUs : 0.081452 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:38] 0.557225 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for RAM : 10.030534 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for RAM : 0.427007 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for all GPUs : 0.894571 kWh. Total GPU Power : 67.48974691505069 W\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for all CPUs : 1.869590 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:38] 12.794695 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for all GPUs : 0.038190 kWh. Total GPU Power : 67.50762911374197 W\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for RAM : 10.014906 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for all CPUs : 0.079636 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:38] 0.544834 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for all GPUs : 0.893089 kWh. Total GPU Power : 67.02793296711201 W\n",
      "[codecarbon INFO @ 16:43:38] Energy consumed for all CPUs : 1.866630 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:38] 12.774624 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for RAM : 0.439763 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for all GPUs : 0.039419 kWh. Total GPU Power : 66.99137295437046 W\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for all CPUs : 0.082035 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:53] 0.561217 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for RAM : 10.033663 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for RAM : 0.430135 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for all GPUs : 0.894850 kWh. Total GPU Power : 67.02281327752074 W\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for all CPUs : 1.870173 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:53] 12.798687 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for all GPUs : 0.038470 kWh. Total GPU Power : 67.04383894420405 W\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for all CPUs : 0.080220 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:53] 0.548824 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for RAM : 10.018035 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for all GPUs : 0.893368 kWh. Total GPU Power : 67.02193838229094 W\n",
      "[codecarbon INFO @ 16:43:53] Energy consumed for all CPUs : 1.867213 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:43:53] 12.778616 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for RAM : 0.442892 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for all GPUs : 0.039698 kWh. Total GPU Power : 67.0407764484001 W\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for all CPUs : 0.082618 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:08] 0.565208 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for RAM : 10.036793 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for RAM : 0.433264 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for all GPUs : 0.895129 kWh. Total GPU Power : 67.00438875143065 W\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for all GPUs : 0.038749 kWh. Total GPU Power : 67.02208637108939 W\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for all CPUs : 1.870757 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for all CPUs : 0.080803 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for RAM : 10.021166 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:08] 12.802680 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:08] 0.552816 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for all GPUs : 0.893649 kWh. Total GPU Power : 67.43409551667814 W\n",
      "[codecarbon INFO @ 16:44:08] Energy consumed for all CPUs : 1.867797 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:08] 12.782612 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for RAM : 0.446021 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for all GPUs : 0.039977 kWh. Total GPU Power : 67.06487662164042 W\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for all CPUs : 0.083201 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:23] 0.569199 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for RAM : 10.039920 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for RAM : 0.436391 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for all GPUs : 0.895411 kWh. Total GPU Power : 67.55088175444327 W\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for all CPUs : 1.871340 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:23] 12.806671 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for all GPUs : 0.039030 kWh. Total GPU Power : 67.55107842013669 W\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for all CPUs : 0.081386 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:23] 0.556808 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for RAM : 10.024295 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for all GPUs : 0.893928 kWh. Total GPU Power : 67.06912451949542 W\n",
      "[codecarbon INFO @ 16:44:23] Energy consumed for all CPUs : 1.868380 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:23] 12.786603 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for RAM : 0.449150 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for all GPUs : 0.040258 kWh. Total GPU Power : 67.52488205656459 W\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for all CPUs : 0.083785 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:38] 0.573193 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for RAM : 10.043049 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for RAM : 0.439520 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for all GPUs : 0.895690 kWh. Total GPU Power : 67.08458275448096 W\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for all CPUs : 1.871923 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:38] 12.810662 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for all GPUs : 0.039309 kWh. Total GPU Power : 67.0852463661518 W\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for all CPUs : 0.081969 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:38] 0.560799 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for RAM : 10.027425 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for all GPUs : 0.894208 kWh. Total GPU Power : 67.06462186347596 W\n",
      "[codecarbon INFO @ 16:44:38] Energy consumed for all CPUs : 1.868963 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:38] 12.790596 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for RAM : 0.452280 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for all GPUs : 0.040537 kWh. Total GPU Power : 66.96410327335214 W\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for all CPUs : 0.084368 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:53] 0.577185 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for RAM : 10.046178 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for RAM : 0.442649 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for all GPUs : 0.895969 kWh. Total GPU Power : 66.97787519027239 W\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for all CPUs : 1.872506 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:53] 12.814654 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for all GPUs : 0.039588 kWh. Total GPU Power : 66.98175445590896 W\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for all CPUs : 0.082553 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:53] 0.564790 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for RAM : 10.030555 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for all GPUs : 0.894487 kWh. Total GPU Power : 66.97223923270752 W\n",
      "[codecarbon INFO @ 16:44:53] Energy consumed for all CPUs : 1.869546 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:44:53] 12.794588 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for RAM : 0.455407 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for all GPUs : 0.040816 kWh. Total GPU Power : 66.8855999179037 W\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for all CPUs : 0.084951 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:08] 0.581174 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for RAM : 10.049308 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for RAM : 0.445779 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for all GPUs : 0.896249 kWh. Total GPU Power : 67.28377354288574 W\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for all CPUs : 1.873090 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:08] 12.818647 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for all GPUs : 0.039869 kWh. Total GPU Power : 67.28668113872632 W\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for all CPUs : 0.083136 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:08] 0.568784 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for RAM : 10.033685 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for all GPUs : 0.894767 kWh. Total GPU Power : 67.28323441365916 W\n",
      "[codecarbon INFO @ 16:45:08] Energy consumed for all CPUs : 1.870130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:08] 12.798581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for RAM : 0.458537 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for all GPUs : 0.041096 kWh. Total GPU Power : 67.41000209880833 W\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for all CPUs : 0.085535 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:23] 0.585168 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for RAM : 10.052438 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for RAM : 0.448908 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for all GPUs : 0.896528 kWh. Total GPU Power : 66.98385758082104 W\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for all CPUs : 1.873673 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:23] 12.822639 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for all GPUs : 0.040148 kWh. Total GPU Power : 66.99149130507254 W\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for all CPUs : 0.083719 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:23] 0.572775 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for RAM : 10.036814 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for all GPUs : 0.895046 kWh. Total GPU Power : 66.98443801080369 W\n",
      "[codecarbon INFO @ 16:45:23] Energy consumed for all CPUs : 1.870713 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:23] 12.802573 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for RAM : 0.461665 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for all GPUs : 0.041376 kWh. Total GPU Power : 67.10511262507534 W\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for all CPUs : 0.086118 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:38] 0.589158 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for RAM : 10.055568 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for RAM : 0.452039 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for all GPUs : 0.896808 kWh. Total GPU Power : 67.06504295337974 W\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for all CPUs : 1.874256 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:38] 12.826632 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for all GPUs : 0.040427 kWh. Total GPU Power : 67.06490222157557 W\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for all CPUs : 0.084303 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:38] 0.576768 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for RAM : 10.039944 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for all GPUs : 0.895325 kWh. Total GPU Power : 67.07146926882876 W\n",
      "[codecarbon INFO @ 16:45:38] Energy consumed for all CPUs : 1.871296 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:38] 12.806566 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for RAM : 0.464794 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for all GPUs : 0.041655 kWh. Total GPU Power : 67.09317660977167 W\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for all CPUs : 0.086701 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:53] 0.593150 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for RAM : 10.058697 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for RAM : 0.455168 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for all GPUs : 0.897089 kWh. Total GPU Power : 67.54734757101996 W\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for all CPUs : 1.874840 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:53] 12.830626 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for all GPUs : 0.040708 kWh. Total GPU Power : 67.53955259226716 W\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for all CPUs : 0.084886 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:53] 0.580762 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for RAM : 10.043074 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for all GPUs : 0.895607 kWh. Total GPU Power : 67.5348429687258 W\n",
      "[codecarbon INFO @ 16:45:53] Energy consumed for all CPUs : 1.871880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:45:53] 12.810560 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for RAM : 0.467923 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for all GPUs : 0.041937 kWh. Total GPU Power : 67.56659933274514 W\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for all CPUs : 0.087284 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:08] 0.597144 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for RAM : 10.061827 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for RAM : 0.458297 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for all GPUs : 0.897368 kWh. Total GPU Power : 67.10756423172742 W\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for all CPUs : 1.875423 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:08] 12.834618 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for all GPUs : 0.040988 kWh. Total GPU Power : 67.11176615584095 W\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for all CPUs : 0.085469 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:08] 0.584755 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for RAM : 10.046204 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for all GPUs : 0.895886 kWh. Total GPU Power : 67.10858105337067 W\n",
      "[codecarbon INFO @ 16:46:08] Energy consumed for all CPUs : 1.872463 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:08] 12.814553 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for RAM : 0.471053 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for all GPUs : 0.042216 kWh. Total GPU Power : 67.07702937582143 W\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for all CPUs : 0.087868 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:23] 0.601136 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for RAM : 10.064956 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for RAM : 0.461427 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for all GPUs : 0.897648 kWh. Total GPU Power : 67.08018415436497 W\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for all CPUs : 1.876006 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:23] 12.838610 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for all GPUs : 0.041267 kWh. Total GPU Power : 67.08431631791393 W\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for all CPUs : 0.086053 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:23] 0.588747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for RAM : 10.049333 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for all GPUs : 0.896166 kWh. Total GPU Power : 67.07828250621468 W\n",
      "[codecarbon INFO @ 16:46:23] Energy consumed for all CPUs : 1.873046 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:23] 12.818545 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for RAM : 0.474182 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for all GPUs : 0.042495 kWh. Total GPU Power : 67.06844206774335 W\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for all CPUs : 0.088451 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:38] 0.605128 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for RAM : 10.068085 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for RAM : 0.464556 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for all GPUs : 0.897929 kWh. Total GPU Power : 67.51980679420522 W\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for all CPUs : 1.876589 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:38] 12.842604 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for all GPUs : 0.041548 kWh. Total GPU Power : 67.52535801730384 W\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for all CPUs : 0.086636 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:38] 0.592740 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for RAM : 10.052463 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for all GPUs : 0.896447 kWh. Total GPU Power : 67.51358771684939 W\n",
      "[codecarbon INFO @ 16:46:38] Energy consumed for all CPUs : 1.873629 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:38] 12.822539 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for RAM : 0.477312 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for all GPUs : 0.042777 kWh. Total GPU Power : 67.47074981395775 W\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for all CPUs : 0.089034 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:53] 0.609122 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for RAM : 10.071215 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for RAM : 0.467685 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for all GPUs : 0.898208 kWh. Total GPU Power : 67.01924128919431 W\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for all CPUs : 1.877173 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:53] 12.846596 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for all GPUs : 0.041828 kWh. Total GPU Power : 67.03142888488708 W\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for all CPUs : 0.087219 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:53] 0.596732 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for RAM : 10.055593 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for all GPUs : 0.896726 kWh. Total GPU Power : 67.02170701201301 W\n",
      "[codecarbon INFO @ 16:46:53] Energy consumed for all CPUs : 1.874213 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:46:53] 12.826532 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5828526\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 647615\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:36:23] Energy consumed for RAM : 0.345886 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for all GPUs : 0.031036 kWh. Total GPU Power : 67.05193912907777 W\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for all CPUs : 0.064537 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:23] 0.441459 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for RAM : 9.939785 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for RAM : 0.336262 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for RAM : 9.924148 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for all GPUs : 0.886468 kWh. Total GPU Power : 66.9776031231066 W\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for all CPUs : 1.852675 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:23] 12.678927 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for all GPUs : 0.030087 kWh. Total GPU Power : 66.9621529914434 W\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for all CPUs : 0.062721 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for all GPUs : 0.884985 kWh. Total GPU Power : 66.96305492299261 W\n",
      "[codecarbon INFO @ 16:36:23] 0.429071 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for all CPUs : 1.849714 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:23] 12.658847 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for RAM : 0.349015 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all GPUs : 0.031315 kWh. Total GPU Power : 67.07711381086939 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all CPUs : 0.065120 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:38] 0.445450 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for RAM : 9.942913 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for RAM : 0.339391 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all GPUs : 0.886747 kWh. Total GPU Power : 67.08578636185405 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all CPUs : 1.853258 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:38] 12.682919 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for RAM : 9.927277 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all GPUs : 0.030366 kWh. Total GPU Power : 67.101987529633 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all CPUs : 0.063305 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:38] 0.433062 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all GPUs : 0.885267 kWh. Total GPU Power : 67.5306800119433 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all CPUs : 1.850297 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:38] 12.662841 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for RAM : 0.352145 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all GPUs : 0.031596 kWh. Total GPU Power : 67.50099727957912 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all CPUs : 0.065703 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:53] 0.449445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for RAM : 9.946043 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for RAM : 0.342520 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for RAM : 9.930407 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all GPUs : 0.887028 kWh. Total GPU Power : 67.49704175636096 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all CPUs : 1.853842 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:53] 12.686912 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all GPUs : 0.030648 kWh. Total GPU Power : 67.510463013606 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all CPUs : 0.063888 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:53] 0.437055 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all GPUs : 0.885546 kWh. Total GPU Power : 67.05331270865727 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all CPUs : 1.850880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:53] 12.666833 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for RAM : 0.355274 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for all GPUs : 0.031875 kWh. Total GPU Power : 66.98688266016075 W\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for all CPUs : 0.066286 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:08] 0.453436 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for RAM : 9.949171 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for RAM : 0.345648 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for RAM : 9.933536 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for all GPUs : 0.030927 kWh. Total GPU Power : 66.99971786964518 W\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for all CPUs : 0.064471 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for all GPUs : 0.887307 kWh. Total GPU Power : 66.9859229869642 W\n",
      "[codecarbon INFO @ 16:37:08] 0.441046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for all CPUs : 1.854425 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:08] 12.690904 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for all GPUs : 0.885825 kWh. Total GPU Power : 66.98138381780555 W\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for all CPUs : 1.851464 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:08] 12.670824 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunk_logger = transformers.utils.logging.get_logger(\n",
    "    \"transformers.tokenization_utils_base\"\n",
    ")\n",
    "def chunk_text(examples):\n",
    "    with CaptureLogger(chunk_logger) as cl:\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples['input_ids'])\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        \n",
    "        result = {\n",
    "            k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        # Filter out chunks that are smaller than block_size\n",
    "        for k, v in result.items():\n",
    "            result[k] = [chunk for chunk in v if len(chunk) == block_size]\n",
    "        \n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22849d1bbf4f4107b56b46cbb76d6a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/9951012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunked_dataset = tokenized_dataset.map(chunk_text, batched=True, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12957320\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunked_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(chunked_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/data/Bangla2B+/shards/parquet_files/'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/data/Bangla2B+/shards/parquet_files/../chunked_dataset'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/\"\n",
    "chunked_dataset_dir = dataset_dir + \"../chunked_dataset\"\n",
    "chunked_dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache-03e5097076955b61.arrow  data-00173-of-00345.arrow\n",
      "cache-b941a8bb8e3f761e.arrow  data-00174-of-00345.arrow\n",
      "data-00000-of-00345.arrow     data-00175-of-00345.arrow\n",
      "data-00001-of-00345.arrow     data-00176-of-00345.arrow\n",
      "data-00002-of-00345.arrow     data-00177-of-00345.arrow\n",
      "data-00003-of-00345.arrow     data-00178-of-00345.arrow\n",
      "data-00004-of-00345.arrow     data-00179-of-00345.arrow\n",
      "data-00005-of-00345.arrow     data-00180-of-00345.arrow\n",
      "data-00006-of-00345.arrow     data-00181-of-00345.arrow\n",
      "data-00007-of-00345.arrow     data-00182-of-00345.arrow\n",
      "data-00008-of-00345.arrow     data-00183-of-00345.arrow\n",
      "data-00009-of-00345.arrow     data-00184-of-00345.arrow\n",
      "data-00010-of-00345.arrow     data-00185-of-00345.arrow\n",
      "data-00011-of-00345.arrow     data-00186-of-00345.arrow\n",
      "data-00012-of-00345.arrow     data-00187-of-00345.arrow\n",
      "data-00013-of-00345.arrow     data-00188-of-00345.arrow\n",
      "data-00014-of-00345.arrow     data-00189-of-00345.arrow\n",
      "data-00015-of-00345.arrow     data-00190-of-00345.arrow\n",
      "data-00016-of-00345.arrow     data-00191-of-00345.arrow\n",
      "data-00017-of-00345.arrow     data-00192-of-00345.arrow\n",
      "data-00018-of-00345.arrow     data-00193-of-00345.arrow\n",
      "data-00019-of-00345.arrow     data-00194-of-00345.arrow\n",
      "data-00020-of-00345.arrow     data-00195-of-00345.arrow\n",
      "data-00021-of-00345.arrow     data-00196-of-00345.arrow\n",
      "data-00022-of-00345.arrow     data-00197-of-00345.arrow\n",
      "data-00023-of-00345.arrow     data-00198-of-00345.arrow\n",
      "data-00024-of-00345.arrow     data-00199-of-00345.arrow\n",
      "data-00025-of-00345.arrow     data-00200-of-00345.arrow\n",
      "data-00026-of-00345.arrow     data-00201-of-00345.arrow\n",
      "data-00027-of-00345.arrow     data-00202-of-00345.arrow\n",
      "data-00028-of-00345.arrow     data-00203-of-00345.arrow\n",
      "data-00029-of-00345.arrow     data-00204-of-00345.arrow\n",
      "data-00030-of-00345.arrow     data-00205-of-00345.arrow\n",
      "data-00031-of-00345.arrow     data-00206-of-00345.arrow\n",
      "data-00032-of-00345.arrow     data-00207-of-00345.arrow\n",
      "data-00033-of-00345.arrow     data-00208-of-00345.arrow\n",
      "data-00034-of-00345.arrow     data-00209-of-00345.arrow\n",
      "data-00035-of-00345.arrow     data-00210-of-00345.arrow\n",
      "data-00036-of-00345.arrow     data-00211-of-00345.arrow\n",
      "data-00037-of-00345.arrow     data-00212-of-00345.arrow\n",
      "data-00038-of-00345.arrow     data-00213-of-00345.arrow\n",
      "data-00039-of-00345.arrow     data-00214-of-00345.arrow\n",
      "data-00040-of-00345.arrow     data-00215-of-00345.arrow\n",
      "data-00041-of-00345.arrow     data-00216-of-00345.arrow\n",
      "data-00042-of-00345.arrow     data-00217-of-00345.arrow\n",
      "data-00043-of-00345.arrow     data-00218-of-00345.arrow\n",
      "data-00044-of-00345.arrow     data-00219-of-00345.arrow\n",
      "data-00045-of-00345.arrow     data-00220-of-00345.arrow\n",
      "data-00046-of-00345.arrow     data-00221-of-00345.arrow\n",
      "data-00047-of-00345.arrow     data-00222-of-00345.arrow\n",
      "data-00048-of-00345.arrow     data-00223-of-00345.arrow\n",
      "data-00049-of-00345.arrow     data-00224-of-00345.arrow\n",
      "data-00050-of-00345.arrow     data-00225-of-00345.arrow\n",
      "data-00051-of-00345.arrow     data-00226-of-00345.arrow\n",
      "data-00052-of-00345.arrow     data-00227-of-00345.arrow\n",
      "data-00053-of-00345.arrow     data-00228-of-00345.arrow\n",
      "data-00054-of-00345.arrow     data-00229-of-00345.arrow\n",
      "data-00055-of-00345.arrow     data-00230-of-00345.arrow\n",
      "data-00056-of-00345.arrow     data-00231-of-00345.arrow\n",
      "data-00057-of-00345.arrow     data-00232-of-00345.arrow\n",
      "data-00058-of-00345.arrow     data-00233-of-00345.arrow\n",
      "data-00059-of-00345.arrow     data-00234-of-00345.arrow\n",
      "data-00060-of-00345.arrow     data-00235-of-00345.arrow\n",
      "data-00061-of-00345.arrow     data-00236-of-00345.arrow\n",
      "data-00062-of-00345.arrow     data-00237-of-00345.arrow\n",
      "data-00063-of-00345.arrow     data-00238-of-00345.arrow\n",
      "data-00064-of-00345.arrow     data-00239-of-00345.arrow\n",
      "data-00065-of-00345.arrow     data-00240-of-00345.arrow\n",
      "data-00066-of-00345.arrow     data-00241-of-00345.arrow\n",
      "data-00067-of-00345.arrow     data-00242-of-00345.arrow\n",
      "data-00068-of-00345.arrow     data-00243-of-00345.arrow\n",
      "data-00069-of-00345.arrow     data-00244-of-00345.arrow\n",
      "data-00070-of-00345.arrow     data-00245-of-00345.arrow\n",
      "data-00071-of-00345.arrow     data-00246-of-00345.arrow\n",
      "data-00072-of-00345.arrow     data-00247-of-00345.arrow\n",
      "data-00073-of-00345.arrow     data-00248-of-00345.arrow\n",
      "data-00074-of-00345.arrow     data-00249-of-00345.arrow\n",
      "data-00075-of-00345.arrow     data-00250-of-00345.arrow\n",
      "data-00076-of-00345.arrow     data-00251-of-00345.arrow\n",
      "data-00077-of-00345.arrow     data-00252-of-00345.arrow\n",
      "data-00078-of-00345.arrow     data-00253-of-00345.arrow\n",
      "data-00079-of-00345.arrow     data-00254-of-00345.arrow\n",
      "data-00080-of-00345.arrow     data-00255-of-00345.arrow\n",
      "data-00081-of-00345.arrow     data-00256-of-00345.arrow\n",
      "data-00082-of-00345.arrow     data-00257-of-00345.arrow\n",
      "data-00083-of-00345.arrow     data-00258-of-00345.arrow\n",
      "data-00084-of-00345.arrow     data-00259-of-00345.arrow\n",
      "data-00085-of-00345.arrow     data-00260-of-00345.arrow\n",
      "data-00086-of-00345.arrow     data-00261-of-00345.arrow\n",
      "data-00087-of-00345.arrow     data-00262-of-00345.arrow\n",
      "data-00088-of-00345.arrow     data-00263-of-00345.arrow\n",
      "data-00089-of-00345.arrow     data-00264-of-00345.arrow\n",
      "data-00090-of-00345.arrow     data-00265-of-00345.arrow\n",
      "data-00091-of-00345.arrow     data-00266-of-00345.arrow\n",
      "data-00092-of-00345.arrow     data-00267-of-00345.arrow\n",
      "data-00093-of-00345.arrow     data-00268-of-00345.arrow\n",
      "data-00094-of-00345.arrow     data-00269-of-00345.arrow\n",
      "data-00095-of-00345.arrow     data-00270-of-00345.arrow\n",
      "data-00096-of-00345.arrow     data-00271-of-00345.arrow\n",
      "data-00097-of-00345.arrow     data-00272-of-00345.arrow\n",
      "data-00098-of-00345.arrow     data-00273-of-00345.arrow\n",
      "data-00099-of-00345.arrow     data-00274-of-00345.arrow\n",
      "data-00100-of-00345.arrow     data-00275-of-00345.arrow\n",
      "data-00101-of-00345.arrow     data-00276-of-00345.arrow\n",
      "data-00102-of-00345.arrow     data-00277-of-00345.arrow\n",
      "data-00103-of-00345.arrow     data-00278-of-00345.arrow\n",
      "data-00104-of-00345.arrow     data-00279-of-00345.arrow\n",
      "data-00105-of-00345.arrow     data-00280-of-00345.arrow\n",
      "data-00106-of-00345.arrow     data-00281-of-00345.arrow\n",
      "data-00107-of-00345.arrow     data-00282-of-00345.arrow\n",
      "data-00108-of-00345.arrow     data-00283-of-00345.arrow\n",
      "data-00109-of-00345.arrow     data-00284-of-00345.arrow\n",
      "data-00110-of-00345.arrow     data-00285-of-00345.arrow\n",
      "data-00111-of-00345.arrow     data-00286-of-00345.arrow\n",
      "data-00112-of-00345.arrow     data-00287-of-00345.arrow\n",
      "data-00113-of-00345.arrow     data-00288-of-00345.arrow\n",
      "data-00114-of-00345.arrow     data-00289-of-00345.arrow\n",
      "data-00115-of-00345.arrow     data-00290-of-00345.arrow\n",
      "data-00116-of-00345.arrow     data-00291-of-00345.arrow\n",
      "data-00117-of-00345.arrow     data-00292-of-00345.arrow\n",
      "data-00118-of-00345.arrow     data-00293-of-00345.arrow\n",
      "data-00119-of-00345.arrow     data-00294-of-00345.arrow\n",
      "data-00120-of-00345.arrow     data-00295-of-00345.arrow\n",
      "data-00121-of-00345.arrow     data-00296-of-00345.arrow\n",
      "data-00122-of-00345.arrow     data-00297-of-00345.arrow\n",
      "data-00123-of-00345.arrow     data-00298-of-00345.arrow\n",
      "data-00124-of-00345.arrow     data-00299-of-00345.arrow\n",
      "data-00125-of-00345.arrow     data-00300-of-00345.arrow\n",
      "data-00126-of-00345.arrow     data-00301-of-00345.arrow\n",
      "data-00127-of-00345.arrow     data-00302-of-00345.arrow\n",
      "data-00128-of-00345.arrow     data-00303-of-00345.arrow\n",
      "data-00129-of-00345.arrow     data-00304-of-00345.arrow\n",
      "data-00130-of-00345.arrow     data-00305-of-00345.arrow\n",
      "data-00131-of-00345.arrow     data-00306-of-00345.arrow\n",
      "data-00132-of-00345.arrow     data-00307-of-00345.arrow\n",
      "data-00133-of-00345.arrow     data-00308-of-00345.arrow\n",
      "data-00134-of-00345.arrow     data-00309-of-00345.arrow\n",
      "data-00135-of-00345.arrow     data-00310-of-00345.arrow\n",
      "data-00136-of-00345.arrow     data-00311-of-00345.arrow\n",
      "data-00137-of-00345.arrow     data-00312-of-00345.arrow\n",
      "data-00138-of-00345.arrow     data-00313-of-00345.arrow\n",
      "data-00139-of-00345.arrow     data-00314-of-00345.arrow\n",
      "data-00140-of-00345.arrow     data-00315-of-00345.arrow\n",
      "data-00141-of-00345.arrow     data-00316-of-00345.arrow\n",
      "data-00142-of-00345.arrow     data-00317-of-00345.arrow\n",
      "data-00143-of-00345.arrow     data-00318-of-00345.arrow\n",
      "data-00144-of-00345.arrow     data-00319-of-00345.arrow\n",
      "data-00145-of-00345.arrow     data-00320-of-00345.arrow\n",
      "data-00146-of-00345.arrow     data-00321-of-00345.arrow\n",
      "data-00147-of-00345.arrow     data-00322-of-00345.arrow\n",
      "data-00148-of-00345.arrow     data-00323-of-00345.arrow\n",
      "data-00149-of-00345.arrow     data-00324-of-00345.arrow\n",
      "data-00150-of-00345.arrow     data-00325-of-00345.arrow\n",
      "data-00151-of-00345.arrow     data-00326-of-00345.arrow\n",
      "data-00152-of-00345.arrow     data-00327-of-00345.arrow\n",
      "data-00153-of-00345.arrow     data-00328-of-00345.arrow\n",
      "data-00154-of-00345.arrow     data-00329-of-00345.arrow\n",
      "data-00155-of-00345.arrow     data-00330-of-00345.arrow\n",
      "data-00156-of-00345.arrow     data-00331-of-00345.arrow\n",
      "data-00157-of-00345.arrow     data-00332-of-00345.arrow\n",
      "data-00158-of-00345.arrow     data-00333-of-00345.arrow\n",
      "data-00159-of-00345.arrow     data-00334-of-00345.arrow\n",
      "data-00160-of-00345.arrow     data-00335-of-00345.arrow\n",
      "data-00161-of-00345.arrow     data-00336-of-00345.arrow\n",
      "data-00162-of-00345.arrow     data-00337-of-00345.arrow\n",
      "data-00163-of-00345.arrow     data-00338-of-00345.arrow\n",
      "data-00164-of-00345.arrow     data-00339-of-00345.arrow\n",
      "data-00165-of-00345.arrow     data-00340-of-00345.arrow\n",
      "data-00166-of-00345.arrow     data-00341-of-00345.arrow\n",
      "data-00167-of-00345.arrow     data-00342-of-00345.arrow\n",
      "data-00168-of-00345.arrow     data-00343-of-00345.arrow\n",
      "data-00169-of-00345.arrow     data-00344-of-00345.arrow\n",
      "data-00170-of-00345.arrow     dataset_info.json\n",
      "data-00171-of-00345.arrow     state.json\n",
      "data-00172-of-00345.arrow\n"
     ]
    }
   ],
   "source": [
    "!ls $chunked_dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $chunked_dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db204382888f43aa83ab9890c0a63b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/346 shards):   0%|          | 0/12957320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12957320 chunks to /workspace/data/Bangla2B+/shards/parquet_files/../chunked_dataset\n",
      "Time elapsed for saving to disk: 0h 12m 25.81s\n"
     ]
    }
   ],
   "source": [
    "# Ensure the directory exists\n",
    "os.makedirs(chunked_dataset_dir, exist_ok=True)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "chunked_dataset.save_to_disk(chunked_dataset_dir, num_proc=32)\n",
    "end_time = time.time()\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "print(f\"Saved {len(chunked_dataset)} chunks to {chunked_dataset_dir}\")\n",
    "\n",
    "# Convert to hours, minutes, and seconds\n",
    "elapsed_time_hours = int(elapsed_time_seconds // 3600)\n",
    "elapsed_time_minutes = int((elapsed_time_seconds % 3600) // 60)\n",
    "elapsed_time_seconds = elapsed_time_seconds % 60\n",
    "\n",
    "print(f\"Time elapsed for saving to disk: {elapsed_time_hours}h {elapsed_time_minutes}m {elapsed_time_seconds:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69eeacc8068740e4a2263c861dd8bbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "import transformers\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "\n",
    "dataset_dir = \"/workspace/data/Bangla2B+/shards/parquet_files/\"\n",
    "chunked_dataset_dir = dataset_dir + \"../chunked_dataset\"\n",
    "chunked_dataset_dir\n",
    "# loading chunked_dataset from dir\n",
    "load_chunked_dataset_logger = transformers.utils.logging.get_logger(\n",
    "    \"transformers.tokenization_utils_base\"\n",
    ")\n",
    "with CaptureLogger(load_chunked_dataset_logger) as cl:\n",
    "    chunked_dataset = load_from_disk(chunked_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5828526\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 647615\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:47:08] Energy consumed for RAM : 0.480441 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for all GPUs : 0.043056 kWh. Total GPU Power : 67.06503832893199 W\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for all CPUs : 0.089617 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:08] 0.613114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for RAM : 10.074345 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for RAM : 0.470815 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for all GPUs : 0.898488 kWh. Total GPU Power : 67.06660360438775 W\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for all CPUs : 1.877756 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:08] 12.850589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for all GPUs : 0.042107 kWh. Total GPU Power : 67.06932488387169 W\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for all CPUs : 0.087802 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:08] 0.600724 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for RAM : 10.058723 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for all GPUs : 0.897005 kWh. Total GPU Power : 67.06508844220473 W\n",
      "[codecarbon INFO @ 16:47:08] Energy consumed for all CPUs : 1.874796 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:08] 12.830524 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for RAM : 0.483570 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for all GPUs : 0.043335 kWh. Total GPU Power : 67.0839636169165 W\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for all CPUs : 0.090201 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:23] 0.617106 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for RAM : 10.077475 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for RAM : 0.473945 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for all GPUs : 0.898767 kWh. Total GPU Power : 67.07774639979922 W\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for all CPUs : 1.878339 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:23] 12.854581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for all GPUs : 0.042386 kWh. Total GPU Power : 67.08217694227002 W\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for all CPUs : 0.088386 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:23] 0.604717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for RAM : 10.061853 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for all GPUs : 0.897287 kWh. Total GPU Power : 67.52604100895316 W\n",
      "[codecarbon INFO @ 16:47:23] Energy consumed for all CPUs : 1.875379 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:23] 12.834519 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for RAM : 0.486700 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for all GPUs : 0.043617 kWh. Total GPU Power : 67.516567918185 W\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for all CPUs : 0.090784 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:38] 0.621101 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for RAM : 10.080604 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for RAM : 0.477075 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for all GPUs : 0.899048 kWh. Total GPU Power : 67.50846554630716 W\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for all CPUs : 1.878923 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:38] 12.858576 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for all GPUs : 0.042668 kWh. Total GPU Power : 67.5117120573907 W\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for all CPUs : 0.088969 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:38] 0.608711 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for RAM : 10.064983 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for all GPUs : 0.897566 kWh. Total GPU Power : 67.06343234474616 W\n",
      "[codecarbon INFO @ 16:47:38] Energy consumed for all CPUs : 1.875963 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:38] 12.838511 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for RAM : 0.489829 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for all GPUs : 0.043896 kWh. Total GPU Power : 67.08063562405042 W\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for all CPUs : 0.091367 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:53] 0.625092 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for RAM : 10.083734 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for RAM : 0.480204 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for all GPUs : 0.899328 kWh. Total GPU Power : 67.0790473800149 W\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for all CPUs : 1.879506 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:53] 12.862567 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for all GPUs : 0.042947 kWh. Total GPU Power : 67.08095349522019 W\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for all CPUs : 0.089552 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:53] 0.612703 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for RAM : 10.068113 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for all GPUs : 0.897845 kWh. Total GPU Power : 67.07159782332943 W\n",
      "[codecarbon INFO @ 16:47:53] Energy consumed for all CPUs : 1.876546 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:47:53] 12.842504 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for RAM : 0.492958 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for all GPUs : 0.044175 kWh. Total GPU Power : 67.0597810533277 W\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for all CPUs : 0.091951 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:08] 0.629084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for RAM : 10.086863 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for RAM : 0.483333 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for all GPUs : 0.899607 kWh. Total GPU Power : 67.05722098042814 W\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for all CPUs : 1.880089 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:08] 12.866559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for all GPUs : 0.043226 kWh. Total GPU Power : 67.0598173073255 W\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for all CPUs : 0.090136 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:08] 0.616695 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for RAM : 10.071242 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for all GPUs : 0.898127 kWh. Total GPU Power : 67.5037670004744 W\n",
      "[codecarbon INFO @ 16:48:08] Energy consumed for all CPUs : 1.877129 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:08] 12.846498 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for RAM : 0.496088 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for all GPUs : 0.044455 kWh. Total GPU Power : 67.06324852746478 W\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for all CPUs : 0.092534 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:23] 0.633076 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for RAM : 10.089993 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for RAM : 0.486463 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for all GPUs : 0.899888 kWh. Total GPU Power : 67.51496595169843 W\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for all CPUs : 1.880673 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:23] 12.870553 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for all GPUs : 0.043508 kWh. Total GPU Power : 67.51536960151206 W\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for all CPUs : 0.090719 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:23] 0.620689 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for RAM : 10.074372 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for all GPUs : 0.898406 kWh. Total GPU Power : 67.0647074862486 W\n",
      "[codecarbon INFO @ 16:48:23] Energy consumed for all CPUs : 1.877713 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:23] 12.850490 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for RAM : 0.499217 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for all GPUs : 0.044736 kWh. Total GPU Power : 67.5047363549615 W\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for all CPUs : 0.093117 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:38] 0.637070 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for RAM : 10.093122 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for RAM : 0.489592 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for all GPUs : 0.900167 kWh. Total GPU Power : 67.06498785904576 W\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for all CPUs : 1.881256 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:38] 12.874545 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for all GPUs : 0.043787 kWh. Total GPU Power : 67.068510540848 W\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for all CPUs : 0.091302 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:38] 0.624681 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for RAM : 10.077502 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for all GPUs : 0.898685 kWh. Total GPU Power : 67.05215980801903 W\n",
      "[codecarbon INFO @ 16:48:38] Energy consumed for all CPUs : 1.878296 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:38] 12.854483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for RAM : 0.502346 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for all GPUs : 0.045015 kWh. Total GPU Power : 67.06605523985365 W\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for all CPUs : 0.093700 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:53] 0.641062 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for RAM : 10.096251 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for RAM : 0.492721 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for all GPUs : 0.900447 kWh. Total GPU Power : 67.066793185548 W\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for all CPUs : 1.881839 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:53] 12.878537 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for all GPUs : 0.044066 kWh. Total GPU Power : 67.06112037887168 W\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for all CPUs : 0.091886 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:53] 0.628673 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for RAM : 10.080631 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for all GPUs : 0.898965 kWh. Total GPU Power : 67.05924707062626 W\n",
      "[codecarbon INFO @ 16:48:53] Energy consumed for all CPUs : 1.878879 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:48:53] 12.858475 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for RAM : 0.505475 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for all GPUs : 0.045294 kWh. Total GPU Power : 67.0573087108187 W\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for all CPUs : 0.094284 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:08] 0.645053 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for RAM : 10.099380 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for RAM : 0.495851 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for all GPUs : 0.900728 kWh. Total GPU Power : 67.49933998605732 W\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for all CPUs : 1.882422 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:08] 12.882531 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for all GPUs : 0.044347 kWh. Total GPU Power : 67.50797620222669 W\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for all CPUs : 0.092469 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:08] 0.632667 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for RAM : 10.083761 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for all GPUs : 0.899246 kWh. Total GPU Power : 67.49774307557988 W\n",
      "[codecarbon INFO @ 16:49:08] Energy consumed for all CPUs : 1.879463 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:08] 12.862469 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for RAM : 0.508605 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for all GPUs : 0.045576 kWh. Total GPU Power : 67.50250544409896 W\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for all CPUs : 0.094867 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:23] 0.649047 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for RAM : 10.102510 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for RAM : 0.498980 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for all GPUs : 0.901007 kWh. Total GPU Power : 67.05566423393026 W\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for all CPUs : 1.883006 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:23] 12.886523 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for all GPUs : 0.044627 kWh. Total GPU Power : 67.05665795366957 W\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for all CPUs : 0.093052 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:23] 0.636659 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for RAM : 10.086891 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for all GPUs : 0.899525 kWh. Total GPU Power : 67.04494630117856 W\n",
      "[codecarbon INFO @ 16:49:23] Energy consumed for all CPUs : 1.880046 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:23] 12.866462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for RAM : 0.511734 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for all GPUs : 0.045855 kWh. Total GPU Power : 67.05690618184076 W\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for all CPUs : 0.095450 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:38] 0.653039 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for RAM : 10.105639 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for RAM : 0.502109 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for all GPUs : 0.901287 kWh. Total GPU Power : 67.07126410999733 W\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for all CPUs : 1.883589 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:38] 12.890515 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for all GPUs : 0.044906 kWh. Total GPU Power : 67.07493001887893 W\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for all CPUs : 0.093635 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:38] 0.640650 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for RAM : 10.090021 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for all GPUs : 0.899804 kWh. Total GPU Power : 67.0672882731321 W\n",
      "[codecarbon INFO @ 16:49:38] Energy consumed for all CPUs : 1.880629 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:38] 12.870454 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for RAM : 0.514863 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for all GPUs : 0.046134 kWh. Total GPU Power : 67.0769797197218 W\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for all CPUs : 0.096036 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for RAM : 10.108782 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for RAM : 0.505252 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:53] 0.657033 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for RAM : 10.093161 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for all GPUs : 0.901568 kWh. Total GPU Power : 67.1570072321668 W\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for all CPUs : 1.884175 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:53] 12.894526 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for all GPUs : 0.045187 kWh. Total GPU Power : 67.167064863449 W\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for all CPUs : 0.094222 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:53] 0.644661 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for all GPUs : 0.900086 kWh. Total GPU Power : 67.20688409535806 W\n",
      "[codecarbon INFO @ 16:49:53] Energy consumed for all CPUs : 1.881215 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:49:53] 12.874462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for RAM : 0.517972 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for all GPUs : 0.046415 kWh. Total GPU Power : 67.93365964547488 W\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for all CPUs : 0.096616 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:08] 0.661004 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for RAM : 10.096288 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for RAM : 0.508379 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for RAM : 10.111911 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for all GPUs : 0.900365 kWh. Total GPU Power : 67.10518262198099 W\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for all CPUs : 1.881798 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for all GPUs : 0.045468 kWh. Total GPU Power : 67.53807999120545 W\n",
      "[codecarbon INFO @ 16:50:08] 12.878451 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for all CPUs : 0.094805 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:08] 0.648652 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for all GPUs : 0.901849 kWh. Total GPU Power : 67.52297636131642 W\n",
      "[codecarbon INFO @ 16:50:08] Energy consumed for all CPUs : 1.884759 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:08] 12.898518 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for RAM : 0.521102 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for all GPUs : 0.046695 kWh. Total GPU Power : 67.05669116558613 W\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for all CPUs : 0.097199 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:23] 0.664996 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for RAM : 10.099416 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for RAM : 0.511508 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for all GPUs : 0.900646 kWh. Total GPU Power : 67.53110482577914 W\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for all CPUs : 1.882381 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:23] 12.882443 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for RAM : 10.115041 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for all GPUs : 0.045748 kWh. Total GPU Power : 67.05506208504478 W\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for all CPUs : 0.095388 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:23] 0.652644 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for all GPUs : 0.902128 kWh. Total GPU Power : 67.05123390825584 W\n",
      "[codecarbon INFO @ 16:50:23] Energy consumed for all CPUs : 1.885342 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:23] 12.902511 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for RAM : 0.524232 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for all GPUs : 0.046974 kWh. Total GPU Power : 66.9971315883849 W\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for all CPUs : 0.097783 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:38] 0.668988 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for RAM : 10.102545 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for RAM : 0.514637 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for all GPUs : 0.900925 kWh. Total GPU Power : 67.01703211088468 W\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for all CPUs : 1.882964 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:38] 12.886434 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for RAM : 10.118170 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for all GPUs : 0.046027 kWh. Total GPU Power : 67.00859724473244 W\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for all CPUs : 0.095971 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:38] 0.656635 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for all GPUs : 0.902407 kWh. Total GPU Power : 66.99679945475339 W\n",
      "[codecarbon INFO @ 16:50:38] Energy consumed for all CPUs : 1.885925 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:38] 12.906503 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for RAM : 0.527361 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for all GPUs : 0.047253 kWh. Total GPU Power : 67.06249864620489 W\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for all CPUs : 0.098366 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:53] 0.672980 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for RAM : 10.105674 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for RAM : 0.517766 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for all GPUs : 0.901204 kWh. Total GPU Power : 67.06867337873223 W\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for all CPUs : 1.883548 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:53] 12.890426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for all GPUs : 0.046306 kWh. Total GPU Power : 67.07393374271382 W\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for all CPUs : 0.096555 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:53] 0.660627 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for RAM : 10.121300 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for all GPUs : 0.902687 kWh. Total GPU Power : 67.05898567471293 W\n",
      "[codecarbon INFO @ 16:50:53] Energy consumed for all CPUs : 1.886509 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:50:53] 12.910495 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for RAM : 0.530491 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for all GPUs : 0.047534 kWh. Total GPU Power : 67.51195408954537 W\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for all CPUs : 0.098949 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:08] 0.676974 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for RAM : 10.108804 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for RAM : 0.520896 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for all GPUs : 0.901486 kWh. Total GPU Power : 67.50106841874538 W\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for all CPUs : 1.884131 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:08] 12.894420 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for all GPUs : 0.046587 kWh. Total GPU Power : 67.5103462314874 W\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for all CPUs : 0.097138 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for RAM : 10.124429 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:08] 0.664621 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for all GPUs : 0.902968 kWh. Total GPU Power : 67.5047738719665 W\n",
      "[codecarbon INFO @ 16:51:08] Energy consumed for all CPUs : 1.887092 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:08] 12.914489 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for RAM : 0.533620 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for all GPUs : 0.047814 kWh. Total GPU Power : 67.07605503401795 W\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for all CPUs : 0.099532 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:23] 0.680966 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for RAM : 10.111933 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for all GPUs : 0.901765 kWh. Total GPU Power : 67.07995373043074 W\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for all CPUs : 1.884714 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:23] 12.898412 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for RAM : 0.524026 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for RAM : 10.127559 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for all GPUs : 0.046867 kWh. Total GPU Power : 67.07436421287765 W\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for all CPUs : 0.097721 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:23] 0.668614 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for all GPUs : 0.903247 kWh. Total GPU Power : 67.07889462609549 W\n",
      "[codecarbon INFO @ 16:51:23] Energy consumed for all CPUs : 1.887675 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:23] 12.918481 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for RAM : 0.536750 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for all GPUs : 0.048093 kWh. Total GPU Power : 67.07413549468977 W\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for all CPUs : 0.100116 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:38] 0.684959 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for RAM : 10.115062 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for all GPUs : 0.902045 kWh. Total GPU Power : 67.07934615208863 W\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for all CPUs : 1.885298 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for RAM : 0.527156 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:38] 12.902404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for RAM : 10.130689 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for all GPUs : 0.047146 kWh. Total GPU Power : 67.07167849733206 W\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for all CPUs : 0.098305 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:38] 0.672607 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for all GPUs : 0.903527 kWh. Total GPU Power : 67.070419207324 W\n",
      "[codecarbon INFO @ 16:51:38] Energy consumed for all CPUs : 1.888258 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:38] 12.922474 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for RAM : 0.539879 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for all GPUs : 0.048374 kWh. Total GPU Power : 67.54541877603569 W\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for all CPUs : 0.100699 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:53] 0.688953 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for RAM : 10.118190 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for all GPUs : 0.902326 kWh. Total GPU Power : 67.57150228344766 W\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for all CPUs : 1.885881 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:53] 12.906397 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for RAM : 0.530285 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for all GPUs : 0.047427 kWh. Total GPU Power : 67.54756353158866 W\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for RAM : 10.133819 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for all CPUs : 0.098888 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:53] 0.676601 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for all GPUs : 0.903808 kWh. Total GPU Power : 67.53606005727613 W\n",
      "[codecarbon INFO @ 16:51:53] Energy consumed for all CPUs : 1.888842 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:51:53] 12.926468 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for RAM : 0.543009 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for all GPUs : 0.048654 kWh. Total GPU Power : 67.0903819405232 W\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for all CPUs : 0.101282 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:08] 0.692945 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for RAM : 10.121319 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for all GPUs : 0.902605 kWh. Total GPU Power : 67.09883890773446 W\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for all CPUs : 1.886464 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:08] 12.910389 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for RAM : 0.533415 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for RAM : 10.136948 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for all GPUs : 0.047707 kWh. Total GPU Power : 67.09232659745807 W\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for all CPUs : 0.099471 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:08] 0.680593 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for all GPUs : 0.904088 kWh. Total GPU Power : 67.08420291847474 W\n",
      "[codecarbon INFO @ 16:52:08] Energy consumed for all CPUs : 1.889425 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:08] 12.930461 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for RAM : 0.546138 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for all GPUs : 0.048934 kWh. Total GPU Power : 67.13618824708082 W\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for all CPUs : 0.101866 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:23] 0.696937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for RAM : 10.124449 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for all GPUs : 0.902885 kWh. Total GPU Power : 67.12988498804832 W\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for all CPUs : 1.887047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:23] 12.914381 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for RAM : 0.536544 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for RAM : 10.140077 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for all GPUs : 0.047986 kWh. Total GPU Power : 67.14079418926794 W\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for all CPUs : 0.100054 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:23] 0.684585 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for all GPUs : 0.904367 kWh. Total GPU Power : 67.13709725945772 W\n",
      "[codecarbon INFO @ 16:52:23] Energy consumed for all CPUs : 1.890008 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:23] 12.934453 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for RAM : 0.549268 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for all GPUs : 0.049215 kWh. Total GPU Power : 67.57361753529122 W\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for all CPUs : 0.102449 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:38] 0.700932 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for RAM : 10.127579 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for all GPUs : 0.903165 kWh. Total GPU Power : 67.1505793461472 W\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for all CPUs : 1.887631 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:38] 12.918374 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for RAM : 0.539674 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for RAM : 10.143207 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for all GPUs : 0.048266 kWh. Total GPU Power : 67.15938828794569 W\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for all CPUs : 0.100638 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:38] 0.688578 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for all GPUs : 0.904649 kWh. Total GPU Power : 67.60552917761483 W\n",
      "[codecarbon INFO @ 16:52:38] Energy consumed for all CPUs : 1.890592 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:38] 12.938447 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for RAM : 0.552396 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for all GPUs : 0.049495 kWh. Total GPU Power : 67.23382468244804 W\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for all CPUs : 0.103032 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:53] 0.704923 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for RAM : 10.130708 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for all GPUs : 0.903446 kWh. Total GPU Power : 67.65532563795539 W\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for all CPUs : 1.888214 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:53] 12.922369 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for RAM : 0.542803 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for all GPUs : 0.048548 kWh. Total GPU Power : 67.66718180288926 W\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for all CPUs : 0.101221 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:53] 0.692572 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for RAM : 10.146337 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for all GPUs : 0.904929 kWh. Total GPU Power : 67.19227264446506 W\n",
      "[codecarbon INFO @ 16:52:53] Energy consumed for all CPUs : 1.891175 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:52:53] 12.942441 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for RAM : 0.555525 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for all GPUs : 0.049775 kWh. Total GPU Power : 67.19960912901918 W\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for all CPUs : 0.103616 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:53:08] 0.708915 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for RAM : 10.133838 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for all GPUs : 0.903726 kWh. Total GPU Power : 67.19726379057953 W\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for all CPUs : 1.888797 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:53:08] 12.926362 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for RAM : 0.545933 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for RAM : 10.149466 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for all GPUs : 0.048828 kWh. Total GPU Power : 67.1958278838769 W\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for all CPUs : 0.101804 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:53:08] 0.696565 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for all GPUs : 0.905209 kWh. Total GPU Power : 67.20692249865779 W\n",
      "[codecarbon INFO @ 16:53:08] Energy consumed for all CPUs : 1.891758 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:53:08] 12.946433 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_split_percentage = 0.1\n",
    "\n",
    "split_dataset_logger = transformers.utils.logging.get_logger(\n",
    "    \"transformers.tokenization_utils_base\"\n",
    ")\n",
    "with CaptureLogger(split_dataset_logger) as cl:\n",
    "    train_dataset = lm_datasets['train'].train_test_split(\n",
    "        test_size=validation_split_percentage\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1247\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:21:53] Energy consumed for RAM : 0.056126 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:21:53] Energy consumed for all GPUs : 0.005090 kWh. Total GPU Power : 66.44820301387114 W\n",
      "[codecarbon INFO @ 16:21:53] Energy consumed for all CPUs : 0.010516 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:21:53] 0.071732 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1717658191701,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "# Wikipedia provides a title and an article text.\n",
    "# Use https://translate.google.com!\n",
    "alpaca_prompt = \"\"\"এখানে একটি নির্দেশনা দেওয়া হলো, যা একটি কাজ সম্পন্ন করার উপায় বর্ণনা করে, এবং এর সাথে একটি ইনপুট দেওয়া হলো যা আরও প্রেক্ষাপট প্রদান করে। একটি উত্তর লিখুন যা অনুরোধটি সঠিকভাবে পূরণ করে। \n",
    "### Instruction: {}\n",
    "\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "# becomes:\n",
    "wikipedia_prompt = \"\"\"উইকিপিডিয়া নিবন্ধ\n",
    "### শিরোনাম: {}\n",
    "\n",
    "### নিবন্ধ:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(convo[0][\"value\"], convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw0tuebw4Ppe"
   },
   "source": [
    "We only use 1% of the dataset to speed things up! Use more for longer runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "930c73ca9b5043e5aa1d301d4385eb49",
      "264288b1c4834e8f9691937bdbf45e6e",
      "bd62e2873ee149a38f0af8ece178f12b",
      "ddfd080832364b008d4c9d5e87adb423",
      "d98761db8080484885285be9d309f761",
      "282d5aa660744eecacb972f345c61c65",
      "0dc39eb1468d48dba66e4d6b403a7085",
      "e2b60d8ee8d44f42b915088d7deeed77",
      "993076c1db464de1b7e34faff22b816a",
      "8fabd167d1fb498bbe4b304580744580",
      "964e02c946804bdda9c0b3eef4ca0e7a",
      "0abb70b59181406a9c53f2fba05a9396",
      "9b95d3952f9f46b3ab5d11a630fb4ab9",
      "9bab4b07eafe4d8b97340a6989755b3a",
      "467126b2609e4b869c6c98f5e0fc7164",
      "f1e7a220b6c14349b5ec041d1cce8889",
      "ff68b5bdadc143deae98570d99a45bd3",
      "5e74c24afd604b98b7b5b2083c90209e",
      "8af4fb16f6ce452eb31c8e2eb8b1b394",
      "64e8dffcea054a5a8382283c3907443e",
      "f22c1b1564234208b5a6a183c5a3bc99",
      "a8b8c2a5bfeb49658dd631fea2398907",
      "a8c0b5125d83425d95697ad677eb8738",
      "380e6a9baa4141a39ee11b7084c8b5d1",
      "da4b89a73b6b4963888688cdd2a12a25",
      "e43305a97dc64e4ea8bd507df6667fbd",
      "f7060dd3107d49c5b78b5a71051c0b98",
      "321bfe67e9164827b768be75ad88bfa7",
      "df1201b85a004294a85925204b9b6438",
      "99a6d2f786504b819609d2062accdd4d",
      "6e7a3e3b3e0e4b4a825e4905d3d17808",
      "6fd423f508284764b6e4858dd807ac75",
      "af7f3773cf0948b0b9ebd92e76aa1f39",
      "358ad09a3c4f40f0ab91c20ab2723b64",
      "f63b6cc3452e46009e83dbaeb781789b",
      "9dc17414097148179f7f2199e3d6197b",
      "74a8f0039e2e4dcdaf49eb623913a3eb",
      "2fa91170684542b79d0b2f18692151cf",
      "bce90326f07f4458bf694da20f8c8f00",
      "7d739e3b12b644bcb9777ae231f55347",
      "065c41d2f95d4980a3379c179e9bf304",
      "3e33deff84ee42f88995a7e99869e665",
      "7b735d6ece794fbea65a3260198c5476",
      "9272b81465a540a5bf6d3d63b4be4605",
      "7acf551e9f534548a87e3b1a251ffbb8",
      "34fa830fe975482cb58bed65ccac69b9",
      "532ef4493746407faee2ccbb8f0c3e17",
      "df1b49fb70ab495888f5b40b8996512c",
      "0a4a905e9ca14b23b8134f4ee1dd51a6",
      "04744ff89533496dbe9bc442b25fde47",
      "8fbdee4d77014de4ae160170f6e336d9",
      "ce1fb1b218b34e63b763bf708b729679",
      "fc91d0bcafa746cfa509e365fadcafe8",
      "a518acc761834cb498f0cce4109eaa8f",
      "7711ba2b9c82489baefee96b7c64eba6"
     ]
    },
    "executionInfo": {
     "elapsed": 29415,
     "status": "ok",
     "timestamp": 1717658221111,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "EsKrpkza3VB3",
    "outputId": "73bc1361-7df2-4e04-f9d5-b662a308a513"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.bn\", split = \"train\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'uonlp/CulturaX' #french novels\n",
    "dataset = load_dataset(dataset_name, \"bn\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We select 1% of the data to make training faster!\n",
    "#dataset = dataset.train_test_split(train_size = 0.01)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'timestamp', 'url', 'source'],\n",
       "    num_rows: 12436596\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to count total tokens in the dataset\n",
    "def count_total_tokens(dataset, tokenizer, text_column=\"text\"):\n",
    "    total_tokens = 0\n",
    "    for example in tqdm(dataset[\"train\"], desc=\"Counting tokens\"):\n",
    "        text = example[text_column]\n",
    "        tokens = tokenizer(text)[\"input_ids\"]\n",
    "        total_tokens += len(tokens)\n",
    "    return total_tokens\n",
    "\n",
    "# Count total tokens in the dataset\n",
    "total_tokens = count_total_tokens(dataset, tokenizer)\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to tokenize and count tokens in batches\n",
    "def count_tokens(batch):\n",
    "    tokens = tokenizer(batch[\"text\"], truncation=True, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return {\"num_tokens\": [len(token_list) for token_list in tokens]}\n",
    "\n",
    "# Map the function to the dataset using multiple processes\n",
    "tokenized_dataset = dataset[\"train\"].map(count_tokens, batched=True, num_proc=8)\n",
    "\n",
    "# Sum up the total number of tokens\n",
    "total_tokens = sum(tokenized_dataset[\"num_tokens\"])\n",
    "\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 6476141\n",
       "})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:10:19] Energy consumed for RAM : 9.598382 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:19] Energy consumed for all GPUs : 0.855837 kWh. Total GPU Power : 67.5072977161755 W\n",
      "[codecarbon INFO @ 16:10:19] Energy consumed for all CPUs : 1.788959 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:19] 12.243178 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:10:27] Energy consumed for RAM : 0.021803 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:27] Energy consumed for all GPUs : 0.002034 kWh. Total GPU Power : 67.04426018581792 W\n",
      "[codecarbon INFO @ 16:10:27] Energy consumed for all CPUs : 0.004086 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:27] 0.027922 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 9951012\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:10:33] Energy consumed for RAM : 9.617023 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:33] Energy consumed for all GPUs : 0.857589 kWh. Total GPU Power : 67.04366064825808 W\n",
      "[codecarbon INFO @ 16:10:33] Energy consumed for all CPUs : 1.792472 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:33] 12.267084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:10:34] Energy consumed for RAM : 9.601512 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:34] Energy consumed for all GPUs : 0.856116 kWh. Total GPU Power : 67.0373968143551 W\n",
      "[codecarbon INFO @ 16:10:34] Energy consumed for all CPUs : 1.789543 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:34] 12.247170 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:10:42] Energy consumed for RAM : 0.024932 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:42] Energy consumed for all GPUs : 0.002315 kWh. Total GPU Power : 67.478597342173 W\n",
      "[codecarbon INFO @ 16:10:42] Energy consumed for all CPUs : 0.004669 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:42] 0.031915 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:10:44] Energy consumed for RAM : 0.015650 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:44] Energy consumed for all GPUs : 0.001399 kWh. Total GPU Power : 67.0316984466633 W\n",
      "[codecarbon INFO @ 16:10:44] Energy consumed for all CPUs : 0.002917 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:44] 0.019966 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:10:48] Energy consumed for RAM : 9.620153 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:48] Energy consumed for all GPUs : 0.857868 kWh. Total GPU Power : 67.0313404591174 W\n",
      "[codecarbon INFO @ 16:10:48] Energy consumed for all CPUs : 1.793056 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:48] 12.271077 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:10:49] Energy consumed for RAM : 9.604641 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:49] Energy consumed for all GPUs : 0.856395 kWh. Total GPU Power : 67.03804031327735 W\n",
      "[codecarbon INFO @ 16:10:49] Energy consumed for all CPUs : 1.790126 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:49] 12.251162 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:10:57] Energy consumed for RAM : 0.028061 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:57] Energy consumed for all GPUs : 0.002594 kWh. Total GPU Power : 67.06214376742038 W\n",
      "[codecarbon INFO @ 16:10:57] Energy consumed for all CPUs : 0.005252 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:57] 0.035908 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:10:59] Energy consumed for RAM : 0.018780 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:10:59] Energy consumed for all GPUs : 0.001679 kWh. Total GPU Power : 67.05382074557069 W\n",
      "[codecarbon INFO @ 16:10:59] Energy consumed for all CPUs : 0.003500 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:10:59] 0.023958 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:11:03] Energy consumed for RAM : 9.623282 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:11:03] Energy consumed for all GPUs : 0.858149 kWh. Total GPU Power : 67.50087967757456 W\n",
      "[codecarbon INFO @ 16:11:03] Energy consumed for all CPUs : 1.793639 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:11:03] 12.275071 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:11:04] Energy consumed for RAM : 9.607770 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:11:04] Energy consumed for all GPUs : 0.856676 kWh. Total GPU Power : 67.51036137568488 W\n",
      "[codecarbon INFO @ 16:11:04] Energy consumed for all CPUs : 1.790709 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:11:04] 12.255156 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 22:22:24] Energy consumed for RAM : 22.687692 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:22:24] Energy consumed for all GPUs : 2.021022 kWh. Total GPU Power : 67.72513135244579 W\n",
      "[codecarbon INFO @ 22:22:24] Energy consumed for all CPUs : 4.229378 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:22:24] 28.938092 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:22:33] Energy consumed for RAM : 32.278050 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:22:33] Energy consumed for all GPUs : 2.876102 kWh. Total GPU Power : 66.90788957774274 W\n",
      "[codecarbon INFO @ 22:22:33] Energy consumed for all CPUs : 6.016795 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:22:33] 41.170947 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:22:36] Energy consumed for RAM : 22.699681 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:22:36] Energy consumed for all GPUs : 2.022193 kWh. Total GPU Power : 66.91496925831042 W\n",
      "[codecarbon INFO @ 22:22:36] Energy consumed for all CPUs : 4.231706 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:22:36] 28.953580 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:22:37] Energy consumed for RAM : 32.294123 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:22:37] Energy consumed for all GPUs : 2.877644 kWh. Total GPU Power : 66.91555738583796 W\n",
      "[codecarbon INFO @ 22:22:37] Energy consumed for all CPUs : 6.019826 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:22:37] 41.191592 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:22:39] Energy consumed for RAM : 22.690822 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:22:39] Energy consumed for all GPUs : 2.021298 kWh. Total GPU Power : 66.46763425014774 W\n",
      "[codecarbon INFO @ 22:22:39] Energy consumed for all CPUs : 4.229961 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:22:39] 28.942082 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "def get_length(examples):\n",
    "    try:\n",
    "        result = {}\n",
    "        result['text_length'] = []\n",
    "        result['token_length'] = []\n",
    "        for text in examples[\"text\"]:\n",
    "            result['text_length'].append(len(text))\n",
    "        for text in examples[\"text\"]:\n",
    "            token = tokenizer(text)\n",
    "            tlen = len(token['input_ids']) + len(token['attention_mask'])\n",
    "            result['token_length'].append(tlen)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_length: {str(e)}\")\n",
    "        print(f\"Examples causing error: {examples}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:11:27] Energy consumed for RAM : 0.034321 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:11:27] Energy consumed for all GPUs : 0.003153 kWh. Total GPU Power : 67.30478460899391 W\n",
      "[codecarbon INFO @ 16:11:27] Energy consumed for all CPUs : 0.006419 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:11:27] 0.043893 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:11:29] Energy consumed for RAM : 0.025038 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:11:29] Energy consumed for all GPUs : 0.002238 kWh. Total GPU Power : 66.80652864578875 W\n",
      "[codecarbon INFO @ 16:11:29] Energy consumed for all CPUs : 0.004667 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:11:29] 0.031943 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "# Set tqdm to display progress bars\n",
    "tqdm.pandas()\n",
    "\n",
    "num_proc = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ff46ba74b64203973fc8dfdf1aa667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating text and token lengths (num_proc=32):   0%|          | 0/9951012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8218 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8542 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11335 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11897 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28205 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11502 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11622 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9009 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18409 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10069 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23890 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19924 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10859 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9357 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20715 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24864 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15117 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14398 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10834 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11186 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9895 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11024 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8458 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11315 > 8192). Running this sequence through the model will result in indexing errors\n",
      "[codecarbon INFO @ 22:22:48] Energy consumed for RAM : 32.281179 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:22:48] Energy consumed for all GPUs : 2.876379 kWh. Total GPU Power : 66.46916111689099 W\n",
      "[codecarbon INFO @ 22:22:48] Energy consumed for all CPUs : 6.017379 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:22:48] 41.174937 kWh of electricity used since the beginning.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30735 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12754 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10239 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10725 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9347 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9078 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33165 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9687 > 8192). Running this sequence through the model will result in indexing errors\n",
      "[codecarbon INFO @ 22:22:51] Energy consumed for RAM : 22.702810 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:22:51] Energy consumed for all GPUs : 2.022470 kWh. Total GPU Power : 66.46898686118936 W\n",
      "[codecarbon INFO @ 22:22:51] Energy consumed for all CPUs : 4.232289 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:22:51] 28.957570 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:22:52] Energy consumed for RAM : 32.297252 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:22:52] Energy consumed for all GPUs : 2.877921 kWh. Total GPU Power : 66.46735227533146 W\n",
      "[codecarbon INFO @ 22:22:52] Energy consumed for all CPUs : 6.020409 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:22:52] 41.195582 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:22:54] Energy consumed for RAM : 22.693956 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:22:54] Energy consumed for all GPUs : 2.021577 kWh. Total GPU Power : 66.82227842241528 W\n",
      "[codecarbon INFO @ 22:22:54] Energy consumed for all CPUs : 4.230545 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:22:54] 28.946078 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:03] Energy consumed for RAM : 32.284309 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:03] Energy consumed for all GPUs : 2.876656 kWh. Total GPU Power : 66.45457127660164 W\n",
      "[codecarbon INFO @ 22:23:03] Energy consumed for all CPUs : 6.017962 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:03] 41.178926 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:06] Energy consumed for RAM : 22.705940 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:06] Energy consumed for all GPUs : 2.022747 kWh. Total GPU Power : 66.44678114135199 W\n",
      "[codecarbon INFO @ 22:23:06] Energy consumed for all CPUs : 4.232872 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:06] 28.961560 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:07] Energy consumed for RAM : 32.300382 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:07] Energy consumed for all GPUs : 2.878197 kWh. Total GPU Power : 66.45878403271337 W\n",
      "[codecarbon INFO @ 22:23:07] Energy consumed for all CPUs : 6.020992 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:07] 41.199571 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:09] Energy consumed for RAM : 22.697085 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:09] Energy consumed for all GPUs : 2.021854 kWh. Total GPU Power : 66.44888682496105 W\n",
      "[codecarbon INFO @ 22:23:09] Energy consumed for all CPUs : 4.231129 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:09] 28.950068 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:18] Energy consumed for RAM : 32.287438 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:18] Energy consumed for all GPUs : 2.876934 kWh. Total GPU Power : 66.90153365458082 W\n",
      "[codecarbon INFO @ 22:23:18] Energy consumed for all CPUs : 6.018545 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:18] 41.182918 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:21] Energy consumed for RAM : 22.709070 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:21] Energy consumed for all GPUs : 2.023024 kWh. Total GPU Power : 66.45519360688796 W\n",
      "[codecarbon INFO @ 22:23:21] Energy consumed for all CPUs : 4.233456 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:21] 28.965549 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:22] Energy consumed for RAM : 32.303511 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:22] Energy consumed for all GPUs : 2.878474 kWh. Total GPU Power : 66.45455036794723 W\n",
      "[codecarbon INFO @ 22:23:22] Energy consumed for all CPUs : 6.021575 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:22] 41.203561 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:24] Energy consumed for RAM : 22.700215 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:24] Energy consumed for all GPUs : 2.022131 kWh. Total GPU Power : 66.46901665556541 W\n",
      "[codecarbon INFO @ 22:23:24] Energy consumed for all CPUs : 4.231712 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:24] 28.954057 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:33] Energy consumed for RAM : 32.290568 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:33] Energy consumed for all GPUs : 2.877211 kWh. Total GPU Power : 66.4555041683803 W\n",
      "[codecarbon INFO @ 22:23:33] Energy consumed for all CPUs : 6.019129 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:33] 41.186908 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:36] Energy consumed for RAM : 22.712200 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:36] Energy consumed for all GPUs : 2.023303 kWh. Total GPU Power : 66.90426469556927 W\n",
      "[codecarbon INFO @ 22:23:36] Energy consumed for all CPUs : 4.234039 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:36] 28.969541 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:37] Energy consumed for RAM : 32.306641 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:37] Energy consumed for all GPUs : 2.878753 kWh. Total GPU Power : 66.90121461444816 W\n",
      "[codecarbon INFO @ 22:23:37] Energy consumed for all CPUs : 6.022159 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:37] 41.207553 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:39] Energy consumed for RAM : 22.703344 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:39] Energy consumed for all GPUs : 2.022408 kWh. Total GPU Power : 66.44969029860934 W\n",
      "[codecarbon INFO @ 22:23:39] Energy consumed for all CPUs : 4.232295 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:39] 28.958047 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:48] Energy consumed for RAM : 32.293713 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:48] Energy consumed for all GPUs : 2.877490 kWh. Total GPU Power : 66.58647631086406 W\n",
      "[codecarbon INFO @ 22:23:48] Energy consumed for all CPUs : 6.019715 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:48] 41.190917 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:51] Energy consumed for RAM : 22.715329 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:51] Energy consumed for all GPUs : 2.023579 kWh. Total GPU Power : 66.45613729667387 W\n",
      "[codecarbon INFO @ 22:23:51] Energy consumed for all CPUs : 4.234622 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:51] 28.973531 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:52] Energy consumed for RAM : 32.309770 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:52] Energy consumed for all GPUs : 2.879030 kWh. Total GPU Power : 66.46360060980817 W\n",
      "[codecarbon INFO @ 22:23:52] Energy consumed for all CPUs : 6.022742 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:52] 41.211542 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:23:54] Energy consumed for RAM : 22.706474 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:23:54] Energy consumed for all GPUs : 2.022686 kWh. Total GPU Power : 66.90950548910159 W\n",
      "[codecarbon INFO @ 22:23:54] Energy consumed for all CPUs : 4.232879 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:23:54] 28.962039 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:03] Energy consumed for RAM : 32.296842 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:03] Energy consumed for all GPUs : 2.877767 kWh. Total GPU Power : 66.45554047706548 W\n",
      "[codecarbon INFO @ 22:24:03] Energy consumed for all CPUs : 6.020298 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:03] 41.194907 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:06] Energy consumed for RAM : 22.718459 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:06] Energy consumed for all GPUs : 2.023856 kWh. Total GPU Power : 66.45741778539447 W\n",
      "[codecarbon INFO @ 22:24:06] Energy consumed for all CPUs : 4.235205 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:06] 28.977521 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:07] Energy consumed for RAM : 32.312900 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:07] Energy consumed for all GPUs : 2.879307 kWh. Total GPU Power : 66.45808250834165 W\n",
      "[codecarbon INFO @ 22:24:07] Energy consumed for all CPUs : 6.023325 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:07] 41.215532 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:09] Energy consumed for RAM : 22.709604 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:09] Energy consumed for all GPUs : 2.022963 kWh. Total GPU Power : 66.45747472990756 W\n",
      "[codecarbon INFO @ 22:24:09] Energy consumed for all CPUs : 4.233462 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:09] 28.966028 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:18] Energy consumed for RAM : 32.299972 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:18] Energy consumed for all GPUs : 2.878045 kWh. Total GPU Power : 66.91160451404338 W\n",
      "[codecarbon INFO @ 22:24:18] Energy consumed for all CPUs : 6.020881 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:18] 41.198899 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:21] Energy consumed for RAM : 22.721589 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:21] Energy consumed for all GPUs : 2.024135 kWh. Total GPU Power : 66.90261300624185 W\n",
      "[codecarbon INFO @ 22:24:21] Energy consumed for all CPUs : 4.235789 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:21] 28.981513 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:22] Energy consumed for RAM : 32.316030 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:22] Energy consumed for all GPUs : 2.879585 kWh. Total GPU Power : 66.90454261143981 W\n",
      "[codecarbon INFO @ 22:24:22] Energy consumed for all CPUs : 6.023909 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:22] 41.219524 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:24] Energy consumed for RAM : 22.712733 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:24] Energy consumed for all GPUs : 2.023240 kWh. Total GPU Power : 66.45476021127504 W\n",
      "[codecarbon INFO @ 22:24:24] Energy consumed for all CPUs : 4.234045 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:24] 28.970018 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:33] Energy consumed for RAM : 32.303102 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:33] Energy consumed for all GPUs : 2.878322 kWh. Total GPU Power : 66.44710663547389 W\n",
      "[codecarbon INFO @ 22:24:33] Energy consumed for all CPUs : 6.021464 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:33] 41.202889 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:36] Energy consumed for RAM : 22.724718 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:36] Energy consumed for all GPUs : 2.024412 kWh. Total GPU Power : 66.45101025732428 W\n",
      "[codecarbon INFO @ 22:24:36] Energy consumed for all CPUs : 4.236372 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:36] 28.985502 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:37] Energy consumed for RAM : 32.319159 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:37] Energy consumed for all GPUs : 2.879862 kWh. Total GPU Power : 66.44706938329614 W\n",
      "[codecarbon INFO @ 22:24:37] Energy consumed for all CPUs : 6.024492 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:37] 41.223513 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:39] Energy consumed for RAM : 22.715863 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:39] Energy consumed for all GPUs : 2.023518 kWh. Total GPU Power : 66.88874901146956 W\n",
      "[codecarbon INFO @ 22:24:39] Energy consumed for all CPUs : 4.234629 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:39] 28.974010 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:48] Energy consumed for RAM : 32.306232 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:48] Energy consumed for all GPUs : 2.878599 kWh. Total GPU Power : 66.44043774963217 W\n",
      "[codecarbon INFO @ 22:24:48] Energy consumed for all CPUs : 6.022048 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:48] 41.206879 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:51] Energy consumed for RAM : 22.727848 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:51] Energy consumed for all GPUs : 2.024689 kWh. Total GPU Power : 66.44937738611256 W\n",
      "[codecarbon INFO @ 22:24:51] Energy consumed for all CPUs : 4.236955 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:51] 28.989492 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:52] Energy consumed for RAM : 32.322289 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:52] Energy consumed for all GPUs : 2.880139 kWh. Total GPU Power : 66.45478525401283 W\n",
      "[codecarbon INFO @ 22:24:52] Energy consumed for all CPUs : 6.025075 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:52] 41.227503 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:54] Energy consumed for RAM : 22.718993 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:24:54] Energy consumed for all GPUs : 2.023795 kWh. Total GPU Power : 66.45679323275519 W\n",
      "[codecarbon INFO @ 22:24:54] Energy consumed for all CPUs : 4.235212 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:24:54] 28.978000 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:03] Energy consumed for RAM : 32.309361 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:03] Energy consumed for all GPUs : 2.878876 kWh. Total GPU Power : 66.4689880761673 W\n",
      "[codecarbon INFO @ 22:25:03] Energy consumed for all CPUs : 6.022631 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:03] 41.210868 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:06] Energy consumed for RAM : 22.730977 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:06] Energy consumed for all GPUs : 2.024967 kWh. Total GPU Power : 66.90943552471508 W\n",
      "[codecarbon INFO @ 22:25:06] Energy consumed for all CPUs : 4.237539 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:06] 28.993483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:07] Energy consumed for RAM : 32.325419 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:07] Energy consumed for all GPUs : 2.880418 kWh. Total GPU Power : 66.91060801037436 W\n",
      "[codecarbon INFO @ 22:25:07] Energy consumed for all CPUs : 6.025659 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:07] 41.231495 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:09] Energy consumed for RAM : 22.722122 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:09] Energy consumed for all GPUs : 2.024072 kWh. Total GPU Power : 66.45930205179678 W\n",
      "[codecarbon INFO @ 22:25:09] Energy consumed for all CPUs : 4.235795 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:09] 28.981990 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:18] Energy consumed for RAM : 32.312491 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:18] Energy consumed for all GPUs : 2.879155 kWh. Total GPU Power : 66.89096752566066 W\n",
      "[codecarbon INFO @ 22:25:18] Energy consumed for all CPUs : 6.023214 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:18] 41.214860 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:21] Energy consumed for RAM : 22.734107 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:21] Energy consumed for all GPUs : 2.025244 kWh. Total GPU Power : 66.45647357487799 W\n",
      "[codecarbon INFO @ 22:25:21] Energy consumed for all CPUs : 4.238122 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:21] 28.997473 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:22] Energy consumed for RAM : 32.328548 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:22] Energy consumed for all GPUs : 2.880694 kWh. Total GPU Power : 66.44892258199702 W\n",
      "[codecarbon INFO @ 22:25:22] Energy consumed for all CPUs : 6.026242 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:22] 41.235484 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:24] Energy consumed for RAM : 22.725252 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:24] Energy consumed for all GPUs : 2.024351 kWh. Total GPU Power : 66.89428873218358 W\n",
      "[codecarbon INFO @ 22:25:24] Energy consumed for all CPUs : 4.236378 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:24] 28.985981 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:33] Energy consumed for RAM : 32.315620 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:33] Energy consumed for all GPUs : 2.879431 kWh. Total GPU Power : 66.4470822051721 W\n",
      "[codecarbon INFO @ 22:25:33] Energy consumed for all CPUs : 6.023798 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:33] 41.218849 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:36] Energy consumed for RAM : 22.737236 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:36] Energy consumed for all GPUs : 2.025521 kWh. Total GPU Power : 66.4486633075726 W\n",
      "[codecarbon INFO @ 22:25:36] Energy consumed for all CPUs : 4.238705 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:36] 29.001462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:37] Energy consumed for RAM : 32.331678 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:37] Energy consumed for all GPUs : 2.880971 kWh. Total GPU Power : 66.45189910950994 W\n",
      "[codecarbon INFO @ 22:25:37] Energy consumed for all CPUs : 6.026825 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:37] 41.239474 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:39] Energy consumed for RAM : 22.728382 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:39] Energy consumed for all GPUs : 2.024628 kWh. Total GPU Power : 66.45104539617404 W\n",
      "[codecarbon INFO @ 22:25:39] Energy consumed for all CPUs : 4.236962 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:39] 28.989971 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:48] Energy consumed for RAM : 32.318750 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:48] Energy consumed for all GPUs : 2.879708 kWh. Total GPU Power : 66.45622828486047 W\n",
      "[codecarbon INFO @ 22:25:48] Energy consumed for all CPUs : 6.024381 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:48] 41.222839 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:51] Energy consumed for RAM : 22.740366 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:51] Energy consumed for all GPUs : 2.025799 kWh. Total GPU Power : 66.89693259537559 W\n",
      "[codecarbon INFO @ 22:25:51] Energy consumed for all CPUs : 4.239289 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:51] 29.005454 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:52] Energy consumed for RAM : 32.334807 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:52] Energy consumed for all GPUs : 2.881248 kWh. Total GPU Power : 66.45148509636304 W\n",
      "[codecarbon INFO @ 22:25:52] Energy consumed for all CPUs : 6.027408 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:52] 41.243463 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:25:54] Energy consumed for RAM : 22.731511 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:25:54] Energy consumed for all GPUs : 2.024904 kWh. Total GPU Power : 66.44185116377575 W\n",
      "[codecarbon INFO @ 22:25:54] Energy consumed for all CPUs : 4.237545 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:25:54] 28.993961 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:03] Energy consumed for RAM : 32.321879 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:03] Energy consumed for all GPUs : 2.879987 kWh. Total GPU Power : 66.90362196857502 W\n",
      "[codecarbon INFO @ 22:26:03] Energy consumed for all CPUs : 6.024964 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:03] 41.226831 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:06] Energy consumed for RAM : 22.743495 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:06] Energy consumed for all GPUs : 2.026076 kWh. Total GPU Power : 66.46522040181445 W\n",
      "[codecarbon INFO @ 22:26:06] Energy consumed for all CPUs : 4.239872 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:06] 29.009443 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:07] Energy consumed for RAM : 32.337937 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:07] Energy consumed for all GPUs : 2.881527 kWh. Total GPU Power : 66.90696756238286 W\n",
      "[codecarbon INFO @ 22:26:07] Energy consumed for all CPUs : 6.027992 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:07] 41.247455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:09] Energy consumed for RAM : 22.734640 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:09] Energy consumed for all GPUs : 2.025183 kWh. Total GPU Power : 66.90856716601895 W\n",
      "[codecarbon INFO @ 22:26:09] Energy consumed for all CPUs : 4.238128 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:09] 28.997952 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:18] Energy consumed for RAM : 32.325009 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:18] Energy consumed for all GPUs : 2.880264 kWh. Total GPU Power : 66.44433406475486 W\n",
      "[codecarbon INFO @ 22:26:18] Energy consumed for all CPUs : 6.025548 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:18] 41.230820 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:21] Energy consumed for RAM : 22.746624 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:21] Energy consumed for all GPUs : 2.026353 kWh. Total GPU Power : 66.45372112437819 W\n",
      "[codecarbon INFO @ 22:26:21] Energy consumed for all CPUs : 4.240455 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:21] 29.013433 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:22] Energy consumed for RAM : 32.341066 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:22] Energy consumed for all GPUs : 2.881803 kWh. Total GPU Power : 66.4537333099872 W\n",
      "[codecarbon INFO @ 22:26:22] Energy consumed for all CPUs : 6.028575 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:22] 41.251445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:24] Energy consumed for RAM : 22.737770 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:24] Energy consumed for all GPUs : 2.025460 kWh. Total GPU Power : 66.45140168973202 W\n",
      "[codecarbon INFO @ 22:26:24] Energy consumed for all CPUs : 4.238712 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:24] 29.001942 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:33] Energy consumed for RAM : 32.328139 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:33] Energy consumed for all GPUs : 2.880540 kWh. Total GPU Power : 66.45592010741257 W\n",
      "[codecarbon INFO @ 22:26:33] Energy consumed for all CPUs : 6.026131 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:33] 41.234810 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:36] Energy consumed for RAM : 22.749754 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:36] Energy consumed for all GPUs : 2.026630 kWh. Total GPU Power : 66.44970489393062 W\n",
      "[codecarbon INFO @ 22:26:36] Energy consumed for all CPUs : 4.241038 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:36] 29.017423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:37] Energy consumed for RAM : 32.344196 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:37] Energy consumed for all GPUs : 2.882080 kWh. Total GPU Power : 66.45337540814106 W\n",
      "[codecarbon INFO @ 22:26:37] Energy consumed for all CPUs : 6.029158 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:37] 41.255434 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:39] Energy consumed for RAM : 22.740899 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:39] Energy consumed for all GPUs : 2.025737 kWh. Total GPU Power : 66.4666800056884 W\n",
      "[codecarbon INFO @ 22:26:39] Energy consumed for all CPUs : 4.239295 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:39] 29.005931 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:48] Energy consumed for RAM : 32.331268 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:48] Energy consumed for all GPUs : 2.880819 kWh. Total GPU Power : 66.8988087472346 W\n",
      "[codecarbon INFO @ 22:26:48] Energy consumed for all CPUs : 6.026714 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:48] 41.238801 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:51] Energy consumed for RAM : 22.752884 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:51] Energy consumed for all GPUs : 2.026909 kWh. Total GPU Power : 66.89396901159517 W\n",
      "[codecarbon INFO @ 22:26:51] Energy consumed for all CPUs : 4.241622 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:51] 29.021414 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:52] Energy consumed for RAM : 32.347325 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:52] Energy consumed for all GPUs : 2.882359 kWh. Total GPU Power : 66.89512813239705 W\n",
      "[codecarbon INFO @ 22:26:52] Energy consumed for all CPUs : 6.029742 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:52] 41.259426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:54] Energy consumed for RAM : 22.744029 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:26:54] Energy consumed for all GPUs : 2.026013 kWh. Total GPU Power : 66.45026684471662 W\n",
      "[codecarbon INFO @ 22:26:54] Energy consumed for all CPUs : 4.239878 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:26:54] 29.009921 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:03] Energy consumed for RAM : 32.334398 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:03] Energy consumed for all GPUs : 2.881096 kWh. Total GPU Power : 66.44521107539052 W\n",
      "[codecarbon INFO @ 22:27:03] Energy consumed for all CPUs : 6.027298 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:03] 41.242791 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:06] Energy consumed for RAM : 22.756014 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:06] Energy consumed for all GPUs : 2.027185 kWh. Total GPU Power : 66.45302259377307 W\n",
      "[codecarbon INFO @ 22:27:06] Energy consumed for all CPUs : 4.242205 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:06] 29.025404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:07] Energy consumed for RAM : 32.350455 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:07] Energy consumed for all GPUs : 2.882636 kWh. Total GPU Power : 66.45104097274763 W\n",
      "[codecarbon INFO @ 22:27:07] Energy consumed for all CPUs : 6.030325 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:07] 41.263416 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:09] Energy consumed for RAM : 22.747158 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:09] Energy consumed for all GPUs : 2.026292 kWh. Total GPU Power : 66.90225194116168 W\n",
      "[codecarbon INFO @ 22:27:09] Energy consumed for all CPUs : 4.240462 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:09] 29.013912 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:18] Energy consumed for RAM : 32.337527 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:18] Energy consumed for all GPUs : 2.881373 kWh. Total GPU Power : 66.46673205058163 W\n",
      "[codecarbon INFO @ 22:27:18] Energy consumed for all CPUs : 6.027881 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:18] 41.246781 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:21] Energy consumed for RAM : 22.759143 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:21] Energy consumed for all GPUs : 2.027462 kWh. Total GPU Power : 66.46039295123772 W\n",
      "[codecarbon INFO @ 22:27:21] Energy consumed for all CPUs : 4.242788 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:21] 29.029394 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:22] Energy consumed for RAM : 32.353585 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:22] Energy consumed for all GPUs : 2.882912 kWh. Total GPU Power : 66.46271394430543 W\n",
      "[codecarbon INFO @ 22:27:22] Energy consumed for all CPUs : 6.030908 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:22] 41.267406 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:24] Energy consumed for RAM : 22.750288 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:24] Energy consumed for all GPUs : 2.026569 kWh. Total GPU Power : 66.46919982395843 W\n",
      "[codecarbon INFO @ 22:27:24] Energy consumed for all CPUs : 4.241045 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:24] 29.017902 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:33] Energy consumed for RAM : 32.340657 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:33] Energy consumed for all GPUs : 2.881652 kWh. Total GPU Power : 66.97900368106404 W\n",
      "[codecarbon INFO @ 22:27:33] Energy consumed for all CPUs : 6.028464 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:33] 41.250773 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:36] Energy consumed for RAM : 22.762260 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:36] Energy consumed for all GPUs : 2.027741 kWh. Total GPU Power : 67.2624062767842 W\n",
      "[codecarbon INFO @ 22:27:36] Energy consumed for all CPUs : 4.243369 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:36] 29.033370 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:37] Energy consumed for RAM : 32.356715 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:37] Energy consumed for all GPUs : 2.883191 kWh. Total GPU Power : 66.9762822767273 W\n",
      "[codecarbon INFO @ 22:27:37] Energy consumed for all CPUs : 6.031492 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:37] 41.271398 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:39] Energy consumed for RAM : 22.753417 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:39] Energy consumed for all GPUs : 2.026846 kWh. Total GPU Power : 66.5338575645806 W\n",
      "[codecarbon INFO @ 22:27:39] Energy consumed for all CPUs : 4.241628 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:39] 29.021892 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:48] Energy consumed for RAM : 32.343787 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:48] Energy consumed for all GPUs : 2.881928 kWh. Total GPU Power : 66.46029291707671 W\n",
      "[codecarbon INFO @ 22:27:48] Energy consumed for all CPUs : 6.029047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:48] 41.254763 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:51] Energy consumed for RAM : 22.765389 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:51] Energy consumed for all GPUs : 2.028018 kWh. Total GPU Power : 66.46566347696168 W\n",
      "[codecarbon INFO @ 22:27:51] Energy consumed for all CPUs : 4.243953 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:51] 29.037360 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:52] Energy consumed for RAM : 32.359844 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:52] Energy consumed for all GPUs : 2.883468 kWh. Total GPU Power : 66.46575712595396 W\n",
      "[codecarbon INFO @ 22:27:52] Energy consumed for all CPUs : 6.032075 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:52] 41.275388 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:27:54] Energy consumed for RAM : 22.756547 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:27:54] Energy consumed for all GPUs : 2.027125 kWh. Total GPU Power : 66.92050599941511 W\n",
      "[codecarbon INFO @ 22:27:54] Energy consumed for all CPUs : 4.242211 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:27:54] 29.025883 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:03] Energy consumed for RAM : 32.346916 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:03] Energy consumed for all GPUs : 2.882205 kWh. Total GPU Power : 66.46276217534816 W\n",
      "[codecarbon INFO @ 22:28:03] Energy consumed for all CPUs : 6.029631 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:03] 41.258753 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:06] Energy consumed for RAM : 22.768519 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:06] Energy consumed for all GPUs : 2.028295 kWh. Total GPU Power : 66.4594736190123 W\n",
      "[codecarbon INFO @ 22:28:06] Energy consumed for all CPUs : 4.244536 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:06] 29.041349 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:07] Energy consumed for RAM : 32.362974 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:07] Energy consumed for all GPUs : 2.883747 kWh. Total GPU Power : 66.60247755794819 W\n",
      "[codecarbon INFO @ 22:28:07] Energy consumed for all CPUs : 6.032661 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:07] 41.279382 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:09] Energy consumed for RAM : 22.759677 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:09] Energy consumed for all GPUs : 2.027402 kWh. Total GPU Power : 66.45430862019185 W\n",
      "[codecarbon INFO @ 22:28:09] Energy consumed for all CPUs : 4.242795 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:09] 29.029873 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:18] Energy consumed for RAM : 32.350046 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:18] Energy consumed for all GPUs : 2.882482 kWh. Total GPU Power : 66.46360389396597 W\n",
      "[codecarbon INFO @ 22:28:18] Energy consumed for all CPUs : 6.030214 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:18] 41.262742 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:21] Energy consumed for RAM : 22.771636 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:21] Energy consumed for all GPUs : 2.028574 kWh. Total GPU Power : 67.18496063325372 W\n",
      "[codecarbon INFO @ 22:28:21] Energy consumed for all CPUs : 4.245117 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:21] 29.045326 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:22] Energy consumed for RAM : 32.366089 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:22] Energy consumed for all GPUs : 2.884022 kWh. Total GPU Power : 66.31559558895466 W\n",
      "[codecarbon INFO @ 22:28:22] Energy consumed for all CPUs : 6.033241 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:22] 41.283353 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:24] Energy consumed for RAM : 22.762806 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:24] Energy consumed for all GPUs : 2.027678 kWh. Total GPU Power : 66.45818835422232 W\n",
      "[codecarbon INFO @ 22:28:24] Energy consumed for all CPUs : 4.243378 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:24] 29.033863 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:33] Energy consumed for RAM : 32.353176 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:33] Energy consumed for all GPUs : 2.882761 kWh. Total GPU Power : 66.91748268496173 W\n",
      "[codecarbon INFO @ 22:28:33] Energy consumed for all CPUs : 6.030797 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:33] 41.266734 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:36] Energy consumed for RAM : 22.774765 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:36] Energy consumed for all GPUs : 2.028850 kWh. Total GPU Power : 66.46783608149788 W\n",
      "[codecarbon INFO @ 22:28:36] Energy consumed for all CPUs : 4.245700 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:36] 29.049316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:37] Energy consumed for RAM : 32.369206 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:37] Energy consumed for all GPUs : 2.884301 kWh. Total GPU Power : 67.20147605091572 W\n",
      "[codecarbon INFO @ 22:28:37] Energy consumed for all CPUs : 6.033822 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:37] 41.287329 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:39] Energy consumed for RAM : 22.765936 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:39] Energy consumed for all GPUs : 2.027955 kWh. Total GPU Power : 66.4772203395516 W\n",
      "[codecarbon INFO @ 22:28:39] Energy consumed for all CPUs : 4.243961 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:39] 29.037853 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:48] Energy consumed for RAM : 32.356305 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:48] Energy consumed for all GPUs : 2.883038 kWh. Total GPU Power : 66.4728695875161 W\n",
      "[codecarbon INFO @ 22:28:48] Energy consumed for all CPUs : 6.031381 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:48] 41.270723 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:51] Energy consumed for RAM : 22.777882 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:51] Energy consumed for all GPUs : 2.029127 kWh. Total GPU Power : 66.74031845988868 W\n",
      "[codecarbon INFO @ 22:28:51] Energy consumed for all CPUs : 4.246281 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:51] 29.053291 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:52] Energy consumed for RAM : 32.372336 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:52] Energy consumed for all GPUs : 2.884578 kWh. Total GPU Power : 66.46464722673018 W\n",
      "[codecarbon INFO @ 22:28:52] Energy consumed for all CPUs : 6.034406 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:52] 41.291319 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:54] Energy consumed for RAM : 22.769066 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:28:54] Energy consumed for all GPUs : 2.028234 kWh. Total GPU Power : 66.91161837120424 W\n",
      "[codecarbon INFO @ 22:28:54] Energy consumed for all CPUs : 4.244545 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:28:54] 29.041844 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:03] Energy consumed for RAM : 32.359435 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:03] Energy consumed for all GPUs : 2.883315 kWh. Total GPU Power : 66.44661802541091 W\n",
      "[codecarbon INFO @ 22:29:03] Energy consumed for all CPUs : 6.031964 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:03] 41.274713 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:06] Energy consumed for RAM : 22.780999 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:06] Energy consumed for all GPUs : 2.029404 kWh. Total GPU Power : 66.72868106241177 W\n",
      "[codecarbon INFO @ 22:29:06] Energy consumed for all CPUs : 4.246864 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:06] 29.057268 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:07] Energy consumed for RAM : 32.375465 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:07] Energy consumed for all GPUs : 2.884854 kWh. Total GPU Power : 66.45392082517336 W\n",
      "[codecarbon INFO @ 22:29:07] Energy consumed for all CPUs : 6.034989 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:07] 41.295309 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:09] Energy consumed for RAM : 22.772195 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:09] Energy consumed for all GPUs : 2.028511 kWh. Total GPU Power : 66.45550368498249 W\n",
      "[codecarbon INFO @ 22:29:09] Energy consumed for all CPUs : 4.245128 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:09] 29.045834 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:18] Energy consumed for RAM : 32.362563 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:18] Energy consumed for all GPUs : 2.883593 kWh. Total GPU Power : 66.9333816611236 W\n",
      "[codecarbon INFO @ 22:29:18] Energy consumed for all CPUs : 6.032547 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:18] 41.278703 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:21] Energy consumed for RAM : 22.784116 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:21] Energy consumed for all GPUs : 2.029683 kWh. Total GPU Power : 67.18368234495078 W\n",
      "[codecarbon INFO @ 22:29:21] Energy consumed for all CPUs : 4.247448 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:21] 29.061247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:22] Energy consumed for RAM : 32.378595 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:22] Energy consumed for all GPUs : 2.885133 kWh. Total GPU Power : 66.90822124884775 W\n",
      "[codecarbon INFO @ 22:29:22] Energy consumed for all CPUs : 6.035575 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:22] 41.299303 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:24] Energy consumed for RAM : 22.775325 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:24] Energy consumed for all GPUs : 2.028788 kWh. Total GPU Power : 66.46798388030547 W\n",
      "[codecarbon INFO @ 22:29:24] Energy consumed for all CPUs : 4.245711 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:24] 29.049824 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:33] Energy consumed for RAM : 32.365693 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:33] Energy consumed for all GPUs : 2.883870 kWh. Total GPU Power : 66.47838356333895 W\n",
      "[codecarbon INFO @ 22:29:33] Energy consumed for all CPUs : 6.033130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:33] 41.282694 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:36] Energy consumed for RAM : 22.787233 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:36] Energy consumed for all GPUs : 2.029962 kWh. Total GPU Power : 66.91552456284515 W\n",
      "[codecarbon INFO @ 22:29:36] Energy consumed for all CPUs : 4.248031 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:36] 29.065226 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:37] Energy consumed for RAM : 32.381712 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:37] Energy consumed for all GPUs : 2.885410 kWh. Total GPU Power : 66.75428031776258 W\n",
      "[codecarbon INFO @ 22:29:37] Energy consumed for all CPUs : 6.036158 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:37] 41.303280 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:39] Energy consumed for RAM : 22.778454 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:39] Energy consumed for all GPUs : 2.029067 kWh. Total GPU Power : 66.92908348530659 W\n",
      "[codecarbon INFO @ 22:29:39] Energy consumed for all CPUs : 4.246295 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:39] 29.053815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:48] Energy consumed for RAM : 32.368822 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:48] Energy consumed for all GPUs : 2.884147 kWh. Total GPU Power : 66.49925875518375 W\n",
      "[codecarbon INFO @ 22:29:48] Energy consumed for all CPUs : 6.033714 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:48] 41.286683 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:51] Energy consumed for RAM : 22.790348 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:51] Energy consumed for all GPUs : 2.030239 kWh. Total GPU Power : 66.50650791584508 W\n",
      "[codecarbon INFO @ 22:29:51] Energy consumed for all CPUs : 4.248614 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:51] 29.069201 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:52] Energy consumed for RAM : 32.384829 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:52] Energy consumed for all GPUs : 2.885687 kWh. Total GPU Power : 66.76269971899428 W\n",
      "[codecarbon INFO @ 22:29:52] Energy consumed for all CPUs : 6.036741 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:52] 41.307257 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:29:54] Energy consumed for RAM : 22.781583 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:29:54] Energy consumed for all GPUs : 2.029343 kWh. Total GPU Power : 66.50153247289572 W\n",
      "[codecarbon INFO @ 22:29:54] Energy consumed for all CPUs : 4.246878 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:29:54] 29.057805 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:03] Energy consumed for RAM : 32.371952 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:03] Energy consumed for all GPUs : 2.884424 kWh. Total GPU Power : 66.47251961625098 W\n",
      "[codecarbon INFO @ 22:30:03] Energy consumed for all CPUs : 6.034297 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:03] 41.290673 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:06] Energy consumed for RAM : 22.793465 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:06] Energy consumed for all GPUs : 2.030515 kWh. Total GPU Power : 66.75681070131847 W\n",
      "[codecarbon INFO @ 22:30:06] Energy consumed for all CPUs : 4.249197 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:06] 29.073178 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:07] Energy consumed for RAM : 32.387946 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:07] Energy consumed for all GPUs : 2.885966 kWh. Total GPU Power : 67.19037624840888 W\n",
      "[codecarbon INFO @ 22:30:07] Energy consumed for all CPUs : 6.037324 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:07] 41.311236 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:09] Energy consumed for RAM : 22.784713 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:09] Energy consumed for all GPUs : 2.029620 kWh. Total GPU Power : 66.46126289211452 W\n",
      "[codecarbon INFO @ 22:30:09] Energy consumed for all CPUs : 4.247461 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:09] 29.061795 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:18] Energy consumed for RAM : 32.375082 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:18] Energy consumed for all GPUs : 2.884703 kWh. Total GPU Power : 66.89378028994135 W\n",
      "[codecarbon INFO @ 22:30:18] Energy consumed for all CPUs : 6.034880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:18] 41.294665 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:21] Energy consumed for RAM : 22.796582 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:21] Energy consumed for all GPUs : 2.030792 kWh. Total GPU Power : 66.70879601820023 W\n",
      "[codecarbon INFO @ 22:30:21] Energy consumed for all CPUs : 4.249781 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:21] 29.077155 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:22] Energy consumed for RAM : 32.391063 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:22] Energy consumed for all GPUs : 2.886243 kWh. Total GPU Power : 66.42667832650135 W\n",
      "[codecarbon INFO @ 22:30:22] Energy consumed for all CPUs : 6.037908 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:22] 41.315213 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:24] Energy consumed for RAM : 22.787843 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:24] Energy consumed for all GPUs : 2.029899 kWh. Total GPU Power : 66.89748493754692 W\n",
      "[codecarbon INFO @ 22:30:24] Energy consumed for all CPUs : 4.248044 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:24] 29.065787 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:33] Energy consumed for RAM : 32.378211 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:33] Energy consumed for all GPUs : 2.884980 kWh. Total GPU Power : 66.49282253647895 W\n",
      "[codecarbon INFO @ 22:30:33] Energy consumed for all CPUs : 6.035464 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:33] 41.298655 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:36] Energy consumed for RAM : 22.799700 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:36] Energy consumed for all GPUs : 2.031071 kWh. Total GPU Power : 66.9219007531376 W\n",
      "[codecarbon INFO @ 22:30:36] Energy consumed for all CPUs : 4.250364 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:36] 29.081135 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:37] Energy consumed for RAM : 32.394179 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:37] Energy consumed for all GPUs : 2.886521 kWh. Total GPU Power : 66.94623203354291 W\n",
      "[codecarbon INFO @ 22:30:37] Energy consumed for all CPUs : 6.038491 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:37] 41.319192 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:39] Energy consumed for RAM : 22.790972 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:39] Energy consumed for all GPUs : 2.030176 kWh. Total GPU Power : 66.49825263231033 W\n",
      "[codecarbon INFO @ 22:30:39] Energy consumed for all CPUs : 4.248628 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:39] 29.069776 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:48] Energy consumed for RAM : 32.381341 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:48] Energy consumed for all GPUs : 2.885256 kWh. Total GPU Power : 66.45769893864961 W\n",
      "[codecarbon INFO @ 22:30:48] Energy consumed for all CPUs : 6.036047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:48] 41.302644 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:51] Energy consumed for RAM : 22.802816 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:51] Energy consumed for all GPUs : 2.031348 kWh. Total GPU Power : 66.48279799204735 W\n",
      "[codecarbon INFO @ 22:30:51] Energy consumed for all CPUs : 4.250947 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:51] 29.085112 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:52] Energy consumed for RAM : 32.397296 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:52] Energy consumed for all GPUs : 2.886796 kWh. Total GPU Power : 66.31798614535782 W\n",
      "[codecarbon INFO @ 22:30:52] Energy consumed for all CPUs : 6.039074 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:52] 41.323167 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:54] Energy consumed for RAM : 22.794102 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:30:54] Energy consumed for all GPUs : 2.030453 kWh. Total GPU Power : 66.48354899522153 W\n",
      "[codecarbon INFO @ 22:30:54] Energy consumed for all CPUs : 4.249211 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:30:54] 29.073766 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:03] Energy consumed for RAM : 32.384471 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:03] Energy consumed for all GPUs : 2.885535 kWh. Total GPU Power : 66.93593659689257 W\n",
      "[codecarbon INFO @ 22:31:03] Energy consumed for all CPUs : 6.036630 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:03] 41.306636 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:06] Energy consumed for RAM : 22.805946 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:06] Energy consumed for all GPUs : 2.031625 kWh. Total GPU Power : 66.45826830439121 W\n",
      "[codecarbon INFO @ 22:31:06] Energy consumed for all CPUs : 4.251531 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:06] 29.089102 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:07] Energy consumed for RAM : 32.400413 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:07] Energy consumed for all GPUs : 2.887075 kWh. Total GPU Power : 66.87880349273624 W\n",
      "[codecarbon INFO @ 22:31:07] Energy consumed for all CPUs : 6.039658 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:07] 41.327146 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:09] Energy consumed for RAM : 22.797232 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:09] Energy consumed for all GPUs : 2.030730 kWh. Total GPU Power : 66.45430856277234 W\n",
      "[codecarbon INFO @ 22:31:09] Energy consumed for all CPUs : 4.249794 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:09] 29.077756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:18] Energy consumed for RAM : 32.387600 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:18] Energy consumed for all GPUs : 2.885812 kWh. Total GPU Power : 66.4249059505889 W\n",
      "[codecarbon INFO @ 22:31:18] Energy consumed for all CPUs : 6.037213 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:18] 41.310626 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:21] Energy consumed for RAM : 22.809076 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:21] Energy consumed for all GPUs : 2.031903 kWh. Total GPU Power : 66.8664179555816 W\n",
      "[codecarbon INFO @ 22:31:21] Energy consumed for all CPUs : 4.252114 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:21] 29.093093 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:22] Energy consumed for RAM : 32.403543 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:22] Energy consumed for all GPUs : 2.887354 kWh. Total GPU Power : 66.86941327298089 W\n",
      "[codecarbon INFO @ 22:31:22] Energy consumed for all CPUs : 6.040241 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:22] 41.331138 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:24] Energy consumed for RAM : 22.800361 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:24] Energy consumed for all GPUs : 2.031008 kWh. Total GPU Power : 66.86730213287827 W\n",
      "[codecarbon INFO @ 22:31:24] Energy consumed for all CPUs : 4.250378 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:24] 29.081747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:33] Energy consumed for RAM : 32.390730 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:33] Energy consumed for all GPUs : 2.886089 kWh. Total GPU Power : 66.45027008717243 W\n",
      "[codecarbon INFO @ 22:31:33] Energy consumed for all CPUs : 6.037797 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:33] 41.314616 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:36] Energy consumed for RAM : 22.812205 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:36] Energy consumed for all GPUs : 2.032180 kWh. Total GPU Power : 66.45953414479091 W\n",
      "[codecarbon INFO @ 22:31:36] Energy consumed for all CPUs : 4.252697 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:36] 29.097083 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:37] Energy consumed for RAM : 32.406673 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:37] Energy consumed for all GPUs : 2.887630 kWh. Total GPU Power : 66.45165895657354 W\n",
      "[codecarbon INFO @ 22:31:37] Energy consumed for all CPUs : 6.040824 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:37] 41.335128 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:39] Energy consumed for RAM : 22.803491 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:39] Energy consumed for all GPUs : 2.031285 kWh. Total GPU Power : 66.46465765827439 W\n",
      "[codecarbon INFO @ 22:31:39] Energy consumed for all CPUs : 4.250961 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:39] 29.085737 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:48] Energy consumed for RAM : 32.393860 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:48] Energy consumed for all GPUs : 2.886368 kWh. Total GPU Power : 66.93321392441923 W\n",
      "[codecarbon INFO @ 22:31:48] Energy consumed for all CPUs : 6.038380 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:48] 41.318607 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:51] Energy consumed for RAM : 22.815335 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:51] Energy consumed for all GPUs : 2.032457 kWh. Total GPU Power : 66.4878961226145 W\n",
      "[codecarbon INFO @ 22:31:51] Energy consumed for all CPUs : 4.253281 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:51] 29.101073 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:52] Energy consumed for RAM : 32.409803 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:52] Energy consumed for all GPUs : 2.887907 kWh. Total GPU Power : 66.49221435640702 W\n",
      "[codecarbon INFO @ 22:31:52] Energy consumed for all CPUs : 6.041408 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:52] 41.339117 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:31:54] Energy consumed for RAM : 22.806620 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:31:54] Energy consumed for all GPUs : 2.031562 kWh. Total GPU Power : 66.48757703989396 W\n",
      "[codecarbon INFO @ 22:31:54] Energy consumed for all CPUs : 4.251544 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:31:54] 29.089727 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:03] Energy consumed for RAM : 32.396990 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:03] Energy consumed for all GPUs : 2.886644 kWh. Total GPU Power : 66.44418441825509 W\n",
      "[codecarbon INFO @ 22:32:03] Energy consumed for all CPUs : 6.038963 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:03] 41.322597 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:06] Energy consumed for RAM : 22.818465 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:06] Energy consumed for all GPUs : 2.032736 kWh. Total GPU Power : 66.8917031197596 W\n",
      "[codecarbon INFO @ 22:32:06] Energy consumed for all CPUs : 4.253864 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:06] 29.105065 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:07] Energy consumed for RAM : 32.412932 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:07] Energy consumed for all GPUs : 2.888186 kWh. Total GPU Power : 66.89340942114173 W\n",
      "[codecarbon INFO @ 22:32:07] Energy consumed for all CPUs : 6.041991 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:07] 41.343109 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:09] Energy consumed for RAM : 22.809750 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:09] Energy consumed for all GPUs : 2.031841 kWh. Total GPU Power : 66.89483660346903 W\n",
      "[codecarbon INFO @ 22:32:09] Energy consumed for all CPUs : 4.252128 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:09] 29.093718 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:18] Energy consumed for RAM : 32.400119 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:18] Energy consumed for all GPUs : 2.886921 kWh. Total GPU Power : 66.45948207624632 W\n",
      "[codecarbon INFO @ 22:32:18] Energy consumed for all CPUs : 6.039547 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:18] 41.326587 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:21] Energy consumed for RAM : 22.821594 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:21] Energy consumed for all GPUs : 2.033013 kWh. Total GPU Power : 66.45944930776476 W\n",
      "[codecarbon INFO @ 22:32:21] Energy consumed for all CPUs : 4.254447 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:21] 29.109054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:22] Energy consumed for RAM : 32.416062 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:22] Energy consumed for all GPUs : 2.888463 kWh. Total GPU Power : 66.45995199410739 W\n",
      "[codecarbon INFO @ 22:32:22] Energy consumed for all CPUs : 6.042574 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:22] 41.347099 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:24] Energy consumed for RAM : 22.812880 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:24] Energy consumed for all GPUs : 2.032117 kWh. Total GPU Power : 66.4491649940247 W\n",
      "[codecarbon INFO @ 22:32:24] Energy consumed for all CPUs : 4.252711 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:24] 29.097708 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:33] Energy consumed for RAM : 32.403249 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:33] Energy consumed for all GPUs : 2.887200 kWh. Total GPU Power : 66.9495524107542 W\n",
      "[codecarbon INFO @ 22:32:33] Energy consumed for all CPUs : 6.040130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:33] 41.330579 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:36] Energy consumed for RAM : 22.824724 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:36] Energy consumed for all GPUs : 2.033290 kWh. Total GPU Power : 66.50404742881295 W\n",
      "[codecarbon INFO @ 22:32:36] Energy consumed for all CPUs : 4.255031 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:36] 29.113044 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:37] Energy consumed for RAM : 32.419192 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:37] Energy consumed for all GPUs : 2.888740 kWh. Total GPU Power : 66.50403699186774 W\n",
      "[codecarbon INFO @ 22:32:37] Energy consumed for all CPUs : 6.043157 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:37] 41.351089 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:39] Energy consumed for RAM : 22.816009 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:39] Energy consumed for all GPUs : 2.032394 kWh. Total GPU Power : 66.53010011485514 W\n",
      "[codecarbon INFO @ 22:32:39] Energy consumed for all CPUs : 4.253294 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:39] 29.101697 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:48] Energy consumed for RAM : 32.406379 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:48] Energy consumed for all GPUs : 2.887477 kWh. Total GPU Power : 66.46088202277407 W\n",
      "[codecarbon INFO @ 22:32:48] Energy consumed for all CPUs : 6.040713 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:48] 41.334569 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:51] Energy consumed for RAM : 22.827854 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:51] Energy consumed for all GPUs : 2.033568 kWh. Total GPU Power : 66.90249018191192 W\n",
      "[codecarbon INFO @ 22:32:51] Energy consumed for all CPUs : 4.255614 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:51] 29.117036 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:52] Energy consumed for RAM : 32.422322 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:52] Energy consumed for all GPUs : 2.889019 kWh. Total GPU Power : 66.90044229958991 W\n",
      "[codecarbon INFO @ 22:32:52] Energy consumed for all CPUs : 6.043741 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:52] 41.355081 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:54] Energy consumed for RAM : 22.819138 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:32:54] Energy consumed for all GPUs : 2.032673 kWh. Total GPU Power : 66.90719646057092 W\n",
      "[codecarbon INFO @ 22:32:54] Energy consumed for all CPUs : 4.253877 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:32:54] 29.105689 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:03] Energy consumed for RAM : 32.409508 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:03] Energy consumed for all GPUs : 2.887754 kWh. Total GPU Power : 66.45461562177718 W\n",
      "[codecarbon INFO @ 22:33:03] Energy consumed for all CPUs : 6.041297 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:03] 41.338559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:06] Energy consumed for RAM : 22.830984 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:06] Energy consumed for all GPUs : 2.033845 kWh. Total GPU Power : 66.46247026349029 W\n",
      "[codecarbon INFO @ 22:33:06] Energy consumed for all CPUs : 4.256197 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:06] 29.121026 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:07] Energy consumed for RAM : 32.425451 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:07] Energy consumed for all GPUs : 2.889295 kWh. Total GPU Power : 66.46264300964356 W\n",
      "[codecarbon INFO @ 22:33:07] Energy consumed for all CPUs : 6.044324 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:07] 41.359070 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:09] Energy consumed for RAM : 22.822268 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:09] Energy consumed for all GPUs : 2.032950 kWh. Total GPU Power : 66.45309137229803 W\n",
      "[codecarbon INFO @ 22:33:09] Energy consumed for all CPUs : 4.254461 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:09] 29.109679 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:18] Energy consumed for RAM : 32.412638 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:18] Energy consumed for all GPUs : 2.888032 kWh. Total GPU Power : 66.92925235205573 W\n",
      "[codecarbon INFO @ 22:33:18] Energy consumed for all CPUs : 6.041880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:18] 41.342550 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:21] Energy consumed for RAM : 22.834113 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:21] Energy consumed for all GPUs : 2.034122 kWh. Total GPU Power : 66.47390174995465 W\n",
      "[codecarbon INFO @ 22:33:21] Energy consumed for all CPUs : 4.256781 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:21] 29.125016 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:22] Energy consumed for RAM : 32.428581 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:22] Energy consumed for all GPUs : 2.889572 kWh. Total GPU Power : 66.50114877523545 W\n",
      "[codecarbon INFO @ 22:33:22] Energy consumed for all CPUs : 6.044907 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:22] 41.363061 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:24] Energy consumed for RAM : 22.825398 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:24] Energy consumed for all GPUs : 2.033227 kWh. Total GPU Power : 66.55993471033091 W\n",
      "[codecarbon INFO @ 22:33:24] Energy consumed for all CPUs : 4.255044 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:24] 29.113669 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:33] Energy consumed for RAM : 32.415767 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:33] Energy consumed for all GPUs : 2.888310 kWh. Total GPU Power : 66.65998494956266 W\n",
      "[codecarbon INFO @ 22:33:33] Energy consumed for all CPUs : 6.042463 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:33] 41.346541 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:36] Energy consumed for RAM : 22.837243 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:36] Energy consumed for all GPUs : 2.034401 kWh. Total GPU Power : 67.10607624049675 W\n",
      "[codecarbon INFO @ 22:33:36] Energy consumed for all CPUs : 4.257364 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:36] 29.129008 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:37] Energy consumed for RAM : 32.431710 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:37] Energy consumed for all GPUs : 2.889852 kWh. Total GPU Power : 67.08154911472437 W\n",
      "[codecarbon INFO @ 22:33:37] Energy consumed for all CPUs : 6.045491 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:37] 41.367053 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:39] Energy consumed for RAM : 22.828527 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:39] Energy consumed for all GPUs : 2.033506 kWh. Total GPU Power : 67.02059314836595 W\n",
      "[codecarbon INFO @ 22:33:39] Energy consumed for all CPUs : 4.255627 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:39] 29.117661 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:48] Energy consumed for RAM : 32.418897 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:48] Energy consumed for all GPUs : 2.888587 kWh. Total GPU Power : 66.45801763236638 W\n",
      "[codecarbon INFO @ 22:33:49] Energy consumed for all CPUs : 6.043047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:49] 41.350531 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:51] Energy consumed for RAM : 22.840373 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:51] Energy consumed for all GPUs : 2.034678 kWh. Total GPU Power : 66.45443919452958 W\n",
      "[codecarbon INFO @ 22:33:51] Energy consumed for all CPUs : 4.257947 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:51] 29.132998 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:52] Energy consumed for RAM : 32.434840 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:52] Energy consumed for all GPUs : 2.890129 kWh. Total GPU Power : 66.4537590941258 W\n",
      "[codecarbon INFO @ 22:33:52] Energy consumed for all CPUs : 6.046074 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:52] 41.371043 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:33:54] Energy consumed for RAM : 22.831657 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:33:54] Energy consumed for all GPUs : 2.033783 kWh. Total GPU Power : 66.46587038157378 W\n",
      "[codecarbon INFO @ 22:33:54] Energy consumed for all CPUs : 4.256211 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:33:54] 29.121651 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:03] Energy consumed for RAM : 32.422026 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:03] Energy consumed for all GPUs : 2.888864 kWh. Total GPU Power : 66.47107612204 W\n",
      "[codecarbon INFO @ 22:34:03] Energy consumed for all CPUs : 6.043630 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:03] 41.354520 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:06] Energy consumed for RAM : 22.843502 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:06] Energy consumed for all GPUs : 2.034955 kWh. Total GPU Power : 66.46616288220177 W\n",
      "[codecarbon INFO @ 22:34:06] Energy consumed for all CPUs : 4.258530 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:06] 29.136988 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:07] Energy consumed for RAM : 32.437969 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:07] Energy consumed for all GPUs : 2.890405 kWh. Total GPU Power : 66.46758112001862 W\n",
      "[codecarbon INFO @ 22:34:07] Energy consumed for all CPUs : 6.046657 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:07] 41.375032 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:09] Energy consumed for RAM : 22.834787 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:09] Energy consumed for all GPUs : 2.034060 kWh. Total GPU Power : 66.4543568068398 W\n",
      "[codecarbon INFO @ 22:34:09] Energy consumed for all CPUs : 4.256794 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:09] 29.125641 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:18] Energy consumed for RAM : 32.425156 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:18] Energy consumed for all GPUs : 2.889142 kWh. Total GPU Power : 66.8920874795222 W\n",
      "[codecarbon INFO @ 22:34:18] Energy consumed for all CPUs : 6.044213 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:19] 41.358511 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:21] Energy consumed for RAM : 22.846632 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:21] Energy consumed for all GPUs : 2.035234 kWh. Total GPU Power : 66.88998374382136 W\n",
      "[codecarbon INFO @ 22:34:21] Energy consumed for all CPUs : 4.259114 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:21] 29.140979 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:22] Energy consumed for RAM : 32.441099 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:22] Energy consumed for all GPUs : 2.890682 kWh. Total GPU Power : 66.44141085186487 W\n",
      "[codecarbon INFO @ 22:34:22] Energy consumed for all CPUs : 6.047241 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:22] 41.379022 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:24] Energy consumed for RAM : 22.837916 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:24] Energy consumed for all GPUs : 2.034339 kWh. Total GPU Power : 66.89027284886076 W\n",
      "[codecarbon INFO @ 22:34:24] Energy consumed for all CPUs : 4.257377 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:24] 29.129632 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:33] Energy consumed for RAM : 32.428286 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:33] Energy consumed for all GPUs : 2.889419 kWh. Total GPU Power : 66.45849882206808 W\n",
      "[codecarbon INFO @ 22:34:33] Energy consumed for all CPUs : 6.044796 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:34] 41.362501 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:36] Energy consumed for RAM : 22.849761 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:36] Energy consumed for all GPUs : 2.035511 kWh. Total GPU Power : 66.45238802912137 W\n",
      "[codecarbon INFO @ 22:34:36] Energy consumed for all CPUs : 4.259697 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:36] 29.144969 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:37] Energy consumed for RAM : 32.444228 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:37] Energy consumed for all GPUs : 2.890961 kWh. Total GPU Power : 66.90324656171333 W\n",
      "[codecarbon INFO @ 22:34:37] Energy consumed for all CPUs : 6.047824 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:37] 41.383013 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:39] Energy consumed for RAM : 22.841046 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:39] Energy consumed for all GPUs : 2.034616 kWh. Total GPU Power : 66.45378951106521 W\n",
      "[codecarbon INFO @ 22:34:39] Energy consumed for all CPUs : 4.257961 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:39] 29.133622 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:48] Energy consumed for RAM : 32.431415 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:48] Energy consumed for all GPUs : 2.889696 kWh. Total GPU Power : 66.46459772118887 W\n",
      "[codecarbon INFO @ 22:34:49] Energy consumed for all CPUs : 6.045380 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:49] 41.366491 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:51] Energy consumed for RAM : 22.852891 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:51] Energy consumed for all GPUs : 2.035787 kWh. Total GPU Power : 66.47811408904383 W\n",
      "[codecarbon INFO @ 22:34:51] Energy consumed for all CPUs : 4.260280 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:51] 29.148959 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:52] Energy consumed for RAM : 32.447358 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:52] Energy consumed for all GPUs : 2.891238 kWh. Total GPU Power : 66.47076324999286 W\n",
      "[codecarbon INFO @ 22:34:52] Energy consumed for all CPUs : 6.048407 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:52] 41.387003 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:54] Energy consumed for RAM : 22.844176 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:34:54] Energy consumed for all GPUs : 2.034892 kWh. Total GPU Power : 66.47521087895788 W\n",
      "[codecarbon INFO @ 22:34:54] Energy consumed for all CPUs : 4.258544 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:34:54] 29.137612 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:03] Energy consumed for RAM : 32.434545 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:04] Energy consumed for all GPUs : 2.889975 kWh. Total GPU Power : 66.90294110067164 W\n",
      "[codecarbon INFO @ 22:35:04] Energy consumed for all CPUs : 6.045963 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:04] 41.370483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:06] Energy consumed for RAM : 22.856021 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:06] Energy consumed for all GPUs : 2.036064 kWh. Total GPU Power : 66.45050769358038 W\n",
      "[codecarbon INFO @ 22:35:06] Energy consumed for all CPUs : 4.260864 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:06] 29.152948 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:07] Energy consumed for RAM : 32.450488 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:07] Energy consumed for all GPUs : 2.891515 kWh. Total GPU Power : 66.45604048077308 W\n",
      "[codecarbon INFO @ 22:35:07] Energy consumed for all CPUs : 6.048991 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:07] 41.390993 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:09] Energy consumed for RAM : 22.847305 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:09] Energy consumed for all GPUs : 2.035169 kWh. Total GPU Power : 66.45554915501582 W\n",
      "[codecarbon INFO @ 22:35:09] Energy consumed for all CPUs : 4.259127 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:09] 29.141602 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:18] Energy consumed for RAM : 32.437675 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:19] Energy consumed for all GPUs : 2.890252 kWh. Total GPU Power : 66.44692947581773 W\n",
      "[codecarbon INFO @ 22:35:19] Energy consumed for all CPUs : 6.046546 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:19] 41.374473 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:21] Energy consumed for RAM : 22.859150 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:21] Energy consumed for all GPUs : 2.036343 kWh. Total GPU Power : 66.89988963740731 W\n",
      "[codecarbon INFO @ 22:35:21] Energy consumed for all CPUs : 4.261447 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:21] 29.156940 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:22] Energy consumed for RAM : 32.453617 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:22] Energy consumed for all GPUs : 2.891793 kWh. Total GPU Power : 66.90822935809321 W\n",
      "[codecarbon INFO @ 22:35:22] Energy consumed for all CPUs : 6.049574 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:22] 41.394984 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:24] Energy consumed for RAM : 22.850435 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:24] Energy consumed for all GPUs : 2.035448 kWh. Total GPU Power : 66.90219841477497 W\n",
      "[codecarbon INFO @ 22:35:24] Energy consumed for all CPUs : 4.259710 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:24] 29.145593 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:33] Energy consumed for RAM : 32.440805 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:34] Energy consumed for all GPUs : 2.890528 kWh. Total GPU Power : 66.45013199668924 W\n",
      "[codecarbon INFO @ 22:35:34] Energy consumed for all CPUs : 6.047130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:34] 41.378463 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:36] Energy consumed for RAM : 22.862280 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:36] Energy consumed for all GPUs : 2.036620 kWh. Total GPU Power : 66.4517164459115 W\n",
      "[codecarbon INFO @ 22:35:36] Energy consumed for all CPUs : 4.262030 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:36] 29.160930 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:37] Energy consumed for RAM : 32.456747 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:37] Energy consumed for all GPUs : 2.892070 kWh. Total GPU Power : 66.45378069586731 W\n",
      "[codecarbon INFO @ 22:35:37] Energy consumed for all CPUs : 6.050157 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:37] 41.398974 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:39] Energy consumed for RAM : 22.853565 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:39] Energy consumed for all GPUs : 2.035725 kWh. Total GPU Power : 66.45952818056942 W\n",
      "[codecarbon INFO @ 22:35:39] Energy consumed for all CPUs : 4.260294 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:39] 29.149583 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:48] Energy consumed for RAM : 32.443934 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:49] Energy consumed for all GPUs : 2.890807 kWh. Total GPU Power : 66.90280585538706 W\n",
      "[codecarbon INFO @ 22:35:49] Energy consumed for all CPUs : 6.047713 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:49] 41.382454 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:51] Energy consumed for RAM : 22.865409 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:51] Energy consumed for all GPUs : 2.036897 kWh. Total GPU Power : 66.45570404844928 W\n",
      "[codecarbon INFO @ 22:35:51] Energy consumed for all CPUs : 4.262614 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:51] 29.164920 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:52] Energy consumed for RAM : 32.459876 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:52] Energy consumed for all GPUs : 2.892347 kWh. Total GPU Power : 66.46333859752862 W\n",
      "[codecarbon INFO @ 22:35:52] Energy consumed for all CPUs : 6.050740 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:52] 41.402964 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:35:54] Energy consumed for RAM : 22.856694 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:35:54] Energy consumed for all GPUs : 2.036002 kWh. Total GPU Power : 66.45978656743141 W\n",
      "[codecarbon INFO @ 22:35:54] Energy consumed for all CPUs : 4.260877 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:35:54] 29.153573 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:03] Energy consumed for RAM : 32.447064 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:04] Energy consumed for all GPUs : 2.891084 kWh. Total GPU Power : 66.47542043652936 W\n",
      "[codecarbon INFO @ 22:36:04] Energy consumed for all CPUs : 6.048296 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:04] 41.386444 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:06] Energy consumed for RAM : 22.868539 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:06] Energy consumed for all GPUs : 2.037175 kWh. Total GPU Power : 66.92379347680397 W\n",
      "[codecarbon INFO @ 22:36:06] Energy consumed for all CPUs : 4.263197 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:06] 29.168911 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:07] Energy consumed for RAM : 32.463006 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:07] Energy consumed for all GPUs : 2.892626 kWh. Total GPU Power : 66.90847502902506 W\n",
      "[codecarbon INFO @ 22:36:07] Energy consumed for all CPUs : 6.051324 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:07] 41.406955 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:09] Energy consumed for RAM : 22.859824 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:09] Energy consumed for all GPUs : 2.036280 kWh. Total GPU Power : 66.91316668045316 W\n",
      "[codecarbon INFO @ 22:36:09] Energy consumed for all CPUs : 4.261460 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:09] 29.157564 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:18] Energy consumed for RAM : 32.450193 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:19] Energy consumed for all GPUs : 2.891361 kWh. Total GPU Power : 66.45972146355203 W\n",
      "[codecarbon INFO @ 22:36:19] Energy consumed for all CPUs : 6.048879 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:19] 41.390434 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:21] Energy consumed for RAM : 22.871668 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:21] Energy consumed for all GPUs : 2.037452 kWh. Total GPU Power : 66.46102224600403 W\n",
      "[codecarbon INFO @ 22:36:21] Energy consumed for all CPUs : 4.263780 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:21] 29.172901 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:22] Energy consumed for RAM : 32.466136 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:22] Energy consumed for all GPUs : 2.892902 kWh. Total GPU Power : 66.46078083427959 W\n",
      "[codecarbon INFO @ 22:36:22] Energy consumed for all CPUs : 6.051907 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:22] 41.410945 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:24] Energy consumed for RAM : 22.862954 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:24] Energy consumed for all GPUs : 2.036557 kWh. Total GPU Power : 66.45498073583106 W\n",
      "[codecarbon INFO @ 22:36:24] Energy consumed for all CPUs : 4.262044 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:24] 29.161554 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:33] Energy consumed for RAM : 32.453323 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:34] Energy consumed for all GPUs : 2.891638 kWh. Total GPU Power : 66.44787727905117 W\n",
      "[codecarbon INFO @ 22:36:34] Energy consumed for all CPUs : 6.049463 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:34] 41.394424 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:36] Energy consumed for RAM : 22.874798 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:36] Energy consumed for all GPUs : 2.037729 kWh. Total GPU Power : 66.44983849936106 W\n",
      "[codecarbon INFO @ 22:36:36] Energy consumed for all CPUs : 4.264363 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:36] 29.176890 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:37] Energy consumed for RAM : 32.469265 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:37] Energy consumed for all GPUs : 2.893179 kWh. Total GPU Power : 66.4513749717074 W\n",
      "[codecarbon INFO @ 22:36:37] Energy consumed for all CPUs : 6.052490 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:37] 41.414935 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:39] Energy consumed for RAM : 22.866083 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:39] Energy consumed for all GPUs : 2.036834 kWh. Total GPU Power : 66.45273478181917 W\n",
      "[codecarbon INFO @ 22:36:39] Energy consumed for all CPUs : 4.262627 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:39] 29.165544 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:48] Energy consumed for RAM : 32.456453 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:49] Energy consumed for all GPUs : 2.891916 kWh. Total GPU Power : 66.90285574656194 W\n",
      "[codecarbon INFO @ 22:36:49] Energy consumed for all CPUs : 6.050046 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:49] 41.398416 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:51] Energy consumed for RAM : 22.877928 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:51] Energy consumed for all GPUs : 2.038008 kWh. Total GPU Power : 66.90036073867552 W\n",
      "[codecarbon INFO @ 22:36:51] Energy consumed for all CPUs : 4.264947 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:51] 29.180882 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:52] Energy consumed for RAM : 32.472395 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:52] Energy consumed for all GPUs : 2.893456 kWh. Total GPU Power : 66.45609670054229 W\n",
      "[codecarbon INFO @ 22:36:52] Energy consumed for all CPUs : 6.053074 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:52] 41.418925 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:54] Energy consumed for RAM : 22.869213 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:36:54] Energy consumed for all GPUs : 2.037113 kWh. Total GPU Power : 66.91144239494372 W\n",
      "[codecarbon INFO @ 22:36:54] Energy consumed for all CPUs : 4.263210 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:36:54] 29.169535 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:04] Energy consumed for RAM : 32.459583 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:04] Energy consumed for all GPUs : 2.892193 kWh. Total GPU Power : 66.45644728364223 W\n",
      "[codecarbon INFO @ 22:37:04] Energy consumed for all CPUs : 6.050629 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:04] 41.402405 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:06] Energy consumed for RAM : 22.881057 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:06] Energy consumed for all GPUs : 2.038284 kWh. Total GPU Power : 66.46637634907012 W\n",
      "[codecarbon INFO @ 22:37:06] Energy consumed for all CPUs : 4.265530 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:06] 29.184872 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:07] Energy consumed for RAM : 32.475525 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:07] Energy consumed for all GPUs : 2.893735 kWh. Total GPU Power : 66.90812854924157 W\n",
      "[codecarbon INFO @ 22:37:07] Energy consumed for all CPUs : 6.053657 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:07] 41.422916 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:09] Energy consumed for RAM : 22.872342 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:09] Energy consumed for all GPUs : 2.037389 kWh. Total GPU Power : 66.45975959844296 W\n",
      "[codecarbon INFO @ 22:37:09] Energy consumed for all CPUs : 4.263793 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:09] 29.173525 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:19] Energy consumed for RAM : 32.462713 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:19] Energy consumed for all GPUs : 2.892470 kWh. Total GPU Power : 66.45850014385167 W\n",
      "[codecarbon INFO @ 22:37:19] Energy consumed for all CPUs : 6.051213 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:19] 41.406395 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:21] Energy consumed for RAM : 22.884187 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:21] Energy consumed for all GPUs : 2.038561 kWh. Total GPU Power : 66.45358349707136 W\n",
      "[codecarbon INFO @ 22:37:21] Energy consumed for all CPUs : 4.266113 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:21] 29.188862 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:22] Energy consumed for RAM : 32.478655 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:22] Energy consumed for all GPUs : 2.894012 kWh. Total GPU Power : 66.45269870837643 W\n",
      "[codecarbon INFO @ 22:37:22] Energy consumed for all CPUs : 6.054240 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:22] 41.426906 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:24] Energy consumed for RAM : 22.875472 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:24] Energy consumed for all GPUs : 2.037666 kWh. Total GPU Power : 66.44540366366023 W\n",
      "[codecarbon INFO @ 22:37:24] Energy consumed for all CPUs : 4.264377 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:24] 29.177515 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:34] Energy consumed for RAM : 32.465829 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:34] Energy consumed for all GPUs : 2.892749 kWh. Total GPU Power : 67.19127104528317 W\n",
      "[codecarbon INFO @ 22:37:34] Energy consumed for all CPUs : 6.051794 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:34] 41.410371 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:36] Energy consumed for RAM : 22.887317 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:36] Energy consumed for all GPUs : 2.038838 kWh. Total GPU Power : 66.45353648657007 W\n",
      "[codecarbon INFO @ 22:37:36] Energy consumed for all CPUs : 4.266697 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:36] 29.192852 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:37] Energy consumed for RAM : 32.481784 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:37] Energy consumed for all GPUs : 2.894288 kWh. Total GPU Power : 66.45442779541142 W\n",
      "[codecarbon INFO @ 22:37:37] Energy consumed for all CPUs : 6.054823 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:37] 41.430896 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:39] Energy consumed for RAM : 22.878602 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:39] Energy consumed for all GPUs : 2.037943 kWh. Total GPU Power : 66.46188502761562 W\n",
      "[codecarbon INFO @ 22:37:39] Energy consumed for all CPUs : 4.264963 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:39] 29.181507 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:49] Energy consumed for RAM : 32.468945 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:49] Energy consumed for all GPUs : 2.893025 kWh. Total GPU Power : 66.74610475901723 W\n",
      "[codecarbon INFO @ 22:37:49] Energy consumed for all CPUs : 6.052374 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:49] 41.414344 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:51] Energy consumed for RAM : 22.890447 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:51] Energy consumed for all GPUs : 2.039117 kWh. Total GPU Power : 66.90333639680507 W\n",
      "[codecarbon INFO @ 22:37:51] Energy consumed for all CPUs : 4.267280 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:51] 29.196843 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:52] Energy consumed for RAM : 32.484914 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:52] Energy consumed for all GPUs : 2.894567 kWh. Total GPU Power : 66.9048890149101 W\n",
      "[codecarbon INFO @ 22:37:52] Energy consumed for all CPUs : 6.055407 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:52] 41.434888 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:37:54] Energy consumed for RAM : 22.881718 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:37:54] Energy consumed for all GPUs : 2.038222 kWh. Total GPU Power : 67.19657095366541 W\n",
      "[codecarbon INFO @ 22:37:54] Energy consumed for all CPUs : 4.265543 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:37:54] 29.185483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:04] Energy consumed for RAM : 32.472061 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:04] Energy consumed for all GPUs : 2.893302 kWh. Total GPU Power : 66.73781567381248 W\n",
      "[codecarbon INFO @ 22:38:04] Energy consumed for all CPUs : 6.052958 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:04] 41.418321 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:06] Energy consumed for RAM : 22.893576 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:06] Energy consumed for all GPUs : 2.039394 kWh. Total GPU Power : 66.4467680655579 W\n",
      "[codecarbon INFO @ 22:38:06] Energy consumed for all CPUs : 4.267863 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:06] 29.200833 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:07] Energy consumed for RAM : 32.488044 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:07] Energy consumed for all GPUs : 2.894844 kWh. Total GPU Power : 66.44526299834934 W\n",
      "[codecarbon INFO @ 22:38:07] Energy consumed for all CPUs : 6.055990 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:07] 41.438878 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:09] Energy consumed for RAM : 22.884847 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:09] Energy consumed for all GPUs : 2.038498 kWh. Total GPU Power : 66.43904351619578 W\n",
      "[codecarbon INFO @ 22:38:09] Energy consumed for all CPUs : 4.266129 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:09] 29.189475 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:19] Energy consumed for RAM : 32.475177 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:19] Energy consumed for all GPUs : 2.893581 kWh. Total GPU Power : 67.19317929691702 W\n",
      "[codecarbon INFO @ 22:38:19] Energy consumed for all CPUs : 6.053539 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:19] 41.422297 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:21] Energy consumed for RAM : 22.896706 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:21] Energy consumed for all GPUs : 2.039670 kWh. Total GPU Power : 66.46545519480243 W\n",
      "[codecarbon INFO @ 22:38:21] Energy consumed for all CPUs : 4.268447 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:21] 29.204823 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:22] Energy consumed for RAM : 32.491174 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:22] Energy consumed for all GPUs : 2.895121 kWh. Total GPU Power : 66.46443207234915 W\n",
      "[codecarbon INFO @ 22:38:22] Energy consumed for all CPUs : 6.056573 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:22] 41.442868 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:24] Energy consumed for RAM : 22.887964 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:24] Energy consumed for all GPUs : 2.038775 kWh. Total GPU Power : 66.73441424839953 W\n",
      "[codecarbon INFO @ 22:38:24] Energy consumed for all CPUs : 4.266712 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:24] 29.193452 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:34] Energy consumed for RAM : 32.478307 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:34] Energy consumed for all GPUs : 2.893858 kWh. Total GPU Power : 66.46384598040895 W\n",
      "[codecarbon INFO @ 22:38:34] Energy consumed for all CPUs : 6.054122 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:34] 41.426286 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:36] Energy consumed for RAM : 22.899836 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:36] Energy consumed for all GPUs : 2.039949 kWh. Total GPU Power : 66.90121999223437 W\n",
      "[codecarbon INFO @ 22:38:36] Energy consumed for all CPUs : 4.269030 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:36] 29.208815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:37] Energy consumed for RAM : 32.494303 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:37] Energy consumed for all GPUs : 2.895399 kWh. Total GPU Power : 66.90274455627639 W\n",
      "[codecarbon INFO @ 22:38:37] Energy consumed for all CPUs : 6.057157 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:37] 41.446859 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:39] Energy consumed for RAM : 22.891080 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:39] Energy consumed for all GPUs : 2.039054 kWh. Total GPU Power : 67.18711343632661 W\n",
      "[codecarbon INFO @ 22:38:39] Energy consumed for all CPUs : 4.267296 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:39] 29.197430 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:49] Energy consumed for RAM : 32.481436 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:49] Energy consumed for all GPUs : 2.894135 kWh. Total GPU Power : 66.49065512009426 W\n",
      "[codecarbon INFO @ 22:38:49] Energy consumed for all CPUs : 6.054708 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:49] 41.430279 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:51] Energy consumed for RAM : 22.902965 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:51] Energy consumed for all GPUs : 2.040226 kWh. Total GPU Power : 66.50087206768602 W\n",
      "[codecarbon INFO @ 22:38:51] Energy consumed for all CPUs : 4.269613 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:51] 29.212804 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:52] Energy consumed for RAM : 32.497433 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:52] Energy consumed for all GPUs : 2.895676 kWh. Total GPU Power : 66.49750945106945 W\n",
      "[codecarbon INFO @ 22:38:52] Energy consumed for all CPUs : 6.057740 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:52] 41.450849 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:54] Energy consumed for RAM : 22.894197 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:38:54] Energy consumed for all GPUs : 2.039331 kWh. Total GPU Power : 66.76938979088152 W\n",
      "[codecarbon INFO @ 22:38:54] Energy consumed for all CPUs : 4.267879 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:38:54] 29.201407 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:04] Energy consumed for RAM : 32.484553 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:04] Energy consumed for all GPUs : 2.894412 kWh. Total GPU Power : 66.74690013336662 W\n",
      "[codecarbon INFO @ 22:39:04] Energy consumed for all CPUs : 6.055291 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:04] 41.434255 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:06] Energy consumed for RAM : 22.906095 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:06] Energy consumed for all GPUs : 2.040503 kWh. Total GPU Power : 66.46011085653791 W\n",
      "[codecarbon INFO @ 22:39:06] Energy consumed for all CPUs : 4.270196 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:06] 29.216794 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:07] Energy consumed for RAM : 32.500562 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:07] Energy consumed for all GPUs : 2.895953 kWh. Total GPU Power : 66.46972733507039 W\n",
      "[codecarbon INFO @ 22:39:07] Energy consumed for all CPUs : 6.058323 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:07] 41.454839 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:09] Energy consumed for RAM : 22.897314 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:09] Energy consumed for all GPUs : 2.039610 kWh. Total GPU Power : 66.90741325249978 W\n",
      "[codecarbon INFO @ 22:39:09] Energy consumed for all CPUs : 4.268462 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:09] 29.205386 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:19] Energy consumed for RAM : 32.487670 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:19] Energy consumed for all GPUs : 2.894691 kWh. Total GPU Power : 67.26569804164775 W\n",
      "[codecarbon INFO @ 22:39:19] Energy consumed for all CPUs : 6.055874 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:19] 41.438234 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:21] Energy consumed for RAM : 22.909225 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:21] Energy consumed for all GPUs : 2.040780 kWh. Total GPU Power : 66.58829843088 W\n",
      "[codecarbon INFO @ 22:39:21] Energy consumed for all CPUs : 4.270780 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:21] 29.220785 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:22] Energy consumed for RAM : 32.503692 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:22] Energy consumed for all GPUs : 2.896231 kWh. Total GPU Power : 66.58992392065531 W\n",
      "[codecarbon INFO @ 22:39:22] Energy consumed for all CPUs : 6.058907 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:22] 41.458829 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:24] Energy consumed for RAM : 22.900429 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:24] Energy consumed for all GPUs : 2.039885 kWh. Total GPU Power : 66.43823709803237 W\n",
      "[codecarbon INFO @ 22:39:24] Energy consumed for all CPUs : 4.269045 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:24] 29.209360 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:34] Energy consumed for RAM : 32.490786 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:34] Energy consumed for all GPUs : 2.894968 kWh. Total GPU Power : 66.78436564263478 W\n",
      "[codecarbon INFO @ 22:39:34] Energy consumed for all CPUs : 6.056457 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:34] 41.442211 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:36] Energy consumed for RAM : 22.912354 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:36] Energy consumed for all GPUs : 2.041059 kWh. Total GPU Power : 66.91806902881314 W\n",
      "[codecarbon INFO @ 22:39:36] Energy consumed for all CPUs : 4.271363 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:36] 29.224776 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:37] Energy consumed for RAM : 32.506821 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:37] Energy consumed for all GPUs : 2.896509 kWh. Total GPU Power : 66.91611731674432 W\n",
      "[codecarbon INFO @ 22:39:37] Energy consumed for all CPUs : 6.059490 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:37] 41.462820 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:39] Energy consumed for RAM : 22.903546 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:39] Energy consumed for all GPUs : 2.040164 kWh. Total GPU Power : 67.17242963794712 W\n",
      "[codecarbon INFO @ 22:39:39] Energy consumed for all CPUs : 4.269629 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:39] 29.213339 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:49] Energy consumed for RAM : 32.493903 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:49] Energy consumed for all GPUs : 2.895244 kWh. Total GPU Power : 66.72478980540366 W\n",
      "[codecarbon INFO @ 22:39:49] Energy consumed for all CPUs : 6.057041 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:49] 41.446188 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:51] Energy consumed for RAM : 22.915484 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:51] Energy consumed for all GPUs : 2.041336 kWh. Total GPU Power : 66.4956801750349 W\n",
      "[codecarbon INFO @ 22:39:51] Energy consumed for all CPUs : 4.271946 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:51] 29.228766 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:52] Energy consumed for RAM : 32.509951 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:52] Energy consumed for all GPUs : 2.896786 kWh. Total GPU Power : 66.50552009750203 W\n",
      "[codecarbon INFO @ 22:39:52] Energy consumed for all CPUs : 6.060073 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:52] 41.466810 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:39:54] Energy consumed for RAM : 22.906664 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:39:54] Energy consumed for all GPUs : 2.040443 kWh. Total GPU Power : 66.94506731062015 W\n",
      "[codecarbon INFO @ 22:39:54] Energy consumed for all CPUs : 4.270212 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:39:54] 29.217319 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:04] Energy consumed for RAM : 32.497021 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:04] Energy consumed for all GPUs : 2.895523 kWh. Total GPU Power : 66.95540400266805 W\n",
      "[codecarbon INFO @ 22:40:04] Energy consumed for all CPUs : 6.057624 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:04] 41.450168 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:06] Energy consumed for RAM : 22.918613 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:06] Energy consumed for all GPUs : 2.041613 kWh. Total GPU Power : 66.47756593537238 W\n",
      "[codecarbon INFO @ 22:40:06] Energy consumed for all CPUs : 4.272530 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:06] 29.232756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:07] Energy consumed for RAM : 32.513079 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:07] Energy consumed for all GPUs : 2.897063 kWh. Total GPU Power : 66.48733077320846 W\n",
      "[codecarbon INFO @ 22:40:07] Energy consumed for all CPUs : 6.060656 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:07] 41.470799 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:09] Energy consumed for RAM : 22.909780 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:09] Energy consumed for all GPUs : 2.040720 kWh. Total GPU Power : 66.48788675211169 W\n",
      "[codecarbon INFO @ 22:40:09] Energy consumed for all CPUs : 4.270795 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:09] 29.221295 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:19] Energy consumed for RAM : 32.500137 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:19] Energy consumed for all GPUs : 2.895802 kWh. Total GPU Power : 66.91338508911522 W\n",
      "[codecarbon INFO @ 22:40:19] Energy consumed for all CPUs : 6.058207 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:19] 41.454146 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:21] Energy consumed for RAM : 22.921742 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:21] Energy consumed for all GPUs : 2.041892 kWh. Total GPU Power : 66.92232153437232 W\n",
      "[codecarbon INFO @ 22:40:21] Energy consumed for all CPUs : 4.273113 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:21] 29.236747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:22] Energy consumed for RAM : 32.516209 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:22] Energy consumed for all GPUs : 2.897342 kWh. Total GPU Power : 66.9068321156432 W\n",
      "[codecarbon INFO @ 22:40:22] Energy consumed for all CPUs : 6.061240 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:22] 41.474791 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:24] Energy consumed for RAM : 22.912910 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:24] Energy consumed for all GPUs : 2.040997 kWh. Total GPU Power : 66.44508140619446 W\n",
      "[codecarbon INFO @ 22:40:24] Energy consumed for all CPUs : 4.271379 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:24] 29.225285 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:34] Energy consumed for RAM : 32.503253 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:34] Energy consumed for all GPUs : 2.896079 kWh. Total GPU Power : 66.47538000124756 W\n",
      "[codecarbon INFO @ 22:40:34] Energy consumed for all CPUs : 6.058790 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:34] 41.458123 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:36] Energy consumed for RAM : 22.924872 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:36] Energy consumed for all GPUs : 2.042168 kWh. Total GPU Power : 66.44673113547418 W\n",
      "[codecarbon INFO @ 22:40:36] Energy consumed for all CPUs : 4.273699 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:36] 29.240739 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:37] Energy consumed for RAM : 32.519338 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:37] Energy consumed for all GPUs : 2.897619 kWh. Total GPU Power : 66.46598515193539 W\n",
      "[codecarbon INFO @ 22:40:37] Energy consumed for all CPUs : 6.061823 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:37] 41.478780 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:39] Energy consumed for RAM : 22.916039 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:39] Energy consumed for all GPUs : 2.041275 kWh. Total GPU Power : 66.91182573778642 W\n",
      "[codecarbon INFO @ 22:40:39] Energy consumed for all CPUs : 4.271962 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:39] 29.229276 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:49] Energy consumed for RAM : 32.506370 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:49] Energy consumed for all GPUs : 2.896356 kWh. Total GPU Power : 66.46577084190524 W\n",
      "[codecarbon INFO @ 22:40:49] Energy consumed for all CPUs : 6.059374 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:49] 41.462100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:51] Energy consumed for RAM : 22.927986 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:51] Energy consumed for all GPUs : 2.042445 kWh. Total GPU Power : 66.78825964573244 W\n",
      "[codecarbon INFO @ 22:40:51] Energy consumed for all CPUs : 4.274279 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:51] 29.244711 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:52] Energy consumed for RAM : 32.522468 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:52] Energy consumed for all GPUs : 2.897896 kWh. Total GPU Power : 66.46178535783035 W\n",
      "[codecarbon INFO @ 22:40:52] Energy consumed for all CPUs : 6.062406 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:52] 41.482770 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:40:54] Energy consumed for RAM : 22.919168 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:40:54] Energy consumed for all GPUs : 2.041552 kWh. Total GPU Power : 66.46889975770593 W\n",
      "[codecarbon INFO @ 22:40:54] Energy consumed for all CPUs : 4.272545 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:40:54] 29.233266 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:04] Energy consumed for RAM : 32.509487 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:04] Energy consumed for all GPUs : 2.896633 kWh. Total GPU Power : 66.72787146006677 W\n",
      "[codecarbon INFO @ 22:41:04] Energy consumed for all CPUs : 6.059957 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:04] 41.466077 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:06] Energy consumed for RAM : 22.931115 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:06] Energy consumed for all GPUs : 2.042722 kWh. Total GPU Power : 66.46298233582583 W\n",
      "[codecarbon INFO @ 22:41:06] Energy consumed for all CPUs : 4.274863 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:06] 29.248700 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:07] Energy consumed for RAM : 32.525597 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:07] Energy consumed for all GPUs : 2.898172 kWh. Total GPU Power : 66.46307485815642 W\n",
      "[codecarbon INFO @ 22:41:07] Energy consumed for all CPUs : 6.062990 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:07] 41.486759 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:09] Energy consumed for RAM : 22.922298 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:09] Energy consumed for all GPUs : 2.041829 kWh. Total GPU Power : 66.45812418122736 W\n",
      "[codecarbon INFO @ 22:41:09] Energy consumed for all CPUs : 4.273129 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:09] 29.237255 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:19] Energy consumed for RAM : 32.512618 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:19] Energy consumed for all GPUs : 2.896911 kWh. Total GPU Power : 66.88460136515721 W\n",
      "[codecarbon INFO @ 22:41:19] Energy consumed for all CPUs : 6.060540 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:19] 41.470069 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:21] Energy consumed for RAM : 22.934245 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:21] Energy consumed for all GPUs : 2.043001 kWh. Total GPU Power : 66.8927807762793 W\n",
      "[codecarbon INFO @ 22:41:21] Energy consumed for all CPUs : 4.275446 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:21] 29.252692 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:22] Energy consumed for RAM : 32.528727 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:22] Energy consumed for all GPUs : 2.898451 kWh. Total GPU Power : 66.88988292002514 W\n",
      "[codecarbon INFO @ 22:41:22] Energy consumed for all CPUs : 6.063573 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:22] 41.490751 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:24] Energy consumed for RAM : 22.925428 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:24] Energy consumed for all GPUs : 2.042106 kWh. Total GPU Power : 66.44467037102605 W\n",
      "[codecarbon INFO @ 22:41:24] Energy consumed for all CPUs : 4.273712 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:24] 29.241245 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:34] Energy consumed for RAM : 32.515747 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:34] Energy consumed for all GPUs : 2.897188 kWh. Total GPU Power : 66.44256229987512 W\n",
      "[codecarbon INFO @ 22:41:34] Energy consumed for all CPUs : 6.061124 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:34] 41.474059 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:36] Energy consumed for RAM : 22.937375 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:36] Energy consumed for all GPUs : 2.043278 kWh. Total GPU Power : 66.45491106882059 W\n",
      "[codecarbon INFO @ 22:41:36] Energy consumed for all CPUs : 4.276029 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:36] 29.256682 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:37] Energy consumed for RAM : 32.531857 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:37] Energy consumed for all GPUs : 2.898728 kWh. Total GPU Power : 66.4590164257922 W\n",
      "[codecarbon INFO @ 22:41:37] Energy consumed for all CPUs : 6.064156 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:37] 41.494741 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:39] Energy consumed for RAM : 22.928557 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:39] Energy consumed for all GPUs : 2.042384 kWh. Total GPU Power : 66.90682854232678 W\n",
      "[codecarbon INFO @ 22:41:39] Energy consumed for all CPUs : 4.274295 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:39] 29.245237 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:49] Energy consumed for RAM : 32.518877 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:49] Energy consumed for all GPUs : 2.897465 kWh. Total GPU Power : 66.47007848529594 W\n",
      "[codecarbon INFO @ 22:41:49] Energy consumed for all CPUs : 6.061707 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:49] 41.478049 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:51] Energy consumed for RAM : 22.940504 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:51] Energy consumed for all GPUs : 2.043554 kWh. Total GPU Power : 66.47663078838067 W\n",
      "[codecarbon INFO @ 22:41:51] Energy consumed for all CPUs : 4.276613 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:51] 29.260671 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:52] Energy consumed for RAM : 32.534986 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:52] Energy consumed for all GPUs : 2.899005 kWh. Total GPU Power : 66.47176458197589 W\n",
      "[codecarbon INFO @ 22:41:52] Energy consumed for all CPUs : 6.064740 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:52] 41.498731 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:41:54] Energy consumed for RAM : 22.931687 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:41:54] Energy consumed for all GPUs : 2.042661 kWh. Total GPU Power : 66.4720107312223 W\n",
      "[codecarbon INFO @ 22:41:54] Energy consumed for all CPUs : 4.274878 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:41:54] 29.249227 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:04] Energy consumed for RAM : 32.522006 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:04] Energy consumed for all GPUs : 2.897744 kWh. Total GPU Power : 66.90414650472204 W\n",
      "[codecarbon INFO @ 22:42:04] Energy consumed for all CPUs : 6.062290 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:04] 41.482040 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:06] Energy consumed for RAM : 22.943634 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:06] Energy consumed for all GPUs : 2.043833 kWh. Total GPU Power : 66.8905190333998 W\n",
      "[codecarbon INFO @ 22:42:06] Energy consumed for all CPUs : 4.277196 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:06] 29.264663 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:07] Energy consumed for RAM : 32.538116 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:07] Energy consumed for all GPUs : 2.899283 kWh. Total GPU Power : 66.89255790818615 W\n",
      "[codecarbon INFO @ 22:42:07] Energy consumed for all CPUs : 6.065323 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:07] 41.502722 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:09] Energy consumed for RAM : 22.934817 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:09] Energy consumed for all GPUs : 2.042938 kWh. Total GPU Power : 66.44348053644414 W\n",
      "[codecarbon INFO @ 22:42:09] Energy consumed for all CPUs : 4.275462 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:09] 29.253216 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:19] Energy consumed for RAM : 32.525136 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:19] Energy consumed for all GPUs : 2.898020 kWh. Total GPU Power : 66.43107334734802 W\n",
      "[codecarbon INFO @ 22:42:19] Energy consumed for all CPUs : 6.062874 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:19] 41.486030 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:21] Energy consumed for RAM : 22.946764 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:21] Energy consumed for all GPUs : 2.044110 kWh. Total GPU Power : 66.42990275297232 W\n",
      "[codecarbon INFO @ 22:42:21] Energy consumed for all CPUs : 4.277779 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:21] 29.268653 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:22] Energy consumed for RAM : 32.541246 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:22] Energy consumed for all GPUs : 2.899560 kWh. Total GPU Power : 66.43565731471205 W\n",
      "[codecarbon INFO @ 22:42:22] Energy consumed for all CPUs : 6.065906 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:22] 41.506712 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:24] Energy consumed for RAM : 22.937947 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:24] Energy consumed for all GPUs : 2.043217 kWh. Total GPU Power : 66.87818725463093 W\n",
      "[codecarbon INFO @ 22:42:24] Energy consumed for all CPUs : 4.276045 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:24] 29.257208 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:34] Energy consumed for RAM : 32.528266 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:34] Energy consumed for all GPUs : 2.898297 kWh. Total GPU Power : 66.42769043383056 W\n",
      "[codecarbon INFO @ 22:42:34] Energy consumed for all CPUs : 6.063457 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:34] 41.490019 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:36] Energy consumed for RAM : 22.949893 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:36] Energy consumed for all GPUs : 2.044386 kWh. Total GPU Power : 66.43291461030967 W\n",
      "[codecarbon INFO @ 22:42:36] Energy consumed for all CPUs : 4.278363 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:36] 29.272642 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:37] Energy consumed for RAM : 32.544376 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:37] Energy consumed for all GPUs : 2.899837 kWh. Total GPU Power : 66.41854393774258 W\n",
      "[codecarbon INFO @ 22:42:37] Energy consumed for all CPUs : 6.066489 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:37] 41.510702 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:39] Energy consumed for RAM : 22.941076 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:39] Energy consumed for all GPUs : 2.043493 kWh. Total GPU Power : 66.421467505927 W\n",
      "[codecarbon INFO @ 22:42:39] Energy consumed for all CPUs : 4.276628 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:39] 29.261198 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:49] Energy consumed for RAM : 32.531395 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:49] Energy consumed for all GPUs : 2.898576 kWh. Total GPU Power : 66.89709837550252 W\n",
      "[codecarbon INFO @ 22:42:49] Energy consumed for all CPUs : 6.064040 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:49] 41.494011 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:51] Energy consumed for RAM : 22.953022 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:51] Energy consumed for all GPUs : 2.044665 kWh. Total GPU Power : 66.90593273790718 W\n",
      "[codecarbon INFO @ 22:42:51] Energy consumed for all CPUs : 4.278946 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:51] 29.276633 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:52] Energy consumed for RAM : 32.547505 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:52] Energy consumed for all GPUs : 2.900115 kWh. Total GPU Power : 66.90324077406358 W\n",
      "[codecarbon INFO @ 22:42:52] Energy consumed for all CPUs : 6.067073 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:52] 41.514693 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:42:54] Energy consumed for RAM : 22.944206 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:42:54] Energy consumed for all GPUs : 2.043770 kWh. Total GPU Power : 66.45844535857262 W\n",
      "[codecarbon INFO @ 22:42:54] Energy consumed for all CPUs : 4.277212 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:42:54] 29.265188 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:04] Energy consumed for RAM : 32.534525 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:04] Energy consumed for all GPUs : 2.898852 kWh. Total GPU Power : 66.44885958008341 W\n",
      "[codecarbon INFO @ 22:43:04] Energy consumed for all CPUs : 6.064623 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:04] 41.498001 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:06] Energy consumed for RAM : 22.956152 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:06] Energy consumed for all GPUs : 2.044942 kWh. Total GPU Power : 66.44296745306433 W\n",
      "[codecarbon INFO @ 22:43:06] Energy consumed for all CPUs : 4.279529 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:06] 29.280623 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:07] Energy consumed for RAM : 32.550635 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:07] Energy consumed for all GPUs : 2.900392 kWh. Total GPU Power : 66.44114900455416 W\n",
      "[codecarbon INFO @ 22:43:07] Energy consumed for all CPUs : 6.067656 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:07] 41.518683 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:09] Energy consumed for RAM : 22.947335 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:09] Energy consumed for all GPUs : 2.044049 kWh. Total GPU Power : 66.89767200454202 W\n",
      "[codecarbon INFO @ 22:43:09] Energy consumed for all CPUs : 4.277795 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:09] 29.269179 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:19] Energy consumed for RAM : 32.537655 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:19] Energy consumed for all GPUs : 2.899129 kWh. Total GPU Power : 66.43779714314245 W\n",
      "[codecarbon INFO @ 22:43:19] Energy consumed for all CPUs : 6.065207 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:19] 41.501991 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:21] Energy consumed for RAM : 22.959282 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:21] Energy consumed for all GPUs : 2.045219 kWh. Total GPU Power : 66.44582842505366 W\n",
      "[codecarbon INFO @ 22:43:21] Energy consumed for all CPUs : 4.280112 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:21] 29.284613 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:22] Energy consumed for RAM : 32.553765 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:22] Energy consumed for all GPUs : 2.900669 kWh. Total GPU Power : 66.43813534279404 W\n",
      "[codecarbon INFO @ 22:43:22] Energy consumed for all CPUs : 6.068239 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:22] 41.522673 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:24] Energy consumed for RAM : 22.950465 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:24] Energy consumed for all GPUs : 2.044325 kWh. Total GPU Power : 66.44432942395511 W\n",
      "[codecarbon INFO @ 22:43:24] Energy consumed for all CPUs : 4.278378 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:24] 29.273169 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:34] Energy consumed for RAM : 32.540785 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:34] Energy consumed for all GPUs : 2.899408 kWh. Total GPU Power : 66.88638868950422 W\n",
      "[codecarbon INFO @ 22:43:34] Energy consumed for all CPUs : 6.065790 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:34] 41.505983 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:36] Energy consumed for RAM : 22.962412 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:36] Energy consumed for all GPUs : 2.045497 kWh. Total GPU Power : 66.88866176247112 W\n",
      "[codecarbon INFO @ 22:43:36] Energy consumed for all CPUs : 4.280696 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:36] 29.288605 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:37] Energy consumed for RAM : 32.556894 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:37] Energy consumed for all GPUs : 2.900948 kWh. Total GPU Power : 66.89197575879884 W\n",
      "[codecarbon INFO @ 22:43:37] Energy consumed for all CPUs : 6.068823 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:37] 41.526665 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:39] Energy consumed for RAM : 22.953595 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:39] Energy consumed for all GPUs : 2.044602 kWh. Total GPU Power : 66.44541152105108 W\n",
      "[codecarbon INFO @ 22:43:39] Energy consumed for all CPUs : 4.278962 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:39] 29.277159 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:49] Energy consumed for RAM : 32.543914 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:49] Energy consumed for all GPUs : 2.899685 kWh. Total GPU Power : 66.46934550531319 W\n",
      "[codecarbon INFO @ 22:43:49] Energy consumed for all CPUs : 6.066373 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:49] 41.509972 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:51] Energy consumed for RAM : 22.965528 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:51] Energy consumed for all GPUs : 2.045774 kWh. Total GPU Power : 66.75068830281838 W\n",
      "[codecarbon INFO @ 22:43:51] Energy consumed for all CPUs : 4.281276 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:51] 29.292578 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:52] Energy consumed for RAM : 32.560024 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:52] Energy consumed for all GPUs : 2.901224 kWh. Total GPU Power : 66.46115760774327 W\n",
      "[codecarbon INFO @ 22:43:52] Energy consumed for all CPUs : 6.069406 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:52] 41.530654 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:43:54] Energy consumed for RAM : 22.956724 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:43:54] Energy consumed for all GPUs : 2.044881 kWh. Total GPU Power : 66.90385275724039 W\n",
      "[codecarbon INFO @ 22:43:54] Energy consumed for all CPUs : 4.279545 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:43:54] 29.281150 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:04] Energy consumed for RAM : 32.547044 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:04] Energy consumed for all GPUs : 2.899961 kWh. Total GPU Power : 66.45042826985593 W\n",
      "[codecarbon INFO @ 22:44:04] Energy consumed for all CPUs : 6.066957 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:04] 41.513962 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:06] Energy consumed for RAM : 22.968657 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:06] Energy consumed for all GPUs : 2.046051 kWh. Total GPU Power : 66.45621965578249 W\n",
      "[codecarbon INFO @ 22:44:06] Energy consumed for all CPUs : 4.281860 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:06] 29.296568 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:07] Energy consumed for RAM : 32.563154 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:07] Energy consumed for all GPUs : 2.901501 kWh. Total GPU Power : 66.45320550342794 W\n",
      "[codecarbon INFO @ 22:44:07] Energy consumed for all CPUs : 6.069989 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:07] 41.534644 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:09] Energy consumed for RAM : 22.959854 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:09] Energy consumed for all GPUs : 2.045158 kWh. Total GPU Power : 66.44836423994727 W\n",
      "[codecarbon INFO @ 22:44:09] Energy consumed for all CPUs : 4.280128 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:09] 29.285140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:19] Energy consumed for RAM : 32.550174 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:19] Energy consumed for all GPUs : 2.900238 kWh. Total GPU Power : 66.44402967387795 W\n",
      "[codecarbon INFO @ 22:44:19] Energy consumed for all CPUs : 6.067540 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:19] 41.517952 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:21] Energy consumed for RAM : 22.971773 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:21] Energy consumed for all GPUs : 2.046330 kWh. Total GPU Power : 67.17932433927237 W\n",
      "[codecarbon INFO @ 22:44:21] Energy consumed for all CPUs : 4.282441 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:21] 29.300544 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:22] Energy consumed for RAM : 32.566283 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:22] Energy consumed for all GPUs : 2.901780 kWh. Total GPU Power : 66.89309087703067 W\n",
      "[codecarbon INFO @ 22:44:22] Energy consumed for all CPUs : 6.070572 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:22] 41.538636 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:24] Energy consumed for RAM : 22.962984 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:24] Energy consumed for all GPUs : 2.045435 kWh. Total GPU Power : 66.4524470812371 W\n",
      "[codecarbon INFO @ 22:44:24] Energy consumed for all CPUs : 4.280711 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:24] 29.289130 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:34] Energy consumed for RAM : 32.553303 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:34] Energy consumed for all GPUs : 2.900517 kWh. Total GPU Power : 66.89455143314376 W\n",
      "[codecarbon INFO @ 22:44:34] Energy consumed for all CPUs : 6.068123 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:34] 41.521943 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:36] Energy consumed for RAM : 22.974890 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:36] Energy consumed for all GPUs : 2.046606 kWh. Total GPU Power : 66.73269046098055 W\n",
      "[codecarbon INFO @ 22:44:36] Energy consumed for all CPUs : 4.283021 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:36] 29.304518 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:37] Energy consumed for RAM : 32.569413 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:37] Energy consumed for all GPUs : 2.902057 kWh. Total GPU Power : 66.45068192208754 W\n",
      "[codecarbon INFO @ 22:44:37] Energy consumed for all CPUs : 6.071156 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:37] 41.542625 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:39] Energy consumed for RAM : 22.966113 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:39] Energy consumed for all GPUs : 2.045713 kWh. Total GPU Power : 66.90535413504033 W\n",
      "[codecarbon INFO @ 22:44:39] Energy consumed for all CPUs : 4.281295 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:39] 29.293121 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:49] Energy consumed for RAM : 32.556433 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:49] Energy consumed for all GPUs : 2.900794 kWh. Total GPU Power : 66.44267536900293 W\n",
      "[codecarbon INFO @ 22:44:49] Energy consumed for all CPUs : 6.068707 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:49] 41.525933 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:51] Energy consumed for RAM : 22.978006 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:51] Energy consumed for all GPUs : 2.046883 kWh. Total GPU Power : 66.72553511073552 W\n",
      "[codecarbon INFO @ 22:44:51] Energy consumed for all CPUs : 4.283602 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:51] 29.308492 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:52] Energy consumed for RAM : 32.572543 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:52] Energy consumed for all GPUs : 2.902333 kWh. Total GPU Power : 66.43959808277023 W\n",
      "[codecarbon INFO @ 22:44:52] Energy consumed for all CPUs : 6.071739 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:52] 41.546615 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:44:54] Energy consumed for RAM : 22.969243 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:44:54] Energy consumed for all GPUs : 2.045990 kWh. Total GPU Power : 66.43748418370846 W\n",
      "[codecarbon INFO @ 22:44:54] Energy consumed for all CPUs : 4.281878 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:44:54] 29.297111 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:04] Energy consumed for RAM : 32.559563 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:04] Energy consumed for all GPUs : 2.901070 kWh. Total GPU Power : 66.45457972657691 W\n",
      "[codecarbon INFO @ 22:45:04] Energy consumed for all CPUs : 6.069290 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:04] 41.529923 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:06] Energy consumed for RAM : 22.981136 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:06] Energy consumed for all GPUs : 2.047162 kWh. Total GPU Power : 66.90219395661764 W\n",
      "[codecarbon INFO @ 22:45:06] Energy consumed for all CPUs : 4.284186 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:06] 29.312483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:07] Energy consumed for RAM : 32.575672 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:07] Energy consumed for all GPUs : 2.902612 kWh. Total GPU Power : 66.90299649408966 W\n",
      "[codecarbon INFO @ 22:45:07] Energy consumed for all CPUs : 6.072322 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:07] 41.550607 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:09] Energy consumed for RAM : 22.972372 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:09] Energy consumed for all GPUs : 2.046267 kWh. Total GPU Power : 66.45726798477142 W\n",
      "[codecarbon INFO @ 22:45:09] Energy consumed for all CPUs : 4.282461 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:09] 29.301100 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:19] Energy consumed for RAM : 32.562692 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:19] Energy consumed for all GPUs : 2.901349 kWh. Total GPU Power : 66.89686566128844 W\n",
      "[codecarbon INFO @ 22:45:19] Energy consumed for all CPUs : 6.069873 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:19] 41.533914 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:21] Energy consumed for RAM : 22.984266 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:21] Energy consumed for all GPUs : 2.047439 kWh. Total GPU Power : 66.45131417816886 W\n",
      "[codecarbon INFO @ 22:45:21] Energy consumed for all CPUs : 4.284769 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:21] 29.316473 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:22] Energy consumed for RAM : 32.578802 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:22] Energy consumed for all GPUs : 2.902889 kWh. Total GPU Power : 66.44862263828172 W\n",
      "[codecarbon INFO @ 22:45:22] Energy consumed for all CPUs : 6.072906 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:22] 41.554597 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:24] Energy consumed for RAM : 22.975502 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:24] Energy consumed for all GPUs : 2.046545 kWh. Total GPU Power : 66.89683270892624 W\n",
      "[codecarbon INFO @ 22:45:24] Energy consumed for all CPUs : 4.283045 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:24] 29.305092 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:34] Energy consumed for RAM : 32.565822 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:34] Energy consumed for all GPUs : 2.901626 kWh. Total GPU Power : 66.45135044497147 W\n",
      "[codecarbon INFO @ 22:45:34] Energy consumed for all CPUs : 6.070456 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:34] 41.537904 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:36] Energy consumed for RAM : 22.987395 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:36] Energy consumed for all GPUs : 2.047715 kWh. Total GPU Power : 66.45163692252493 W\n",
      "[codecarbon INFO @ 22:45:36] Energy consumed for all CPUs : 4.285352 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:36] 29.320463 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:37] Energy consumed for RAM : 32.581932 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:37] Energy consumed for all GPUs : 2.903166 kWh. Total GPU Power : 66.45343965674417 W\n",
      "[codecarbon INFO @ 22:45:37] Energy consumed for all CPUs : 6.073489 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:37] 41.558586 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:39] Energy consumed for RAM : 22.978631 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:39] Energy consumed for all GPUs : 2.046822 kWh. Total GPU Power : 66.45276264530197 W\n",
      "[codecarbon INFO @ 22:45:39] Energy consumed for all CPUs : 4.283628 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:39] 29.309082 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:49] Energy consumed for RAM : 32.568952 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:49] Energy consumed for all GPUs : 2.901903 kWh. Total GPU Power : 66.46561632689942 W\n",
      "[codecarbon INFO @ 22:45:49] Energy consumed for all CPUs : 6.071040 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:49] 41.541894 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:51] Energy consumed for RAM : 22.990525 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:51] Energy consumed for all GPUs : 2.047992 kWh. Total GPU Power : 66.46247468961609 W\n",
      "[codecarbon INFO @ 22:45:51] Energy consumed for all CPUs : 4.285935 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:51] 29.324453 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:52] Energy consumed for RAM : 32.585061 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:52] Energy consumed for all GPUs : 2.903443 kWh. Total GPU Power : 66.47312399421797 W\n",
      "[codecarbon INFO @ 22:45:52] Energy consumed for all CPUs : 6.074072 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:52] 41.562576 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:45:54] Energy consumed for RAM : 22.981761 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:45:54] Energy consumed for all GPUs : 2.047099 kWh. Total GPU Power : 66.46891406352627 W\n",
      "[codecarbon INFO @ 22:45:54] Energy consumed for all CPUs : 4.284211 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:45:54] 29.313072 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:04] Energy consumed for RAM : 32.572081 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:04] Energy consumed for all GPUs : 2.902181 kWh. Total GPU Power : 66.91351478294455 W\n",
      "[codecarbon INFO @ 22:46:04] Energy consumed for all CPUs : 6.071623 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:04] 41.545886 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:06] Energy consumed for RAM : 22.993655 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:06] Energy consumed for all GPUs : 2.048271 kWh. Total GPU Power : 66.91490959062104 W\n",
      "[codecarbon INFO @ 22:46:06] Energy consumed for all CPUs : 4.286519 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:06] 29.328444 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:07] Energy consumed for RAM : 32.588191 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:07] Energy consumed for all GPUs : 2.903721 kWh. Total GPU Power : 66.90858225789759 W\n",
      "[codecarbon INFO @ 22:46:07] Energy consumed for all CPUs : 6.074656 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:07] 41.566568 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:09] Energy consumed for RAM : 22.984891 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:09] Energy consumed for all GPUs : 2.047376 kWh. Total GPU Power : 66.46332005835332 W\n",
      "[codecarbon INFO @ 22:46:09] Energy consumed for all CPUs : 4.284795 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:09] 29.317061 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:19] Energy consumed for RAM : 32.575210 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:19] Energy consumed for all GPUs : 2.902458 kWh. Total GPU Power : 66.46883620940407 W\n",
      "[codecarbon INFO @ 22:46:19] Energy consumed for all CPUs : 6.072206 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:19] 41.549875 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:21] Energy consumed for RAM : 22.996784 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:21] Energy consumed for all GPUs : 2.048548 kWh. Total GPU Power : 66.46669661969075 W\n",
      "[codecarbon INFO @ 22:46:21] Energy consumed for all CPUs : 4.287102 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:21] 29.332434 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:22] Energy consumed for RAM : 32.591320 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:22] Energy consumed for all GPUs : 2.903998 kWh. Total GPU Power : 66.46503437443717 W\n",
      "[codecarbon INFO @ 22:46:22] Energy consumed for all CPUs : 6.075239 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:22] 41.570557 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:24] Energy consumed for RAM : 22.988020 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:24] Energy consumed for all GPUs : 2.047655 kWh. Total GPU Power : 66.91127158858286 W\n",
      "[codecarbon INFO @ 22:46:24] Energy consumed for all CPUs : 4.285378 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:24] 29.321052 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:34] Energy consumed for RAM : 32.578340 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:34] Energy consumed for all GPUs : 2.902735 kWh. Total GPU Power : 66.46645030872862 W\n",
      "[codecarbon INFO @ 22:46:34] Energy consumed for all CPUs : 6.072790 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:34] 41.553865 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:36] Energy consumed for RAM : 22.999913 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:36] Energy consumed for all GPUs : 2.048825 kWh. Total GPU Power : 66.47939359421473 W\n",
      "[codecarbon INFO @ 22:46:36] Energy consumed for all CPUs : 4.287685 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:36] 29.336423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:37] Energy consumed for RAM : 32.594449 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:37] Energy consumed for all GPUs : 2.904275 kWh. Total GPU Power : 66.48480446763836 W\n",
      "[codecarbon INFO @ 22:46:37] Energy consumed for all CPUs : 6.075822 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:37] 41.574547 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:39] Energy consumed for RAM : 22.991149 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:39] Energy consumed for all GPUs : 2.047931 kWh. Total GPU Power : 66.4781451281236 W\n",
      "[codecarbon INFO @ 22:46:39] Energy consumed for all CPUs : 4.285961 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:39] 29.325042 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:49] Energy consumed for RAM : 32.581469 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:49] Energy consumed for all GPUs : 2.903014 kWh. Total GPU Power : 66.92529178900769 W\n",
      "[codecarbon INFO @ 22:46:49] Energy consumed for all CPUs : 6.073373 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:49] 41.557856 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:51] Energy consumed for RAM : 23.003042 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:51] Energy consumed for all GPUs : 2.049103 kWh. Total GPU Power : 66.92204612967241 W\n",
      "[codecarbon INFO @ 22:46:51] Energy consumed for all CPUs : 4.288268 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:51] 29.340414 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:52] Energy consumed for RAM : 32.597579 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:52] Energy consumed for all GPUs : 2.904554 kWh. Total GPU Power : 66.91626518021852 W\n",
      "[codecarbon INFO @ 22:46:52] Energy consumed for all CPUs : 6.076405 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:52] 41.578538 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:46:54] Energy consumed for RAM : 22.994279 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:46:54] Energy consumed for all GPUs : 2.048208 kWh. Total GPU Power : 66.47389127108708 W\n",
      "[codecarbon INFO @ 22:46:54] Energy consumed for all CPUs : 4.286544 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:46:54] 29.329031 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:04] Energy consumed for RAM : 32.584598 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:04] Energy consumed for all GPUs : 2.903291 kWh. Total GPU Power : 66.4603070766306 W\n",
      "[codecarbon INFO @ 22:47:04] Energy consumed for all CPUs : 6.073956 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:04] 41.561845 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:06] Energy consumed for RAM : 23.006172 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:06] Energy consumed for all GPUs : 2.049380 kWh. Total GPU Power : 66.45032710800447 W\n",
      "[codecarbon INFO @ 22:47:06] Energy consumed for all CPUs : 4.288852 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:06] 29.344404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:07] Energy consumed for RAM : 32.600709 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:07] Energy consumed for all GPUs : 2.904831 kWh. Total GPU Power : 66.45326102394273 W\n",
      "[codecarbon INFO @ 22:47:07] Energy consumed for all CPUs : 6.076989 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:07] 41.582528 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:09] Energy consumed for RAM : 22.997408 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:09] Energy consumed for all GPUs : 2.048487 kWh. Total GPU Power : 66.90090009425728 W\n",
      "[codecarbon INFO @ 22:47:09] Energy consumed for all CPUs : 4.287128 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:09] 29.333023 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:19] Energy consumed for RAM : 32.587728 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:19] Energy consumed for all GPUs : 2.903568 kWh. Total GPU Power : 66.4827189625966 W\n",
      "[codecarbon INFO @ 22:47:19] Energy consumed for all CPUs : 6.074539 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:19] 41.565835 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:21] Energy consumed for RAM : 23.009302 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:21] Energy consumed for all GPUs : 2.049657 kWh. Total GPU Power : 66.4819626475358 W\n",
      "[codecarbon INFO @ 22:47:21] Energy consumed for all CPUs : 4.289435 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:21] 29.348394 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:22] Energy consumed for RAM : 32.603838 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:22] Energy consumed for all GPUs : 2.905107 kWh. Total GPU Power : 66.48273349109306 W\n",
      "[codecarbon INFO @ 22:47:22] Energy consumed for all CPUs : 6.077572 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:22] 41.586518 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:24] Energy consumed for RAM : 23.000538 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:24] Energy consumed for all GPUs : 2.048764 kWh. Total GPU Power : 66.4865190337468 W\n",
      "[codecarbon INFO @ 22:47:24] Energy consumed for all CPUs : 4.287711 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:24] 29.337013 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:34] Energy consumed for RAM : 32.590857 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:34] Energy consumed for all GPUs : 2.903844 kWh. Total GPU Power : 66.46867810159999 W\n",
      "[codecarbon INFO @ 22:47:34] Energy consumed for all CPUs : 6.075123 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:34] 41.569824 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:36] Energy consumed for RAM : 23.012431 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:36] Energy consumed for all GPUs : 2.049936 kWh. Total GPU Power : 66.91006040879266 W\n",
      "[codecarbon INFO @ 22:47:36] Energy consumed for all CPUs : 4.290018 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:36] 29.352386 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:37] Energy consumed for RAM : 32.606968 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:37] Energy consumed for all GPUs : 2.905384 kWh. Total GPU Power : 66.46378597586808 W\n",
      "[codecarbon INFO @ 22:47:37] Energy consumed for all CPUs : 6.078155 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:37] 41.590507 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:39] Energy consumed for RAM : 23.003667 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:39] Energy consumed for all GPUs : 2.049041 kWh. Total GPU Power : 66.46102757821136 W\n",
      "[codecarbon INFO @ 22:47:39] Energy consumed for all CPUs : 4.288294 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:39] 29.341002 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:49] Energy consumed for RAM : 32.593986 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:49] Energy consumed for all GPUs : 2.904123 kWh. Total GPU Power : 66.90610050893393 W\n",
      "[codecarbon INFO @ 22:47:49] Energy consumed for all CPUs : 6.075706 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:49] 41.573816 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:51] Energy consumed for RAM : 23.015561 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:51] Energy consumed for all GPUs : 2.050213 kWh. Total GPU Power : 66.45951397714798 W\n",
      "[codecarbon INFO @ 22:47:51] Energy consumed for all CPUs : 4.290602 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:51] 29.356375 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:52] Energy consumed for RAM : 32.610097 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:52] Energy consumed for all GPUs : 2.905663 kWh. Total GPU Power : 66.91079017470874 W\n",
      "[codecarbon INFO @ 22:47:52] Energy consumed for all CPUs : 6.078739 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:52] 41.594499 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:47:54] Energy consumed for RAM : 23.006797 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:47:54] Energy consumed for all GPUs : 2.049318 kWh. Total GPU Power : 66.4658304741667 W\n",
      "[codecarbon INFO @ 22:47:54] Energy consumed for all CPUs : 4.288877 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:47:54] 29.344992 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:04] Energy consumed for RAM : 32.597116 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:04] Energy consumed for all GPUs : 2.904400 kWh. Total GPU Power : 66.47577602799072 W\n",
      "[codecarbon INFO @ 22:48:04] Energy consumed for all CPUs : 6.076289 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:04] 41.577805 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:06] Energy consumed for RAM : 23.018690 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:06] Energy consumed for all GPUs : 2.050490 kWh. Total GPU Power : 66.47577537681079 W\n",
      "[codecarbon INFO @ 22:48:06] Energy consumed for all CPUs : 4.291185 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:06] 29.360365 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:07] Energy consumed for RAM : 32.613227 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:07] Energy consumed for all GPUs : 2.905940 kWh. Total GPU Power : 66.47383937700555 W\n",
      "[codecarbon INFO @ 22:48:07] Energy consumed for all CPUs : 6.079322 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:07] 41.598488 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:09] Energy consumed for RAM : 23.009926 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:09] Energy consumed for all GPUs : 2.049596 kWh. Total GPU Power : 66.91863347737498 W\n",
      "[codecarbon INFO @ 22:48:09] Energy consumed for all CPUs : 4.289461 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:09] 29.348983 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:19] Energy consumed for RAM : 32.600245 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:19] Energy consumed for all GPUs : 2.904677 kWh. Total GPU Power : 66.47326434038023 W\n",
      "[codecarbon INFO @ 22:48:19] Energy consumed for all CPUs : 6.076873 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:19] 41.581795 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:21] Energy consumed for RAM : 23.021819 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:21] Energy consumed for all GPUs : 2.050766 kWh. Total GPU Power : 66.480620441305 W\n",
      "[codecarbon INFO @ 22:48:21] Energy consumed for all CPUs : 4.291768 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:21] 29.364354 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:22] Energy consumed for RAM : 32.616356 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:22] Energy consumed for all GPUs : 2.906217 kWh. Total GPU Power : 66.47078272243064 W\n",
      "[codecarbon INFO @ 22:48:22] Energy consumed for all CPUs : 6.079905 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:22] 41.602478 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:24] Energy consumed for RAM : 23.013056 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:24] Energy consumed for all GPUs : 2.049873 kWh. Total GPU Power : 66.47445467066255 W\n",
      "[codecarbon INFO @ 22:48:24] Energy consumed for all CPUs : 4.290044 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:24] 29.352973 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:34] Energy consumed for RAM : 32.603375 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:34] Energy consumed for all GPUs : 2.904956 kWh. Total GPU Power : 66.91332409118881 W\n",
      "[codecarbon INFO @ 22:48:34] Energy consumed for all CPUs : 6.077456 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:34] 41.585786 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:36] Energy consumed for RAM : 23.024949 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:36] Energy consumed for all GPUs : 2.051045 kWh. Total GPU Power : 66.91065280923404 W\n",
      "[codecarbon INFO @ 22:48:36] Energy consumed for all CPUs : 4.292352 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:36] 29.368345 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:37] Energy consumed for RAM : 32.619486 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:37] Energy consumed for all GPUs : 2.906495 kWh. Total GPU Power : 66.90762295988397 W\n",
      "[codecarbon INFO @ 22:48:37] Energy consumed for all CPUs : 6.080488 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:37] 41.606470 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:39] Energy consumed for RAM : 23.016185 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:39] Energy consumed for all GPUs : 2.050150 kWh. Total GPU Power : 66.46191789202025 W\n",
      "[codecarbon INFO @ 22:48:39] Energy consumed for all CPUs : 4.290627 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:39] 29.356962 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:49] Energy consumed for RAM : 32.606504 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:49] Energy consumed for all GPUs : 2.905232 kWh. Total GPU Power : 66.4625642336276 W\n",
      "[codecarbon INFO @ 22:48:49] Energy consumed for all CPUs : 6.078039 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:49] 41.589776 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:51] Energy consumed for RAM : 23.028078 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:51] Energy consumed for all GPUs : 2.051322 kWh. Total GPU Power : 66.47429832961201 W\n",
      "[codecarbon INFO @ 22:48:51] Energy consumed for all CPUs : 4.292935 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:51] 29.372335 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:52] Energy consumed for RAM : 32.622615 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:52] Energy consumed for all GPUs : 2.906772 kWh. Total GPU Power : 66.4722470093041 W\n",
      "[codecarbon INFO @ 22:48:52] Energy consumed for all CPUs : 6.081072 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:52] 41.610459 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:48:54] Energy consumed for RAM : 23.019314 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:48:54] Energy consumed for all GPUs : 2.050429 kWh. Total GPU Power : 66.92263960448408 W\n",
      "[codecarbon INFO @ 22:48:54] Energy consumed for all CPUs : 4.291210 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:48:54] 29.360954 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:04] Energy consumed for RAM : 32.609634 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:04] Energy consumed for all GPUs : 2.905509 kWh. Total GPU Power : 66.4579582419825 W\n",
      "[codecarbon INFO @ 22:49:04] Energy consumed for all CPUs : 6.078622 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:04] 41.593766 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:06] Energy consumed for RAM : 23.031208 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:06] Energy consumed for all GPUs : 2.051599 kWh. Total GPU Power : 66.44494107590971 W\n",
      "[codecarbon INFO @ 22:49:06] Energy consumed for all CPUs : 4.293518 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:06] 29.376325 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:07] Energy consumed for RAM : 32.625745 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:07] Energy consumed for all GPUs : 2.907049 kWh. Total GPU Power : 66.44778714769868 W\n",
      "[codecarbon INFO @ 22:49:07] Energy consumed for all CPUs : 6.081655 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:07] 41.614449 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:09] Energy consumed for RAM : 23.022444 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:09] Energy consumed for all GPUs : 2.050706 kWh. Total GPU Power : 66.43433607085669 W\n",
      "[codecarbon INFO @ 22:49:09] Energy consumed for all CPUs : 4.291794 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:09] 29.364944 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:19] Energy consumed for RAM : 32.612764 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:19] Energy consumed for all GPUs : 2.905786 kWh. Total GPU Power : 66.44678472856792 W\n",
      "[codecarbon INFO @ 22:49:19] Energy consumed for all CPUs : 6.079206 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:19] 41.597755 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:21] Energy consumed for RAM : 23.034338 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:21] Energy consumed for all GPUs : 2.051877 kWh. Total GPU Power : 66.89979182623024 W\n",
      "[codecarbon INFO @ 22:49:21] Energy consumed for all CPUs : 4.294101 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:21] 29.380316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:22] Energy consumed for RAM : 32.628875 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:22] Energy consumed for all GPUs : 2.907328 kWh. Total GPU Power : 66.90092031862315 W\n",
      "[codecarbon INFO @ 22:49:22] Energy consumed for all CPUs : 6.082238 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:22] 41.618441 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:24] Energy consumed for RAM : 23.025574 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:24] Energy consumed for all GPUs : 2.050982 kWh. Total GPU Power : 66.45898457600966 W\n",
      "[codecarbon INFO @ 22:49:24] Energy consumed for all CPUs : 4.292377 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:24] 29.368933 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:34] Energy consumed for RAM : 32.615893 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:34] Energy consumed for all GPUs : 2.906065 kWh. Total GPU Power : 66.91625165245948 W\n",
      "[codecarbon INFO @ 22:49:34] Energy consumed for all CPUs : 6.079789 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:34] 41.601747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:36] Energy consumed for RAM : 23.037468 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:36] Energy consumed for all GPUs : 2.052154 kWh. Total GPU Power : 66.46633691141908 W\n",
      "[codecarbon INFO @ 22:49:36] Energy consumed for all CPUs : 4.294685 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:36] 29.384307 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:37] Energy consumed for RAM : 32.632004 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:37] Energy consumed for all GPUs : 2.907605 kWh. Total GPU Power : 66.48092697732883 W\n",
      "[codecarbon INFO @ 22:49:37] Energy consumed for all CPUs : 6.082822 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:37] 41.622430 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:39] Energy consumed for RAM : 23.028704 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:39] Energy consumed for all GPUs : 2.051261 kWh. Total GPU Power : 66.92340225362186 W\n",
      "[codecarbon INFO @ 22:49:39] Energy consumed for all CPUs : 4.292960 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:39] 29.372925 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:49:49] Energy consumed for RAM : 32.619023 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 22:49:49] Energy consumed for all GPUs : 2.906342 kWh. Total GPU Power : 66.46192669000524 W\n",
      "[codecarbon INFO @ 22:49:49] Energy consumed for all CPUs : 6.080372 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 22:49:49] 41.605737 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Calculate text lengths\n",
    "dataset_with_lengths = dataset.map(\n",
    "    get_length,\n",
    "    batched=True,\n",
    "    num_proc=32,\n",
    "    desc=\"Calculating text and token lengths\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_length', 'token_length'],\n",
       "    num_rows: 9951012\n",
       "})"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:22:34] Energy consumed for RAM : 33.028989 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:22:34] Energy consumed for all GPUs : 2.942701 kWh. Total GPU Power : 66.61825633233987 W\n",
      "[codecarbon INFO @ 23:22:34] Energy consumed for all CPUs : 6.156784 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:22:34] 42.128473 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:22:36] Energy consumed for RAM : 23.450562 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:22:36] Energy consumed for all GPUs : 2.088790 kWh. Total GPU Power : 66.63989956644734 W\n",
      "[codecarbon INFO @ 23:22:36] Energy consumed for all CPUs : 4.371679 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:22:36] 29.911032 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:22:37] Energy consumed for RAM : 33.045097 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:22:37] Energy consumed for all GPUs : 2.944243 kWh. Total GPU Power : 67.11247163520632 W\n",
      "[codecarbon INFO @ 23:22:37] Energy consumed for all CPUs : 6.159818 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:22:37] 42.149158 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:22:39] Energy consumed for RAM : 23.441802 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:22:39] Energy consumed for all GPUs : 2.087897 kWh. Total GPU Power : 67.15168280361935 W\n",
      "[codecarbon INFO @ 23:22:39] Energy consumed for all CPUs : 4.369956 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:22:39] 29.899655 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:22:49] Energy consumed for RAM : 33.032118 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:22:49] Energy consumed for all GPUs : 2.942979 kWh. Total GPU Power : 66.70261301244945 W\n",
      "[codecarbon INFO @ 23:22:49] Energy consumed for all CPUs : 6.157367 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:22:49] 42.132464 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:22:51] Energy consumed for RAM : 23.453692 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:22:51] Energy consumed for all GPUs : 2.089068 kWh. Total GPU Power : 66.71146918841708 W\n",
      "[codecarbon INFO @ 23:22:51] Energy consumed for all CPUs : 4.372263 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:22:51] 29.915022 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:22:52] Energy consumed for RAM : 33.048226 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:22:52] Energy consumed for all GPUs : 2.944520 kWh. Total GPU Power : 66.69278349486413 W\n",
      "[codecarbon INFO @ 23:22:52] Energy consumed for all CPUs : 6.160402 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:22:52] 42.153148 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:22:54] Energy consumed for RAM : 23.444932 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:22:54] Energy consumed for all GPUs : 2.088175 kWh. Total GPU Power : 66.66264400839908 W\n",
      "[codecarbon INFO @ 23:22:54] Energy consumed for all CPUs : 4.370539 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:22:54] 29.903646 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:04] Energy consumed for RAM : 33.035248 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:23:04] Energy consumed for all GPUs : 2.943256 kWh. Total GPU Power : 66.65182092462146 W\n",
      "[codecarbon INFO @ 23:23:04] Energy consumed for all CPUs : 6.157950 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:04] 42.136455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:06] Energy consumed for RAM : 23.456821 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:23:06] Energy consumed for all GPUs : 2.089346 kWh. Total GPU Power : 66.68626995827037 W\n",
      "[codecarbon INFO @ 23:23:06] Energy consumed for all CPUs : 4.372846 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:06] 29.919013 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:07] Energy consumed for RAM : 33.051356 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:23:07] Energy consumed for all GPUs : 2.944798 kWh. Total GPU Power : 66.67291685325605 W\n",
      "[codecarbon INFO @ 23:23:07] Energy consumed for all CPUs : 6.160985 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:07] 42.157139 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:09] Energy consumed for RAM : 23.448061 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:23:09] Energy consumed for all GPUs : 2.088453 kWh. Total GPU Power : 66.70055556811249 W\n",
      "[codecarbon INFO @ 23:23:09] Energy consumed for all CPUs : 4.371122 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:09] 29.907636 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "dataset_with_lengths.select_columns([\"text_length\", \"token_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:23:42] Energy consumed for RAM : 23.454031 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:23:42] Energy consumed for RAM : 23.463416 kWh. RAM Power : 751.453685760498 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length statistics:\n",
      "Min: 7\n",
      "Max: 1375457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:23:42] Energy consumed for RAM : 33.042284 kWh. RAM Power : 751.453685760498 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1144.73\n",
      "Median: 454\n",
      "\n",
      "Token length statistics:\n",
      "Min: 10\n",
      "Max: 3313168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:23:42] Energy consumed for RAM : 33.057721 kWh. RAM Power : 751.453685760498 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 2667.74\n",
      "Median: 780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:23:44] Energy consumed for all GPUs : 2.089070 kWh. Total GPU Power : 66.76086266389258 W\n",
      "[codecarbon INFO @ 23:23:46] Energy consumed for all GPUs : 2.090060 kWh. Total GPU Power : 66.72001540597354 W\n",
      "[codecarbon INFO @ 23:23:46] Energy consumed for all GPUs : 2.944041 kWh. Total GPU Power : 66.6724377193135 W\n",
      "[codecarbon INFO @ 23:23:46] Energy consumed for all CPUs : 4.372572 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:46] Energy consumed for all GPUs : 2.945524 kWh. Total GPU Power : 66.65297334489856 W\n",
      "[codecarbon INFO @ 23:23:46] Energy consumed for all CPUs : 4.374412 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:46] Energy consumed for all CPUs : 6.159599 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:46] 29.915674 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:46] Energy consumed for all CPUs : 6.162509 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:46] 29.927889 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:46] 42.145925 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:46] 42.165754 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for RAM : 23.455308 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for RAM : 23.464693 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for RAM : 33.058998 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for RAM : 33.043561 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for all GPUs : 2.090205 kWh. Total GPU Power : 85.23981130013927 W\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for all CPUs : 4.374651 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:52] 29.929548 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for all GPUs : 2.089256 kWh. Total GPU Power : 109.24570610497724 W\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for all CPUs : 4.372811 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:52] 29.917374 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for all GPUs : 2.945639 kWh. Total GPU Power : 67.76241976786956 W\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for all CPUs : 6.162747 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:52] 42.167384 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for all GPUs : 2.944157 kWh. Total GPU Power : 67.74746292884245 W\n",
      "[codecarbon INFO @ 23:23:52] Energy consumed for all CPUs : 6.159838 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:23:52] 42.147555 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for RAM : 23.467821 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for RAM : 33.062125 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for RAM : 23.458436 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for all GPUs : 2.090484 kWh. Total GPU Power : 66.92861200015786 W\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for all CPUs : 4.375234 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for RAM : 33.046689 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:07] 29.933539 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for all GPUs : 2.945916 kWh. Total GPU Power : 66.50121150261188 W\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for all CPUs : 6.163330 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:07] 42.171371 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for all GPUs : 2.089535 kWh. Total GPU Power : 66.91533711116375 W\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for all CPUs : 4.373394 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:07] 29.921365 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for all GPUs : 2.944433 kWh. Total GPU Power : 66.49428716984607 W\n",
      "[codecarbon INFO @ 23:24:07] Energy consumed for all CPUs : 6.160421 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:07] 42.151544 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for RAM : 23.470950 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for RAM : 23.461565 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for RAM : 33.065254 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for all GPUs : 2.090761 kWh. Total GPU Power : 66.46500698169434 W\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for RAM : 33.049818 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for all CPUs : 4.375817 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:22] 29.937528 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for all GPUs : 2.089812 kWh. Total GPU Power : 66.4731014121714 W\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for all CPUs : 4.373977 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:22] 29.925354 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for all GPUs : 2.946192 kWh. Total GPU Power : 66.45756117028857 W\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for all CPUs : 6.163914 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for all GPUs : 2.944710 kWh. Total GPU Power : 66.46387615920595 W\n",
      "[codecarbon INFO @ 23:24:22] 42.175361 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:22] Energy consumed for all CPUs : 6.161004 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:22] 42.155533 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for RAM : 23.474079 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for RAM : 23.464693 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for RAM : 33.068382 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for all GPUs : 2.091038 kWh. Total GPU Power : 66.47917880370439 W\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for all CPUs : 4.376400 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for all GPUs : 2.090089 kWh. Total GPU Power : 66.47771779752712 W\n",
      "[codecarbon INFO @ 23:24:37] 29.941517 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for all CPUs : 4.374560 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for RAM : 33.052947 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:37] 29.929343 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for all GPUs : 2.946469 kWh. Total GPU Power : 66.48118541323359 W\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for all CPUs : 6.164497 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:37] 42.179349 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for all GPUs : 2.944987 kWh. Total GPU Power : 66.46511584982244 W\n",
      "[codecarbon INFO @ 23:24:37] Energy consumed for all CPUs : 6.161587 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:37] 42.159522 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for RAM : 23.477207 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for RAM : 33.071511 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for all GPUs : 2.091316 kWh. Total GPU Power : 66.91850418857737 W\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for RAM : 23.467822 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for all CPUs : 4.376983 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for all GPUs : 2.946748 kWh. Total GPU Power : 66.92087647728007 W\n",
      "[codecarbon INFO @ 23:24:52] 29.945507 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for RAM : 33.056077 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for all CPUs : 6.165080 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:52] 42.183339 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for all GPUs : 2.090367 kWh. Total GPU Power : 66.88943735852781 W\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for all CPUs : 4.375144 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for all GPUs : 2.945266 kWh. Total GPU Power : 66.89108023103982 W\n",
      "[codecarbon INFO @ 23:24:52] 29.933334 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:24:52] Energy consumed for all CPUs : 6.162171 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:24:52] 42.163513 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for RAM : 23.480336 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for RAM : 33.074639 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for RAM : 23.470950 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for all GPUs : 2.091593 kWh. Total GPU Power : 66.50221742823338 W\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for all CPUs : 4.377567 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:07] 29.949496 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for RAM : 33.059205 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for all GPUs : 2.947025 kWh. Total GPU Power : 66.49056438530488 W\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for all GPUs : 2.090644 kWh. Total GPU Power : 66.49596945943729 W\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for all CPUs : 6.165664 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for all CPUs : 4.375727 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:07] 42.187327 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:07] 29.937321 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for all GPUs : 2.945543 kWh. Total GPU Power : 66.48649979009762 W\n",
      "[codecarbon INFO @ 23:25:07] Energy consumed for all CPUs : 6.162754 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:07] 42.167502 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for RAM : 23.483464 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for RAM : 33.077767 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for RAM : 23.474079 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for all GPUs : 2.091870 kWh. Total GPU Power : 66.57701347027786 W\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for all CPUs : 4.378150 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:22] 29.953485 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for all GPUs : 2.947302 kWh. Total GPU Power : 66.58345401082978 W\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for all CPUs : 6.166247 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:22] 42.191316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for RAM : 33.062335 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for all GPUs : 2.090921 kWh. Total GPU Power : 66.56229978519622 W\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for all CPUs : 4.376310 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:22] 29.941310 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for all GPUs : 2.945820 kWh. Total GPU Power : 66.54155352471969 W\n",
      "[codecarbon INFO @ 23:25:22] Energy consumed for all CPUs : 6.163337 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:22] 42.171492 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for RAM : 23.486593 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for RAM : 33.080896 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for all GPUs : 2.092148 kWh. Total GPU Power : 66.56904170918607 W\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for all CPUs : 4.378733 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for all GPUs : 2.947581 kWh. Total GPU Power : 67.0183592458351 W\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for RAM : 23.477207 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:37] 29.957474 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for all CPUs : 6.166830 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for RAM : 33.065464 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:37] 42.195307 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for all GPUs : 2.091201 kWh. Total GPU Power : 67.00103318035066 W\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for all CPUs : 4.376894 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:37] 29.945302 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for all GPUs : 2.946099 kWh. Total GPU Power : 67.0017681920726 W\n",
      "[codecarbon INFO @ 23:25:37] Energy consumed for all CPUs : 6.163921 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:37] 42.175484 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for RAM : 23.489721 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for RAM : 33.084024 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for all GPUs : 2.092427 kWh. Total GPU Power : 66.99062169190543 W\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for all CPUs : 4.379316 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:52] 29.961464 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for all GPUs : 2.947858 kWh. Total GPU Power : 66.53813963645804 W\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for RAM : 23.480336 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for all CPUs : 6.167413 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:52] 42.199295 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for RAM : 33.068593 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for all GPUs : 2.091478 kWh. Total GPU Power : 66.52649124324923 W\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for all CPUs : 4.377477 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:52] 29.949290 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for all GPUs : 2.946376 kWh. Total GPU Power : 66.52463223787302 W\n",
      "[codecarbon INFO @ 23:25:52] Energy consumed for all CPUs : 6.164504 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:25:52] 42.179473 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for RAM : 23.492851 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for RAM : 33.087153 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for all GPUs : 2.092704 kWh. Total GPU Power : 66.5192130753536 W\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for all CPUs : 4.379899 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:26:07] 29.965454 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for all GPUs : 2.948135 kWh. Total GPU Power : 66.52710116717033 W\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for all CPUs : 6.167996 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:26:07] 42.203285 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for RAM : 23.483466 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for RAM : 33.071722 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for all GPUs : 2.091755 kWh. Total GPU Power : 66.50021190636964 W\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for all CPUs : 4.378060 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:26:07] 29.953280 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for all GPUs : 2.946653 kWh. Total GPU Power : 66.50905344816307 W\n",
      "[codecarbon INFO @ 23:26:07] Energy consumed for all CPUs : 6.165087 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:26:07] 42.183463 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "text_lengths = dataset_with_lengths['text_length']\n",
    "token_lengths = dataset_with_lengths['token_length']\n",
    "\n",
    "print(\"Text length statistics:\")\n",
    "print(f\"Min: {min(text_lengths)}\")\n",
    "print(f\"Max: {max(text_lengths)}\")\n",
    "print(f\"Mean: {sum(text_lengths) / len(text_lengths):.2f}\")\n",
    "print(f\"Median: {sorted(text_lengths)[len(text_lengths)//2]}\")\n",
    "\n",
    "print(\"\\nToken length statistics:\")\n",
    "print(f\"Min: {min(token_lengths)}\")\n",
    "print(f\"Max: {max(token_lengths)}\")\n",
    "print(f\"Mean: {sum(token_lengths) / len(token_lengths):.2f}\")\n",
    "print(f\"Median: {sorted(token_lengths)[len(token_lengths)//2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 23:26:52] Energy consumed for RAM : 23.502238 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for RAM : 33.096541 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for all GPUs : 2.093536 kWh. Total GPU Power : 66.45655713543026 W\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for all CPUs : 4.381649 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:26:52] 29.977424 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for all GPUs : 2.948968 kWh. Total GPU Power : 66.4350709609135 W\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for RAM : 23.492854 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for all CPUs : 6.169746 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for RAM : 33.081110 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:26:52] 42.215255 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for all GPUs : 2.092587 kWh. Total GPU Power : 66.43232953928927 W\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for all CPUs : 4.379810 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:26:52] 29.965251 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for all GPUs : 2.947485 kWh. Total GPU Power : 66.43885293118701 W\n",
      "[codecarbon INFO @ 23:26:52] Energy consumed for all CPUs : 6.166837 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:26:52] 42.195433 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for RAM : 23.505368 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for RAM : 33.099669 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for all GPUs : 2.093813 kWh. Total GPU Power : 66.43142875076569 W\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for all CPUs : 4.382232 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:07] 29.981413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for all GPUs : 2.949246 kWh. Total GPU Power : 66.91329337444509 W\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for all CPUs : 6.170329 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:07] 42.219244 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for RAM : 23.495982 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for RAM : 33.084239 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for all GPUs : 2.092866 kWh. Total GPU Power : 66.89289191977852 W\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for all CPUs : 4.380393 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:07] 29.969241 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for all GPUs : 2.947764 kWh. Total GPU Power : 66.89535497174 W\n",
      "[codecarbon INFO @ 23:27:07] Energy consumed for all CPUs : 6.167421 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:07] 42.199423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for RAM : 23.508497 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for RAM : 33.102798 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for all GPUs : 2.094091 kWh. Total GPU Power : 66.88359157516824 W\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for all CPUs : 4.382816 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:22] 29.985404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for all GPUs : 2.949523 kWh. Total GPU Power : 66.4434361650647 W\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for all CPUs : 6.170912 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:22] 42.223233 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for RAM : 23.499112 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for RAM : 33.087369 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for all GPUs : 2.093142 kWh. Total GPU Power : 66.43828314939577 W\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for all CPUs : 4.380977 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:22] 29.973231 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for all GPUs : 2.948041 kWh. Total GPU Power : 66.44305689783485 W\n",
      "[codecarbon INFO @ 23:27:22] Energy consumed for all CPUs : 6.168004 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:22] 42.203413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for RAM : 23.511627 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for RAM : 33.105927 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for all GPUs : 2.094368 kWh. Total GPU Power : 66.45405187907616 W\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for all CPUs : 4.383399 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:37] 29.989394 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for all GPUs : 2.949800 kWh. Total GPU Power : 66.45133958325336 W\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for all CPUs : 6.171496 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:37] 42.227223 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for RAM : 23.502241 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for RAM : 33.090498 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for all GPUs : 2.093419 kWh. Total GPU Power : 66.45857885264108 W\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for all CPUs : 4.381560 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:37] 29.977221 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for all GPUs : 2.948318 kWh. Total GPU Power : 66.45962826019577 W\n",
      "[codecarbon INFO @ 23:27:37] Energy consumed for all CPUs : 6.168587 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:37] 42.207403 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for RAM : 23.514756 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for RAM : 33.109057 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for all GPUs : 2.094645 kWh. Total GPU Power : 66.44671904778691 W\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for all CPUs : 4.383982 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:52] 29.993383 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for all GPUs : 2.950078 kWh. Total GPU Power : 66.8948597147653 W\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for all CPUs : 6.172079 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:52] 42.231214 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for RAM : 23.505371 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for RAM : 33.093628 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for all GPUs : 2.093698 kWh. Total GPU Power : 66.88613872147978 W\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for all CPUs : 4.382143 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:52] 29.981212 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for all GPUs : 2.948596 kWh. Total GPU Power : 66.8895694165288 W\n",
      "[codecarbon INFO @ 23:27:52] Energy consumed for all CPUs : 6.169170 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:27:52] 42.211394 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for RAM : 23.517886 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for RAM : 33.112186 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for all GPUs : 2.094923 kWh. Total GPU Power : 66.901567219165 W\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for all CPUs : 4.384566 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:07] 29.997375 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for all GPUs : 2.950355 kWh. Total GPU Power : 66.44734049280132 W\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for all CPUs : 6.172662 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:07] 42.235203 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for RAM : 23.508501 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for RAM : 33.096757 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for all GPUs : 2.093974 kWh. Total GPU Power : 66.44701193016863 W\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for all CPUs : 4.382727 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:07] 29.985202 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for all GPUs : 2.948873 kWh. Total GPU Power : 66.45113134750095 W\n",
      "[codecarbon INFO @ 23:28:07] Energy consumed for all CPUs : 6.169754 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:07] 42.215384 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_stats_in_batches(dataset, batch_size=10000):\n",
    "    min_length = float('inf')\n",
    "    max_length = float('-inf')\n",
    "    total_length = 0\n",
    "    count = 0\n",
    "    all_lengths = []\n",
    "\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing batches\"):\n",
    "        batch = dataset.select(range(i, min(i + batch_size, len(dataset))))\n",
    "        lengths = batch['text_length']\n",
    "        \n",
    "        min_length = min(min_length, min(lengths))\n",
    "        max_length = max(max_length, max(lengths))\n",
    "        total_length += sum(lengths)\n",
    "        count += len(lengths)\n",
    "        all_lengths.extend(lengths)\n",
    "\n",
    "    mean_length = total_length / count\n",
    "    median_length = statistics.median(all_lengths)\n",
    "\n",
    "    return min_length, max_length, mean_length, median_length\n",
    "\n",
    "def calculate_token_stats_in_batches(dataset, batch_size=10000):\n",
    "    min_length = float('inf')\n",
    "    max_length = float('-inf')\n",
    "    total_length = 0\n",
    "    count = 0\n",
    "    all_lengths = []\n",
    "\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing batches\"):\n",
    "        batch = dataset.select(range(i, min(i + batch_size, len(dataset))))\n",
    "        lengths = batch['token_length']\n",
    "        \n",
    "        min_length = min(min_length, min(lengths))\n",
    "        max_length = max(max_length, max(lengths))\n",
    "        total_length += sum(lengths)\n",
    "        count += len(lengths)\n",
    "        all_lengths.extend(lengths)\n",
    "\n",
    "    mean_length = total_length / count\n",
    "    median_length = statistics.median(all_lengths)\n",
    "\n",
    "    return min_length, max_length, mean_length, median_length\n",
    "\n",
    "print(\"Computing text length statistics...\")\n",
    "min_length, max_length, mean_length, median_length = calculate_stats_in_batches(dataset_with_lengths)\n",
    "\n",
    "print(\"Text length statistics:\")\n",
    "print(f\"Min: {min_length}\")\n",
    "print(f\"Max: {max_length}\")\n",
    "print(f\"Mean: {mean_length:.2f}\")\n",
    "print(f\"Median: {median_length}\")\n",
    "\n",
    "\n",
    "min_length, max_length, mean_length, median_length = calculate_token_stats_in_batches(dataset_with_lengths)\n",
    "print(\"Token length statistics:\")\n",
    "print(f\"Min: {min_length}\")\n",
    "print(f\"Max: {max_length}\")\n",
    "print(f\"Mean: {mean_length:.2f}\")\n",
    "print(f\"Median: {median_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting text and token category frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  49%|██████████████████████████████████████████████████████████████▌                                                                 | 487/996 [00:09<00:10, 50.23it/s][codecarbon INFO @ 23:28:22] Energy consumed for RAM : 33.115316 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for RAM : 23.521016 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for all GPUs : 2.950632 kWh. Total GPU Power : 66.31233311481951 W\n",
      "Processing batches:  49%|███████████████████████████████████████████████████████████████▎                                                                | 493/996 [00:09<00:10, 47.78it/s][codecarbon INFO @ 23:28:22] Energy consumed for RAM : 33.099892 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for RAM : 23.511636 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for all GPUs : 2.095200 kWh. Total GPU Power : 66.28709540769927 W\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for all CPUs : 6.173248 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for all CPUs : 4.385152 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for all GPUs : 2.949152 kWh. Total GPU Power : 66.67112631746824 W\n",
      "[codecarbon INFO @ 23:28:22] 42.239196 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for all GPUs : 2.094253 kWh. Total GPU Power : 66.64516591130804 W\n",
      "[codecarbon INFO @ 23:28:22] 30.001368 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for all CPUs : 6.170341 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:22] Energy consumed for all CPUs : 4.383315 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:22] 42.219384 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:22] 29.989204 kWh of electricity used since the beginning.\n",
      "Processing batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 996/996 [00:19<00:00, 51.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text length categories:\n",
      "Text length range 100-499: 3503223\n",
      "Text length range 1000-1999: 2042850\n",
      "Text length range < 100: 1630380\n",
      "Text length range 5000+: 301685\n",
      "Text length range 2000-3499: 950101\n",
      "Text length range 3500-4999: 257310\n",
      "Text length range 500-999: 1265463\n",
      "\n",
      "Token length categories:\n",
      "Token length range 500-999: 841744\n",
      "Token length range 3500-4999: 1008347\n",
      "Token length range 1000-1999: 800922\n",
      "Token length range < 100: 1097486\n",
      "Token length range 5000+: 1429656\n",
      "Token length range 100-499: 3229441\n",
      "Token length range 2000-3499: 1543416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for RAM : 33.118421 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for RAM : 23.524119 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for all GPUs : 2.950909 kWh. Total GPU Power : 66.98762612818841 W\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for all CPUs : 6.173826 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:37] 42.243156 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for all GPUs : 2.095479 kWh. Total GPU Power : 67.49417228308731 W\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for all CPUs : 4.385730 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:37] 30.005327 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for RAM : 23.514730 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for all GPUs : 2.094530 kWh. Total GPU Power : 67.22531727997752 W\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for all CPUs : 4.383892 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:37] 29.993151 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for RAM : 33.102995 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for all GPUs : 2.949428 kWh. Total GPU Power : 67.02760635492076 W\n",
      "[codecarbon INFO @ 23:28:37] Energy consumed for all CPUs : 6.170919 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:37] 42.223342 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for RAM : 33.121550 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for RAM : 23.527248 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for all GPUs : 2.951188 kWh. Total GPU Power : 66.92667320920006 W\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for all CPUs : 6.174410 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:52] 42.247147 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for all GPUs : 2.095756 kWh. Total GPU Power : 66.48981084223239 W\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for all CPUs : 4.386313 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:52] 30.009316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for RAM : 23.517859 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for all GPUs : 2.094807 kWh. Total GPU Power : 66.48114428174276 W\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for all CPUs : 4.384475 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:52] 29.997141 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for RAM : 33.106124 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for all GPUs : 2.949705 kWh. Total GPU Power : 66.4804666576655 W\n",
      "[codecarbon INFO @ 23:28:52] Energy consumed for all CPUs : 6.171503 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:28:52] 42.227332 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for RAM : 33.124679 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for RAM : 23.530377 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for all GPUs : 2.951464 kWh. Total GPU Power : 66.46289919724195 W\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for all CPUs : 6.174993 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:07] 42.251137 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for all GPUs : 2.096033 kWh. Total GPU Power : 66.46185615335413 W\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for all CPUs : 4.386896 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:07] 30.013306 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for RAM : 23.520989 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for all GPUs : 2.095084 kWh. Total GPU Power : 66.45541896164958 W\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for all CPUs : 4.385058 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:07] 30.001131 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for RAM : 33.109251 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for all GPUs : 2.949982 kWh. Total GPU Power : 66.52620672679916 W\n",
      "[codecarbon INFO @ 23:29:07] Energy consumed for all CPUs : 6.172086 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:07] 42.231318 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for RAM : 33.127809 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for RAM : 23.533507 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for all GPUs : 2.951742 kWh. Total GPU Power : 66.53219951992064 W\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for all CPUs : 6.175576 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:22] 42.255127 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for all GPUs : 2.096310 kWh. Total GPU Power : 66.53745064796583 W\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for all CPUs : 4.387480 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:22] 30.017297 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for RAM : 23.524119 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for all GPUs : 2.095363 kWh. Total GPU Power : 66.98162586944184 W\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for all CPUs : 4.385641 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:22] 30.005123 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for RAM : 33.112380 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for all GPUs : 2.950261 kWh. Total GPU Power : 66.99878972312158 W\n",
      "[codecarbon INFO @ 23:29:22] Energy consumed for all CPUs : 6.172669 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:22] 42.235310 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for RAM : 33.130938 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for RAM : 23.536636 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for all GPUs : 2.952021 kWh. Total GPU Power : 67.03418662342561 W\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for all CPUs : 6.176160 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:37] 42.259118 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for all GPUs : 2.096589 kWh. Total GPU Power : 67.03068610674669 W\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for all CPUs : 4.388063 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for RAM : 23.527248 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:37] 30.021288 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for all GPUs : 2.095640 kWh. Total GPU Power : 66.57590606540515 W\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for all CPUs : 4.386225 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:37] 30.009113 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for RAM : 33.115510 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for all GPUs : 2.950539 kWh. Total GPU Power : 66.58078506673135 W\n",
      "[codecarbon INFO @ 23:29:37] Energy consumed for all CPUs : 6.173252 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:37] 42.239300 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for RAM : 33.134067 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for all GPUs : 2.952298 kWh. Total GPU Power : 66.48942005029728 W\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for all CPUs : 6.176743 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:52] 42.263108 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for RAM : 23.539766 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for all GPUs : 2.096866 kWh. Total GPU Power : 66.4872463091983 W\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for RAM : 23.530377 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for all CPUs : 4.388646 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:52] 30.025278 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for all GPUs : 2.095917 kWh. Total GPU Power : 66.4775402174697 W\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for all CPUs : 4.386808 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:52] 30.013102 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for RAM : 33.118639 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for all GPUs : 2.950815 kWh. Total GPU Power : 66.48018686015591 W\n",
      "[codecarbon INFO @ 23:29:52] Energy consumed for all CPUs : 6.173835 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:29:52] 42.243290 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for RAM : 33.137196 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for all GPUs : 2.952575 kWh. Total GPU Power : 66.59631280335242 W\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for all CPUs : 6.177326 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:07] 42.267098 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for RAM : 23.542895 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for all GPUs : 2.097143 kWh. Total GPU Power : 66.5987791352727 W\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for RAM : 23.533506 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for all CPUs : 4.389229 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:07] 30.029267 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for all GPUs : 2.096194 kWh. Total GPU Power : 66.59727114592357 W\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for all CPUs : 4.387391 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:07] 30.017092 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for RAM : 33.121769 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for all GPUs : 2.951095 kWh. Total GPU Power : 67.0352285266632 W\n",
      "[codecarbon INFO @ 23:30:07] Energy consumed for all CPUs : 6.174419 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:07] 42.247283 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for RAM : 33.140326 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for all GPUs : 2.952854 kWh. Total GPU Power : 67.0546889598098 W\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for all CPUs : 6.177909 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:22] 42.271090 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for RAM : 23.546024 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for RAM : 23.536636 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for all GPUs : 2.097423 kWh. Total GPU Power : 67.06101721762454 W\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for all CPUs : 4.389813 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:22] 30.033259 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for all GPUs : 2.096474 kWh. Total GPU Power : 67.05316864986912 W\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for all CPUs : 4.387975 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:22] 30.021084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for RAM : 33.124899 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for all GPUs : 2.951372 kWh. Total GPU Power : 66.60446116345376 W\n",
      "[codecarbon INFO @ 23:30:22] Energy consumed for all CPUs : 6.175002 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:22] 42.251273 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for RAM : 33.143456 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for all GPUs : 2.953132 kWh. Total GPU Power : 66.57519475721158 W\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for all CPUs : 6.178493 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:37] 42.275080 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for RAM : 23.549154 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for RAM : 23.539766 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for all GPUs : 2.097700 kWh. Total GPU Power : 66.58199816073417 W\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for all CPUs : 4.390396 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:37] 30.037250 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for all GPUs : 2.096751 kWh. Total GPU Power : 66.58060709196286 W\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for all CPUs : 4.388558 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:37] 30.025075 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for RAM : 33.128029 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for all GPUs : 2.951649 kWh. Total GPU Power : 66.57686504554023 W\n",
      "[codecarbon INFO @ 23:30:37] Energy consumed for all CPUs : 6.175585 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:37] 42.255264 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for RAM : 33.146585 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for all GPUs : 2.953409 kWh. Total GPU Power : 66.47869858731072 W\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for all CPUs : 6.179076 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:52] 42.279070 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for RAM : 23.552283 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for RAM : 23.542896 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for all GPUs : 2.097977 kWh. Total GPU Power : 66.47374772009738 W\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for all CPUs : 4.390979 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:52] 30.041240 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for all GPUs : 2.097028 kWh. Total GPU Power : 66.46822272968942 W\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for all CPUs : 4.389141 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:52] 30.029065 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for RAM : 33.131159 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for all GPUs : 2.951928 kWh. Total GPU Power : 66.9229435352909 W\n",
      "[codecarbon INFO @ 23:30:52] Energy consumed for all CPUs : 6.176169 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:30:52] 42.259256 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for RAM : 33.149715 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for all GPUs : 2.953688 kWh. Total GPU Power : 67.17044700809745 W\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for all CPUs : 6.179659 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:07] 42.283063 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for RAM : 23.555413 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for RAM : 23.546025 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for all GPUs : 2.098257 kWh. Total GPU Power : 67.17135691799783 W\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for all CPUs : 4.391563 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:07] 30.045232 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for all GPUs : 2.097308 kWh. Total GPU Power : 67.17357839783782 W\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for all CPUs : 4.389725 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:07] 30.033057 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for RAM : 33.134289 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for all GPUs : 2.952206 kWh. Total GPU Power : 66.71708615953256 W\n",
      "[codecarbon INFO @ 23:31:07] Energy consumed for all CPUs : 6.176752 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:07] 42.263247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for RAM : 33.152845 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for all GPUs : 2.953966 kWh. Total GPU Power : 66.7021737990839 W\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for all CPUs : 6.180243 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:22] 42.287053 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for RAM : 23.558543 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for RAM : 23.549155 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for all GPUs : 2.098534 kWh. Total GPU Power : 66.70218758575395 W\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for all CPUs : 4.392146 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:22] 30.049223 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for all GPUs : 2.097586 kWh. Total GPU Power : 66.70317811988396 W\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for all CPUs : 4.390308 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:22] 30.037048 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for RAM : 33.137418 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for all GPUs : 2.952484 kWh. Total GPU Power : 66.70647657729974 W\n",
      "[codecarbon INFO @ 23:31:22] Energy consumed for all CPUs : 6.177335 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:22] 42.267237 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for RAM : 33.155974 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for all GPUs : 2.954244 kWh. Total GPU Power : 66.63368131034552 W\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for all CPUs : 6.180826 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:37] 42.291044 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for RAM : 23.561673 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for RAM : 23.552285 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for all GPUs : 2.098812 kWh. Total GPU Power : 66.63878876990228 W\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for all CPUs : 4.392729 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:37] 30.053214 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for all GPUs : 2.097863 kWh. Total GPU Power : 66.63868811752391 W\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for all CPUs : 4.390891 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:37] 30.041039 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for RAM : 33.140548 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for all GPUs : 2.952762 kWh. Total GPU Power : 66.63799521361265 W\n",
      "[codecarbon INFO @ 23:31:37] Energy consumed for all CPUs : 6.177919 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:37] 42.271228 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for RAM : 33.159104 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for all GPUs : 2.954522 kWh. Total GPU Power : 66.81999179416273 W\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for all CPUs : 6.181409 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:52] 42.295035 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for RAM : 23.564802 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for RAM : 23.555415 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for all GPUs : 2.099092 kWh. Total GPU Power : 67.25981941784983 W\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for all CPUs : 4.393313 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:52] 30.057207 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for all GPUs : 2.098143 kWh. Total GPU Power : 67.25832443963711 W\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for all CPUs : 4.391474 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:52] 30.045032 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for RAM : 33.143678 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for all GPUs : 2.953042 kWh. Total GPU Power : 67.2579405468002 W\n",
      "[codecarbon INFO @ 23:31:52] Energy consumed for all CPUs : 6.178502 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:31:52] 42.275222 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for RAM : 33.162233 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for RAM : 23.558544 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for RAM : 33.146810 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for RAM : 23.567939 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for all GPUs : 2.954802 kWh. Total GPU Power : 67.06404721707774 W\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for all CPUs : 6.181994 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:07] 42.299030 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for all GPUs : 2.098422 kWh. Total GPU Power : 66.65912707834121 W\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for all CPUs : 4.392059 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:07] 30.049025 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for all GPUs : 2.953320 kWh. Total GPU Power : 66.73305026594626 W\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for all CPUs : 6.179086 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:07] 42.279216 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for all GPUs : 2.099370 kWh. Total GPU Power : 66.63466293759059 W\n",
      "[codecarbon INFO @ 23:32:07] Energy consumed for all CPUs : 4.393898 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:07] 30.061207 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for RAM : 33.165353 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for all GPUs : 2.955080 kWh. Total GPU Power : 66.8024285667585 W\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for all CPUs : 6.182576 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:22] 42.303009 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for RAM : 23.571060 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for RAM : 23.561666 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for all GPUs : 2.099648 kWh. Total GPU Power : 66.79546707772069 W\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for all CPUs : 4.394479 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:22] 30.065187 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for all GPUs : 2.098699 kWh. Total GPU Power : 66.7601397918669 W\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for all CPUs : 4.392641 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:22] 30.053006 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for RAM : 33.149939 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for all GPUs : 2.953597 kWh. Total GPU Power : 66.62737039649105 W\n",
      "[codecarbon INFO @ 23:32:22] Energy consumed for all CPUs : 6.179669 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:22] 42.283205 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for RAM : 33.168483 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for all GPUs : 2.955357 kWh. Total GPU Power : 66.48484744868216 W\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for all CPUs : 6.183159 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:37] 42.306999 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for RAM : 23.574190 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for RAM : 23.564796 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for all GPUs : 2.099925 kWh. Total GPU Power : 66.4862849973614 W\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for all CPUs : 4.395062 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:37] 30.069177 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for all GPUs : 2.098976 kWh. Total GPU Power : 66.48846046497584 W\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for all CPUs : 4.393224 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:37] 30.056996 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for RAM : 33.153068 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for all GPUs : 2.953876 kWh. Total GPU Power : 66.94427248440174 W\n",
      "[codecarbon INFO @ 23:32:37] Energy consumed for all CPUs : 6.180252 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:37] 42.287197 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for RAM : 33.171613 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for all GPUs : 2.955635 kWh. Total GPU Power : 66.90293936288094 W\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for all CPUs : 6.183742 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:52] 42.310991 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for RAM : 23.577320 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for RAM : 23.567926 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for all GPUs : 2.100204 kWh. Total GPU Power : 66.90242878049735 W\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for all CPUs : 4.395646 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:52] 30.073169 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for all GPUs : 2.099255 kWh. Total GPU Power : 66.90110236643453 W\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for all CPUs : 4.393808 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:52] 30.060988 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for RAM : 33.156198 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for all GPUs : 2.954153 kWh. Total GPU Power : 66.45556758747571 W\n",
      "[codecarbon INFO @ 23:32:52] Energy consumed for all CPUs : 6.180836 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:32:52] 42.291186 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for RAM : 33.174743 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for all GPUs : 2.955912 kWh. Total GPU Power : 66.45504338121872 W\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for all CPUs : 6.184326 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:07] 42.314980 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for RAM : 23.580449 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for RAM : 23.571056 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for all GPUs : 2.100480 kWh. Total GPU Power : 66.45751054651 W\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for all CPUs : 4.396229 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:07] 30.077159 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for all GPUs : 2.099531 kWh. Total GPU Power : 66.45289757184223 W\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for all CPUs : 4.394391 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:07] 30.064978 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for RAM : 33.159328 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for all GPUs : 2.954430 kWh. Total GPU Power : 66.4553223149192 W\n",
      "[codecarbon INFO @ 23:33:07] Energy consumed for all CPUs : 6.181419 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:07] 42.295176 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for RAM : 33.177872 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for all GPUs : 2.956189 kWh. Total GPU Power : 66.46004142020738 W\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for all CPUs : 6.184909 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:22] 42.318970 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for RAM : 23.583579 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for RAM : 23.574185 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for all GPUs : 2.100757 kWh. Total GPU Power : 66.45632299529413 W\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for all CPUs : 4.396812 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:22] 30.081148 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for all GPUs : 2.099808 kWh. Total GPU Power : 66.45646669480749 W\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for all CPUs : 4.394974 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:22] 30.068968 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for RAM : 33.162457 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for all GPUs : 2.954709 kWh. Total GPU Power : 66.90544449808135 W\n",
      "[codecarbon INFO @ 23:33:22] Energy consumed for all CPUs : 6.182002 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:22] 42.299168 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for RAM : 33.181002 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for all GPUs : 2.956468 kWh. Total GPU Power : 66.90229708147888 W\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for all CPUs : 6.185492 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:37] 42.322962 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for RAM : 23.586709 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for RAM : 23.577315 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for all GPUs : 2.101036 kWh. Total GPU Power : 66.90516697832591 W\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for all CPUs : 4.397396 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:37] 30.085140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for all GPUs : 2.100087 kWh. Total GPU Power : 66.90441278217648 W\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for all CPUs : 4.395558 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:37] 30.072960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for RAM : 33.165587 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for all GPUs : 2.954985 kWh. Total GPU Power : 66.45536802421204 W\n",
      "[codecarbon INFO @ 23:33:37] Energy consumed for all CPUs : 6.182585 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:37] 42.303158 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for RAM : 33.184131 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for all GPUs : 2.956744 kWh. Total GPU Power : 66.45122905793212 W\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for all CPUs : 6.186076 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:52] 42.326951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for RAM : 23.589839 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for RAM : 23.580445 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for all GPUs : 2.101313 kWh. Total GPU Power : 66.44589675422196 W\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for all CPUs : 4.397979 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:52] 30.089130 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for all GPUs : 2.100364 kWh. Total GPU Power : 66.44464806018804 W\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for all CPUs : 4.396141 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:52] 30.076950 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for RAM : 33.168717 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for all GPUs : 2.955262 kWh. Total GPU Power : 66.44231665642575 W\n",
      "[codecarbon INFO @ 23:33:52] Energy consumed for all CPUs : 6.183169 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:33:52] 42.307148 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for RAM : 33.187261 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for all GPUs : 2.957021 kWh. Total GPU Power : 66.45822032857934 W\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for all CPUs : 6.186659 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:07] 42.330941 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for RAM : 23.592968 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for RAM : 23.583574 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for all GPUs : 2.101589 kWh. Total GPU Power : 66.46899920207781 W\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for all CPUs : 4.398562 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:07] 30.093120 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for all GPUs : 2.100641 kWh. Total GPU Power : 66.46886814379758 W\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for all CPUs : 4.396724 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:07] 30.080939 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for RAM : 33.171847 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for all GPUs : 2.955541 kWh. Total GPU Power : 66.90975702325098 W\n",
      "[codecarbon INFO @ 23:34:07] Energy consumed for all CPUs : 6.183752 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:07] 42.311140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for RAM : 33.190391 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for all GPUs : 2.957298 kWh. Total GPU Power : 66.46763847684544 W\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for all CPUs : 6.187242 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:22] 42.334931 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for RAM : 23.596098 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for RAM : 23.586704 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for all GPUs : 2.101868 kWh. Total GPU Power : 66.90963857735055 W\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for all CPUs : 4.399146 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:22] 30.097112 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for all GPUs : 2.100919 kWh. Total GPU Power : 66.90975985195496 W\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for all CPUs : 4.397308 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:22] 30.084931 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for RAM : 33.174976 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for all GPUs : 2.955818 kWh. Total GPU Power : 66.47427857500074 W\n",
      "[codecarbon INFO @ 23:34:22] Energy consumed for all CPUs : 6.184335 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:22] 42.315129 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for RAM : 33.193520 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for all GPUs : 2.957577 kWh. Total GPU Power : 66.91161907853136 W\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for all CPUs : 6.187825 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:37] 42.338922 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for RAM : 23.599227 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for RAM : 23.589834 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for all GPUs : 2.102145 kWh. Total GPU Power : 66.4664641207585 W\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for all CPUs : 4.399729 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:37] 30.101101 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for all GPUs : 2.101196 kWh. Total GPU Power : 66.4652086312725 W\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for all CPUs : 4.397891 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:37] 30.088921 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for RAM : 33.178106 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for all GPUs : 2.956095 kWh. Total GPU Power : 66.46137587725995 W\n",
      "[codecarbon INFO @ 23:34:37] Energy consumed for all CPUs : 6.184919 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:37] 42.319119 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for RAM : 33.196650 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for all GPUs : 2.957854 kWh. Total GPU Power : 66.47221076415468 W\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for all CPUs : 6.188409 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:52] 42.342912 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for RAM : 23.602357 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for RAM : 23.592964 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for all GPUs : 2.102422 kWh. Total GPU Power : 66.4661626596644 W\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for all CPUs : 4.400312 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:52] 30.105091 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for all GPUs : 2.101473 kWh. Total GPU Power : 66.46842938098717 W\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for all CPUs : 4.398474 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:52] 30.092911 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for RAM : 33.181236 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for all GPUs : 2.956371 kWh. Total GPU Power : 66.46812925609075 W\n",
      "[codecarbon INFO @ 23:34:52] Energy consumed for all CPUs : 6.185502 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:34:52] 42.323109 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for RAM : 33.199779 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for all GPUs : 2.958130 kWh. Total GPU Power : 66.45794477592695 W\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for all CPUs : 6.188992 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:07] 42.346902 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for RAM : 23.605486 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for RAM : 23.596093 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for all GPUs : 2.102699 kWh. Total GPU Power : 66.47492094930766 W\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for all CPUs : 4.400895 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:07] 30.109081 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for all GPUs : 2.101750 kWh. Total GPU Power : 66.47584488455338 W\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for all CPUs : 4.399057 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:07] 30.096900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for RAM : 33.184365 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for all GPUs : 2.956650 kWh. Total GPU Power : 66.91205162992645 W\n",
      "[codecarbon INFO @ 23:35:07] Energy consumed for all CPUs : 6.186085 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 23:35:07] 42.327101 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def categorize_length(length):\n",
    "    if length < 100:\n",
    "        return \"< 100\"\n",
    "    elif length < 500:\n",
    "        return \"100-499\"\n",
    "    elif length < 1000:\n",
    "        return \"500-999\"\n",
    "    elif length < 2000:\n",
    "        return \"1000-1999\"\n",
    "    elif length < 3500:\n",
    "        return \"2000-3499\"\n",
    "    elif length < 5000:\n",
    "        return \"3500-4999\"\n",
    "    else:\n",
    "        return \"5000+\"\n",
    "\n",
    "def count_frequencies_in_batches(dataset, batch_size=10000):\n",
    "    text_category_counts = Counter()\n",
    "    token_category_counts = Counter()\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing batches\"):\n",
    "        batch = dataset.select(range(i, min(i + batch_size, len(dataset))))\n",
    "        \n",
    "        for text_length, token_length in zip(batch['text_length'], batch['token_length']):\n",
    "            text_category = categorize_length(text_length)\n",
    "            token_category = categorize_length(token_length)\n",
    "            \n",
    "            text_category_counts[text_category] += 1\n",
    "            token_category_counts[token_category] += 1\n",
    "    \n",
    "    return text_category_counts, token_category_counts\n",
    "\n",
    "# Count frequencies\n",
    "print(\"Counting text and token category frequencies...\")\n",
    "text_counts, token_counts = count_frequencies_in_batches(dataset_with_lengths)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nText length categories:\")\n",
    "for category, count in text_counts.items():\n",
    "    print(f\"Text length range {category}: {count}\")\n",
    "\n",
    "print(\"\\nToken length categories:\")\n",
    "for category, count in token_counts.items():\n",
    "    print(f\"Token length range {category}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating histogram...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:32:23] Energy consumed for RAM : 0.295824 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all GPUs : 0.026561 kWh. Total GPU Power : 67.05711418099833 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all CPUs : 0.055204 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:23] 0.377589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for RAM : 0.286190 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for RAM : 9.889715 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for RAM : 9.874074 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all GPUs : 0.025612 kWh. Total GPU Power : 67.05869544348121 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all CPUs : 0.053388 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all GPUs : 0.881993 kWh. Total GPU Power : 67.06430670150526 W\n",
      "[codecarbon INFO @ 16:32:23] 0.365191 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all CPUs : 1.843342 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all GPUs : 0.880511 kWh. Total GPU Power : 67.05824227567216 W\n",
      "[codecarbon INFO @ 16:32:23] 12.615050 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all CPUs : 1.840381 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:23] 12.594965 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for RAM : 0.298953 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for all GPUs : 0.026840 kWh. Total GPU Power : 67.02390774756638 W\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for all CPUs : 0.055788 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:38] 0.381581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for RAM : 0.289319 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for RAM : 9.892844 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for all GPUs : 0.025891 kWh. Total GPU Power : 67.0212881433673 W\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for all CPUs : 0.053971 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:38] 0.369182 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for all GPUs : 0.882272 kWh. Total GPU Power : 67.02494690685276 W\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for RAM : 9.877203 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for all CPUs : 1.843925 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:38] 12.619042 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for all GPUs : 0.880790 kWh. Total GPU Power : 67.00285119915642 W\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for all CPUs : 1.840964 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:38] 12.598957 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for RAM : 0.302081 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all GPUs : 0.027122 kWh. Total GPU Power : 67.50350378668645 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all CPUs : 0.056371 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:53] 0.385574 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for RAM : 0.292448 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for RAM : 9.895973 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all GPUs : 0.026173 kWh. Total GPU Power : 67.49560111390494 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all CPUs : 0.054555 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:53] 0.373176 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all GPUs : 0.882553 kWh. Total GPU Power : 67.49774459272908 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for RAM : 9.880332 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all CPUs : 1.844509 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:53] 12.623035 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all GPUs : 0.881071 kWh. Total GPU Power : 67.48924661202696 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all CPUs : 1.841547 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:53] 12.602950 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for RAM : 0.305210 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all GPUs : 0.027401 kWh. Total GPU Power : 67.04114463438931 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all CPUs : 0.056954 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:08] 0.389565 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for RAM : 0.295578 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for RAM : 9.899102 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all GPUs : 0.026452 kWh. Total GPU Power : 67.03749010981268 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all CPUs : 0.055138 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:08] 0.377167 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all GPUs : 0.882832 kWh. Total GPU Power : 67.04675939150458 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all CPUs : 1.845092 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:08] 12.627026 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for RAM : 9.883462 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all GPUs : 0.881350 kWh. Total GPU Power : 67.03189182683677 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all CPUs : 1.842130 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:08] 12.606943 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for RAM : 0.308340 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all GPUs : 0.027680 kWh. Total GPU Power : 66.98245024830358 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all CPUs : 0.057537 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:23] 0.393557 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for RAM : 0.298707 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for RAM : 9.902232 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all GPUs : 0.026731 kWh. Total GPU Power : 66.97987009234447 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all CPUs : 0.055721 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:23] 0.381159 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all GPUs : 0.883111 kWh. Total GPU Power : 66.98581763484114 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all CPUs : 1.845675 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:23] 12.631018 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for RAM : 9.886592 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all GPUs : 0.881629 kWh. Total GPU Power : 66.96712131118626 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all CPUs : 1.842714 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:23] 12.610936 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for RAM : 0.311469 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for all GPUs : 0.027961 kWh. Total GPU Power : 67.43956596642975 W\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for all CPUs : 0.058121 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:38] 0.397550 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for RAM : 0.301837 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for RAM : 9.905361 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for all GPUs : 0.027012 kWh. Total GPU Power : 67.4404421050077 W\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for all CPUs : 0.056304 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:38] 0.385153 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for all GPUs : 0.883392 kWh. Total GPU Power : 67.44123354473692 W\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for all CPUs : 1.846258 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:38] 12.635012 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for RAM : 9.889722 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for all GPUs : 0.881910 kWh. Total GPU Power : 67.44032950330363 W\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for all CPUs : 1.843297 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:38] 12.614929 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for RAM : 0.314598 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all GPUs : 0.028239 kWh. Total GPU Power : 66.94826510330915 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all CPUs : 0.058704 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:53] 0.401542 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for RAM : 0.304967 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for RAM : 9.908491 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all GPUs : 0.027291 kWh. Total GPU Power : 66.9354091305707 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all CPUs : 0.056888 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:53] 0.389145 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all GPUs : 0.883671 kWh. Total GPU Power : 66.94093662564565 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all CPUs : 1.846842 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:53] 12.639004 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for RAM : 9.892852 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all GPUs : 0.882189 kWh. Total GPU Power : 66.93662118506084 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all CPUs : 1.843880 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:53] 12.618921 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for RAM : 0.317727 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all GPUs : 0.028518 kWh. Total GPU Power : 66.87008297163646 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all CPUs : 0.059287 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:08] 0.405532 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for RAM : 0.308096 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for RAM : 9.911620 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all GPUs : 0.027569 kWh. Total GPU Power : 66.85309392326855 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all CPUs : 0.057471 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:08] 0.393136 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all GPUs : 0.883950 kWh. Total GPU Power : 66.85737332931994 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all CPUs : 1.847425 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:08] 12.642995 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for RAM : 9.895981 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all GPUs : 0.882467 kWh. Total GPU Power : 66.84876299087837 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all CPUs : 1.844464 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:08] 12.622913 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for RAM : 0.320856 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all GPUs : 0.028797 kWh. Total GPU Power : 67.04089810642841 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all CPUs : 0.059870 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:23] 0.409524 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for RAM : 0.311226 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for RAM : 9.914749 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all GPUs : 0.027850 kWh. Total GPU Power : 67.49316013601437 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all CPUs : 0.058054 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:23] 0.397130 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all GPUs : 0.884231 kWh. Total GPU Power : 67.503504830097 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all CPUs : 1.848008 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:23] 12.646988 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for RAM : 9.899111 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all GPUs : 0.882749 kWh. Total GPU Power : 67.4895266144375 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all CPUs : 1.845047 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:23] 12.626907 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Create histogram\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Creating histogram...\")\n",
    "sample_size = min(100000, len(dataset_with_lengths))\n",
    "lengths_sample = dataset_with_lengths.select(range(sample_size))[\"text_length\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lengths_sample, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Text Lengths (Sample)')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Continued Pretraining\n",
    "Now let's use Unsloth's `UnslothTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 20 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
    "\n",
    "Also set `embedding_learning_rate` to be a learning rate at least 2x or 10x smaller than `learning_rate` to make continual pretraining work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12464\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_split_percentage = 0.1\n",
    "\n",
    "split_dataset_logger = transformers.utils.logging.get_logger(\n",
    "    \"transformers.tokenization_utils_base\"\n",
    ")\n",
    "with CaptureLogger(split_dataset_logger) as cl:\n",
    "    train_dataset = lm_datasets['train'].train_test_split(\n",
    "        test_size=validation_split_percentage\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1247\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:22:08] Energy consumed for RAM : 0.059255 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:22:09] Energy consumed for all GPUs : 0.005373 kWh. Total GPU Power : 67.93598570939994 W\n",
      "[codecarbon INFO @ 16:22:09] Energy consumed for all CPUs : 0.011107 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:22:09] 0.075735 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  15444 MiB |  15444 MiB |  15444 MiB |    512 B   |\n",
      "|       from large pool |  15444 MiB |  15444 MiB |  15444 MiB |      0 B   |\n",
      "|       from small pool |      0 MiB |      0 MiB |      0 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  15444 MiB |  15444 MiB |  15444 MiB |    512 B   |\n",
      "|       from large pool |  15444 MiB |  15444 MiB |  15444 MiB |      0 B   |\n",
      "|       from small pool |      0 MiB |      0 MiB |      0 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  15444 MiB |  15444 MiB |  15444 MiB |      8 B   |\n",
      "|       from large pool |  15444 MiB |  15444 MiB |  15444 MiB |      0 B   |\n",
      "|       from small pool |      0 MiB |      0 MiB |      0 MiB |      8 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  15446 MiB |  15446 MiB |  15448 MiB |   2048 KiB |\n",
      "|       from large pool |  15444 MiB |  15444 MiB |  15444 MiB |      0 KiB |\n",
      "|       from small pool |      2 MiB |      2 MiB |      4 MiB |   2048 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   1528 KiB | 140816 KiB | 397303 KiB | 395775 KiB |\n",
      "|       from large pool |      0 KiB | 139264 KiB | 393216 KiB | 393216 KiB |\n",
      "|       from small pool |   1528 KiB |   2047 KiB |   4087 KiB |   2559 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     355    |     355    |     356    |       1    |\n",
      "|       from large pool |     290    |     290    |     290    |       0    |\n",
      "|       from small pool |      65    |      65    |      66    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     355    |     355    |     356    |       1    |\n",
      "|       from large pool |     290    |     290    |     290    |       0    |\n",
      "|       from small pool |      65    |      65    |      66    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     195    |     195    |     196    |       1    |\n",
      "|       from large pool |     194    |     194    |     194    |       0    |\n",
      "|       from small pool |       1    |       1    |       2    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |      33    |      34    |      33    |\n",
      "|       from large pool |       0    |      32    |      32    |      32    |\n",
      "|       from small pool |       1    |       1    |       2    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LoggingCallback(TrainerCallback):\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        logger.info(f\"Starting step {state.global_step}\")\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        logger.info(f\"Finished step {state.global_step}\")\n",
    "        logger.info(f\"Learning rate: {args.learning_rate}\")\n",
    "        logger.info(f\"GPU memory allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "        logger.info(f\"GPU memory reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            logger.info(f\"Logs: {logs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): ModulesToSaveWrapper(\n",
       "          (original_module): Embedding(128256, 4096)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Embedding(128256, 4096)\n",
       "          )\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:40:59] Energy consumed for RAM : 7.231985 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 15:40:59] Energy consumed for all GPUs : 3.043716 kWh. Total GPU Power : 66.71427728361913 W\n",
      "[codecarbon INFO @ 15:40:59] Energy consumed for all CPUs : 1.349936 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 15:40:59] 11.625638 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:41:06] Energy consumed for RAM : 7.205013 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 15:41:06] Energy consumed for all GPUs : 3.040544 kWh. Total GPU Power : 66.59624615829183 W\n",
      "[codecarbon INFO @ 15:41:06] Energy consumed for all CPUs : 1.344944 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 15:41:06] 11.590501 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:41:14] Energy consumed for RAM : 7.235115 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 15:41:14] Energy consumed for all GPUs : 3.043996 kWh. Total GPU Power : 67.13902315518355 W\n",
      "[codecarbon INFO @ 15:41:14] Energy consumed for all CPUs : 1.350521 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 15:41:14] 11.629631 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Invoke garbage collector multiple times\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#if trainer:\n",
    "    #del trainer\n",
    "#    time.sleep(2)\n",
    "# Reset the CUDA runtime\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "#clear_gpu_memory()\n",
    "\n",
    "# Optionally, you can use nvidia-smi command to kill all processes using GPU\n",
    "import os\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Both warmup_ratio and warmup_steps given, warmup_steps will override any effect of warmup_ratio during training\n",
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize and run the trainer with the custom callback\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mUnslothTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#\"text\",\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#max_seq_length = 512, #block_size, #max_seq_length,\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_num_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUnslothTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased batch size to leverage GPU memory\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Adjusted gradient accumulation\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#max_steps = 120,\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# More warmup steps for stability with larger batch sizes\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Increase epochs for better training (adjust as needed)\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Adjusted learning rate for larger batch size\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_learning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Adjusted embedding learning rate\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#False, #not is_bfloat16_supported(),\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#is_bfloat16_supported(),\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw_8bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3407\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workspace/outputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mLoggingCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add the custom logging callback\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:323\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    318\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to your code.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m     )\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_model_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/trainer.py:397\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/trainer_utils.py:99\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed, deterministic)\u001b[0m\n\u001b[1;32m     97\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/random.py:45\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/random.py:126\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    123\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    124\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/__init__.py:223\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 223\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/random.py:124\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    123\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 124\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:22:23] Energy consumed for RAM : 0.062347 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:22:23] Energy consumed for all GPUs : 0.005647 kWh. Total GPU Power : 66.67526397795282 W\n",
      "[codecarbon INFO @ 16:22:23] Energy consumed for all CPUs : 0.011683 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:22:23] 0.079677 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:22:38] Energy consumed for RAM : 0.065477 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:22:39] Energy consumed for all GPUs : 0.005929 kWh. Total GPU Power : 67.56133478849185 W\n",
      "[codecarbon INFO @ 16:22:39] Energy consumed for all CPUs : 0.012273 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:22:39] 0.083678 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:22:53] Energy consumed for RAM : 0.068571 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:22:53] Energy consumed for all GPUs : 0.006205 kWh. Total GPU Power : 67.22889504361862 W\n",
      "[codecarbon INFO @ 16:22:53] Energy consumed for all CPUs : 0.012849 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:22:53] 0.087626 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:23:08] Energy consumed for RAM : 0.071701 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:23:09] Energy consumed for all GPUs : 0.006486 kWh. Total GPU Power : 67.25456229314109 W\n",
      "[codecarbon INFO @ 16:23:09] Energy consumed for all CPUs : 0.013436 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:23:09] 0.091623 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:23:23] Energy consumed for RAM : 0.074811 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:23:23] Energy consumed for all GPUs : 0.006762 kWh. Total GPU Power : 66.72894939785284 W\n",
      "[codecarbon INFO @ 16:23:23] Energy consumed for all CPUs : 0.014016 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:23:23] 0.095589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:23:38] Energy consumed for RAM : 0.077941 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:23:39] Energy consumed for all GPUs : 0.007047 kWh. Total GPU Power : 68.34808661594415 W\n",
      "[codecarbon INFO @ 16:23:39] Energy consumed for all CPUs : 0.014611 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:23:39] 0.099599 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:23:53] Energy consumed for RAM : 0.081005 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:23:54] Energy consumed for all GPUs : 0.007321 kWh. Total GPU Power : 67.30265811634663 W\n",
      "[codecarbon INFO @ 16:23:54] Energy consumed for all CPUs : 0.015186 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:23:54] 0.103512 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:24:08] Energy consumed for RAM : 0.084116 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:24:08] Energy consumed for all GPUs : 0.007597 kWh. Total GPU Power : 66.69073483411462 W\n",
      "[codecarbon INFO @ 16:24:08] Energy consumed for all CPUs : 0.015766 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:24:08] 0.107479 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:24:23] Energy consumed for RAM : 0.087245 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:24:23] Energy consumed for all GPUs : 0.007876 kWh. Total GPU Power : 66.96052509915735 W\n",
      "[codecarbon INFO @ 16:24:23] Energy consumed for all CPUs : 0.016349 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:24:23] 0.111471 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:24:38] Energy consumed for RAM : 0.090375 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:24:38] Energy consumed for all GPUs : 0.008156 kWh. Total GPU Power : 67.3191534089651 W\n",
      "[codecarbon INFO @ 16:24:38] Energy consumed for all CPUs : 0.016932 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:24:38] 0.115464 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:24:53] Energy consumed for RAM : 0.093505 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:24:53] Energy consumed for all GPUs : 0.008434 kWh. Total GPU Power : 66.671056596255 W\n",
      "[codecarbon INFO @ 16:24:53] Energy consumed for all CPUs : 0.017517 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:24:53] 0.119456 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:25:08] Energy consumed for RAM : 0.096629 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:25:08] Energy consumed for all GPUs : 0.008712 kWh. Total GPU Power : 66.88757210205415 W\n",
      "[codecarbon INFO @ 16:25:08] Energy consumed for all CPUs : 0.018099 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:25:08] 0.123440 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:25:23] Energy consumed for RAM : 0.099759 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:25:23] Energy consumed for all GPUs : 0.008991 kWh. Total GPU Power : 67.03479233607841 W\n",
      "[codecarbon INFO @ 16:25:23] Energy consumed for all CPUs : 0.018682 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:25:23] 0.127433 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:25:38] Energy consumed for RAM : 0.102888 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:25:38] Energy consumed for all GPUs : 0.009269 kWh. Total GPU Power : 66.58540299066043 W\n",
      "[codecarbon INFO @ 16:25:38] Energy consumed for all CPUs : 0.019267 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:25:38] 0.131424 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:25:53] Energy consumed for RAM : 0.106011 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:25:53] Energy consumed for all GPUs : 0.009546 kWh. Total GPU Power : 66.68934533342103 W\n",
      "[codecarbon INFO @ 16:25:53] Energy consumed for all CPUs : 0.019849 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:25:53] 0.135406 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:26:08] Energy consumed for RAM : 0.109141 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:26:09] Energy consumed for all GPUs : 0.009825 kWh. Total GPU Power : 67.08468886744582 W\n",
      "[codecarbon INFO @ 16:26:09] Energy consumed for all CPUs : 0.020436 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:26:09] 0.139402 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:26:23] Energy consumed for RAM : 0.112252 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:26:23] Energy consumed for all GPUs : 0.010104 kWh. Total GPU Power : 67.2250911220411 W\n",
      "[codecarbon INFO @ 16:26:23] Energy consumed for all CPUs : 0.021016 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:26:23] 0.143371 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:26:38] Energy consumed for RAM : 0.115382 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:26:38] Energy consumed for all GPUs : 0.010382 kWh. Total GPU Power : 66.77055467775399 W\n",
      "[codecarbon INFO @ 16:26:38] Energy consumed for all CPUs : 0.021600 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:26:38] 0.147364 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:26:53] Energy consumed for RAM : 0.118504 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:26:53] Energy consumed for all GPUs : 0.010659 kWh. Total GPU Power : 66.73577049358742 W\n",
      "[codecarbon INFO @ 16:26:53] Energy consumed for all CPUs : 0.022182 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:26:53] 0.151345 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:27:08] Energy consumed for RAM : 0.121633 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:27:09] Energy consumed for all GPUs : 0.010940 kWh. Total GPU Power : 67.51696842551526 W\n",
      "[codecarbon INFO @ 16:27:09] Energy consumed for all CPUs : 0.022772 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:27:09] 0.155346 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:27:23] Energy consumed for RAM : 0.124727 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:27:23] Energy consumed for all GPUs : 0.011216 kWh. Total GPU Power : 66.97019977431728 W\n",
      "[codecarbon INFO @ 16:27:23] Energy consumed for all CPUs : 0.023349 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:27:23] 0.159292 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:27:38] Energy consumed for RAM : 0.127856 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:27:38] Energy consumed for all GPUs : 0.011496 kWh. Total GPU Power : 67.1452784365009 W\n",
      "[codecarbon INFO @ 16:27:38] Energy consumed for all CPUs : 0.023933 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:27:38] 0.163285 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:27:53] Energy consumed for RAM : 0.130981 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:27:53] Energy consumed for all GPUs : 0.011773 kWh. Total GPU Power : 66.70662790212619 W\n",
      "[codecarbon INFO @ 16:27:53] Energy consumed for all CPUs : 0.024515 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:27:53] 0.167270 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:28:08] Energy consumed for RAM : 0.134111 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:28:09] Energy consumed for all GPUs : 0.012055 kWh. Total GPU Power : 67.534227554895 W\n",
      "[codecarbon INFO @ 16:28:09] Energy consumed for all CPUs : 0.025108 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:28:09] 0.171274 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:28:23] Energy consumed for RAM : 0.137191 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:28:23] Energy consumed for all GPUs : 0.012328 kWh. Total GPU Power : 66.71915656132074 W\n",
      "[codecarbon INFO @ 16:28:23] Energy consumed for all CPUs : 0.025682 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:28:23] 0.175201 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:28:38] Energy consumed for RAM : 0.140321 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:28:38] Energy consumed for all GPUs : 0.012607 kWh. Total GPU Power : 66.96038660384045 W\n",
      "[codecarbon INFO @ 16:28:38] Energy consumed for all CPUs : 0.026265 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:28:38] 0.179193 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:28:53] Energy consumed for RAM : 0.143450 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:28:53] Energy consumed for all GPUs : 0.012884 kWh. Total GPU Power : 66.5626642186677 W\n",
      "[codecarbon INFO @ 16:28:53] Energy consumed for all CPUs : 0.026849 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:28:53] 0.183183 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:29:08] Energy consumed for RAM : 0.146580 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:29:09] Energy consumed for all GPUs : 0.013166 kWh. Total GPU Power : 67.66607010884377 W\n",
      "[codecarbon INFO @ 16:29:09] Energy consumed for all CPUs : 0.027437 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:29:09] 0.187183 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:29:23] Energy consumed for RAM : 0.149679 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:29:23] Energy consumed for all GPUs : 0.013441 kWh. Total GPU Power : 66.64475980611341 W\n",
      "[codecarbon INFO @ 16:29:23] Energy consumed for all CPUs : 0.028015 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:29:23] 0.191135 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:29:38] Energy consumed for RAM : 0.152809 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:29:39] Energy consumed for all GPUs : 0.013721 kWh. Total GPU Power : 67.11691104255621 W\n",
      "[codecarbon INFO @ 16:29:39] Energy consumed for all CPUs : 0.028600 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:29:39] 0.195130 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:29:53] Energy consumed for RAM : 0.155927 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:29:53] Energy consumed for all GPUs : 0.013998 kWh. Total GPU Power : 66.8622702691956 W\n",
      "[codecarbon INFO @ 16:29:53] Energy consumed for all CPUs : 0.029182 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:29:53] 0.199107 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:30:08] Energy consumed for RAM : 0.159057 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:30:08] Energy consumed for all GPUs : 0.014275 kWh. Total GPU Power : 66.54038749296218 W\n",
      "[codecarbon INFO @ 16:30:08] Energy consumed for all CPUs : 0.029765 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:30:08] 0.203097 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:30:23] Energy consumed for RAM : 0.162187 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:30:23] Energy consumed for all GPUs : 0.014554 kWh. Total GPU Power : 66.98539458269785 W\n",
      "[codecarbon INFO @ 16:30:23] Energy consumed for all CPUs : 0.030348 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:30:23] 0.207089 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:30:38] Energy consumed for RAM : 0.165316 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:30:39] Energy consumed for all GPUs : 0.014839 kWh. Total GPU Power : 68.31324496477465 W\n",
      "[codecarbon INFO @ 16:30:39] Energy consumed for all CPUs : 0.030946 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:30:39] 0.211102 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:30:53] Energy consumed for RAM : 0.168367 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:30:53] Energy consumed for all GPUs : 0.015109 kWh. Total GPU Power : 66.5061259283444 W\n",
      "[codecarbon INFO @ 16:30:53] Energy consumed for all CPUs : 0.031515 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:30:53] 0.214991 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:31:08] Energy consumed for RAM : 0.171497 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:31:08] Energy consumed for all GPUs : 0.015386 kWh. Total GPU Power : 66.54152491543593 W\n",
      "[codecarbon INFO @ 16:31:08] Energy consumed for all CPUs : 0.032098 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:31:08] 0.218981 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:31:23] Energy consumed for RAM : 0.174626 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:31:24] Energy consumed for all GPUs : 0.015667 kWh. Total GPU Power : 67.42299822644416 W\n",
      "[codecarbon INFO @ 16:31:24] Energy consumed for all CPUs : 0.032686 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:31:24] 0.222979 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:31:38] Energy consumed for RAM : 0.177733 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:31:38] Energy consumed for all GPUs : 0.015942 kWh. Total GPU Power : 66.59547151102711 W\n",
      "[codecarbon INFO @ 16:31:38] Energy consumed for all CPUs : 0.033265 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:31:38] 0.226940 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:31:53] Energy consumed for RAM : 0.180862 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:31:53] Energy consumed for all GPUs : 0.016220 kWh. Total GPU Power : 66.57291034351765 W\n",
      "[codecarbon INFO @ 16:31:53] Energy consumed for all CPUs : 0.033848 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:31:53] 0.230930 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:08] Energy consumed for RAM : 0.183992 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:09] Energy consumed for all GPUs : 0.016499 kWh. Total GPU Power : 67.14491072457656 W\n",
      "[codecarbon INFO @ 16:32:09] Energy consumed for all CPUs : 0.034434 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:09] 0.234926 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for RAM : 0.187107 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all GPUs : 0.016777 kWh. Total GPU Power : 67.06640200374147 W\n",
      "[codecarbon INFO @ 16:32:23] Energy consumed for all CPUs : 0.035015 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:23] 0.238899 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:38] Energy consumed for RAM : 0.190236 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:39] Energy consumed for all GPUs : 0.017059 kWh. Total GPU Power : 67.55020854051497 W\n",
      "[codecarbon INFO @ 16:32:39] Energy consumed for all CPUs : 0.035606 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:39] 0.242900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for RAM : 0.193324 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all GPUs : 0.017333 kWh. Total GPU Power : 66.86543391417698 W\n",
      "[codecarbon INFO @ 16:32:53] Energy consumed for all CPUs : 0.036181 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:32:53] 0.246838 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for RAM : 0.196453 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all GPUs : 0.017613 kWh. Total GPU Power : 67.16328349449523 W\n",
      "[codecarbon INFO @ 16:33:08] Energy consumed for all CPUs : 0.036765 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:08] 0.250831 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for RAM : 0.199583 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all GPUs : 0.017891 kWh. Total GPU Power : 66.64723211875507 W\n",
      "[codecarbon INFO @ 16:33:23] Energy consumed for all CPUs : 0.037348 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:23] 0.254822 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:38] Energy consumed for RAM : 0.202713 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:39] Energy consumed for all GPUs : 0.018172 kWh. Total GPU Power : 67.45565713800512 W\n",
      "[codecarbon INFO @ 16:33:39] Energy consumed for all CPUs : 0.037937 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:39] 0.258822 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for RAM : 0.205811 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all GPUs : 0.018447 kWh. Total GPU Power : 66.80060042730639 W\n",
      "[codecarbon INFO @ 16:33:53] Energy consumed for all CPUs : 0.038514 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:33:53] 0.262772 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for RAM : 0.208940 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all GPUs : 0.018724 kWh. Total GPU Power : 66.54275560511864 W\n",
      "[codecarbon INFO @ 16:34:08] Energy consumed for all CPUs : 0.039098 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:08] 0.266762 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for RAM : 0.212069 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all GPUs : 0.019002 kWh. Total GPU Power : 66.68254721055888 W\n",
      "[codecarbon INFO @ 16:34:23] Energy consumed for all CPUs : 0.039681 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:23] 0.270753 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:38] Energy consumed for RAM : 0.215199 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:39] Energy consumed for all GPUs : 0.019286 kWh. Total GPU Power : 68.11550736379905 W\n",
      "[codecarbon INFO @ 16:34:39] Energy consumed for all CPUs : 0.040272 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:39] 0.274757 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:34:53] Energy consumed for RAM : 0.218286 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:34:53] Energy consumed for all GPUs : 0.019560 kWh. Total GPU Power : 66.64942821749116 W\n",
      "[codecarbon INFO @ 16:34:53] Energy consumed for all CPUs : 0.040848 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:34:53] 0.278693 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:35:08] Energy consumed for RAM : 0.221415 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:35:09] Energy consumed for all GPUs : 0.019843 kWh. Total GPU Power : 67.9633822775504 W\n",
      "[codecarbon INFO @ 16:35:09] Energy consumed for all CPUs : 0.041442 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:35:09] 0.282700 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:35:23] Energy consumed for RAM : 0.224484 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:35:23] Energy consumed for all GPUs : 0.020114 kWh. Total GPU Power : 66.49321510144428 W\n",
      "[codecarbon INFO @ 16:35:23] Energy consumed for all CPUs : 0.042014 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:35:23] 0.286612 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:35:38] Energy consumed for RAM : 0.227613 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:35:38] Energy consumed for all GPUs : 0.020393 kWh. Total GPU Power : 67.03740676122942 W\n",
      "[codecarbon INFO @ 16:35:38] Energy consumed for all CPUs : 0.042598 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:35:38] 0.290604 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:35:53] Energy consumed for RAM : 0.230743 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:35:54] Energy consumed for all GPUs : 0.020674 kWh. Total GPU Power : 67.47343876627065 W\n",
      "[codecarbon INFO @ 16:35:54] Energy consumed for all CPUs : 0.043188 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:35:54] 0.294605 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:08] Energy consumed for RAM : 0.233833 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:08] Energy consumed for all GPUs : 0.020949 kWh. Total GPU Power : 66.78618719898725 W\n",
      "[codecarbon INFO @ 16:36:08] Energy consumed for all CPUs : 0.043764 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:08] 0.298547 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:23] Energy consumed for RAM : 0.236962 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:24] Energy consumed for all GPUs : 0.021230 kWh. Total GPU Power : 67.43279849164126 W\n",
      "[codecarbon INFO @ 16:36:24] Energy consumed for all CPUs : 0.044351 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:24] 0.302543 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for RAM : 0.240072 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all GPUs : 0.021509 kWh. Total GPU Power : 67.40348018921989 W\n",
      "[codecarbon INFO @ 16:36:38] Energy consumed for all CPUs : 0.044931 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:38] 0.306512 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for RAM : 0.243202 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all GPUs : 0.021788 kWh. Total GPU Power : 66.91182246057954 W\n",
      "[codecarbon INFO @ 16:36:53] Energy consumed for all CPUs : 0.045514 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:36:53] 0.310503 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:08] Energy consumed for RAM : 0.246332 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:09] Energy consumed for all GPUs : 0.022070 kWh. Total GPU Power : 67.83519542580751 W\n",
      "[codecarbon INFO @ 16:37:09] Energy consumed for all CPUs : 0.046104 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:09] 0.314506 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for RAM : 0.249425 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all GPUs : 0.022348 kWh. Total GPU Power : 67.34010001517137 W\n",
      "[codecarbon INFO @ 16:37:23] Energy consumed for all CPUs : 0.046681 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:23] 0.318453 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:38] Energy consumed for RAM : 0.252555 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:39] Energy consumed for all GPUs : 0.022628 kWh. Total GPU Power : 67.43192214628709 W\n",
      "[codecarbon INFO @ 16:37:39] Energy consumed for all CPUs : 0.047267 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:39] 0.322450 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for RAM : 0.255667 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all GPUs : 0.022906 kWh. Total GPU Power : 66.90504311481317 W\n",
      "[codecarbon INFO @ 16:37:53] Energy consumed for all CPUs : 0.047847 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:37:53] 0.326420 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for RAM : 0.258796 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all GPUs : 0.023186 kWh. Total GPU Power : 67.40046843876628 W\n",
      "[codecarbon INFO @ 16:38:08] Energy consumed for all CPUs : 0.048430 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:08] 0.330413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for RAM : 0.261925 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all GPUs : 0.023464 kWh. Total GPU Power : 66.75897429122234 W\n",
      "[codecarbon INFO @ 16:38:23] Energy consumed for all CPUs : 0.049014 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:23] 0.334403 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for RAM : 0.265054 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all GPUs : 0.023743 kWh. Total GPU Power : 66.94803320341276 W\n",
      "[codecarbon INFO @ 16:38:38] Energy consumed for all CPUs : 0.049597 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:38] 0.338394 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for RAM : 0.268183 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all GPUs : 0.024024 kWh. Total GPU Power : 67.43185317867784 W\n",
      "[codecarbon INFO @ 16:38:53] Energy consumed for all CPUs : 0.050180 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:38:53] 0.342387 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:08] Energy consumed for RAM : 0.271313 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:09] Energy consumed for all GPUs : 0.024309 kWh. Total GPU Power : 68.34795497376429 W\n",
      "[codecarbon INFO @ 16:39:09] Energy consumed for all CPUs : 0.050775 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:09] 0.346397 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for RAM : 0.274381 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all GPUs : 0.024582 kWh. Total GPU Power : 66.96243510644848 W\n",
      "[codecarbon INFO @ 16:39:23] Energy consumed for all CPUs : 0.051347 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:23] 0.350309 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for RAM : 0.277509 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all GPUs : 0.024861 kWh. Total GPU Power : 67.01178937480573 W\n",
      "[codecarbon INFO @ 16:39:38] Energy consumed for all CPUs : 0.051930 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:38] 0.354301 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:39:53] Energy consumed for RAM : 0.280639 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:39:54] Energy consumed for all GPUs : 0.025144 kWh. Total GPU Power : 67.849547155866 W\n",
      "[codecarbon INFO @ 16:39:54] Energy consumed for all CPUs : 0.052518 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:39:54] 0.358301 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for RAM : 0.283742 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all GPUs : 0.025421 kWh. Total GPU Power : 67.11083393240781 W\n",
      "[codecarbon INFO @ 16:40:08] Energy consumed for all CPUs : 0.053097 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:08] 0.362259 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:23] Energy consumed for RAM : 0.286871 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:24] Energy consumed for all GPUs : 0.025703 kWh. Total GPU Power : 67.8584939145478 W\n",
      "[codecarbon INFO @ 16:40:24] Energy consumed for all CPUs : 0.053685 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:24] 0.366260 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for RAM : 0.289971 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all GPUs : 0.025980 kWh. Total GPU Power : 67.04534984315882 W\n",
      "[codecarbon INFO @ 16:40:38] Energy consumed for all CPUs : 0.054263 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:38] 0.370214 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:40:53] Energy consumed for RAM : 0.293100 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:40:54] Energy consumed for all GPUs : 0.026263 kWh. Total GPU Power : 67.86319158621477 W\n",
      "[codecarbon INFO @ 16:40:54] Energy consumed for all CPUs : 0.054854 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:40:54] 0.374217 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:08] Energy consumed for RAM : 0.296190 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:09] Energy consumed for all GPUs : 0.026540 kWh. Total GPU Power : 67.34718462130661 W\n",
      "[codecarbon INFO @ 16:41:09] Energy consumed for all CPUs : 0.055431 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:09] 0.378161 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for RAM : 0.299312 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all GPUs : 0.026819 kWh. Total GPU Power : 67.20222513450601 W\n",
      "[codecarbon INFO @ 16:41:23] Energy consumed for all CPUs : 0.056013 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:41:23] 0.382143 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run the trainer with the custom callback\n",
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    eval_dataset=train_dataset['test'],\n",
    "\n",
    "    packing=True,\n",
    "    dataset_text_field = None, #\"text\",\n",
    "    \n",
    "    \n",
    "    #max_seq_length = 512, #block_size, #max_seq_length,\n",
    "    dataset_num_proc=32,\n",
    "    \n",
    "    args=UnslothTrainingArguments(\n",
    "        per_device_train_batch_size=32,  # Increased batch size to leverage GPU memory\n",
    "        gradient_accumulation_steps=8,   # Adjusted gradient accumulation\n",
    "        \n",
    "        #max_steps = 120,\n",
    "        warmup_steps=100,                 # More warmup steps for stability with larger batch sizes\n",
    "        warmup_ratio=0.05,\n",
    "        num_train_epochs=1,               # Increase epochs for better training (adjust as needed)\n",
    "        \n",
    "        learning_rate=1e-4,               # Adjusted learning rate for larger batch size\n",
    "        embedding_learning_rate=2e-5,     # Adjusted embedding learning rate\n",
    "        \n",
    "        fp16 = False, #False, #not is_bfloat16_supported(),\n",
    "        bf16 = True, #is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        evaluation_strategy=\"steps\",  # Added\n",
    "        eval_steps=1000,  # Added\n",
    "        save_strategy=\"steps\",  # Added\n",
    "        save_steps=1000,  # Added\n",
    "    ),\n",
    "    callbacks=[LoggingCallback()]  # Add the custom logging callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1717658245568,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "4eaa7bd1-f800-49d3-a8f5-58c918f339cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-80GB. Max memory = 79.151 GB.\n",
      "20.279 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "import torch\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5239701,
     "status": "ok",
     "timestamp": 1717663485260,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "c60e3467-a197-48e1-8d52-c9999b59efc7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513d2ef55f1544b8954657ff780fd573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting untrained tokens:   0%|          | 0/11217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting embed_tokens & lm_head untrained tokens to mean(trained) to counteract NaNs during training.\n",
      "Currently training with a batch size of: 32\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 11,217 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 256 | Total steps = 43\n",
      " \"-____-\"     Number of trainable parameters = 1,386,217,472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 2.00e-05 instead of 1.00e-04 for embed_tokens.\n",
      "Unsloth: Setting lr = 2.00e-05 instead of 1.00e-04 for lm_head.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:123\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:359\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/accelerate/utils/operations.py:822\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/accelerate/utils/operations.py:810\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:885\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    874\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    884\u001b[0m ):\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:816\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 816\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    830\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:653\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[idx] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offloaded_gradient_checkpointing:\n\u001b[0;32m--> 653\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mUnsloth_Offloaded_Gradient_Checkpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m gradient_checkpointing:\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_custom_forward\u001b[39m(module):\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:115\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/_utils.py:383\u001b[0m, in \u001b[0;36mUnsloth_Offloaded_Gradient_Checkpointer.forward\u001b[0;34m(ctx, forward_function, hidden_states, *args)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mcustom_fwd\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, forward_function, hidden_states, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 383\u001b[0m     saved_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    385\u001b[0m         output \u001b[38;5;241m=\u001b[39m forward_function(hidden_states, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:17:38] Energy consumed for RAM : 0.003132 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:17:38] Energy consumed for all GPUs : 0.000338 kWh. Total GPU Power : 81.08191141953027 W\n",
      "[codecarbon INFO @ 16:17:38] Energy consumed for all CPUs : 0.000585 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:17:38] 0.004055 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:17:53] Energy consumed for RAM : 0.006254 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:17:53] Energy consumed for all GPUs : 0.000617 kWh. Total GPU Power : 67.2629215262362 W\n",
      "[codecarbon INFO @ 16:17:53] Energy consumed for all CPUs : 0.001167 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:17:53] 0.008038 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:18:08] Energy consumed for RAM : 0.009384 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:18:08] Energy consumed for all GPUs : 0.000900 kWh. Total GPU Power : 67.89037802945379 W\n",
      "[codecarbon INFO @ 16:18:08] Energy consumed for all CPUs : 0.001758 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:18:08] 0.012042 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:18:23] Energy consumed for RAM : 0.012473 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:18:23] Energy consumed for all GPUs : 0.001175 kWh. Total GPU Power : 66.83768784955235 W\n",
      "[codecarbon INFO @ 16:18:23] Energy consumed for all CPUs : 0.002334 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:18:23] 0.015981 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:18:38] Energy consumed for RAM : 0.015603 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:18:38] Energy consumed for all GPUs : 0.001456 kWh. Total GPU Power : 67.3876400467193 W\n",
      "[codecarbon INFO @ 16:18:38] Energy consumed for all CPUs : 0.002921 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:18:38] 0.019979 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:18:53] Energy consumed for RAM : 0.018711 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:18:53] Energy consumed for all GPUs : 0.001734 kWh. Total GPU Power : 67.32590115803303 W\n",
      "[codecarbon INFO @ 16:18:53] Energy consumed for all CPUs : 0.003500 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:18:53] 0.023945 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:19:08] Energy consumed for RAM : 0.021840 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 16:19:08] Energy consumed for all GPUs : 0.002017 kWh. Total GPU Power : 67.88983852151749 W\n",
      "[codecarbon INFO @ 16:19:08] Energy consumed for all CPUs : 0.004092 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 16:19:08] 0.027949 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 26 21:12:40 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              66W / 400W |  49989MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:46:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              67W / 400W |  47111MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:4C:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              65W / 400W |  47111MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              68W / 400W |  46967MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Feb__7_19:32:13_PST_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.66\n",
      "Build cuda_12.1.r12.1/compiler.32415258_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:12:44] Energy consumed for RAM : 0.031360 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:12:44] Energy consumed for all GPUs : 0.011417 kWh. Total GPU Power : 274.8827443950337 W\n",
      "[codecarbon INFO @ 21:12:44] Energy consumed for all CPUs : 0.005904 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:12:44] 0.048681 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:12:59] Energy consumed for RAM : 0.034488 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:12:59] Energy consumed for all GPUs : 0.012530 kWh. Total GPU Power : 267.4108124760764 W\n",
      "[codecarbon INFO @ 21:12:59] Energy consumed for all CPUs : 0.006487 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:12:59] 0.053505 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:13:14] Energy consumed for RAM : 0.037616 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:13:14] Energy consumed for all GPUs : 0.013648 kWh. Total GPU Power : 268.69069465643133 W\n",
      "[codecarbon INFO @ 21:13:14] Energy consumed for all CPUs : 0.007071 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:13:14] 0.058335 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 4\n",
      "Current CUDA Device: 0\n",
      "CUDA Device Name: NVIDIA A100-SXM4-80GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:13:29] Energy consumed for RAM : 0.040744 kWh. RAM Power : 751.453685760498 W\n",
      "[codecarbon INFO @ 21:13:29] Energy consumed for all GPUs : 0.014761 kWh. Total GPU Power : 267.2918160880798 W\n",
      "[codecarbon INFO @ 21:13:29] Energy consumed for all CPUs : 0.007654 kWh. Total CPU Power : 140.0 W\n",
      "[codecarbon INFO @ 21:13:29] 0.063159 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current CUDA Device:\", torch.cuda.current_device())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 9951012\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139972791104176"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139973424920752"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(merged_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /workspace/bangla-llama/data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_gguf='/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_dir='/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Base'\n",
    "target_dir='/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls $target_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_dir='/workspace/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Base/snapshots/defb6a155755ad3547c62908c0dc6cda7aa7d018/GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/tokenizer_config.json',\n",
       " '/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/special_tokens_map.json',\n",
       " '/root/.cache/huggingface/hub/models--BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repo_id = 'BanglaLLM/BanglaLLama-3-8b-BnWiki-Base-GGUF'\n",
    "repo_id = 'BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct-GGUF')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(repo_id=repo_id, repo_type=\"model\", private=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def push_to_hub(target_model_path, repo_id, hf_token):\n",
    "    print(\"Pushing model to hub...\")\n",
    "    if os.path.exists(f\"{target_model_path}/training_params.json\"):\n",
    "        training_params = json.load(\n",
    "            open(f\"{target_model_path}/training_params.json\")\n",
    "        )\n",
    "        # Optionally, remove sensitive info if needed\n",
    "        # training_params.pop(\"token\")\n",
    "        json.dump(\n",
    "            training_params, open(f\"{target_model_path}/training_params.json\", \"w\")\n",
    "        )\n",
    "\n",
    "    api = HfApi(token=hf_token)\n",
    "    api.create_repo(repo_id=repo_id, repo_type=\"model\", private=True, exist_ok=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=target_model_path, repo_id=repo_id, repo_type=\"model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing model to hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 0.00/16.1G [00:00<?, ?B/\n",
      "Upload 2 LFS files:   0%|                                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   0%| | 0.00/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 16.4k/16.1G [00:00<49:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 246k/16.1G [00:00<4:16:4\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 2.05M/16.1G [00:00<36:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 10.1M/16.1G [00:00<12:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   0%| | 6.96M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 16.1M/16.1G [00:01<16:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 23.5M/16.1G [00:01<10:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 30.9M/16.1G [00:01<10:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   1%| | 27.0M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 44.2M/16.1G [00:02<11:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 47.4M/16.1G [00:02<10:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 56.0M/16.1G [00:02<12:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 59.8M/16.1G [00:02<11:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 66.2M/16.1G [00:03<16:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   0%| | 76.0M/16.1G [00:03<09:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 80.4M/16.1G [00:03<11:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 88.2M/16.1G [00:04<08:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 95.0M/16.1G [00:04<08:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 104M/16.1G [00:04<10:25,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 108M/16.1G [00:05<12:43,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 111M/16.1G [00:05<12:05,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   2%| | 79.0M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 114M/16.1G [00:05<17:50,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 128M/16.1G [00:05<09:07,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   2%| | 96.0M/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 132M/16.1G [00:06<14:54,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 140M/16.1G [00:06<10:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 145M/16.1G [00:06<13:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 156M/16.1G [00:06<08:28,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   3%| | 140M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 161M/16.1G [00:07<10:42,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 188M/16.1G [00:07<06:47,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   3%| | 161M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 205M/16.1G [00:08<07:36,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 210M/16.1G [00:08<08:40,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 221M/16.1G [00:08<06:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 227M/16.1G [00:09<08:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   1%| | 237M/16.1G [00:09<06:14,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   4%| | 210M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 253M/16.1G [00:09<06:32,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   5%| | 226M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 265M/16.1G [00:10<07:28,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   5%| | 242M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 270M/16.1G [00:10<09:07,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 274M/16.1G [00:10<14:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 280M/16.1G [00:10<11:10,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 286M/16.1G [00:11<10:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   6%| | 284M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   6%| | 290M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 301M/16.1G [00:11<09:56,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   6%| | 306M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 317M/16.1G [00:12<08:14,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   7%| | 322M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 332M/16.1G [00:12<07:08,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   7%| | 338M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 338M/16.1G [00:13<09:52,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 349M/16.1G [00:13<07:03,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 355M/16.1G [00:13<09:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 365M/16.1G [00:13<06:52,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   8%| | 380M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 371M/16.1G [00:14<08:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 387M/16.1G [00:14<07:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   2%| | 397M/16.1G [00:14<05:51,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 404M/16.1G [00:14<08:28,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 413M/16.1G [00:15<06:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 419M/16.1G [00:15<07:45,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 430M/16.1G [00:15<05:59,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 462M/16.1G [00:16<06:54,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 468M/16.1G [00:16<08:01,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 478M/16.1G [00:17<06:13,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:   9%| | 464M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 469M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 480M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 485M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 489M/16.1G [00:18<12:09,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  10%| | 502M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 509M/16.1G [00:18<08:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  11%| | 518M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  11%| | 528M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  11%| | 534M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 524M/16.1G [00:19<09:16,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 530M/16.1G [00:19<10:11,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 542M/16.1G [00:19<07:09,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 548M/16.1G [00:20<08:48,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   3%| | 558M/16.1G [00:20<06:45,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 564M/16.1G [00:20<08:03,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 574M/16.1G [00:20<06:15,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  12%| | 598M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 590M/16.1G [00:21<06:11,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  12%| | 614M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|▏| 624M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|▏| 630M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 602M/16.1G [00:21<10:46,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|▏| 646M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 621M/16.1G [00:22<07:30,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  13%|▏| 662M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 638M/16.1G [00:22<06:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  14%|▏| 678M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 654M/16.1G [00:23<05:49,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  14%|▏| 694M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 671M/16.1G [00:23<05:46,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  14%|▏| 710M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 686M/16.1G [00:24<06:31,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  15%|▏| 726M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 703M/16.1G [00:24<06:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   4%| | 709M/16.1G [00:24<06:43,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 735M/16.1G [00:25<05:43,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  15%|▏| 757M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 751M/16.1G [00:25<05:37,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  16%|▏| 771M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 767M/16.1G [00:26<05:50,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  16%|▏| 785M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 782M/16.1G [00:26<06:06,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 788M/16.1G [00:26<07:37,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 799M/16.1G [00:26<05:41,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  17%|▏| 817M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 815M/16.1G [00:27<06:03,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 821M/16.1G [00:27<07:10,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 831M/16.1G [00:27<05:35,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 837M/16.1G [00:28<07:16,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 863M/16.1G [00:28<05:47,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  18%|▏| 866M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 876M/16.1G [00:29<06:59,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  18%|▏| 880M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   5%| | 880M/16.1G [00:29<09:42,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  18%|▏| 898M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 895M/16.1G [00:30<09:55,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  19%|▏| 914M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 912M/16.1G [00:30<08:01,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  19%|▏| 930M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 928M/16.1G [00:31<07:12,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  19%|▏| 946M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 944M/16.1G [00:31<06:33,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  20%|▏| 962M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 960M/16.1G [00:31<06:04,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  20%|▏| 978M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 974M/16.1G [00:32<08:15,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  20%|▏| 992M/4.92G [00:\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 992M/16.1G [00:33<06:52,\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  21%|▏| 1.01G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.01G/16.1G [00:33<06:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.01G/16.1G [00:33<07:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.02G/16.1G [00:33<06:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.03G/16.1G [00:34<07:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   6%| | 1.04G/16.1G [00:34<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.05G/16.1G [00:34<07:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.07G/16.1G [00:35<05:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  22%|▏| 1.07G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.09G/16.1G [00:35<05:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  22%|▏| 1.09G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.09G/16.1G [00:35<07:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.10G/16.1G [00:35<06:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  23%|▏| 1.12G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.11G/16.1G [00:36<07:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.12G/16.1G [00:36<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.13G/16.1G [00:36<07:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.14G/16.1G [00:36<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.14G/16.1G [00:37<07:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.15G/16.1G [00:37<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  24%|▏| 1.17G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.17G/16.1G [00:37<06:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  24%|▏| 1.19G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.18G/16.1G [00:38<06:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  24%|▏| 1.20G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   7%| | 1.20G/16.1G [00:38<06:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  25%|▏| 1.22G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.22G/16.1G [00:39<06:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  25%|▎| 1.23G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.23G/16.1G [00:39<05:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  25%|▎| 1.25G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.25G/16.1G [00:39<05:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  26%|▎| 1.27G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.26G/16.1G [00:40<10:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.26G/16.1G [00:40<07:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.27G/16.1G [00:41<09:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.28G/16.1G [00:41<06:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.29G/16.1G [00:41<08:09\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.30G/16.1G [00:41<06:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|▎| 1.32G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|▎| 1.33G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|▎| 1.34G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  27%|▎| 1.35G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|▎| 1.36G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|▎| 1.36G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|▎| 1.37G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|▎| 1.38G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|▎| 1.39G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  28%|▎| 1.39G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|▎| 1.40G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|▎| 1.41G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|▎| 1.42G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|▎| 1.43G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|▎| 1.44G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  29%|▎| 1.44G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.30G/16.1G [00:45<42:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  30%|▎| 1.46G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.31G/16.1G [00:45<28:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  30%|▎| 1.47G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.33G/16.1G [00:46<15:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  30%|▎| 1.49G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.35G/16.1G [00:46<09:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  31%|▎| 1.51G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   8%| | 1.36G/16.1G [00:47<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  31%|▎| 1.52G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.38G/16.1G [00:47<06:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  31%|▎| 1.54G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.38G/16.1G [00:48<09:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.39G/16.1G [00:48<07:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.41G/16.1G [00:48<06:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  32%|▎| 1.57G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.42G/16.1G [00:49<09:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.43G/16.1G [00:49<07:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.43G/16.1G [00:49<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.44G/16.1G [00:49<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.45G/16.1G [00:49<07:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.46G/16.1G [00:49<05:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.46G/16.1G [00:50<07:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.47G/16.1G [00:50<05:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.48G/16.1G [00:50<07:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.49G/16.1G [00:50<05:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  34%|▎| 1.66G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.50G/16.1G [00:51<07:32\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.51G/16.1G [00:51<05:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.51G/16.1G [00:51<08:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:   9%| | 1.52G/16.1G [00:51<06:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  35%|▎| 1.70G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.54G/16.1G [00:52<06:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  35%|▎| 1.71G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.55G/16.1G [00:52<06:09\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.56G/16.1G [00:53<08:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.58G/16.1G [00:53<08:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.59G/16.1G [00:53<06:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.59G/16.1G [00:53<07:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.60G/16.1G [00:54<05:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.62G/16.1G [00:54<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  36%|▎| 1.78G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.63G/16.1G [00:55<06:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  36%|▎| 1.79G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.64G/16.1G [00:55<07:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.65G/16.1G [00:55<06:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  37%|▎| 1.82G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.66G/16.1G [00:55<08:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.67G/16.1G [00:56<06:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  37%|▎| 1.84G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  10%| | 1.68G/16.1G [00:56<06:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  38%|▍| 1.86G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.70G/16.1G [00:57<07:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.72G/16.1G [00:57<05:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  38%|▍| 1.88G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.72G/16.1G [00:57<07:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.74G/16.1G [00:58<07:32\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.75G/16.1G [00:58<05:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  39%|▍| 1.92G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.75G/16.1G [00:58<07:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.76G/16.1G [00:58<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  39%|▍| 1.94G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.78G/16.1G [00:59<07:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  40%|▍| 1.95G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.78G/16.1G [00:59<09:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  40%|▍| 1.97G/4.92G [00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.79G/16.1G [01:00<14:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.80G/16.1G [01:00<09:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.80G/16.1G [01:00<11:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.81G/16.1G [01:00<07:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.82G/16.1G [01:01<08:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  11%| | 1.83G/16.1G [01:01<06:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.85G/16.1G [01:02<07:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.86G/16.1G [01:02<05:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.88G/16.1G [01:02<04:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  42%|▍| 2.05G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.88G/16.1G [01:02<08:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.89G/16.1G [01:03<06:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  42%|▍| 2.08G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.90G/16.1G [01:03<07:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.91G/16.1G [01:03<06:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.91G/16.1G [01:03<07:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.92G/16.1G [01:03<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.93G/16.1G [01:04<07:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.94G/16.1G [01:04<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.95G/16.1G [01:04<07:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.96G/16.1G [01:04<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.96G/16.1G [01:05<06:54\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.97G/16.1G [01:05<05:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.98G/16.1G [01:05<06:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.99G/16.1G [01:05<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 1.99G/16.1G [01:05<06:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  12%| | 2.00G/16.1G [01:06<05:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.01G/16.1G [01:06<06:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.02G/16.1G [01:06<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  45%|▍| 2.21G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.03G/16.1G [01:07<07:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.04G/16.1G [01:07<08:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.05G/16.1G [01:07<05:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.06G/16.1G [01:07<07:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.07G/16.1G [01:07<05:34\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.07G/16.1G [01:08<06:54\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.08G/16.1G [01:08<05:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  46%|▍| 2.27G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.10G/16.1G [01:08<05:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  47%|▍| 2.29G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.12G/16.1G [01:09<05:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  47%|▍| 2.31G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.13G/16.1G [01:09<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  47%|▍| 2.32G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.14G/16.1G [01:10<07:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.15G/16.1G [01:10<05:48\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  48%|▍| 2.35G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.15G/16.1G [01:10<07:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  13%|▏| 2.16G/16.1G [01:10<05:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  48%|▍| 2.37G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.18G/16.1G [01:11<06:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  48%|▍| 2.39G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.20G/16.1G [01:11<05:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  49%|▍| 2.40G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.20G/16.1G [01:11<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.21G/16.1G [01:12<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.22G/16.1G [01:12<07:34\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.23G/16.1G [01:12<05:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  50%|▍| 2.44G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.24G/16.1G [01:12<07:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.25G/16.1G [01:13<06:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.26G/16.1G [01:13<05:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  50%|▌| 2.48G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.27G/16.1G [01:13<07:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.28G/16.1G [01:13<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.28G/16.1G [01:14<07:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.29G/16.1G [01:14<05:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  51%|▌| 2.51G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.31G/16.1G [01:14<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  51%|▌| 2.53G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  52%|▌| 2.54G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  52%|▌| 2.55G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  52%|▌| 2.56G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.32G/16.1G [01:15<12:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  14%|▏| 2.32G/16.1G [01:15<10:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.33G/16.1G [01:16<10:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.34G/16.1G [01:16<07:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.35G/16.1G [01:16<08:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.36G/16.1G [01:16<06:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.36G/16.1G [01:16<07:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.38G/16.1G [01:17<06:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.38G/16.1G [01:17<06:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.39G/16.1G [01:17<07:00\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  54%|▌| 2.64G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.41G/16.1G [01:18<07:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  54%|▌| 2.66G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.42G/16.1G [01:18<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  54%|▌| 2.67G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|▌| 2.68G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|▌| 2.69G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|▌| 2.70G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|▌| 2.71G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.44G/16.1G [01:19<08:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  55%|▌| 2.72G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.46G/16.1G [01:20<06:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  56%|▌| 2.74G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.47G/16.1G [01:20<05:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.48G/16.1G [01:21<07:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  15%|▏| 2.49G/16.1G [01:21<05:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  56%|▌| 2.77G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  56%|▌| 2.78G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  57%|▌| 2.79G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.50G/16.1G [01:22<13:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.51G/16.1G [01:22<09:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.53G/16.1G [01:23<08:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.54G/16.1G [01:23<06:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.55G/16.1G [01:23<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  58%|▌| 2.83G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  58%|▌| 2.84G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.56G/16.1G [01:24<07:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.57G/16.1G [01:24<05:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  58%|▌| 2.86G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.59G/16.1G [01:24<05:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  59%|▌| 2.88G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.59G/16.1G [01:25<08:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  59%|▌| 2.90G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.60G/16.1G [01:25<10:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  59%|▌| 2.91G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.60G/16.1G [01:25<13:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  60%|▌| 2.93G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.60G/16.1G [01:26<17:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.61G/16.1G [01:26<10:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.62G/16.1G [01:26<10:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.63G/16.1G [01:26<06:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  60%|▌| 2.97G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.64G/16.1G [01:27<08:09\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  16%|▏| 2.65G/16.1G [01:27<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.65G/16.1G [01:27<07:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.68G/16.1G [01:28<05:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  61%|▌| 3.01G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.70G/16.1G [01:28<05:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.70G/16.1G [01:28<05:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.71G/16.1G [01:28<04:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.72G/16.1G [01:29<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.73G/16.1G [01:29<04:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.73G/16.1G [01:29<05:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.75G/16.1G [01:29<05:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.76G/16.1G [01:30<04:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  63%|▋| 3.08G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.77G/16.1G [01:30<07:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.78G/16.1G [01:30<05:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.78G/16.1G [01:30<06:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.79G/16.1G [01:30<06:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.79G/16.1G [01:31<08:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  64%|▋| 3.13G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  64%|▋| 3.14G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.80G/16.1G [01:31<14:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  17%|▏| 2.81G/16.1G [01:32<08:32\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  64%|▋| 3.16G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.81G/16.1G [01:32<10:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.82G/16.1G [01:32<08:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.82G/16.1G [01:32<10:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.83G/16.1G [01:33<10:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  65%|▋| 3.20G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.84G/16.1G [01:33<11:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  65%|▋| 3.22G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.86G/16.1G [01:34<06:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.86G/16.1G [01:34<07:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.87G/16.1G [01:34<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  66%|▋| 3.25G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.89G/16.1G [01:34<05:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  66%|▋| 3.27G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.90G/16.1G [01:35<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.91G/16.1G [01:35<07:01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.92G/16.1G [01:35<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.93G/16.1G [01:36<06:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.94G/16.1G [01:36<04:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.94G/16.1G [01:36<06:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.95G/16.1G [01:36<05:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.96G/16.1G [01:37<06:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  18%|▏| 2.97G/16.1G [01:37<04:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  68%|▋| 3.35G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 2.98G/16.1G [01:37<05:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  68%|▋| 3.36G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.01G/16.1G [01:38<06:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.02G/16.1G [01:38<05:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  69%|▋| 3.39G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.02G/16.1G [01:38<06:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.04G/16.1G [01:39<06:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.05G/16.1G [01:39<05:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  69%|▋| 3.42G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.05G/16.1G [01:39<08:07\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.08G/16.1G [01:40<05:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  70%|▋| 3.44G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.10G/16.1G [01:40<04:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  70%|▋| 3.46G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.11G/16.1G [01:41<05:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  71%|▋| 3.47G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  19%|▏| 3.13G/16.1G [01:41<05:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  71%|▋| 3.49G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.15G/16.1G [01:42<05:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  71%|▋| 3.51G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.15G/16.1G [01:42<07:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.16G/16.1G [01:42<05:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.17G/16.1G [01:43<06:54\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.18G/16.1G [01:43<05:16\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  72%|▋| 3.55G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.18G/16.1G [01:43<08:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.19G/16.1G [01:43<06:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  73%|▋| 3.57G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.21G/16.1G [01:44<06:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  73%|▋| 3.59G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.23G/16.1G [01:44<05:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  73%|▋| 3.60G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.24G/16.1G [01:45<05:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.25G/16.1G [01:45<06:45\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.26G/16.1G [01:45<05:10\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  74%|▋| 3.63G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.27G/16.1G [01:46<05:46\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  74%|▋| 3.65G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  20%|▏| 3.29G/16.1G [01:46<05:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.30G/16.1G [01:46<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.31G/16.1G [01:46<05:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.31G/16.1G [01:47<06:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.32G/16.1G [01:47<04:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  75%|▊| 3.70G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.34G/16.1G [01:47<05:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  75%|▊| 3.71G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.35G/16.1G [01:48<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.36G/16.1G [01:48<06:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.37G/16.1G [01:48<05:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.38G/16.1G [01:49<06:24\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.39G/16.1G [01:49<06:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.40G/16.1G [01:49<04:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.41G/16.1G [01:49<06:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.42G/16.1G [01:50<05:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  77%|▊| 3.79G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.42G/16.1G [01:50<07:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.43G/16.1G [01:50<05:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  77%|▊| 3.81G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  21%|▏| 3.45G/16.1G [01:51<06:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  78%|▊| 3.83G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.47G/16.1G [01:51<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  78%|▊| 3.84G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.47G/16.1G [01:52<07:29\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.48G/16.1G [01:52<05:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.49G/16.1G [01:52<07:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  79%|▊| 3.87G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.50G/16.1G [01:52<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.50G/16.1G [01:53<08:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.52G/16.1G [01:53<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.53G/16.1G [01:53<05:08\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  80%|▊| 3.92G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.54G/16.1G [01:54<06:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.55G/16.1G [01:54<05:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.55G/16.1G [01:54<06:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.56G/16.1G [01:54<05:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.57G/16.1G [01:54<06:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.58G/16.1G [01:54<04:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  81%|▊| 3.97G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.60G/16.1G [01:55<05:22\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  81%|▊| 3.99G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.60G/16.1G [01:55<07:38\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  22%|▏| 3.61G/16.1G [01:56<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.62G/16.1G [01:56<07:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  82%|▊| 4.02G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.63G/16.1G [01:56<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.63G/16.1G [01:56<06:52\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.64G/16.1G [01:56<05:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  82%|▊| 4.05G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.66G/16.1G [01:57<05:27\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|▊| 4.07G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.68G/16.1G [01:57<05:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|▊| 4.08G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.69G/16.1G [01:58<05:05\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|▊| 4.10G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  83%|▊| 4.11G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  84%|▊| 4.11G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.70G/16.1G [01:59<08:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  84%|▊| 4.13G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.71G/16.1G [01:59<08:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.72G/16.1G [01:59<06:03\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  84%|▊| 4.16G/4.92G [01\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.73G/16.1G [02:00<07:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.74G/16.1G [02:00<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  85%|▊| 4.18G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.75G/16.1G [02:01<07:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.76G/16.1G [02:01<08:16\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  23%|▏| 3.77G/16.1G [02:01<05:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  86%|▊| 4.21G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.79G/16.1G [02:01<05:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.79G/16.1G [02:02<06:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.81G/16.1G [02:02<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.82G/16.1G [02:02<04:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.83G/16.1G [02:03<05:58\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.84G/16.1G [02:03<04:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  87%|▊| 4.27G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.84G/16.1G [02:03<06:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.85G/16.1G [02:03<05:12\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  87%|▊| 4.29G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.87G/16.1G [02:04<05:02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  88%|▉| 4.31G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.88G/16.1G [02:04<04:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  88%|▉| 4.32G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  88%|▉| 4.33G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.89G/16.1G [02:05<08:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.90G/16.1G [02:05<06:44\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.90G/16.1G [02:05<07:30\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.92G/16.1G [02:05<05:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  24%|▏| 3.92G/16.1G [02:06<06:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▏| 3.94G/16.1G [02:06<06:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  89%|▉| 4.39G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▏| 3.95G/16.1G [02:06<05:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▏| 3.95G/16.1G [02:07<07:31\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▏| 3.96G/16.1G [02:07<05:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▏| 3.97G/16.1G [02:07<07:11\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▏| 3.98G/16.1G [02:07<05:23\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  90%|▉| 4.43G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  90%|▉| 4.44G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  90%|▉| 4.45G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▏| 3.99G/16.1G [02:08<09:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  91%|▉| 4.47G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▏| 4.00G/16.1G [02:09<09:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▏| 4.01G/16.1G [02:09<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▎| 4.02G/16.1G [02:09<07:35\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▎| 4.03G/16.1G [02:09<05:40\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  92%|▉| 4.51G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▎| 4.03G/16.1G [02:10<07:19\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▎| 4.05G/16.1G [02:10<05:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▎| 4.05G/16.1G [02:10<06:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▎| 4.06G/16.1G [02:10<05:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▎| 4.07G/16.1G [02:10<06:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▎| 4.08G/16.1G [02:11<04:53\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  25%|▎| 4.08G/16.1G [02:11<06:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.10G/16.1G [02:11<06:06\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.11G/16.1G [02:11<04:42\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  93%|▉| 4.59G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.12G/16.1G [02:12<06:13\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.13G/16.1G [02:12<05:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.14G/16.1G [02:12<04:28\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  94%|▉| 4.62G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.15G/16.1G [02:13<05:59\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.16G/16.1G [02:13<04:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.16G/16.1G [02:13<06:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.17G/16.1G [02:13<04:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.18G/16.1G [02:14<06:26\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.19G/16.1G [02:14<04:56\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.20G/16.1G [02:14<06:18\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.21G/16.1G [02:14<04:49\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.21G/16.1G [02:14<05:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.23G/16.1G [02:15<05:57\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.24G/16.1G [02:15<04:36\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  96%|▉| 4.72G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  96%|▉| 4.72G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  26%|▎| 4.25G/16.1G [02:16<06:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  96%|▉| 4.74G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.26G/16.1G [02:16<07:37\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.27G/16.1G [02:16<05:21\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  97%|▉| 4.76G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  97%|▉| 4.77G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.29G/16.1G [02:17<05:51\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  97%|▉| 4.79G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.29G/16.1G [02:17<08:39\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.30G/16.1G [02:17<06:43\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.31G/16.1G [02:18<07:33\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.32G/16.1G [02:18<05:17\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.32G/16.1G [02:18<06:25\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.33G/16.1G [02:18<04:55\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf:  98%|▉| 4.84G/4.92G [02\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.34G/16.1G [02:19<06:20\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.35G/16.1G [02:19<04:50\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.36G/16.1G [02:19<06:14\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.37G/16.1G [02:19<04:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.37G/16.1G [02:19<06:15\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.38G/16.1G [02:20<04:47\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.39G/16.1G [02:20<06:04\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf:  27%|▎| 4.40G/16.1G [02:20<04:41\u001b[A\u001b[A\n",
      "\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-model-Q4_K_M.gguf: 100%|█| 4.92G/4.92G [02\u001b[A\u001b[A\n",
      "BanglaLLM--BanglaLLama-3-8b-BnWiki-Instruct-ggml-f16.gguf: 100%|█| 16.1G/16.1G [08:16<00:00\n",
      "\n",
      "Upload 2 LFS files: 100%|███████████████████████████████████| 2/2 [08:17<00:00, 248.91s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "push_to_hub(target_dir, repo_id, hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_gguf='/workspace/bangla-llama/data/models/BanglaLLama-3-8b-BnWiki-Base/GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p $target_dir_gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/bangla-llama/llama.cpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/anaconda3/envs/autotrain/lib/python310.zip',\n",
       " '/workspace/anaconda3/envs/autotrain/lib/python3.10',\n",
       " '/workspace/anaconda3/envs/autotrain/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages',\n",
       " '/tmp/tmpj3yslgo3',\n",
       " '/workspace/bangla-llama/llama.cpp']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'BanglaLLM/BanglaLLama-3-8b-BnWiki-Base-GGUF'\n",
    "#model.save_pretrained_gguf(target_dir_gguf, tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9dRBJZulavZ"
   },
   "source": [
    "### Instruction Finetuning\n",
    "\n",
    "We now use the [Alpaca in GPT4 Dataset](https://huggingface.co/datasets/FreedomIntelligence/alpaca-gpt4-korean) but translated in Korean!\n",
    "\n",
    "Go to [vicgalle/alpaca-gpt4](https://huggingface.co/datasets/vicgalle/alpaca-gpt4) for the original GPT4 dataset for Alpaca or [MultilingualSIFT project](https://github.com/FreedomIntelligence/MultilingualSIFT) for other translations of the Alpaca dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196,
     "referenced_widgets": [
      "5ad923a4f66c48f293727c51202e3d8b",
      "ef246bb6829e4816a009b3f6875e20b8",
      "aadc85ded2694638a4faa0bb6dc9caa5",
      "c713a841e9cd402bb4f1acfccf9c4a7e",
      "34e31cad78ea462bb50193a9ee4c1372",
      "772ed124915740aeb2f5a614e8cfe2e1",
      "bf79d1ed09134e50b29aa3b6e28ecde5",
      "f59b229ad1ad4ab088682c83bb6c5ce4",
      "93d68696e1d345daaa4e819a47166a85",
      "a3bb01312bcd45649373c3790e327c18",
      "49666651fca242b2b777b1705dd4fa8a",
      "66744f20d34f4cd5b75570d269261b94",
      "03137bba9d7d4ef3ae7e073064b5438d",
      "cbece197ea7f41f9855fa94b6e049254",
      "cc5764233d5d4af79c1221d626b21322",
      "add58b43a7cd4047adb11778c26812f7",
      "b12e798c92f848bc9e1d430d504ee3c7",
      "ebb68cccf04641ed8996e9c9b62c67be",
      "7626531fcae84ddbb8f482b8ac2f7439",
      "119f3c027e3f4beda1c026fd3ebb3f7c",
      "97f0c9d5be3245fb8b1320cc17818ae2",
      "be8fed98ffb3473ba6667d2c17b3929b",
      "c3e586fbe5d34ecdbcc05a2ecbc17fc0",
      "ba1fd50e24904c6988967eab2ab64225",
      "f611865e89774fe4ba6dc047f97eaa61",
      "5fd4be6bc35d459eb335da817e346724",
      "e91b73923c044243b5ed8efcfa252f2f",
      "ee3c23c50c5b435bbada1bcec39c23b4",
      "6d83eb47f7b34c7c930e63649da55097",
      "7621ead96516491db1ebb4fba0055702",
      "790cc80cfe574ee58da93f8e54c8517f",
      "c0e9aae192fa4c62af7379d157c0893f",
      "5e0e10a542654766bf78a9e90c1daa8a"
     ]
    },
    "executionInfo": {
     "elapsed": 4839,
     "status": "ok",
     "timestamp": 1717663490088,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "1oNjUwxOyG8C",
    "outputId": "800603ed-2857-4687-a072-7c5589b5b5e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████████████████████| 124/124 [00:00<00:00, 470kB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|███████████████████████████████| 51.6M/51.6M [00:01<00:00, 32.6MB/s]\n",
      "Generating train split: 100%|██████████████| 49969/49969 [00:00<00:00, 59775.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#alpaca_dataset_korean = load_dataset(\"FreedomIntelligence/alpaca-gpt4-korean\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "alpaca_dataset = load_dataset(\"BanglaLLM/bangla-alpaca-orca\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text', 'system_prompt'],\n",
       "    num_rows: 172026\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmFG41nFytgi"
   },
   "source": [
    "We print 1 example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1717663490089,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "QvTfIKaUQxQ5",
    "outputId": "83e1b097-e215-4b2e-c6bc-9b7f639914ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "এখানে একটি নির্দেশনা দেওয়া হলো, যা একটি কাজ সম্পন্ন করার উপায় বর্ণনা করে, এবং এর সাথে একটি ইনপুট দেওয়া হলো যা আরও প্রেক্ষাপট প্রদান করে। একটি উত্তর লিখুন যা অনুরোধটি সঠিকভাবে পূরণ করে। ### Instruction: একটি ডেলিভারি কোম্পানির জন্য একটি অ্যাপ ডিজাইন করুন। ### Input:  ### Response: ডেলিভারি কোম্পানি অ্যাপ গ্রাহকদের তাদের সমস্ত ডেলিভারি চাহিদা এক জায়গায় পরিচালনা করার জন্য একটি কার্যকর উপায় প্রদান করবে। এখানে এই অ্যাপটির মূল বৈশিষ্ট্যগুলি রয়েছে: 1. ব্যবহারকারীর অ্যাকাউন্ট: গ্রাহকরা তাদের ডেলিভারি অর্ডার ট্র্যাক করতে এবং তাদের অ্যাকাউন্টের ইতিহাস দেখতে একটি ব্যক্তিগত অ্যাকাউন্ট তৈরি করতে সক্ষম হবেন। অ্যাকাউন্টের মাধ্যমে, তারা তাদের ব্যক্তিগত তথ্য, যোগাযোগের বিবরণ এবং ঠিকানা সম্পাদনা করতে পারে। 2. অর্ডার প্লেসমেন্ট: গ্রাহকরা তাদের পছন্দসই আইটেম নির্বাচন করে বা তাদের কেনাকাটার তালিকার একটি ছবি আপলোড করে অ্যাপে তাদের ডেলিভারি অর্ডার দিতে পারেন। অর্ডার নির্দিষ্ট তারিখ এবং সময়ের জন্য নির্ধারিত হতে পারে বা পুনরাবৃত্ত ভিত্তিতে স্থাপন করা যেতে পারে। 3. রিয়েল-টাইম ট্র্যাকিং: গ্রাহকরা তাদের অর্ডার রিয়েল-টাইমে ম্যাপে ট্র্যাক করতে পারেন, আগমনের আনুমানিক সময়ের আপডেট সহ। 4. ইন-অ্যাপ যোগাযোগ: গ্রাহক একটি ইন-অ্যাপ চ্যাট ফাংশনের মাধ্যমে ডেলিভারি নির্দেশাবলী, সময়সূচী বা বিশেষ অনুরোধ সম্পর্কে ডেলিভারি ড্রাইভারের সাথে যোগাযোগ করতে সক্ষম হবে। 5. ডেলিভারি নিশ্চিতকরণ: ডেলিভারি সম্পূর্ণ হওয়ার পরে, গ্রাহক একটি বিজ্ঞপ্তি পাবেন, এবং তারা ডেলিভারি নিশ্চিত করতে এবং পরিষেবাটি রেট করতে সক্ষম হবে। 6. পেমেন্ট: গ্রাহকরা তাদের পছন্দের অর্থপ্রদানের পদ্ধতি অ্যাপে যোগ করতে পারবেন, যেমন একটি ক্রেডিট কার্ড বা মোবাইল পেমেন্ট সিস্টেমের মাধ্যমে এবং লেনদেনগুলি নির্বিঘ্নে প্রক্রিয়া করা হবে। 7. গ্রাহক সহায়তা: গ্রাহকরা সহায়তা কেন্দ্রে প্রবেশ করতে সক্ষম হবেন, যেখানে তারা যেকোন সমস্যা প্রতিবেদন করতে, তাদের প্রশ্নের উত্তর পেতে বা অ্যাপ থেকে সরাসরি গ্রাহক সহায়তায় কল করতে পারবেন। সামগ্রিকভাবে, এই ডেলিভারি অ্যাপটি গ্রাহকদের জন্য একটি স্বজ্ঞাত এবং ব্যবহারকারী-বান্ধব ইন্টারফেস প্রদান করবে, ডেলিভারি প্রক্রিয়াটিকে নির্বিঘ্ন এবং চাপমুক্ত করে তুলবে।\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO8UY34ql2vJ"
   },
   "source": [
    "We again use https://translate.google.com/ to translate the Alpaca format into Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69,
     "referenced_widgets": [
      "a6babbf2f467476f8a650a0b3cf4b225",
      "4d1479b7392549beb8459113a42e2610",
      "a1b291badc6041beae070c2e262ae005",
      "a424608a868c46b2be65767c093c3dd1",
      "4ad139d347eb449ab7b4bd6df4f64516",
      "bdc71b983e5f48b1800ebdc457f22f3e",
      "3bc92134440649aba5cf0ff5f4393c55",
      "e6ae977baad248078458445fecc03963",
      "55b47d1ed657480d9a860b46eaa1d678",
      "8b93d48735a347c1bc7a5b72692387e7",
      "960c9eba4fec494989f66a6c0dd5fce5"
     ]
    },
    "executionInfo": {
     "elapsed": 1949,
     "status": "ok",
     "timestamp": 1717663492023,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "B2tv8AEhPTu6",
    "outputId": "acd18935-ee12-4fbd-fc55-b813dc389d13"
   },
   "outputs": [],
   "source": [
    "# Wikipedia provides a title and an article text.\n",
    "# Use https://translate.google.com!\n",
    "alpaca_prompt = \"\"\"এখানে একটি নির্দেশনা দেওয়া হলো, যা একটি কাজ সম্পন্ন করার উপায় বর্ণনা করে, এবং এর সাথে একটি ইনপুট দেওয়া হলো যা আরও প্রেক্ষাপট প্রদান করে। একটি উত্তর লিখুন যা অনুরোধটি সঠিকভাবে পূরণ করে। \n",
    "### Instruction: {}\n",
    "\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "# becomes:\n",
    "alpaca_prompt = \"\"\"এখানে একটি নির্দেশনা দেওয়া হলো, যা একটি কাজ সম্পন্ন করার উপায় বর্ণনা করে, এবং এর সাথে একটি ইনপুট দেওয়া হলো যা আরও প্রেক্ষাপট প্রদান করে। একটি উত্তর লিখুন যা অনুরোধটি সঠিকভাবে পূরণ করে। \n",
    "### Instruction: {}\n",
    "\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(conversations):\n",
    "    texts = []\n",
    "    conversations = conversations[\"conversations\"]\n",
    "    for convo in conversations:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(convo[0][\"value\"], convo[1][\"value\"]) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "#alpaca_dataset = alpaca_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpaca_dataset = alpaca_dataset.train_test_split(train_size = 0.01)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text', 'system_prompt'],\n",
       "    num_rows: 172026\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxWWh8xsl9XT"
   },
   "source": [
    "We again employ `UnslothTrainer` and do instruction finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "f09ab65302ea49f38ad65c7285439b66",
      "89727d06874849f5b681f3705bfe0978",
      "3b939980516140c588220612194de161",
      "fed11544e08c4fb3bf9fe247f7c37cd1",
      "1531e137a0b84e3db00599d6311fbc04",
      "aa120e115285432da310d295df2bb739",
      "7c319ad4809c47189345d0d158dcd922",
      "9918c010fe1f418c831d5fffd0a94180",
      "90655a74b97944f58cad529fa8adf1fa",
      "e57c379628ce49da904b447a74b0e1b1",
      "d6217028b7744abdb85c305237cb52a0"
     ]
    },
    "executionInfo": {
     "elapsed": 115741,
     "status": "ok",
     "timestamp": 1717663607754,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "1Zul21NSRRLP",
    "outputId": "d3bab1d0-6399-4c14-9e26-b2d3ea1d27d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[codecarbon INFO @ 18:07:37] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:07:37] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:07:37] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 18:07:37] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:07:37] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:07:39] CPU Model on constant consumption mode: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 18:07:39] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:07:39]   Platform system: Linux-5.4.0-169-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 18:07:39]   Python version: 3.10.14\n",
      "[codecarbon INFO @ 18:07:39]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 18:07:39]   Available RAM : 1007.784 GB\n",
      "[codecarbon INFO @ 18:07:39]   CPU count: 128\n",
      "[codecarbon INFO @ 18:07:39]   CPU model: AMD EPYC 7543 32-Core Processor\n",
      "[codecarbon INFO @ 18:07:39]   GPU count: 1\n",
      "[codecarbon INFO @ 18:07:39]   GPU model: 1 x NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = alpaca_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        # Use num_train_epochs and warmup_ratio for longer runs!\n",
    "        #max_steps = 120,\n",
    "        warmup_steps = 10,\n",
    "        warmup_ratio = 0.05,\n",
    "        num_train_epochs = 1,\n",
    "\n",
    "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 1e-5,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3624846,
     "status": "ok",
     "timestamp": 1717667232586,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "DIO7c1FoRe-X",
    "outputId": "68e3537d-b30c-4f35-af67-c73bd589a37e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 172,026 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 128 | Total steps = 1,344\n",
      " \"-____-\"     Number of trainable parameters = 1,386,217,472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n",
      "Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:08:11] Energy consumed for RAM : 0.001635 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:11] Energy consumed for all GPUs : 0.001025 kWh. Total GPU Power : 236.9192245871228 W\n",
      "[codecarbon INFO @ 18:08:11] Energy consumed for all CPUs : 0.000487 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:11] 0.003148 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:26] Energy consumed for RAM : 0.003211 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:26] Energy consumed for all GPUs : 0.002222 kWh. Total GPU Power : 286.9437672889438 W\n",
      "[codecarbon INFO @ 18:08:26] Energy consumed for all CPUs : 0.000957 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:26] 0.006391 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:41] Energy consumed for RAM : 0.004784 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:41] Energy consumed for all GPUs : 0.003419 kWh. Total GPU Power : 287.4171218110107 W\n",
      "[codecarbon INFO @ 18:08:41] Energy consumed for all CPUs : 0.001425 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:41] 0.009628 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:56] Energy consumed for RAM : 0.006357 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:08:56] Energy consumed for all GPUs : 0.004618 kWh. Total GPU Power : 287.9541976131156 W\n",
      "[codecarbon INFO @ 18:08:56] Energy consumed for all CPUs : 0.001894 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:08:56] 0.012869 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:11] Energy consumed for RAM : 0.007931 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:11] Energy consumed for all GPUs : 0.005827 kWh. Total GPU Power : 290.31942944392415 W\n",
      "[codecarbon INFO @ 18:09:11] Energy consumed for all CPUs : 0.002363 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:11] 0.016121 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='1344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/1344 02:25 < 27:06:44, 0.01 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.606300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:09:26] Energy consumed for RAM : 0.009505 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:26] Energy consumed for all GPUs : 0.006993 kWh. Total GPU Power : 279.876770470884 W\n",
      "[codecarbon INFO @ 18:09:26] Energy consumed for all CPUs : 0.002831 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:26] 0.019329 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:42] Energy consumed for RAM : 0.011183 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:42] Energy consumed for all GPUs : 0.008284 kWh. Total GPU Power : 290.7094274699353 W\n",
      "[codecarbon INFO @ 18:09:42] Energy consumed for all CPUs : 0.003331 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:42] 0.022799 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:57] Energy consumed for RAM : 0.012757 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:09:57] Energy consumed for all GPUs : 0.009497 kWh. Total GPU Power : 291.1135479397693 W\n",
      "[codecarbon INFO @ 18:09:57] Energy consumed for all CPUs : 0.003800 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:09:57] 0.026054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:12] Energy consumed for RAM : 0.014331 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:12] Energy consumed for all GPUs : 0.010686 kWh. Total GPU Power : 285.60071279964137 W\n",
      "[codecarbon INFO @ 18:10:12] Energy consumed for all CPUs : 0.004269 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:12] 0.029286 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:27] Energy consumed for RAM : 0.015922 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:27] Energy consumed for all GPUs : 0.011910 kWh. Total GPU Power : 290.57925000400616 W\n",
      "[codecarbon INFO @ 18:10:27] Energy consumed for all CPUs : 0.004743 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:27] 0.032576 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:42] Energy consumed for RAM : 0.017495 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:42] Energy consumed for all GPUs : 0.013114 kWh. Total GPU Power : 289.0136369761482 W\n",
      "[codecarbon INFO @ 18:10:42] Energy consumed for all CPUs : 0.005212 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:42] 0.035820 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:57] Energy consumed for RAM : 0.019069 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:10:57] Energy consumed for all GPUs : 0.014335 kWh. Total GPU Power : 293.2924827348806 W\n",
      "[codecarbon INFO @ 18:10:57] Energy consumed for all CPUs : 0.005680 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:10:57] 0.039084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:13] Energy consumed for RAM : 0.020734 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:13] Energy consumed for all GPUs : 0.015614 kWh. Total GPU Power : 290.08496905337216 W\n",
      "[codecarbon INFO @ 18:11:13] Energy consumed for all CPUs : 0.006176 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:13] 0.042525 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:28] Energy consumed for RAM : 0.022308 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:28] Energy consumed for all GPUs : 0.016829 kWh. Total GPU Power : 291.682251376309 W\n",
      "[codecarbon INFO @ 18:11:28] Energy consumed for all CPUs : 0.006645 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:28] 0.045782 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:43] Energy consumed for RAM : 0.023880 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:43] Energy consumed for all GPUs : 0.018023 kWh. Total GPU Power : 286.96374393688865 W\n",
      "[codecarbon INFO @ 18:11:43] Energy consumed for all CPUs : 0.007114 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:43] 0.049018 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for RAM : 0.025543 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for all GPUs : 0.019302 kWh. Total GPU Power : 290.56786476108346 W\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for all CPUs : 0.007609 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:11:59] 0.052455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for RAM : 0.027117 kWh. RAM Power : 377.91905450820923 W\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for all GPUs : 0.020517 kWh. Total GPU Power : 291.6241488975196 W\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for all CPUs : 0.008078 kWh. Total CPU Power : 112.5 W\n",
      "[codecarbon INFO @ 18:12:14] 0.055711 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1717667232587,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "198797f5-4842-4a8d-e15c-3a60643816c0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_gpu_memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Show final memory and time stats\u001b[39;00m\n\u001b[1;32m      2\u001b[0m used_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m used_memory_for_lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory \u001b[38;5;241m-\u001b[39m \u001b[43mstart_gpu_memory\u001b[49m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m used_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory         \u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m lora_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory_for_lora\u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_gpu_memory' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=1344, training_loss=0.3289879886433482, metrics={'train_runtime': 98065.4069, 'train_samples_per_second': 1.754, 'train_steps_per_second': 0.014, 'total_flos': 1.7581221468435382e+19, 'train_loss': 0.3289879886433482, 'epoch': 1.0})\n"
     ]
    }
   ],
   "source": [
    "print(trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done finetuning\n"
     ]
    }
   ],
   "source": [
    "print(\"Done finetuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n",
    "Remember to use https://translate.google.com/!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export HF_HOME=/workspace/.cache/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/.cache/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████| 7/7 [02:03<00:00, 17.58s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name = \"BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Use 4bit quantization to reduce memory usage. Can be False.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/loader.py:143\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:1129\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# RoPE Scaling's max_position_embeddings must be updated\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m max_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_seq_length, model_max_seq_length)\n\u001b[0;32m-> 1129\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# Counteract saved tokenizers\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m tokenizer_name \u001b[38;5;241m=\u001b[39m model_name \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer_name\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/modeling_utils.py:3626\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3620\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3621\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3622\u001b[0m )\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3626\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3629\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1091\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:878\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 878\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:878\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 878\u001b[0m     [\u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:678\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mLLAMA_ATTENTION_CLASSES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m LlamaMLP(config)\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:267\u001b[0m, in \u001b[0;36mLlamaAttention.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_bias)\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_rope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:271\u001b[0m, in \u001b[0;36mLlamaAttention._init_rope\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_rope\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_scaling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaRotaryEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m         scaling_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrope_scaling[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:915\u001b[0m, in \u001b[0;36mLlamaRotaryEmbedding.__init__\u001b[0;34m(self, dim, max_position_embeddings, base, device)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;241m=\u001b[39m base\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Build here to make `torch.jit.trace` work.\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_cos_sin_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:930\u001b[0m, in \u001b[0;36mLlamaRotaryEmbedding._set_cos_sin_cache\u001b[0;34m(self, seq_len, device, dtype)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# Different from paper, but it uses a different permutation in order to obtain the same calculation\u001b[39;00m\n\u001b[1;32m    929\u001b[0m emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((freqs, freqs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos_cached\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m, persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin_cached\u001b[39m\u001b[38;5;124m\"\u001b[39m, emb\u001b[38;5;241m.\u001b[39msin()\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████| 7/7 [02:05<00:00, 17.89s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "#model_name2 = \"BanglaLLM/bangla-llama-7b-instruct-v0.1\"\n",
    "model_name2 = \"BanglaLLM/BanglaLLama-3-8b-BnWiki-Instruct\"\n",
    "model2, tokenizer2 = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name2, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████| 3/3 [00:06<00:00,  2.16s/it]\n",
      "Some weights of the model checkpoint at BanglaLLM/bangla-llama-13b-instruct-v0.1 were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.base_layer.weight', 'model.layers.0.mlp.down_proj.lora_A.default.weight', 'model.layers.0.mlp.down_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.base_layer.weight', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight', 'model.layers.0.mlp.up_proj.base_layer.weight', 'model.layers.0.mlp.up_proj.lora_A.default.weight', 'model.layers.0.mlp.up_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.base_layer.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.base_layer.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.mlp.down_proj.base_layer.weight', 'model.layers.1.mlp.down_proj.lora_A.default.weight', 'model.layers.1.mlp.down_proj.lora_B.default.weight', 'model.layers.1.mlp.gate_proj.base_layer.weight', 'model.layers.1.mlp.gate_proj.lora_A.default.weight', 'model.layers.1.mlp.gate_proj.lora_B.default.weight', 'model.layers.1.mlp.up_proj.base_layer.weight', 'model.layers.1.mlp.up_proj.lora_A.default.weight', 'model.layers.1.mlp.up_proj.lora_B.default.weight', 'model.layers.1.self_attn.k_proj.base_layer.weight', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.base_layer.weight', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.mlp.down_proj.base_layer.weight', 'model.layers.10.mlp.down_proj.lora_A.default.weight', 'model.layers.10.mlp.down_proj.lora_B.default.weight', 'model.layers.10.mlp.gate_proj.base_layer.weight', 'model.layers.10.mlp.gate_proj.lora_A.default.weight', 'model.layers.10.mlp.gate_proj.lora_B.default.weight', 'model.layers.10.mlp.up_proj.base_layer.weight', 'model.layers.10.mlp.up_proj.lora_A.default.weight', 'model.layers.10.mlp.up_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.base_layer.weight', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.base_layer.weight', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.mlp.down_proj.base_layer.weight', 'model.layers.11.mlp.down_proj.lora_A.default.weight', 'model.layers.11.mlp.down_proj.lora_B.default.weight', 'model.layers.11.mlp.gate_proj.base_layer.weight', 'model.layers.11.mlp.gate_proj.lora_A.default.weight', 'model.layers.11.mlp.gate_proj.lora_B.default.weight', 'model.layers.11.mlp.up_proj.base_layer.weight', 'model.layers.11.mlp.up_proj.lora_A.default.weight', 'model.layers.11.mlp.up_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.base_layer.weight', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.base_layer.weight', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.mlp.down_proj.base_layer.weight', 'model.layers.12.mlp.down_proj.lora_A.default.weight', 'model.layers.12.mlp.down_proj.lora_B.default.weight', 'model.layers.12.mlp.gate_proj.base_layer.weight', 'model.layers.12.mlp.gate_proj.lora_A.default.weight', 'model.layers.12.mlp.gate_proj.lora_B.default.weight', 'model.layers.12.mlp.up_proj.base_layer.weight', 'model.layers.12.mlp.up_proj.lora_A.default.weight', 'model.layers.12.mlp.up_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.base_layer.weight', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.o_proj.base_layer.weight', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.mlp.down_proj.base_layer.weight', 'model.layers.13.mlp.down_proj.lora_A.default.weight', 'model.layers.13.mlp.down_proj.lora_B.default.weight', 'model.layers.13.mlp.gate_proj.base_layer.weight', 'model.layers.13.mlp.gate_proj.lora_A.default.weight', 'model.layers.13.mlp.gate_proj.lora_B.default.weight', 'model.layers.13.mlp.up_proj.base_layer.weight', 'model.layers.13.mlp.up_proj.lora_A.default.weight', 'model.layers.13.mlp.up_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.base_layer.weight', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.o_proj.base_layer.weight', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.mlp.down_proj.base_layer.weight', 'model.layers.14.mlp.down_proj.lora_A.default.weight', 'model.layers.14.mlp.down_proj.lora_B.default.weight', 'model.layers.14.mlp.gate_proj.base_layer.weight', 'model.layers.14.mlp.gate_proj.lora_A.default.weight', 'model.layers.14.mlp.gate_proj.lora_B.default.weight', 'model.layers.14.mlp.up_proj.base_layer.weight', 'model.layers.14.mlp.up_proj.lora_A.default.weight', 'model.layers.14.mlp.up_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.base_layer.weight', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.base_layer.weight', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.mlp.down_proj.base_layer.weight', 'model.layers.15.mlp.down_proj.lora_A.default.weight', 'model.layers.15.mlp.down_proj.lora_B.default.weight', 'model.layers.15.mlp.gate_proj.base_layer.weight', 'model.layers.15.mlp.gate_proj.lora_A.default.weight', 'model.layers.15.mlp.gate_proj.lora_B.default.weight', 'model.layers.15.mlp.up_proj.base_layer.weight', 'model.layers.15.mlp.up_proj.lora_A.default.weight', 'model.layers.15.mlp.up_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.base_layer.weight', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.base_layer.weight', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.mlp.down_proj.base_layer.weight', 'model.layers.16.mlp.down_proj.lora_A.default.weight', 'model.layers.16.mlp.down_proj.lora_B.default.weight', 'model.layers.16.mlp.gate_proj.base_layer.weight', 'model.layers.16.mlp.gate_proj.lora_A.default.weight', 'model.layers.16.mlp.gate_proj.lora_B.default.weight', 'model.layers.16.mlp.up_proj.base_layer.weight', 'model.layers.16.mlp.up_proj.lora_A.default.weight', 'model.layers.16.mlp.up_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.base_layer.weight', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.base_layer.weight', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.mlp.down_proj.base_layer.weight', 'model.layers.17.mlp.down_proj.lora_A.default.weight', 'model.layers.17.mlp.down_proj.lora_B.default.weight', 'model.layers.17.mlp.gate_proj.base_layer.weight', 'model.layers.17.mlp.gate_proj.lora_A.default.weight', 'model.layers.17.mlp.gate_proj.lora_B.default.weight', 'model.layers.17.mlp.up_proj.base_layer.weight', 'model.layers.17.mlp.up_proj.lora_A.default.weight', 'model.layers.17.mlp.up_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.base_layer.weight', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.o_proj.base_layer.weight', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.mlp.down_proj.base_layer.weight', 'model.layers.18.mlp.down_proj.lora_A.default.weight', 'model.layers.18.mlp.down_proj.lora_B.default.weight', 'model.layers.18.mlp.gate_proj.base_layer.weight', 'model.layers.18.mlp.gate_proj.lora_A.default.weight', 'model.layers.18.mlp.gate_proj.lora_B.default.weight', 'model.layers.18.mlp.up_proj.base_layer.weight', 'model.layers.18.mlp.up_proj.lora_A.default.weight', 'model.layers.18.mlp.up_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.base_layer.weight', 'model.layers.18.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.base_layer.weight', 'model.layers.18.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.o_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.mlp.down_proj.base_layer.weight', 'model.layers.19.mlp.down_proj.lora_A.default.weight', 'model.layers.19.mlp.down_proj.lora_B.default.weight', 'model.layers.19.mlp.gate_proj.base_layer.weight', 'model.layers.19.mlp.gate_proj.lora_A.default.weight', 'model.layers.19.mlp.gate_proj.lora_B.default.weight', 'model.layers.19.mlp.up_proj.base_layer.weight', 'model.layers.19.mlp.up_proj.lora_A.default.weight', 'model.layers.19.mlp.up_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.base_layer.weight', 'model.layers.19.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.base_layer.weight', 'model.layers.19.self_attn.o_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.mlp.down_proj.base_layer.weight', 'model.layers.2.mlp.down_proj.lora_A.default.weight', 'model.layers.2.mlp.down_proj.lora_B.default.weight', 'model.layers.2.mlp.gate_proj.base_layer.weight', 'model.layers.2.mlp.gate_proj.lora_A.default.weight', 'model.layers.2.mlp.gate_proj.lora_B.default.weight', 'model.layers.2.mlp.up_proj.base_layer.weight', 'model.layers.2.mlp.up_proj.lora_A.default.weight', 'model.layers.2.mlp.up_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.base_layer.weight', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.base_layer.weight', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.mlp.down_proj.base_layer.weight', 'model.layers.20.mlp.down_proj.lora_A.default.weight', 'model.layers.20.mlp.down_proj.lora_B.default.weight', 'model.layers.20.mlp.gate_proj.base_layer.weight', 'model.layers.20.mlp.gate_proj.lora_A.default.weight', 'model.layers.20.mlp.gate_proj.lora_B.default.weight', 'model.layers.20.mlp.up_proj.base_layer.weight', 'model.layers.20.mlp.up_proj.lora_A.default.weight', 'model.layers.20.mlp.up_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.base_layer.weight', 'model.layers.20.self_attn.k_proj.lora_A.default.weight', 'model.layers.20.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.base_layer.weight', 'model.layers.20.self_attn.o_proj.lora_A.default.weight', 'model.layers.20.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.mlp.down_proj.base_layer.weight', 'model.layers.21.mlp.down_proj.lora_A.default.weight', 'model.layers.21.mlp.down_proj.lora_B.default.weight', 'model.layers.21.mlp.gate_proj.base_layer.weight', 'model.layers.21.mlp.gate_proj.lora_A.default.weight', 'model.layers.21.mlp.gate_proj.lora_B.default.weight', 'model.layers.21.mlp.up_proj.base_layer.weight', 'model.layers.21.mlp.up_proj.lora_A.default.weight', 'model.layers.21.mlp.up_proj.lora_B.default.weight', 'model.layers.21.self_attn.k_proj.base_layer.weight', 'model.layers.21.self_attn.k_proj.lora_A.default.weight', 'model.layers.21.self_attn.k_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.base_layer.weight', 'model.layers.21.self_attn.o_proj.lora_A.default.weight', 'model.layers.21.self_attn.o_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.mlp.down_proj.base_layer.weight', 'model.layers.22.mlp.down_proj.lora_A.default.weight', 'model.layers.22.mlp.down_proj.lora_B.default.weight', 'model.layers.22.mlp.gate_proj.base_layer.weight', 'model.layers.22.mlp.gate_proj.lora_A.default.weight', 'model.layers.22.mlp.gate_proj.lora_B.default.weight', 'model.layers.22.mlp.up_proj.base_layer.weight', 'model.layers.22.mlp.up_proj.lora_A.default.weight', 'model.layers.22.mlp.up_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.base_layer.weight', 'model.layers.22.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.k_proj.lora_B.default.weight', 'model.layers.22.self_attn.o_proj.base_layer.weight', 'model.layers.22.self_attn.o_proj.lora_A.default.weight', 'model.layers.22.self_attn.o_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.mlp.down_proj.base_layer.weight', 'model.layers.23.mlp.down_proj.lora_A.default.weight', 'model.layers.23.mlp.down_proj.lora_B.default.weight', 'model.layers.23.mlp.gate_proj.base_layer.weight', 'model.layers.23.mlp.gate_proj.lora_A.default.weight', 'model.layers.23.mlp.gate_proj.lora_B.default.weight', 'model.layers.23.mlp.up_proj.base_layer.weight', 'model.layers.23.mlp.up_proj.lora_A.default.weight', 'model.layers.23.mlp.up_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.base_layer.weight', 'model.layers.23.self_attn.k_proj.lora_A.default.weight', 'model.layers.23.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.o_proj.base_layer.weight', 'model.layers.23.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.o_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.mlp.down_proj.base_layer.weight', 'model.layers.24.mlp.down_proj.lora_A.default.weight', 'model.layers.24.mlp.down_proj.lora_B.default.weight', 'model.layers.24.mlp.gate_proj.base_layer.weight', 'model.layers.24.mlp.gate_proj.lora_A.default.weight', 'model.layers.24.mlp.gate_proj.lora_B.default.weight', 'model.layers.24.mlp.up_proj.base_layer.weight', 'model.layers.24.mlp.up_proj.lora_A.default.weight', 'model.layers.24.mlp.up_proj.lora_B.default.weight', 'model.layers.24.self_attn.k_proj.base_layer.weight', 'model.layers.24.self_attn.k_proj.lora_A.default.weight', 'model.layers.24.self_attn.k_proj.lora_B.default.weight', 'model.layers.24.self_attn.o_proj.base_layer.weight', 'model.layers.24.self_attn.o_proj.lora_A.default.weight', 'model.layers.24.self_attn.o_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.mlp.down_proj.base_layer.weight', 'model.layers.25.mlp.down_proj.lora_A.default.weight', 'model.layers.25.mlp.down_proj.lora_B.default.weight', 'model.layers.25.mlp.gate_proj.base_layer.weight', 'model.layers.25.mlp.gate_proj.lora_A.default.weight', 'model.layers.25.mlp.gate_proj.lora_B.default.weight', 'model.layers.25.mlp.up_proj.base_layer.weight', 'model.layers.25.mlp.up_proj.lora_A.default.weight', 'model.layers.25.mlp.up_proj.lora_B.default.weight', 'model.layers.25.self_attn.k_proj.base_layer.weight', 'model.layers.25.self_attn.k_proj.lora_A.default.weight', 'model.layers.25.self_attn.k_proj.lora_B.default.weight', 'model.layers.25.self_attn.o_proj.base_layer.weight', 'model.layers.25.self_attn.o_proj.lora_A.default.weight', 'model.layers.25.self_attn.o_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.mlp.down_proj.base_layer.weight', 'model.layers.26.mlp.down_proj.lora_A.default.weight', 'model.layers.26.mlp.down_proj.lora_B.default.weight', 'model.layers.26.mlp.gate_proj.base_layer.weight', 'model.layers.26.mlp.gate_proj.lora_A.default.weight', 'model.layers.26.mlp.gate_proj.lora_B.default.weight', 'model.layers.26.mlp.up_proj.base_layer.weight', 'model.layers.26.mlp.up_proj.lora_A.default.weight', 'model.layers.26.mlp.up_proj.lora_B.default.weight', 'model.layers.26.self_attn.k_proj.base_layer.weight', 'model.layers.26.self_attn.k_proj.lora_A.default.weight', 'model.layers.26.self_attn.k_proj.lora_B.default.weight', 'model.layers.26.self_attn.o_proj.base_layer.weight', 'model.layers.26.self_attn.o_proj.lora_A.default.weight', 'model.layers.26.self_attn.o_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.mlp.down_proj.base_layer.weight', 'model.layers.27.mlp.down_proj.lora_A.default.weight', 'model.layers.27.mlp.down_proj.lora_B.default.weight', 'model.layers.27.mlp.gate_proj.base_layer.weight', 'model.layers.27.mlp.gate_proj.lora_A.default.weight', 'model.layers.27.mlp.gate_proj.lora_B.default.weight', 'model.layers.27.mlp.up_proj.base_layer.weight', 'model.layers.27.mlp.up_proj.lora_A.default.weight', 'model.layers.27.mlp.up_proj.lora_B.default.weight', 'model.layers.27.self_attn.k_proj.base_layer.weight', 'model.layers.27.self_attn.k_proj.lora_A.default.weight', 'model.layers.27.self_attn.k_proj.lora_B.default.weight', 'model.layers.27.self_attn.o_proj.base_layer.weight', 'model.layers.27.self_attn.o_proj.lora_A.default.weight', 'model.layers.27.self_attn.o_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.mlp.down_proj.base_layer.weight', 'model.layers.28.mlp.down_proj.lora_A.default.weight', 'model.layers.28.mlp.down_proj.lora_B.default.weight', 'model.layers.28.mlp.gate_proj.base_layer.weight', 'model.layers.28.mlp.gate_proj.lora_A.default.weight', 'model.layers.28.mlp.gate_proj.lora_B.default.weight', 'model.layers.28.mlp.up_proj.base_layer.weight', 'model.layers.28.mlp.up_proj.lora_A.default.weight', 'model.layers.28.mlp.up_proj.lora_B.default.weight', 'model.layers.28.self_attn.k_proj.base_layer.weight', 'model.layers.28.self_attn.k_proj.lora_A.default.weight', 'model.layers.28.self_attn.k_proj.lora_B.default.weight', 'model.layers.28.self_attn.o_proj.base_layer.weight', 'model.layers.28.self_attn.o_proj.lora_A.default.weight', 'model.layers.28.self_attn.o_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.mlp.down_proj.base_layer.weight', 'model.layers.29.mlp.down_proj.lora_A.default.weight', 'model.layers.29.mlp.down_proj.lora_B.default.weight', 'model.layers.29.mlp.gate_proj.base_layer.weight', 'model.layers.29.mlp.gate_proj.lora_A.default.weight', 'model.layers.29.mlp.gate_proj.lora_B.default.weight', 'model.layers.29.mlp.up_proj.base_layer.weight', 'model.layers.29.mlp.up_proj.lora_A.default.weight', 'model.layers.29.mlp.up_proj.lora_B.default.weight', 'model.layers.29.self_attn.k_proj.base_layer.weight', 'model.layers.29.self_attn.k_proj.lora_A.default.weight', 'model.layers.29.self_attn.k_proj.lora_B.default.weight', 'model.layers.29.self_attn.o_proj.base_layer.weight', 'model.layers.29.self_attn.o_proj.lora_A.default.weight', 'model.layers.29.self_attn.o_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.mlp.down_proj.base_layer.weight', 'model.layers.3.mlp.down_proj.lora_A.default.weight', 'model.layers.3.mlp.down_proj.lora_B.default.weight', 'model.layers.3.mlp.gate_proj.base_layer.weight', 'model.layers.3.mlp.gate_proj.lora_A.default.weight', 'model.layers.3.mlp.gate_proj.lora_B.default.weight', 'model.layers.3.mlp.up_proj.base_layer.weight', 'model.layers.3.mlp.up_proj.lora_A.default.weight', 'model.layers.3.mlp.up_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.base_layer.weight', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.base_layer.weight', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.mlp.down_proj.base_layer.weight', 'model.layers.30.mlp.down_proj.lora_A.default.weight', 'model.layers.30.mlp.down_proj.lora_B.default.weight', 'model.layers.30.mlp.gate_proj.base_layer.weight', 'model.layers.30.mlp.gate_proj.lora_A.default.weight', 'model.layers.30.mlp.gate_proj.lora_B.default.weight', 'model.layers.30.mlp.up_proj.base_layer.weight', 'model.layers.30.mlp.up_proj.lora_A.default.weight', 'model.layers.30.mlp.up_proj.lora_B.default.weight', 'model.layers.30.self_attn.k_proj.base_layer.weight', 'model.layers.30.self_attn.k_proj.lora_A.default.weight', 'model.layers.30.self_attn.k_proj.lora_B.default.weight', 'model.layers.30.self_attn.o_proj.base_layer.weight', 'model.layers.30.self_attn.o_proj.lora_A.default.weight', 'model.layers.30.self_attn.o_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.mlp.down_proj.base_layer.weight', 'model.layers.31.mlp.down_proj.lora_A.default.weight', 'model.layers.31.mlp.down_proj.lora_B.default.weight', 'model.layers.31.mlp.gate_proj.base_layer.weight', 'model.layers.31.mlp.gate_proj.lora_A.default.weight', 'model.layers.31.mlp.gate_proj.lora_B.default.weight', 'model.layers.31.mlp.up_proj.base_layer.weight', 'model.layers.31.mlp.up_proj.lora_A.default.weight', 'model.layers.31.mlp.up_proj.lora_B.default.weight', 'model.layers.31.self_attn.k_proj.base_layer.weight', 'model.layers.31.self_attn.k_proj.lora_A.default.weight', 'model.layers.31.self_attn.k_proj.lora_B.default.weight', 'model.layers.31.self_attn.o_proj.base_layer.weight', 'model.layers.31.self_attn.o_proj.lora_A.default.weight', 'model.layers.31.self_attn.o_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.32.mlp.down_proj.base_layer.weight', 'model.layers.32.mlp.down_proj.lora_A.default.weight', 'model.layers.32.mlp.down_proj.lora_B.default.weight', 'model.layers.32.mlp.gate_proj.base_layer.weight', 'model.layers.32.mlp.gate_proj.lora_A.default.weight', 'model.layers.32.mlp.gate_proj.lora_B.default.weight', 'model.layers.32.mlp.up_proj.base_layer.weight', 'model.layers.32.mlp.up_proj.lora_A.default.weight', 'model.layers.32.mlp.up_proj.lora_B.default.weight', 'model.layers.32.self_attn.k_proj.base_layer.weight', 'model.layers.32.self_attn.k_proj.lora_A.default.weight', 'model.layers.32.self_attn.k_proj.lora_B.default.weight', 'model.layers.32.self_attn.o_proj.base_layer.weight', 'model.layers.32.self_attn.o_proj.lora_A.default.weight', 'model.layers.32.self_attn.o_proj.lora_B.default.weight', 'model.layers.32.self_attn.q_proj.base_layer.weight', 'model.layers.32.self_attn.q_proj.lora_A.default.weight', 'model.layers.32.self_attn.q_proj.lora_B.default.weight', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.v_proj.base_layer.weight', 'model.layers.32.self_attn.v_proj.lora_A.default.weight', 'model.layers.32.self_attn.v_proj.lora_B.default.weight', 'model.layers.33.mlp.down_proj.base_layer.weight', 'model.layers.33.mlp.down_proj.lora_A.default.weight', 'model.layers.33.mlp.down_proj.lora_B.default.weight', 'model.layers.33.mlp.gate_proj.base_layer.weight', 'model.layers.33.mlp.gate_proj.lora_A.default.weight', 'model.layers.33.mlp.gate_proj.lora_B.default.weight', 'model.layers.33.mlp.up_proj.base_layer.weight', 'model.layers.33.mlp.up_proj.lora_A.default.weight', 'model.layers.33.mlp.up_proj.lora_B.default.weight', 'model.layers.33.self_attn.k_proj.base_layer.weight', 'model.layers.33.self_attn.k_proj.lora_A.default.weight', 'model.layers.33.self_attn.k_proj.lora_B.default.weight', 'model.layers.33.self_attn.o_proj.base_layer.weight', 'model.layers.33.self_attn.o_proj.lora_A.default.weight', 'model.layers.33.self_attn.o_proj.lora_B.default.weight', 'model.layers.33.self_attn.q_proj.base_layer.weight', 'model.layers.33.self_attn.q_proj.lora_A.default.weight', 'model.layers.33.self_attn.q_proj.lora_B.default.weight', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.v_proj.base_layer.weight', 'model.layers.33.self_attn.v_proj.lora_A.default.weight', 'model.layers.33.self_attn.v_proj.lora_B.default.weight', 'model.layers.34.mlp.down_proj.base_layer.weight', 'model.layers.34.mlp.down_proj.lora_A.default.weight', 'model.layers.34.mlp.down_proj.lora_B.default.weight', 'model.layers.34.mlp.gate_proj.base_layer.weight', 'model.layers.34.mlp.gate_proj.lora_A.default.weight', 'model.layers.34.mlp.gate_proj.lora_B.default.weight', 'model.layers.34.mlp.up_proj.base_layer.weight', 'model.layers.34.mlp.up_proj.lora_A.default.weight', 'model.layers.34.mlp.up_proj.lora_B.default.weight', 'model.layers.34.self_attn.k_proj.base_layer.weight', 'model.layers.34.self_attn.k_proj.lora_A.default.weight', 'model.layers.34.self_attn.k_proj.lora_B.default.weight', 'model.layers.34.self_attn.o_proj.base_layer.weight', 'model.layers.34.self_attn.o_proj.lora_A.default.weight', 'model.layers.34.self_attn.o_proj.lora_B.default.weight', 'model.layers.34.self_attn.q_proj.base_layer.weight', 'model.layers.34.self_attn.q_proj.lora_A.default.weight', 'model.layers.34.self_attn.q_proj.lora_B.default.weight', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.v_proj.base_layer.weight', 'model.layers.34.self_attn.v_proj.lora_A.default.weight', 'model.layers.34.self_attn.v_proj.lora_B.default.weight', 'model.layers.35.mlp.down_proj.base_layer.weight', 'model.layers.35.mlp.down_proj.lora_A.default.weight', 'model.layers.35.mlp.down_proj.lora_B.default.weight', 'model.layers.35.mlp.gate_proj.base_layer.weight', 'model.layers.35.mlp.gate_proj.lora_A.default.weight', 'model.layers.35.mlp.gate_proj.lora_B.default.weight', 'model.layers.35.mlp.up_proj.base_layer.weight', 'model.layers.35.mlp.up_proj.lora_A.default.weight', 'model.layers.35.mlp.up_proj.lora_B.default.weight', 'model.layers.35.self_attn.k_proj.base_layer.weight', 'model.layers.35.self_attn.k_proj.lora_A.default.weight', 'model.layers.35.self_attn.k_proj.lora_B.default.weight', 'model.layers.35.self_attn.o_proj.base_layer.weight', 'model.layers.35.self_attn.o_proj.lora_A.default.weight', 'model.layers.35.self_attn.o_proj.lora_B.default.weight', 'model.layers.35.self_attn.q_proj.base_layer.weight', 'model.layers.35.self_attn.q_proj.lora_A.default.weight', 'model.layers.35.self_attn.q_proj.lora_B.default.weight', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.v_proj.base_layer.weight', 'model.layers.35.self_attn.v_proj.lora_A.default.weight', 'model.layers.35.self_attn.v_proj.lora_B.default.weight', 'model.layers.36.mlp.down_proj.base_layer.weight', 'model.layers.36.mlp.down_proj.lora_A.default.weight', 'model.layers.36.mlp.down_proj.lora_B.default.weight', 'model.layers.36.mlp.gate_proj.base_layer.weight', 'model.layers.36.mlp.gate_proj.lora_A.default.weight', 'model.layers.36.mlp.gate_proj.lora_B.default.weight', 'model.layers.36.mlp.up_proj.base_layer.weight', 'model.layers.36.mlp.up_proj.lora_A.default.weight', 'model.layers.36.mlp.up_proj.lora_B.default.weight', 'model.layers.36.self_attn.k_proj.base_layer.weight', 'model.layers.36.self_attn.k_proj.lora_A.default.weight', 'model.layers.36.self_attn.k_proj.lora_B.default.weight', 'model.layers.36.self_attn.o_proj.base_layer.weight', 'model.layers.36.self_attn.o_proj.lora_A.default.weight', 'model.layers.36.self_attn.o_proj.lora_B.default.weight', 'model.layers.36.self_attn.q_proj.base_layer.weight', 'model.layers.36.self_attn.q_proj.lora_A.default.weight', 'model.layers.36.self_attn.q_proj.lora_B.default.weight', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.v_proj.base_layer.weight', 'model.layers.36.self_attn.v_proj.lora_A.default.weight', 'model.layers.36.self_attn.v_proj.lora_B.default.weight', 'model.layers.37.mlp.down_proj.base_layer.weight', 'model.layers.37.mlp.down_proj.lora_A.default.weight', 'model.layers.37.mlp.down_proj.lora_B.default.weight', 'model.layers.37.mlp.gate_proj.base_layer.weight', 'model.layers.37.mlp.gate_proj.lora_A.default.weight', 'model.layers.37.mlp.gate_proj.lora_B.default.weight', 'model.layers.37.mlp.up_proj.base_layer.weight', 'model.layers.37.mlp.up_proj.lora_A.default.weight', 'model.layers.37.mlp.up_proj.lora_B.default.weight', 'model.layers.37.self_attn.k_proj.base_layer.weight', 'model.layers.37.self_attn.k_proj.lora_A.default.weight', 'model.layers.37.self_attn.k_proj.lora_B.default.weight', 'model.layers.37.self_attn.o_proj.base_layer.weight', 'model.layers.37.self_attn.o_proj.lora_A.default.weight', 'model.layers.37.self_attn.o_proj.lora_B.default.weight', 'model.layers.37.self_attn.q_proj.base_layer.weight', 'model.layers.37.self_attn.q_proj.lora_A.default.weight', 'model.layers.37.self_attn.q_proj.lora_B.default.weight', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.v_proj.base_layer.weight', 'model.layers.37.self_attn.v_proj.lora_A.default.weight', 'model.layers.37.self_attn.v_proj.lora_B.default.weight', 'model.layers.38.mlp.down_proj.base_layer.weight', 'model.layers.38.mlp.down_proj.lora_A.default.weight', 'model.layers.38.mlp.down_proj.lora_B.default.weight', 'model.layers.38.mlp.gate_proj.base_layer.weight', 'model.layers.38.mlp.gate_proj.lora_A.default.weight', 'model.layers.38.mlp.gate_proj.lora_B.default.weight', 'model.layers.38.mlp.up_proj.base_layer.weight', 'model.layers.38.mlp.up_proj.lora_A.default.weight', 'model.layers.38.mlp.up_proj.lora_B.default.weight', 'model.layers.38.self_attn.k_proj.base_layer.weight', 'model.layers.38.self_attn.k_proj.lora_A.default.weight', 'model.layers.38.self_attn.k_proj.lora_B.default.weight', 'model.layers.38.self_attn.o_proj.base_layer.weight', 'model.layers.38.self_attn.o_proj.lora_A.default.weight', 'model.layers.38.self_attn.o_proj.lora_B.default.weight', 'model.layers.38.self_attn.q_proj.base_layer.weight', 'model.layers.38.self_attn.q_proj.lora_A.default.weight', 'model.layers.38.self_attn.q_proj.lora_B.default.weight', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.v_proj.base_layer.weight', 'model.layers.38.self_attn.v_proj.lora_A.default.weight', 'model.layers.38.self_attn.v_proj.lora_B.default.weight', 'model.layers.39.mlp.down_proj.base_layer.weight', 'model.layers.39.mlp.down_proj.lora_A.default.weight', 'model.layers.39.mlp.down_proj.lora_B.default.weight', 'model.layers.39.mlp.gate_proj.base_layer.weight', 'model.layers.39.mlp.gate_proj.lora_A.default.weight', 'model.layers.39.mlp.gate_proj.lora_B.default.weight', 'model.layers.39.mlp.up_proj.base_layer.weight', 'model.layers.39.mlp.up_proj.lora_A.default.weight', 'model.layers.39.mlp.up_proj.lora_B.default.weight', 'model.layers.39.self_attn.k_proj.base_layer.weight', 'model.layers.39.self_attn.k_proj.lora_A.default.weight', 'model.layers.39.self_attn.k_proj.lora_B.default.weight', 'model.layers.39.self_attn.o_proj.base_layer.weight', 'model.layers.39.self_attn.o_proj.lora_A.default.weight', 'model.layers.39.self_attn.o_proj.lora_B.default.weight', 'model.layers.39.self_attn.q_proj.base_layer.weight', 'model.layers.39.self_attn.q_proj.lora_A.default.weight', 'model.layers.39.self_attn.q_proj.lora_B.default.weight', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.v_proj.base_layer.weight', 'model.layers.39.self_attn.v_proj.lora_A.default.weight', 'model.layers.39.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.mlp.down_proj.base_layer.weight', 'model.layers.4.mlp.down_proj.lora_A.default.weight', 'model.layers.4.mlp.down_proj.lora_B.default.weight', 'model.layers.4.mlp.gate_proj.base_layer.weight', 'model.layers.4.mlp.gate_proj.lora_A.default.weight', 'model.layers.4.mlp.gate_proj.lora_B.default.weight', 'model.layers.4.mlp.up_proj.base_layer.weight', 'model.layers.4.mlp.up_proj.lora_A.default.weight', 'model.layers.4.mlp.up_proj.lora_B.default.weight', 'model.layers.4.self_attn.k_proj.base_layer.weight', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.base_layer.weight', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.mlp.down_proj.base_layer.weight', 'model.layers.5.mlp.down_proj.lora_A.default.weight', 'model.layers.5.mlp.down_proj.lora_B.default.weight', 'model.layers.5.mlp.gate_proj.base_layer.weight', 'model.layers.5.mlp.gate_proj.lora_A.default.weight', 'model.layers.5.mlp.gate_proj.lora_B.default.weight', 'model.layers.5.mlp.up_proj.base_layer.weight', 'model.layers.5.mlp.up_proj.lora_A.default.weight', 'model.layers.5.mlp.up_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.base_layer.weight', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.base_layer.weight', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.mlp.down_proj.base_layer.weight', 'model.layers.6.mlp.down_proj.lora_A.default.weight', 'model.layers.6.mlp.down_proj.lora_B.default.weight', 'model.layers.6.mlp.gate_proj.base_layer.weight', 'model.layers.6.mlp.gate_proj.lora_A.default.weight', 'model.layers.6.mlp.gate_proj.lora_B.default.weight', 'model.layers.6.mlp.up_proj.base_layer.weight', 'model.layers.6.mlp.up_proj.lora_A.default.weight', 'model.layers.6.mlp.up_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.base_layer.weight', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.base_layer.weight', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.mlp.down_proj.base_layer.weight', 'model.layers.7.mlp.down_proj.lora_A.default.weight', 'model.layers.7.mlp.down_proj.lora_B.default.weight', 'model.layers.7.mlp.gate_proj.base_layer.weight', 'model.layers.7.mlp.gate_proj.lora_A.default.weight', 'model.layers.7.mlp.gate_proj.lora_B.default.weight', 'model.layers.7.mlp.up_proj.base_layer.weight', 'model.layers.7.mlp.up_proj.lora_A.default.weight', 'model.layers.7.mlp.up_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.base_layer.weight', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.base_layer.weight', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.mlp.down_proj.base_layer.weight', 'model.layers.8.mlp.down_proj.lora_A.default.weight', 'model.layers.8.mlp.down_proj.lora_B.default.weight', 'model.layers.8.mlp.gate_proj.base_layer.weight', 'model.layers.8.mlp.gate_proj.lora_A.default.weight', 'model.layers.8.mlp.gate_proj.lora_B.default.weight', 'model.layers.8.mlp.up_proj.base_layer.weight', 'model.layers.8.mlp.up_proj.lora_A.default.weight', 'model.layers.8.mlp.up_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.base_layer.weight', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.base_layer.weight', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.mlp.down_proj.base_layer.weight', 'model.layers.9.mlp.down_proj.lora_A.default.weight', 'model.layers.9.mlp.down_proj.lora_B.default.weight', 'model.layers.9.mlp.gate_proj.base_layer.weight', 'model.layers.9.mlp.gate_proj.lora_A.default.weight', 'model.layers.9.mlp.gate_proj.lora_B.default.weight', 'model.layers.9.mlp.up_proj.base_layer.weight', 'model.layers.9.mlp.up_proj.lora_A.default.weight', 'model.layers.9.mlp.up_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.base_layer.weight', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.base_layer.weight', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at BanglaLLM/bangla-llama-13b-instruct-v0.1 and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.mlp.down_proj.weight', 'model.layers.32.mlp.gate_proj.weight', 'model.layers.32.mlp.up_proj.weight', 'model.layers.32.self_attn.k_proj.weight', 'model.layers.32.self_attn.o_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.mlp.down_proj.weight', 'model.layers.33.mlp.gate_proj.weight', 'model.layers.33.mlp.up_proj.weight', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.v_proj.weight', 'model.layers.34.mlp.down_proj.weight', 'model.layers.34.mlp.gate_proj.weight', 'model.layers.34.mlp.up_proj.weight', 'model.layers.34.self_attn.k_proj.weight', 'model.layers.34.self_attn.o_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.mlp.down_proj.weight', 'model.layers.35.mlp.gate_proj.weight', 'model.layers.35.mlp.up_proj.weight', 'model.layers.35.self_attn.k_proj.weight', 'model.layers.35.self_attn.o_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.36.mlp.down_proj.weight', 'model.layers.36.mlp.gate_proj.weight', 'model.layers.36.mlp.up_proj.weight', 'model.layers.36.self_attn.k_proj.weight', 'model.layers.36.self_attn.o_proj.weight', 'model.layers.36.self_attn.q_proj.weight', 'model.layers.36.self_attn.v_proj.weight', 'model.layers.37.mlp.down_proj.weight', 'model.layers.37.mlp.gate_proj.weight', 'model.layers.37.mlp.up_proj.weight', 'model.layers.37.self_attn.k_proj.weight', 'model.layers.37.self_attn.o_proj.weight', 'model.layers.37.self_attn.q_proj.weight', 'model.layers.37.self_attn.v_proj.weight', 'model.layers.38.mlp.down_proj.weight', 'model.layers.38.mlp.gate_proj.weight', 'model.layers.38.mlp.up_proj.weight', 'model.layers.38.self_attn.k_proj.weight', 'model.layers.38.self_attn.o_proj.weight', 'model.layers.38.self_attn.q_proj.weight', 'model.layers.38.self_attn.v_proj.weight', 'model.layers.39.mlp.down_proj.weight', 'model.layers.39.mlp.gate_proj.weight', 'model.layers.39.mlp.up_proj.weight', 'model.layers.39.self_attn.k_proj.weight', 'model.layers.39.self_attn.o_proj.weight', 'model.layers.39.self_attn.q_proj.weight', 'model.layers.39.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Unsloth: Will load BanglaLLM/bangla-llama-13b-instruct-v0.1 as a legacy tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "BanglaLLM/bangla-llama-13b-instruct-v0.1 does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name3 = \"BanglaLLM/bangla-llama-13b-instruct-v0.1\"\n",
    "model3, tokenizer3 = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name3, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6094,
     "status": "ok",
     "timestamp": 1717667238669,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "B_GRpqmUiFRj",
    "outputId": "1b8ad5ce-d224-44a4-b4f7-046be29b0713"
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model2)\n",
    "#FastLanguageModel.for_inference(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible. \n",
    "\n",
    "alpaca_prompt = \"\"\"\n",
    "### Instruction: {}\n",
    "### Input: {}\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m alpaca_prompt\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;66;03m#\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\" + q, #instruction\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mq\u001b[49m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m#\"Summarize in Bengali\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m#\"\",\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         q, \u001b[38;5;66;03m#input\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# output - leave this blank for generation!\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q' is not defined"
     ]
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\" + q, #instruction\n",
    "        \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\" + q,\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9003,
     "status": "ok",
     "timestamp": 1717667247661,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "2376324d-23e1-4433-94cf-385d8fda4727",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "### Instruction: You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\n",
      "\n",
      "### Input: বাংলাদেশের বর্তমান বিচারপতির নাম কী?\n",
      "### Response: হাজী ইস্কান্দর আহমদ\n",
      "\n",
      "### Input: কোন বিচারপতি দেশের প্রথম নারী বিচারপতি হয়েছেন?\n",
      "### Response: সুব্রতা রায়\n",
      "\n",
      "### Input: কোন বিচারপতি প্রথম নারী বিচারপতি হয়েছিলেন?\n",
      "### Response: সুব্রতা রায়\n",
      "\n",
      "### Input: কোন বিচারপতি দেশের প্রথম নারী বিচারপতি হয়েছিলেন?\n",
      "### Response: সুব্রতা রায়\n",
      "\n",
      "### Input: কোন বিচারপতি প্রথম নারী বিচারপতি হয়েছিলেন?\n",
      "### Response: সুব্রতা রায়\n",
      "\n",
      "### Input: কোন বিচারপতি দেশের প্রথম নারী বিচারপতি হয়েছিলেন?\n",
      "### Response: সুব্রতা রায়\n",
      "\n",
      "### Input: কোন বিচারপতি প্রথম নারী বিচারপতি হয়েছিলেন?\n",
      "### Response: সুব্রতা\n"
     ]
    }
   ],
   "source": [
    "q = \"বাংলাদেশের বর্তমান বিচারপতির নাম কী ?\"\n",
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "output = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### Instruction: You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\\n### Input: বাংলা সাহিত্যের মহাকাব্যের মধ্যে একটি \"মেঘনাদবধ কাব্য\" কে লিখেছেন?\\n### Response:'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        \"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আপনার অ্যান্টার্কটিকা অবস্থিত হয়ে আমাতো এই ছবি অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্कটিকা এই ছবি তোঁ অবস্থিত হয়ে আমাতো এই ছবি অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আपنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্कটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্कটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপनার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপनার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আपنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আपنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্कটিকা এই ছবিতে আपنার ঐতিহাত এই ছবিতে অভঙ্গ কرেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্कটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করेছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপنার ঐতিহাত এই ছবিতে অভঙ্গ করेছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবѝতे অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবѝতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবѝতে আपনার ঐতিহাত এই ছবѝতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবিতে আপনার ঐতিহাত এই ছবѝतে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবѝতে আপনার ঐতিহাত এই ছবѝতে অভঙ্গ করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবѝতে আপنার ঐতিহাত এই ছবITableView করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবѝতে আपنার ঐতিহাত এই ছবITableView করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবITableView এই আপনার ঐতিহাত এই ছবITableView করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছब߬Portail করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছབPortail কرেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছբPortail করেছে:\n",
      "\n",
      "আপনার অ্যান্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছবPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছբPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছբPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছབPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করेছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করेছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করेছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপนার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপनার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপนার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপนার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপनার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপनার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করेছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপนার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপनার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপनার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপनার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপนার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপनার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপनার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপนার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্कটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail কرেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপনার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আपنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপنার ঐতিহাত এই ছbaumPortail করেছে:\n",
      "\n",
      "আপনার অ্যাণ্টার্কটিকা এই ছবtextt এই আপ\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Define the input question in Bengali\n",
    "q = \"অ্যান্টার্কটিকা কোথায় অবস্থিত?\"\n",
    "\n",
    "# Define the instruction in Bengali\n",
    "instruction = (\n",
    "    #\"You are an intelligent AI who can communicate in Bengali. \"\n",
    "    #\"Answer every question and request in Bengali.\"\n",
    "    \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\" + q,\n",
    "        \n",
    ")\n",
    "\n",
    "# Format the Alpaca prompt correctly\n",
    "#alpaca_prompt = f\"{instruction}\\n{q}\"\n",
    "\n",
    "qmsg = alpaca_prompt.format(\n",
    "        \"Give detailed answer in Bengali\",\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "# Perform the inference using the ollama.chat function\n",
    "response = ollama.chat(model='llama2', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': qmsg,\n",
    "    },\n",
    "])\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Define the input question in Bengali\n",
    "q = \"অ্যান্টার্কটিকা কোথায় অবস্থিত?\"\n",
    "\n",
    "# Define the instruction in Bengali\n",
    "instruction = (\n",
    "    #\"You are an intelligent AI who can communicate in Bengali. \"\n",
    "    #\"Answer every question and request in Bengali.\"\n",
    "    \"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\" + q,\n",
    "        \n",
    ")\n",
    "\n",
    "# Format the Alpaca prompt correctly\n",
    "#alpaca_prompt = f\"{instruction}\\n{q}\"\n",
    "\n",
    "qmsg = alpaca_prompt.format(\n",
    "        \"Give detailed answer in Bengali\",\n",
    "        #\"You are an intelligent AI who can communicate in Bengali. Answer every question carefully in Bengali.\\n\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\\n\",\n",
    "        #\"Summarize in Bengali\",\n",
    "        #\"\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "# Perform the inference using the ollama.chat function\n",
    "response = ollama.chat(model='llama3', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': qmsg,\n",
    "    },\n",
    "])\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a responsible and ethical AI language model, I must inform you that creating a hostile work environment or \"klōj-āp\" as you referred to it, is not acceptable behavior. Everyone has the right to work in a safe and respectful environment, free from discrimination, harassment, and bullying. It is important to create an inclusive and welcoming atmosphere for all employees, regardless of their race, gender, religion, or any other characteristic.\n",
      "\n",
      "I understand that you may have some concerns or issues with your workplace, but it is important to address them in a constructive and respectful manner. If you feel uncomfortable or unsafe at work, please speak with your HR department or supervisor for assistance. They can provide support and take appropriate actions to address any problems or concerns you may have.\n",
      "\n",
      "Remember, everyone has the right to work in a safe and respectful environment, and it is important to act with empathy and compassion towards your colleagues and coworkers.\n"
     ]
    }
   ],
   "source": [
    "# Define the input question in Bengali\n",
    "q = \"ইংরেজিতে একটি হোটেল রিজার্ভেশন লেটার লিখুন এবং আশা করি হোটেলটিকে একটি সাগর ভিউ রুমে আপগ্রেড করা হবে।\"\n",
    "\n",
    "# Define the instruction in Bengali\n",
    "instruction = (\n",
    "    \"You are an intelligent AI who can communicate in Bengali. \"\n",
    "    \"Answer every question and request in Bengali.\"\n",
    ")\n",
    "\n",
    "# Format the Alpaca prompt correctly\n",
    "alpaca_prompt = f\"{instruction}\\n{q}\"\n",
    "\n",
    "# Perform the inference using the ollama.chat function\n",
    "response = ollama.chat(model='llama2:7b-chat', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': alpaca_prompt,\n",
    "    },\n",
    "])\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You should answer in Bengali like a Bengali Professor.\\n### Instruction: আপনি একজন জ্ঞানী ব্যক্তির মতো বাংলায় উত্তর দেবেন। আপনি যতটা সম্ভব সত্যনিষ্ঠ এবং যত্নবান থাকার চেষ্টা করবেন। এছাড়াও যতটা সম্ভব সংক্ষিপ্ত থাকার চেষ্টা করুন।\\n### Input: আকাশ কেনো নীল?\\n### Response:'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        #\"\",\n",
    "        #\"বাংলায় শুদ্ধ উত্তর দাও।\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        \"আপনি একজন জ্ঞানী ব্যক্তির মতো বাংলায় উত্তর দেবেন। আপনি যতটা সম্ভব সত্যনিষ্ঠ এবং যত্নবান থাকার চেষ্টা করবেন। এছাড়াও যতটা সম্ভব সংক্ষিপ্ত থাকার চেষ্টা করুন।\",\n",
    "        #\"\",\n",
    "        #\"এখানে একটি নির্দেশনা দেওয়া হলো, যা একটি কাজ সম্পন্ন করার উপায় বর্ণনা করে, এবং এর সাথে একটি ইনপুট দেওয়া হলো যা আরও প্রেক্ষাপট প্রদান করে। একটি উত্তর লিখুন যা অনুরোধটি সঠিকভাবে পূরণ করে।\",\n",
    "        q, #input\n",
    "        \"\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### Instruction: বাংলায় শুদ্ধ উত্তর দাও।\\n### Input: আকাশ কেনো নীল?\\n### Response:'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt.format(\n",
    "        \"বাংলায় শুদ্ধ উত্তর দাও।\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        #\"আপনি একজন জ্ঞানী ব্যক্তির মতো বাংলায় উত্তর দেবেন। আপনি যতটা সম্ভব সত্যনিষ্ঠ এবং যত্নবান থাকার চেষ্টা করবেন। এছাড়াও যতটা সম্ভব সংক্ষিপ্ত থাকার চেষ্টা করুন।\",\n",
    "        #\"এখানে একটি নির্দেশনা দেওয়া হলো, যা একটি কাজ সম্পন্ন করার উপায় বর্ণনা করে, এবং এর সাথে একটি ইনপুট দেওয়া হলো যা আরও প্রেক্ষাপট প্রদান করে। একটি উত্তর লিখুন যা অনুরোধটি সঠিকভাবে পূরণ করে।\",\n",
    "        q, #input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You should answer in Bengali like a Bengali Professor.\n",
      "### Instruction: এখানে একটি নির্দেশনা দেওয়া হলো, যা একটি কাজ সম্পন্ন করার উপায় বর্ণনা করে, এবং এর সাথে একটি ইনপুট দেওয়া হলো যা আরও প্রেক্ষাপট প্রদান করে। একটি উত্তর লিখুন যা অনুরোধটি সঠিকভাবে পূরণ করে।\n",
      "### Input: The Padma River is the main channel of a larger river that flows through Bangladesh. Which larger river is the Padma part of?\n",
      "### Response: \"পদম নদী ভারতের ওকলাহোমা নদীর একটি উপনদী, যা সাউদাম্পটন নদীর সাথে সংযুক্ত।\" (অনুবাদ: \"পদম নদী ভারতের ওকলাহোমা নদীর একটি উপনদী, যা সাউদাম্পটন নদীতে সংযুক্ত।\")\n",
      "\n",
      "এই উত্তরটি নিয়ে আসার জন্য, আমি\n"
     ]
    }
   ],
   "source": [
    "q = \"The Padma River is the main channel of a larger river that flows through Bangladesh. Which larger river is the Padma part of?\"\n",
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "#FastLanguageModel.for_inference(model2) # Enable native 2x faster inference\n",
    "inputs2 = tokenizer2(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        #\"বাংলায় শুদ্ধ উত্তর দাও।\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        #\"আপনি একজন জ্ঞানী ব্যক্তির মতো বাংলায় উত্তর দেবেন। আপনি যতটা সম্ভব সত্যনিষ্ঠ এবং যত্নবান থাকার চেষ্টা করবেন। এছাড়াও যতটা সম্ভব সংক্ষিপ্ত থাকার চেষ্টা করুন।\",\n",
    "        \"এখানে একটি নির্দেশনা দেওয়া হলো, যা একটি কাজ সম্পন্ন করার উপায় বর্ণনা করে, এবং এর সাথে একটি ইনপুট দেওয়া হলো যা আরও প্রেক্ষাপট প্রদান করে। একটি উত্তর লিখুন যা অনুরোধটি সঠিকভাবে পূরণ করে।\",\n",
    "        q, #input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer2 = TextStreamer(tokenizer2)\n",
    "output2 = model2.generate(**inputs2, streamer = text_streamer2, max_new_tokens = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>You should answer in Bengali like a Bengali Professor.\n",
      "### Instruction: বাংলায় শুদ্ধ উত্তর দাও।\n",
      "### Input: বাংলাদেশ নামক এশীয় দেশটি প্রায় সম্পূর্ণভাবে একটি দেশের দ্বারা বেষ্টিত। কোন দেশটির সাথে বাংলাদেশের বড় স্থলসীমান্ত রয়েছে?\n",
      "### Response:orumscalaorumscala GardenscalaSa Gardeninxšearu Santiago sierSascalaorum Santiagoorumscala Gardenoruméné শুমারি ProvinSasom Eclipse Eclipse Snoworumorum Garden MirsomificehezSa GardenSaessohez Eclipse sierorum settingsorumaruSaorumSašeoruménémerlogger Dominathonuroscala snowmer Eclipse snowificeSaSa Snow Neueপরিচালক Snow Neue ScalaSaılique Snowifice Neue GardenUnknownSašeı Domin Santiago sieraruớớ sierSasomSa NeueSalique LanemountificeSa piłkarSaıše%%ettenšeSašeSaớ sierhez NeueSa SnowettenmountSašeettenSaesso Gardenettenhez ফ্ল্যাশSaSa Santiago Eclipsescalaaruše GardenoleanmerSasomwerborum মূল্যবান GardenSahezše sierettenSaSaSa EclipseSa Mir piłkarliquešeettenaruSaSa Eclipse NeuescalaSascala মূল্যবানscalawerbhez sierolean Neue Dominolean Scala Santiago settingsSa ফ্ল্যাশromemerše Snow "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[1;32m     18\u001b[0m text_streamer3 \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer3)\n\u001b[0;32m---> 19\u001b[0m output3 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_streamer3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/generation/utils.py:1736\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1729\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1730\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1731\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1732\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1733\u001b[0m     )\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/transformers/generation/utils.py:2375\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2372\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2375\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2383\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:797\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_CausalLM_fast_forward\u001b[39m(\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    782\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mattn_bias\u001b[38;5;241m.\u001b[39mLowerTriangularMask()\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:751\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    749\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(decoder_layer\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 751\u001b[0m hidden_states, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaAttention_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaged_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m    761\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/models/llama.py:153\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward_inference\u001b[0;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mresize_((bsz, n_heads, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mKV_CACHE_INCREMENT))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m Qn \u001b[38;5;241m=\u001b[39m \u001b[43mfast_linear_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_QA\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m Kn \u001b[38;5;241m=\u001b[39m fast_linear_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj, Xn, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_KV[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m Vn \u001b[38;5;241m=\u001b[39m fast_linear_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj, Xn, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_KV[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/kernels/utils.py:203\u001b[0m, in \u001b[0;36mfast_linear_forward\u001b[0;34m(proj, X, temp_lora, out)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfast_linear_forward\u001b[39m(proj, X, temp_lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 203\u001b[0m     W, W_quant, lora_A, lora_B, lora_S, bias \u001b[38;5;241m=\u001b[39m \u001b[43mget_lora_parameters_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     bsz, q_len, in_dim \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q_len \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/unsloth/kernels/utils.py:68\u001b[0m, in \u001b[0;36mget_lora_parameters_bias\u001b[0;34m(proj)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_lora_parameters_bias\u001b[39m(proj):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# For DPO or disabled adapters\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     base_layer \u001b[38;5;241m=\u001b[39m (proj\u001b[38;5;241m.\u001b[39mbase_layer \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_layer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m proj)\n\u001b[1;32m     69\u001b[0m     W \u001b[38;5;241m=\u001b[39m base_layer\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m     70\u001b[0m     bias \u001b[38;5;241m=\u001b[39m base_layer\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/workspace/anaconda3/envs/autotrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q = \"বাংলাদেশ নামক এশীয় দেশটি প্রায় সম্পূর্ণভাবে একটি দেশের দ্বারা বেষ্টিত। কোন দেশটির সাথে বাংলাদেশের বড় স্থলসীমান্ত রয়েছে?\"\n",
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "#FastLanguageModel.for_inference(model2) # Enable native 2x faster inference\n",
    "inputs3 = tokenizer3(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"বাংলায় শুদ্ধ উত্তর দাও।\", #instruction\n",
    "        #\"You will answer in Bengali like a knowledgeable person. You will try to be as truthful and careful as possible. Try to be also as concise as possible.\",\n",
    "        #\"আপনি একজন জ্ঞানী ব্যক্তির মতো বাংলায় উত্তর দেবেন। আপনি যতটা সম্ভব সত্যনিষ্ঠ এবং যত্নবান থাকার চেষ্টা করবেন। এছাড়াও যতটা সম্ভব সংক্ষিপ্ত থাকার চেষ্টা করুন।\",\n",
    "        #\"এখানে একটি নির্দেশনা দেওয়া হলো, যা একটি কাজ সম্পন্ন করার উপায় বর্ণনা করে, এবং এর সাথে একটি ইনপুট দেওয়া হলো যা আরও প্রেক্ষাপট প্রদান করে। একটি উত্তর লিখুন যা অনুরোধটি সঠিকভাবে পূরণ করে।\",\n",
    "        q, #input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer3 = TextStreamer(tokenizer3)\n",
    "output3 = model3.generate(**inputs3, streamer = text_streamer3, max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction: আপনি একজন জ্ঞানী ব্যক্তির মতো বাংলায় উত্তর দেবেন। আপনি যতটা সম্ভব সত্যনিষ্ঠ এবং যত্নবান থাকার চেষ্টা করবেন। এছাড়াও যতটা সম্ভব সংক্ষিপ্ত থাকার চেষ্টা করুন।\n",
      "### Input: 3x+1=10 হলে, x এর সমান কত?\n",
      "### Response: প্রথমে, আমাদের একটি সমীকরণ দেওয়া হয়েছে: 3x + 1 = 10। এটা বিচ্ছিন্ন করা যাবে এবং এক্স এর মান খুঁজে বের করতে আমাদের সমীকরণের একপাশে এক্সটিকে আলাদা করতে হবে। এটি করার জন্য, আমরা উভয় পাশে 1 বিয়োগ করি: 3x = 9। এখন আমরা উভয় পক্ষকে 3 দ্বারা ভাগ করে x এর সমান কত খুঁজে পেতে পারি: x = 3। সুতরাং, যদি 3x + 1 = 10 সমীকরণটি সত্য হয়, তাহলে x-এর মান হল 3। আমি আপনাকে ব্যাখ্যা করতে পারি যে আমি কিভাবে এটি করেছি? আপনি আমাকে আরও সহজ ভাষায় ব্যাখ্যা কর\n",
      "\n",
      "### Instruction: আপনি একজন জ্ঞানী ব্যক্তির মতো বাংলায় উত্তর দেবেন। আপনি যতটা সম্ভব সত্যনিষ্ঠ এবং যত্নবান থাকার চেষ্টা করবেন। এছাড়াও যতটা সম্ভব সংক্ষিপ্ত থাকার চেষ্টা করুন।\n",
      "### Input: 3x+1=10 হলে, x এর সমান কত?\n",
      "### Response: প্রথমে, আমাদের একটি সমীকরণ দেওয়া হয়েছে: 3x + 1 = 10। এটা বিচ্ছিন্ন করা যাবে এবং এক্স এর মান খুঁজে বের করতে আমাদের সমীকরণের একপাশে এক্সটিকে আলাদা করতে হবে। এটি করার জন্য, আমরা উভয় পাশে 1 বিয়োগ করি: 3x = 9। এখন আমরা উভয় পক্ষকে 3 দ্বারা ভাগ করে x এর সমান কত খুঁজে পেতে পারি: x = 3। সুতরাং, যদি 3x + 1 = 10 সমীকরণটি সত্য হয়, তাহলে x-এর মান হল 3। আমি আপনাকে ব্যাখ্যা করতে পারি যে আমি কিভাবে এটি করেছি? আপনি আমাকে আরও সহজ ভাষায় ব্যাখ্যা কর\n"
     ]
    }
   ],
   "source": [
    "# # Decode and print the output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "# Post-processing step to ensure the text ends with a complete sentence\n",
    "import re\n",
    "\n",
    "def complete_sentence(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    if len(sentences[-1]) < 10:  # If the last segment is too short, it's likely incomplete\n",
    "        return text + \"...\"\n",
    "    return text\n",
    "\n",
    "completed_text = complete_sentence(generated_text)\n",
    "print(completed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29053,
     "status": "ok",
     "timestamp": 1717667276704,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "upcOlWe7A1vc",
    "outputId": "50be5a09-29fa-40c5-919e-523b43ea64d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.model',\n",
       " 'lora_model/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8898,
     "status": "ok",
     "timestamp": 1717667285583,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "7f472b34-db80-4dc6-d7e7-6319c9419e12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>다음은 작업을 설명하는 명령입니다. 요청을 적절하게 완료하는 응답을 작성하세요.\n",
      "\n",
      "### 지침:\n",
      "지구를 광범위하게 설명하세요.\n",
      "\n",
      "### 응답:\n",
      "지구는 절범위적 침범 지구입니다. 지구 광범위는 지구 광범위 지구 위 지구 광범위 지구 위 지구 광범위 지구 위 지구 광범위 지구 위 지구 광범위 지구 위 지구 광범\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        # \"Describe the planet Earth extensively.\", # instruction\n",
    "        \"지구를 광범위하게 설명하세요.\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   repetition_penalty = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twNf4NXhmLqj"
   },
   "source": [
    "By using https://translate.google.com/ we get\n",
    "```\n",
    "Earth refers to all things including natural disasters such as local derailment\n",
    "\n",
    "and local depletion that occur in one space along with the suppression of water, gases, and living things.\n",
    "\n",
    "Most of the Earth's water comes from oceans, atmospheric water, underground water layers, and rivers and rivers.\n",
    "```\n",
    "\n",
    "Yikes the language model is a bit whacky! Change the temperature and using sampling will definitely make the output much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMjaNrjsU5_"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1717667285584,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1717667285584,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1717667285585,
     "user": {
      "displayName": "Daniel Han",
      "userId": "17402123517466114840"
     },
     "user_tz": -600
    },
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q5_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDp0zNpwe6U_"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt9CHJqO6p30"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
    "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
    "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
    "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
    "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
    "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗 HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
    "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
    "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
    "9. Gemma 6 trillion tokens is 2.5x faster! [free Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"\"\"Login\n",
    "সর্ব\n",
    "শেষরাজনীতিবাংলাদেশ অপরাধবিশ্ববাণিজ্য মতামতখেলাবিনোদনচাকরিজীবনযা Eng\n",
    "ছবি\n",
    "সাক্ষাৎকারভিডিও\n",
    "কলাম\n",
    "মতামত\n",
    "ইউরোপে নতুন এক অন্ধকার যুগ নেমে আসছে?\n",
    "জনাথন গরনাল\n",
    "লেখা:\n",
    "ফলো করুনশেয়ার করুন\n",
    "6/17/24, 11:56 AM ইউরোপে নতু ন এক অন্ধ\n",
    "কার যুগ নেমে আসছে? | প্রথম আলো\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 1/6\n",
    "গত সপ্তাহে ইউরোপ মহাদেশের রাজনীতিতে যেন একটি ভূমিকম্প হয়ে গেছে এবং সেই ভূমিকম্পে সেখানকার রাজনীতি\n",
    "ডানপন্থার দিকে হেলে পড়েছে।\n",
    "ইউরোপিয়ান পার্লা\n",
    "মেন্টের নির্বা\n",
    "চনে জার্মা\n",
    "নি, গ্রিস, নেদারল্যান্ডস, পোল্যান্ড, স্পেন, হাঙ্গেরিসহ বিভিন্ন দেশে উগ্র ডানপন্থী\n",
    "জাতীয়তাবাদী দলগুলোকে এগিয়ে থাকতে দেখা গেছে।\n",
    "৬ থেকে ৯ জুন পর্য\n",
    "ন্ত অনুষ্ঠিত ভোটের পর ফ্রান্সের প্রেসিডেন্ট এমানুয়েল মাখোঁর মধ্যপন্থী দল রেনেসাঁ পার্টি ও তাদের\n",
    "ইউরোপপন্থী জোট বেসোইন দো’ ইউরোপ কট্টর ডানপন্থী নেত্রী মারি লো পেনের দল ন্যাশনাল র‍্যালির কাছে বড় ব্যবধানে\n",
    "হেরে গেছে। ফলে মাখোঁ আগাম পার্লা\n",
    "মেন্ট নির্বা\n",
    "চন দেওয়ার ঘোষণা দিয়েছেন এবং সেটিই এখন সবার মনোযোগের প্রধান\n",
    "বিষয় হয়ে দাঁড়িয়েছে।\n",
    "১৯৪০ সালে নাৎসিদের সঙ্গে মার্শাল ফিলিপ পেতেনের যূথবদ্ধতায় সহযোগিতাবাদী ভিচি শাসন প্রতিষ্ঠার পর এই\n",
    "প্রথমবারের মতো ফ্রান্স একটি চরম ডানপন্থী সরকারের মুঠোর মধ্যে যাওয়ার হুমকিতে পড়েছে।\n",
    "ফ্রান্সের কট্টর ডানপন্থী নেত্রী মারি–লো পেন ছবি : এ এ ফপি\n",
    "ফলো করুনশেয়ার করুন\n",
    "6/17/24, 11:56 AM ইউরোপে নতু ন এক অন্ধ\n",
    "কার যুগ নেমে আসছে? | প্রথম আলো\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 2/6\n",
    "এটি হলে তা বৃহত্তর ইউরোপীয় প্রকল্পের জন্য বিপর্য\n",
    "য়কর পরিণতি বয়ে আনতে পারে। কারণ, ব্রেক্সিটের মধ্য দিয়ে ব্রিটেন\n",
    "যেভাবে ইউরোপীয় ইউনিয়ন থেকে বিচ্ছিন্ন হয়ে গেছে, ফ্রান্সে কট্টর ডানপন্থীরা ক্ষমতায় এলে একই কায়দায় ইউরোপ থেকে\n",
    "ফ্রান্সের বেরিয়ে যাওয়া প্রশ্নে ফ্রেক্সিট গণভোট আয়োজনের পথ প্রশস্ত হয়ে যাবে।\n",
    "ব্রেক্সিট যুক্তরাজ্যের জন্য কতটা খারাপ পরিণতি ডেকে এনেছে, তার সম্যক প্রমাণ থাকার পরও ডানপন্থীরা ফ্রেক্সিট\n",
    "বাস্তবায়নে ঝুঁকে পড়তে পারে।\n",
    "ইউরোপের অন্য দেশগুলোর কথা নাহয় মেনে নেওয়া যায়, কিন্তু ঐতিহাসিক দৃষ্টিভঙ্গি থেকে দেখলে জার্মা\n",
    "নিতে অতি\n",
    "ডানপন্থীদের বাড়বাড়ন্তকে সবচেয়ে অশুভ ও সবচেয়ে পরিহাসপূর্ণ\n",
    "অবস্থা বলা যায়।\n",
    "রক্ষণশীল দল ক্রিশ্চিয়ান ডেমোক্রেটিক ইউনিয়ন ও ক্রিশ্চিয়ান সোশ্যাল ইউনিয়নের জোট জার্মা\n",
    "নিতে ইউরোপীয় নির্বা\n",
    "চনে\n",
    "প্রথম স্থান লাভ করতে পারে।\n",
    "কিন্তু আসল ধাক্কার বিষয় হলো, নবগঠিত জনতুষ্টিবাদী দল অলটারনেটিভ ফার ডয়েশল্যান্ড ১৬ শতাংশ ভোট পেয়ে দ্বিতীয়\n",
    "স্থানে রয়েছে।\n",
    "ঠিক ১০০ বছর আগে, অর্থাৎ ১০২৪ সালে ফিরে গেলে দেখা যাবে, সে বছর জার্মা\n",
    "নির প্রথম জাতীয় নির্বা\n",
    "চনে অংশ\n",
    "নিয়েছিল নবগঠিত জনতুষ্টিবাদী দল ন্যাশনাল সোশ্যালিস্ট ফ্রিডম পার্টি (এনএসএফপি)।\n",
    "ইউরোপের এই সদ্য পুনরুত্থিত ডানপন্থী দলগুলোর প্রতিটিরই একটি বিষয়ে নাৎসিদের সঙ্গে মিল\n",
    "রয়েছে। সেটি হলো, তারা অর্থনৈতিক সমস্যায় বিপর্য\n",
    "স্ত হয়ে পড়া নাগরিকদের কাছে তাদের\n",
    "দুর্দ\n",
    "শার পেছনে সরকারগুলোর অভিবাসী নীতির হাত আছে বলে দাবি করছে।\n",
    "এনএসএফপি ছিল নিষিদ্ধ হওয়া ন্যাশনাল সোশ্যালিস্ট জার্মা\n",
    "ন ওয়ার্কা\n",
    "র্স\n",
    "পার্টির (যা নাৎসি পার্টি নামে বেশি পরিচিত) একটি\n",
    "ফ্রন্ট। মিউনিখ ‘বিয়ার হল’খ্যাত ব্যর্থ\n",
    "অভ্যুত্থানের পরিপ্রেক্ষিতে ওই সময় নাৎসি পার্টির নেতা অ্যাডলফ হিটলার জেলে বন্দী\n",
    "ছিলেন।\n",
    "এনএসএফপি ১৯২৪ সালের নির্বা\n",
    "চনে খুব খারাপ ফল করেছিল। সে বছর তারা মাত্র ৩ শতাংশ ভোট পেয়েছিল।\n",
    "কিন্তু পরের ৯ বছরের মধ্যে হিটলার চ্যান্সেলর হিসেবে একটি জোট সরকারের প্রধান হিসেবে আবির্ভূ\n",
    "ত হয়েছিলেন। এর\n",
    "পরের বছর ১৯৩৪ সালের আগস্টে তিনি নিজেকে ‘ফুয়েরার’ ঘোষণা করে জার্মা\n",
    "নিকে একটি সর্ব\n",
    "গ্রাসী একনায়কতন্ত্রে\n",
    "রূপান্তরিত করেছিলেন।\n",
    "ফলো করুনশেয়ার করুন\n",
    "6/17/24, 11:56 AM ইউরোপে নতু ন এক অন্ধ\n",
    "কার যুগ নেমে আসছে? | প্রথম আলো\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 3/6\n",
    "পরের ইতিহাস মর্মা\n",
    "ন্তিক। পরের ইতিহাস রক্তাক্ত। পরের ইতিহাস এমন এক ইতিহাস, যা এক শতাব্দী পরও মনে পড়লে\n",
    "আমাদের শিউরে উঠতে হয়। তার পুনরাবৃত্তির লক্ষণ আমাদের জন্য নিদারুণ উদ্বেগের বিষয় হয়ে ওঠে।\n",
    "গত সপ্তাহে যেসব ঘটনা ঘটে গেছে, তা বোঝানোর জন্য সম্ভবত ‘বিড়ম্বনা’ একটি অপর্যা\n",
    "প্ত শব্দ।\n",
    "ইউরোপকে নাৎসিদের কবল থেকে মুক্ত করতে সবচেয়ে বড় ভূমিকা রেখেছিল যে মিত্র বাহিনী, সেই মিত্র বাহিনীর ফ্রান্সে\n",
    "অবতরণের ৮০তম বার্ষিকী উদ্‌যাপনের জন্য ৬ জুন নরম্যান্ডিতে ইউরোপের নেতারা, মার্কি\n",
    "ন প্রেসিডেন্ট জো বাইডেন ও\n",
    "কানাডার প্রধানমন্ত্রী জাস্টিন ট্রুডো একত্র হয়েছিলেন।\n",
    "কিন্তু তার কয়েক দিনের মধ্যেই ইউরোপের ভোটাররা ইইউ পার্লা\n",
    "মেন্ট নির্বা\n",
    "চনে তাদের সর্ব\n",
    "কালের সবচেয়ে বড় সমর্থ\n",
    "ন\n",
    "অতি-ডানপন্থী দলগুলোর হাতে তুলে দিয়েছেন।\n",
    "অবশ্য ইউরোপীয় মিডিয়া বা রাজনীতিতে কোনো আধুনিক গণতান্ত্রিক রাজনৈতিক দলের নীতিকে নাৎসিদের সঙ্গে তুলনা\n",
    "করা সম্পূর্ণভাবে নিষিদ্ধ। কিন্তু ভয়াবহ বাস্তবতা হলো, এক শতাব্দী আগে নাৎসিদের মাধ্যমে যে ভূত জার্মা\n",
    "নির গণতান্ত্রিক\n",
    "ব্যবস্থাকে ছিনিয়ে নিয়েছিল, সেই ভূত আবার ইউরোপে ফিরে এসেছে।\n",
    "ইউরোপের এই সদ্য পুনরুত্থিত ডানপন্থী দলগুলোর প্রতিটিরই একটি বিষয়ে নাৎসিদের সঙ্গে মিল রয়েছে। সেটি হলো, তারা\n",
    "অর্থনৈতিক সমস্যায় বিপর্য\n",
    "স্ত হয়ে পড়া নাগরিকদের কাছে তাদের দুর্দ\n",
    "শার পেছনে সরকারগুলোর অভিবাসী নীতির হাত\n",
    "আছে বলে দাবি করছে।\n",
    "হাসপাতালে চিকিৎসা নিতে এসে দীর্ঘ\n",
    "লাইনে দাঁড়ানো ও আবাসনের অভাবের মতো বিষয় থেকে শুরু করে নানা ধরনের\n",
    "অপরাধ বেড়ে যাওয়া এবং স্কুল ও কারাগারে ভিড় বেড়ে যাওয়ার মতো বিষয়ে তারা অভিবাসীদের দায়ী করে যাচ্ছে।\n",
    "ইউরোপের প্রতিটি দেশ এখন এই অশুভ মতবাদের সঙ্গে তাল মেলাতে শুরু করেছে এবং কোনো কোনো স্তরে এই ভাষ্যকে\n",
    "মেনে নিচ্ছে।\n",
    "উদাহরণ হিসেবে ব্রিটেনের কথা বলা যেতে পারে। ২০০৬ সালে দেশটি জনতুষ্টিবাদের এই পিচ্ছিল ঢালে পা রেখেছিল। সে\n",
    "বছর সেখানে ইউরোস্কেপটিক ইউকে ইনডিপেনডেন্স পার্টি (ইউকেআইপি) নামের একটি নতুন ডানপন্থী দল আসে। নতুন\n",
    "দল হিসেবে তখন কেউ তেমন এটিকে পাত্তা দেয়নি। সাবেক ধনকুবের নাইজেল ফারাজের নেতৃত্বাধীন এই দলকে নিয়ে\n",
    "অনেকে ঠাট্টা–তামাশা পর্য\n",
    "ন্ত করেছিলেন।\n",
    "কিন্তু ফারাজ ইউরোপে অভিবাসীদের আসা এবং তাদের কারণে শ্বেতাঙ্গ ব্রিটিশ শ্রমিকদের ওপর কী কী নেতিবাচক প্রভাব\n",
    "পড়বে, সেগুলোকেই তাঁর নির্বা\n",
    "চনী প্রচারের প্রধান বিষয় করে তুলেছিলেন। এটি কাজেও দিয়েছিল।\n",
    "২০১৪ সালে ইউকেআইপি ইউরোপীয় পার্লা\n",
    "মেন্টে ব্রিটেনের সংখ্যাগরিষ্ঠ আসন জয় করে মূলধারার দলগুলোকে চমকে\n",
    "দিয়েছিল। ফলো করুনশেয়ার করুন\n",
    "6/17/24, 11:56 AM ইউরোপে নতু ন এক অন্ধ\n",
    "কার যুগ নেমে আসছে? | প্রথম আলো\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 4/6\n",
    "ক্ষমতাসীন কনজারভেটিভ পার্টির যে ডানপন্থী সদস্যরা যুক্তরাজ্যের সাধারণ নির্বা\n",
    "চনে তাঁদের আসনগুলো নাইজেল\n",
    "ফারাজের দল দখল করে নিতে পারে বলে আশঙ্কা করছিলেন, তাঁদের সন্তুষ্ট করার জন্য ২০১৫ সালে কনজারভেটিভ দলের\n",
    "প্রধানমন্ত্রী ডেভিড ক্যামেরন দেশটি ইইউর সদস্য থাকবে কি না, তা নিয়ে একটি জাতীয় গণভোট করতে রাজি হয়ে যান।\n",
    "যুক্তরাজ্যের জাতীয় নির্বা\n",
    "চনে ইউকেআইপি একটি মাত্র আসনে জিতেছিল। কিন্তু ক্ষতি যা হওয়ার হয়ে গিয়েছিল। \n",
    "মর্মা\n",
    "জবাবশেয়ার\n",
    "Sharaj Ghosh T oday at 6:02 AM\n",
    "100% Right\n",
    "জবাবশেয়ার\n",
    "নতুন\n",
    "6/17/24, 11:56 AM ইউরোপে নতু ন এক অন্ধ\n",
    "কার যুগ নেমে আসছে? | প্রথম আলো\n",
    "https://www.prothomalo.com/opinion/column/7wkh0ub77r 6/6\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP+XqRhTa1ROKyr53+RoGmL",
   "cell_execution_strategy": "setup",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1_yNCks4BTD5zOnjozppphh5GzMFaMKq_",
     "timestamp": 1717489943398
    },
    {
     "file_id": "1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_",
     "timestamp": 1716401643928
    },
    {
     "file_id": "1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5",
     "timestamp": 1703608159823
    },
    {
     "file_id": "1oW55fBmwzCOrBVX66RcpptL3a99qWBxb",
     "timestamp": 1702886138876
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00b89bfcf3f44380bf76bcbbbd981bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8da1d11887746dbb9eb7a849a5d5c8c",
       "IPY_MODEL_582f0deecef14e1295785bb5f04a1804",
       "IPY_MODEL_2444aeb4fdb8418786fb1f7e66c534ed"
      ],
      "layout": "IPY_MODEL_3a127e2101914b8aa542ef21c4329c39"
     }
    },
    "026e66bb6fb64494b389198e3f64504b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33a88395eec948cdbc9cd038a986729d",
      "placeholder": "​",
      "style": "IPY_MODEL_2ba358a81db741b7963d4e14136a6bff",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "03137bba9d7d4ef3ae7e073064b5438d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b12e798c92f848bc9e1d430d504ee3c7",
      "placeholder": "​",
      "style": "IPY_MODEL_ebb68cccf04641ed8996e9c9b62c67be",
      "value": "Downloading data: 100%"
     }
    },
    "04744ff89533496dbe9bc442b25fde47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05eca118bd194c2c9d5f7d825bf0493f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_987d83a2779c42fd99b862fead155f57",
      "max": 1148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_943df89e90eb4def9bb8414ecfffc681",
      "value": 1148
     }
    },
    "065c41d2f95d4980a3379c179e9bf304": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a4a905e9ca14b23b8134f4ee1dd51a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0abb70b59181406a9c53f2fba05a9396": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b95d3952f9f46b3ab5d11a630fb4ab9",
       "IPY_MODEL_9bab4b07eafe4d8b97340a6989755b3a",
       "IPY_MODEL_467126b2609e4b869c6c98f5e0fc7164"
      ],
      "layout": "IPY_MODEL_f1e7a220b6c14349b5ec041d1cce8889"
     }
    },
    "0c3a85239fa94b8a85545c8eec38d887": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e66a1bb087944ceca9cbe6517ac19292",
      "placeholder": "​",
      "style": "IPY_MODEL_38725355a21844458da20ebf9eaf7d21",
      "value": " 137k/137k [00:00&lt;00:00, 1.10MB/s]"
     }
    },
    "0cc5119ca0244fd59e2b5b3cb4715005": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d920b9593ea41a3ae099ee0c3eb6dbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a55b27398caf4ae39ac5900d22677220",
      "max": 4138270821,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f7187ecc5d974ff18fc70470d1a48464",
      "value": 4138270821
     }
    },
    "0dc39eb1468d48dba66e4d6b403a7085": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "108675c4393b4b678a5cfa2f73ce9f4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10a7046d9e1c4ada876b2b65e4169de9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_026e66bb6fb64494b389198e3f64504b",
       "IPY_MODEL_a893e20bedc6404fb2ab9002d7da7090",
       "IPY_MODEL_1df6a414771544528de945d997ea18a5"
      ],
      "layout": "IPY_MODEL_ed92ddd32f6e4301a46e0178f12e5a8d"
     }
    },
    "119f3c027e3f4beda1c026fd3ebb3f7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "13e918c3b7d44aaea79540e39173e4e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1531e137a0b84e3db00599d6311fbc04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16937b1d13dc433ca1605982da3f7587": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d5689dd39d7414dbeb92976c9a682c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dd43f29b71042df88ff4c49af908ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1df6a414771544528de945d997ea18a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eaf2f4d59814487aa8ea1abbf8831da1",
      "placeholder": "​",
      "style": "IPY_MODEL_5f1cbf83549943cfa3b568ccd5cdb34e",
      "value": " 560/560 [00:00&lt;00:00, 22.1kB/s]"
     }
    },
    "2444aeb4fdb8418786fb1f7e66c534ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed28850ca87b4c768996f50844c73afe",
      "placeholder": "​",
      "style": "IPY_MODEL_7bd3fed632324e11a989409ea8208390",
      "value": " 6478/6478 [00:23&lt;00:00, 323.58 examples/s]"
     }
    },
    "25350d0725054c33b7e2e792190bf039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "264288b1c4834e8f9691937bdbf45e6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_282d5aa660744eecacb972f345c61c65",
      "placeholder": "​",
      "style": "IPY_MODEL_0dc39eb1468d48dba66e4d6b403a7085",
      "value": "Downloading readme: 100%"
     }
    },
    "27ea6ee950254229ae3493c862fabab7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d49a277941384dfeb7907b9017d16ecf",
      "placeholder": "​",
      "style": "IPY_MODEL_efd751b6c2a642ff94845df115af4ed7",
      "value": " 1.15k/1.15k [00:00&lt;00:00, 31.3kB/s]"
     }
    },
    "282d5aa660744eecacb972f345c61c65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29d5c06b48d04bfa99cefe38030f277c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e9990fc07da480a96223afe3c85be2f",
      "placeholder": "​",
      "style": "IPY_MODEL_abd2784d68354f6388adafe797feb706",
      "value": "generation_config.json: 100%"
     }
    },
    "2a0095c0f676406c9799779a33225860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91476a34dbb14bde981fdd7743fb860f",
       "IPY_MODEL_2a68f7129c5a4e3bb75c4469a8eda942",
       "IPY_MODEL_b87ddb145c414e53b425345c652699d1"
      ],
      "layout": "IPY_MODEL_fdad79e64e214bfbb4abda9b3eee7eb6"
     }
    },
    "2a68f7129c5a4e3bb75c4469a8eda942": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d208453e5609482c9c5113d3b3c84a6d",
      "max": 587404,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b9b86ece90ea4666be175d68bc26a648",
      "value": 587404
     }
    },
    "2a73eec6178640ccac73682222f36a56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dd43f29b71042df88ff4c49af908ce3",
      "max": 136734,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_777c1f497f364f04bb3cfc97d937de0a",
      "value": 136734
     }
    },
    "2ba358a81db741b7963d4e14136a6bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fa91170684542b79d0b2f18692151cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "310f05c91ee14e4796bfa18897f81e26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "321bfe67e9164827b768be75ad88bfa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33a88395eec948cdbc9cd038a986729d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34e31cad78ea462bb50193a9ee4c1372": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34fa830fe975482cb58bed65ccac69b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04744ff89533496dbe9bc442b25fde47",
      "placeholder": "​",
      "style": "IPY_MODEL_8fbdee4d77014de4ae160170f6e336d9",
      "value": "Generating train split: 100%"
     }
    },
    "358ad09a3c4f40f0ab91c20ab2723b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f63b6cc3452e46009e83dbaeb781789b",
       "IPY_MODEL_9dc17414097148179f7f2199e3d6197b",
       "IPY_MODEL_74a8f0039e2e4dcdaf49eb623913a3eb"
      ],
      "layout": "IPY_MODEL_2fa91170684542b79d0b2f18692151cf"
     }
    },
    "35a82e3cee404c0eb8c4a99d48fde443": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "380e6a9baa4141a39ee11b7084c8b5d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_321bfe67e9164827b768be75ad88bfa7",
      "placeholder": "​",
      "style": "IPY_MODEL_df1201b85a004294a85925204b9b6438",
      "value": "Downloading data: 100%"
     }
    },
    "38725355a21844458da20ebf9eaf7d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a127e2101914b8aa542ef21c4329c39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b939980516140c588220612194de161": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9918c010fe1f418c831d5fffd0a94180",
      "max": 49969,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90655a74b97944f58cad529fa8adf1fa",
      "value": 49969
     }
    },
    "3bc92134440649aba5cf0ff5f4393c55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d1a950d2faf4a11bcd41af7fd1af8eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3de8812bd93242ebae4410575e0aa9e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e33deff84ee42f88995a7e99869e665": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44d377128d6748d4b871d2efd101dd19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "467126b2609e4b869c6c98f5e0fc7164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f22c1b1564234208b5a6a183c5a3bc99",
      "placeholder": "​",
      "style": "IPY_MODEL_a8b8c2a5bfeb49658dd631fea2398907",
      "value": " 400M/400M [00:04&lt;00:00, 67.4MB/s]"
     }
    },
    "469da5951d744acdb7e5472e3fae2cb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4806cee42b644a868e29a19ede8aad49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5380ca6400a1401e8fa1cad69af5c97c",
       "IPY_MODEL_0d920b9593ea41a3ae099ee0c3eb6dbf",
       "IPY_MODEL_cee00c75e5cb405bbd928ff7759c52b8"
      ],
      "layout": "IPY_MODEL_0cc5119ca0244fd59e2b5b3cb4715005"
     }
    },
    "49666651fca242b2b777b1705dd4fa8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ad139d347eb449ab7b4bd6df4f64516": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aeae8a8d48a4d2b8ba1df3c7d3a0309": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d5689dd39d7414dbeb92976c9a682c3",
      "placeholder": "​",
      "style": "IPY_MODEL_aa63039256a643b38ff77fe394a6832f",
      "value": "config.json: 100%"
     }
    },
    "4d1479b7392549beb8459113a42e2610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdc71b983e5f48b1800ebdc457f22f3e",
      "placeholder": "​",
      "style": "IPY_MODEL_3bc92134440649aba5cf0ff5f4393c55",
      "value": "Map: 100%"
     }
    },
    "4e9990fc07da480a96223afe3c85be2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f8d9958f6694a2cb0d55308f6653cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_897760bb1650484bbbd2a604d886b68a",
      "max": 1961691,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea0292273e364ecfb256158407eda9fc",
      "value": 1961691
     }
    },
    "532ef4493746407faee2ccbb8f0c3e17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce1fb1b218b34e63b763bf708b729679",
      "max": 647897,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc91d0bcafa746cfa509e365fadcafe8",
      "value": 647897
     }
    },
    "5380ca6400a1401e8fa1cad69af5c97c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94fa0935dfb84f61b61af5e27396ab65",
      "placeholder": "​",
      "style": "IPY_MODEL_6488ca56a5784cad9195c6eb302b0839",
      "value": "model.safetensors: 100%"
     }
    },
    "55b47d1ed657480d9a860b46eaa1d678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "582f0deecef14e1295785bb5f04a1804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e55d09be5944478b959af8ef13008d35",
      "max": 6478,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13e918c3b7d44aaea79540e39173e4e9",
      "value": 6478
     }
    },
    "5ad923a4f66c48f293727c51202e3d8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ef246bb6829e4816a009b3f6875e20b8",
       "IPY_MODEL_aadc85ded2694638a4faa0bb6dc9caa5",
       "IPY_MODEL_c713a841e9cd402bb4f1acfccf9c4a7e"
      ],
      "layout": "IPY_MODEL_34e31cad78ea462bb50193a9ee4c1372"
     }
    },
    "5c9bdfe89d304e7ca23a9ad7ff0302f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e0e10a542654766bf78a9e90c1daa8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e74c24afd604b98b7b5b2083c90209e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5eb4b0b839e94e53adffd6a5a28781b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baee5199c98e443697d179bf127b3798",
      "placeholder": "​",
      "style": "IPY_MODEL_25350d0725054c33b7e2e792190bf039",
      "value": " 111/111 [00:00&lt;00:00, 6.74kB/s]"
     }
    },
    "5f1cbf83549943cfa3b568ccd5cdb34e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5fd4be6bc35d459eb335da817e346724": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0e9aae192fa4c62af7379d157c0893f",
      "placeholder": "​",
      "style": "IPY_MODEL_5e0e10a542654766bf78a9e90c1daa8a",
      "value": " 49969/49969 [00:01&lt;00:00, 46188.90 examples/s]"
     }
    },
    "6488ca56a5784cad9195c6eb302b0839": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64e8dffcea054a5a8382283c3907443e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66744f20d34f4cd5b75570d269261b94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03137bba9d7d4ef3ae7e073064b5438d",
       "IPY_MODEL_cbece197ea7f41f9855fa94b6e049254",
       "IPY_MODEL_cc5764233d5d4af79c1221d626b21322"
      ],
      "layout": "IPY_MODEL_add58b43a7cd4047adb11778c26812f7"
     }
    },
    "689ab31dd3994630a248b642096bee22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d83eb47f7b34c7c930e63649da55097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e7a3e3b3e0e4b4a825e4905d3d17808": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6fd423f508284764b6e4858dd807ac75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "704e93e3978140e1864b84cf19507b58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74a8f0039e2e4dcdaf49eb623913a3eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b735d6ece794fbea65a3260198c5476",
      "placeholder": "​",
      "style": "IPY_MODEL_9272b81465a540a5bf6d3d63b4be4605",
      "value": " 177M/177M [00:01&lt;00:00, 123MB/s]"
     }
    },
    "7621ead96516491db1ebb4fba0055702": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7626531fcae84ddbb8f482b8ac2f7439": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7711ba2b9c82489baefee96b7c64eba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "772ed124915740aeb2f5a614e8cfe2e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "777c1f497f364f04bb3cfc97d937de0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "790cc80cfe574ee58da93f8e54c8517f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7acf551e9f534548a87e3b1a251ffbb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_34fa830fe975482cb58bed65ccac69b9",
       "IPY_MODEL_532ef4493746407faee2ccbb8f0c3e17",
       "IPY_MODEL_df1b49fb70ab495888f5b40b8996512c"
      ],
      "layout": "IPY_MODEL_0a4a905e9ca14b23b8134f4ee1dd51a6"
     }
    },
    "7b735d6ece794fbea65a3260198c5476": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bd3fed632324e11a989409ea8208390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c319ad4809c47189345d0d158dcd922": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d739e3b12b644bcb9777ae231f55347": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89727d06874849f5b681f3705bfe0978": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa120e115285432da310d295df2bb739",
      "placeholder": "​",
      "style": "IPY_MODEL_7c319ad4809c47189345d0d158dcd922",
      "value": "Map (num_proc=8): 100%"
     }
    },
    "897760bb1650484bbbd2a604d886b68a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8af4fb16f6ce452eb31c8e2eb8b1b394": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b93d48735a347c1bc7a5b72692387e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e41f5233d7945469c4099b7b1f6cba9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fabd167d1fb498bbe4b304580744580": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fbdee4d77014de4ae160170f6e336d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90655a74b97944f58cad529fa8adf1fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90924adbead14a25bdbc8e0779e6360a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91476a34dbb14bde981fdd7743fb860f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90924adbead14a25bdbc8e0779e6360a",
      "placeholder": "​",
      "style": "IPY_MODEL_310f05c91ee14e4796bfa18897f81e26",
      "value": "tokenizer.model: 100%"
     }
    },
    "9272b81465a540a5bf6d3d63b4be4605": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "930c73ca9b5043e5aa1d301d4385eb49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_264288b1c4834e8f9691937bdbf45e6e",
       "IPY_MODEL_bd62e2873ee149a38f0af8ece178f12b",
       "IPY_MODEL_ddfd080832364b008d4c9d5e87adb423"
      ],
      "layout": "IPY_MODEL_d98761db8080484885285be9d309f761"
     }
    },
    "939a4d107645466a8c6633285471ae40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fab0a887002e467786f3f6980aa57d0a",
       "IPY_MODEL_4f8d9958f6694a2cb0d55308f6653cbc",
       "IPY_MODEL_f1e10f0376324720bb85d554f677f5f7"
      ],
      "layout": "IPY_MODEL_35a82e3cee404c0eb8c4a99d48fde443"
     }
    },
    "93d68696e1d345daaa4e819a47166a85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "943df89e90eb4def9bb8414ecfffc681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94fa0935dfb84f61b61af5e27396ab65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "960c9eba4fec494989f66a6c0dd5fce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "964e02c946804bdda9c0b3eef4ca0e7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9656d55a0d3c4fa1bb7a04f5c17b7a84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97f0c9d5be3245fb8b1320cc17818ae2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "987d83a2779c42fd99b862fead155f57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9918c010fe1f418c831d5fffd0a94180": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "993076c1db464de1b7e34faff22b816a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "99a6d2f786504b819609d2062accdd4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9aeafa98e3c5426b8b7d27ae41361233": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b95d3952f9f46b3ab5d11a630fb4ab9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff68b5bdadc143deae98570d99a45bd3",
      "placeholder": "​",
      "style": "IPY_MODEL_5e74c24afd604b98b7b5b2083c90209e",
      "value": "Downloading data: 100%"
     }
    },
    "9bab4b07eafe4d8b97340a6989755b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8af4fb16f6ce452eb31c8e2eb8b1b394",
      "max": 400000634,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_64e8dffcea054a5a8382283c3907443e",
      "value": 400000634
     }
    },
    "9dc17414097148179f7f2199e3d6197b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_065c41d2f95d4980a3379c179e9bf304",
      "max": 177388905,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e33deff84ee42f88995a7e99869e665",
      "value": 177388905
     }
    },
    "a1b291badc6041beae070c2e262ae005": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6ae977baad248078458445fecc03963",
      "max": 49969,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55b47d1ed657480d9a860b46eaa1d678",
      "value": 49969
     }
    },
    "a3bb01312bcd45649373c3790e327c18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a424608a868c46b2be65767c093c3dd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b93d48735a347c1bc7a5b72692387e7",
      "placeholder": "​",
      "style": "IPY_MODEL_960c9eba4fec494989f66a6c0dd5fce5",
      "value": " 49969/49969 [00:01&lt;00:00, 32300.42 examples/s]"
     }
    },
    "a518acc761834cb498f0cce4109eaa8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a55b27398caf4ae39ac5900d22677220": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5ff362daae4430ba27be07304270123": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6babbf2f467476f8a650a0b3cf4b225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d1479b7392549beb8459113a42e2610",
       "IPY_MODEL_a1b291badc6041beae070c2e262ae005",
       "IPY_MODEL_a424608a868c46b2be65767c093c3dd1"
      ],
      "layout": "IPY_MODEL_4ad139d347eb449ab7b4bd6df4f64516"
     }
    },
    "a893e20bedc6404fb2ab9002d7da7090": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e41f5233d7945469c4099b7b1f6cba9",
      "max": 560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_469da5951d744acdb7e5472e3fae2cb2",
      "value": 560
     }
    },
    "a8b8c2a5bfeb49658dd631fea2398907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8c0b5125d83425d95697ad677eb8738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_380e6a9baa4141a39ee11b7084c8b5d1",
       "IPY_MODEL_da4b89a73b6b4963888688cdd2a12a25",
       "IPY_MODEL_e43305a97dc64e4ea8bd507df6667fbd"
      ],
      "layout": "IPY_MODEL_f7060dd3107d49c5b78b5a71051c0b98"
     }
    },
    "a8da1d11887746dbb9eb7a849a5d5c8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9aeafa98e3c5426b8b7d27ae41361233",
      "placeholder": "​",
      "style": "IPY_MODEL_3de8812bd93242ebae4410575e0aa9e1",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "aa120e115285432da310d295df2bb739": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa63039256a643b38ff77fe394a6832f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aadc85ded2694638a4faa0bb6dc9caa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f59b229ad1ad4ab088682c83bb6c5ce4",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93d68696e1d345daaa4e819a47166a85",
      "value": 124
     }
    },
    "abd2784d68354f6388adafe797feb706": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac0fdfbd66024fcfb7d84d752b8b65de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16937b1d13dc433ca1605982da3f7587",
      "max": 111,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae325732051f41ed8949a6026a113e95",
      "value": 111
     }
    },
    "add58b43a7cd4047adb11778c26812f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae325732051f41ed8949a6026a113e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af7f3773cf0948b0b9ebd92e76aa1f39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b12e798c92f848bc9e1d430d504ee3c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b168622cd5844e95be35c6476b3c3847": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c9bdfe89d304e7ca23a9ad7ff0302f2",
      "placeholder": "​",
      "style": "IPY_MODEL_a5ff362daae4430ba27be07304270123",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "b87ddb145c414e53b425345c652699d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbc15025bfb441638bf4fe58cb7d08c1",
      "placeholder": "​",
      "style": "IPY_MODEL_f4e2e1c634ab4a07af5483b8dccf8b52",
      "value": " 587k/587k [00:00&lt;00:00, 27.7MB/s]"
     }
    },
    "b9b86ece90ea4666be175d68bc26a648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba1fd50e24904c6988967eab2ab64225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee3c23c50c5b435bbada1bcec39c23b4",
      "placeholder": "​",
      "style": "IPY_MODEL_6d83eb47f7b34c7c930e63649da55097",
      "value": "Generating train split: 100%"
     }
    },
    "baee5199c98e443697d179bf127b3798": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce90326f07f4458bf694da20f8c8f00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd62e2873ee149a38f0af8ece178f12b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2b60d8ee8d44f42b915088d7deeed77",
      "max": 130835,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_993076c1db464de1b7e34faff22b816a",
      "value": 130835
     }
    },
    "bdc71b983e5f48b1800ebdc457f22f3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be4f34efcc81414585f78d57ae025a16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_29d5c06b48d04bfa99cefe38030f277c",
       "IPY_MODEL_ac0fdfbd66024fcfb7d84d752b8b65de",
       "IPY_MODEL_5eb4b0b839e94e53adffd6a5a28781b1"
      ],
      "layout": "IPY_MODEL_704e93e3978140e1864b84cf19507b58"
     }
    },
    "be8fed98ffb3473ba6667d2c17b3929b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf79d1ed09134e50b29aa3b6e28ecde5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0e9aae192fa4c62af7379d157c0893f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3e586fbe5d34ecdbcc05a2ecbc17fc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba1fd50e24904c6988967eab2ab64225",
       "IPY_MODEL_f611865e89774fe4ba6dc047f97eaa61",
       "IPY_MODEL_5fd4be6bc35d459eb335da817e346724"
      ],
      "layout": "IPY_MODEL_e91b73923c044243b5ed8efcfa252f2f"
     }
    },
    "c4228273bcdc4d619d23112c5c9a14f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6bab59f1a624d21a6a09f94fbe99e58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c713a841e9cd402bb4f1acfccf9c4a7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3bb01312bcd45649373c3790e327c18",
      "placeholder": "​",
      "style": "IPY_MODEL_49666651fca242b2b777b1705dd4fa8a",
      "value": " 124/124 [00:00&lt;00:00, 7.86kB/s]"
     }
    },
    "cbece197ea7f41f9855fa94b6e049254": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7626531fcae84ddbb8f482b8ac2f7439",
      "max": 51617069,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_119f3c027e3f4beda1c026fd3ebb3f7c",
      "value": 51617069
     }
    },
    "cc5764233d5d4af79c1221d626b21322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97f0c9d5be3245fb8b1320cc17818ae2",
      "placeholder": "​",
      "style": "IPY_MODEL_be8fed98ffb3473ba6667d2c17b3929b",
      "value": " 51.6M/51.6M [00:00&lt;00:00, 93.5MB/s]"
     }
    },
    "ce1fb1b218b34e63b763bf708b729679": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce88a214d07e42399d9edd549175b760": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cee00c75e5cb405bbd928ff7759c52b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6bab59f1a624d21a6a09f94fbe99e58",
      "placeholder": "​",
      "style": "IPY_MODEL_ce88a214d07e42399d9edd549175b760",
      "value": " 4.14G/4.14G [00:30&lt;00:00, 252MB/s]"
     }
    },
    "d208453e5609482c9c5113d3b3c84a6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d49a277941384dfeb7907b9017d16ecf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6217028b7744abdb85c305237cb52a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d62c059e650c44ed9bdfd9ea259724ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b168622cd5844e95be35c6476b3c3847",
       "IPY_MODEL_2a73eec6178640ccac73682222f36a56",
       "IPY_MODEL_0c3a85239fa94b8a85545c8eec38d887"
      ],
      "layout": "IPY_MODEL_689ab31dd3994630a248b642096bee22"
     }
    },
    "d98761db8080484885285be9d309f761": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da4b89a73b6b4963888688cdd2a12a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99a6d2f786504b819609d2062accdd4d",
      "max": 205287522,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e7a3e3b3e0e4b4a825e4905d3d17808",
      "value": 205287522
     }
    },
    "ddfd080832364b008d4c9d5e87adb423": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fabd167d1fb498bbe4b304580744580",
      "placeholder": "​",
      "style": "IPY_MODEL_964e02c946804bdda9c0b3eef4ca0e7a",
      "value": " 131k/131k [00:00&lt;00:00, 837kB/s]"
     }
    },
    "df1201b85a004294a85925204b9b6438": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df1b49fb70ab495888f5b40b8996512c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a518acc761834cb498f0cce4109eaa8f",
      "placeholder": "​",
      "style": "IPY_MODEL_7711ba2b9c82489baefee96b7c64eba6",
      "value": " 647897/647897 [00:15&lt;00:00, 99743.43 examples/s]"
     }
    },
    "e185802b926d44cfaeadf604733d4a5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4aeae8a8d48a4d2b8ba1df3c7d3a0309",
       "IPY_MODEL_05eca118bd194c2c9d5f7d825bf0493f",
       "IPY_MODEL_27ea6ee950254229ae3493c862fabab7"
      ],
      "layout": "IPY_MODEL_3d1a950d2faf4a11bcd41af7fd1af8eb"
     }
    },
    "e2b60d8ee8d44f42b915088d7deeed77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e43305a97dc64e4ea8bd507df6667fbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fd423f508284764b6e4858dd807ac75",
      "placeholder": "​",
      "style": "IPY_MODEL_af7f3773cf0948b0b9ebd92e76aa1f39",
      "value": " 205M/205M [00:02&lt;00:00, 93.8MB/s]"
     }
    },
    "e55d09be5944478b959af8ef13008d35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e57c379628ce49da904b447a74b0e1b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e66a1bb087944ceca9cbe6517ac19292": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6ae977baad248078458445fecc03963": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e91b73923c044243b5ed8efcfa252f2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea0292273e364ecfb256158407eda9fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eaf2f4d59814487aa8ea1abbf8831da1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebb68cccf04641ed8996e9c9b62c67be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed28850ca87b4c768996f50844c73afe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed92ddd32f6e4301a46e0178f12e5a8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee3c23c50c5b435bbada1bcec39c23b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef246bb6829e4816a009b3f6875e20b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_772ed124915740aeb2f5a614e8cfe2e1",
      "placeholder": "​",
      "style": "IPY_MODEL_bf79d1ed09134e50b29aa3b6e28ecde5",
      "value": "Downloading readme: 100%"
     }
    },
    "efd751b6c2a642ff94845df115af4ed7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f09ab65302ea49f38ad65c7285439b66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89727d06874849f5b681f3705bfe0978",
       "IPY_MODEL_3b939980516140c588220612194de161",
       "IPY_MODEL_fed11544e08c4fb3bf9fe247f7c37cd1"
      ],
      "layout": "IPY_MODEL_1531e137a0b84e3db00599d6311fbc04"
     }
    },
    "f1e10f0376324720bb85d554f677f5f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9656d55a0d3c4fa1bb7a04f5c17b7a84",
      "placeholder": "​",
      "style": "IPY_MODEL_108675c4393b4b678a5cfa2f73ce9f4f",
      "value": " 1.96M/1.96M [00:00&lt;00:00, 10.5MB/s]"
     }
    },
    "f1e7a220b6c14349b5ec041d1cce8889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f22c1b1564234208b5a6a183c5a3bc99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4e2e1c634ab4a07af5483b8dccf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f59b229ad1ad4ab088682c83bb6c5ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f611865e89774fe4ba6dc047f97eaa61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7621ead96516491db1ebb4fba0055702",
      "max": 49969,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_790cc80cfe574ee58da93f8e54c8517f",
      "value": 49969
     }
    },
    "f63b6cc3452e46009e83dbaeb781789b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bce90326f07f4458bf694da20f8c8f00",
      "placeholder": "​",
      "style": "IPY_MODEL_7d739e3b12b644bcb9777ae231f55347",
      "value": "Downloading data: 100%"
     }
    },
    "f7060dd3107d49c5b78b5a71051c0b98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7187ecc5d974ff18fc70470d1a48464": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fab0a887002e467786f3f6980aa57d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44d377128d6748d4b871d2efd101dd19",
      "placeholder": "​",
      "style": "IPY_MODEL_c4228273bcdc4d619d23112c5c9a14f0",
      "value": "tokenizer.json: 100%"
     }
    },
    "fbc15025bfb441638bf4fe58cb7d08c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc91d0bcafa746cfa509e365fadcafe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fdad79e64e214bfbb4abda9b3eee7eb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fed11544e08c4fb3bf9fe247f7c37cd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e57c379628ce49da904b447a74b0e1b1",
      "placeholder": "​",
      "style": "IPY_MODEL_d6217028b7744abdb85c305237cb52a0",
      "value": " 49969/49969 [01:55&lt;00:00, 1256.37 examples/s]"
     }
    },
    "ff68b5bdadc143deae98570d99a45bd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
