{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brishtiteveja/bangla-llama/blob/main/BanglaLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkTeasOKJ4Ha",
        "outputId": "4192cdbf-0ec1-4760-e687-c89cb7176326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive for data projects\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUQARDlIKZuD",
        "outputId": "bad861e1-c4c5-4029-bd76-169cb8a225b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Projects\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My\\ Drive/Projects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPL11_AqKykY",
        "outputId": "60dbdbe5-9206-4b7c-bab7-89ab902c197e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Projects/bangla-llama\n"
          ]
        }
      ],
      "source": [
        "# git clone bangla llama here\n",
        "#git clone https://github.com/brishtiteveja/bangla-llama.git\n",
        "%cd bangla-llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZrfaYEiLf4z",
        "outputId": "945bd165-ce91-48e6-ba2a-9adcf3c5a7fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assets\tCITATION.cff  data  download_culturax.sh  LICENSE  README.md  requirements.txt\tscripts\n"
          ]
        }
      ],
      "source": [
        "!ls ../bangla-llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QXbRcMGfNkX5",
        "outputId": "52447f87-c63a-4e51-f54c-086ac9b268a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==2.0.0\n",
            "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/130.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: absl-py\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires absl-py<2.0.0,>=0.9, but you have absl-py 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-2.0.0\n",
            "Collecting accelerate==0.21.0\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n",
            "Collecting aiofiles==23.2.1\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: aiofiles\n",
            "Successfully installed aiofiles-23.2.1\n",
            "Collecting aiohttp==3.8.6\n",
            "  Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.6) (23.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.6) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.6) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.6) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.6) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.6) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.6) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.8.6) (3.6)\n",
            "Installing collected packages: aiohttp\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.9.1\n",
            "    Uninstalling aiohttp-3.9.1:\n",
            "      Successfully uninstalled aiohttp-3.9.1\n",
            "Successfully installed aiohttp-3.8.6\n",
            "Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from aiosignal==1.3.1) (1.4.1)\n",
            "Requirement already satisfied: albumentations==1.3.1 in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1) (4.9.0.80)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations==1.3.1) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations==1.3.1) (4.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (3.2.0)\n",
            "Collecting alembic==1.12.0\n",
            "  Downloading alembic-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from alembic==1.12.0) (2.0.24)\n",
            "Collecting Mako (from alembic==1.12.0)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic==1.12.0) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.3.0->alembic==1.12.0) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic==1.12.0) (2.1.3)\n",
            "Installing collected packages: Mako, alembic\n",
            "Successfully installed Mako-1.3.0 alembic-1.12.0\n",
            "Collecting altair==5.1.2\n",
            "  Downloading altair-5.1.2-py3-none-any.whl (516 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.2/516.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair==5.1.2) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair==5.1.2) (4.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair==5.1.2) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair==5.1.2) (23.2)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from altair==5.1.2) (1.5.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair==5.1.2) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from altair==5.1.2) (4.5.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==5.1.2) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==5.1.2) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==5.1.2) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==5.1.2) (0.16.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->altair==5.1.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->altair==5.1.2) (2023.3.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair==5.1.2) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.25->altair==5.1.2) (1.16.0)\n",
            "Installing collected packages: altair\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed altair-5.1.2\n",
            "Requirement already satisfied: anyio==3.7.1 in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio==3.7.1) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio==3.7.1) (1.2.0)\n",
            "Collecting arrow==1.3.0\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow==1.3.0) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow==1.3.0)\n",
            "  Downloading types_python_dateutil-2.8.19.20240106-py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow==1.3.0) (1.16.0)\n",
            "Installing collected packages: types-python-dateutil, arrow\n",
            "Successfully installed arrow-1.3.0 types-python-dateutil-2.8.19.20240106\n",
            "Requirement already satisfied: async-timeout==4.0.3 in /usr/local/lib/python3.10/dist-packages (4.0.3)\n",
            "Collecting attrs==23.1.0\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: attrs\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.2.0\n",
            "    Uninstalling attrs-23.2.0:\n",
            "      Successfully uninstalled attrs-23.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0\n",
            "Collecting autotrain-advanced==0.6.37\n",
            "  Downloading autotrain_advanced-0.6.37-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.4/130.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: albumentations==1.3.1 in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced==0.6.37) (1.3.1)\n",
            "Collecting codecarbon==2.2.3 (from autotrain-advanced==0.6.37)\n",
            "  Downloading codecarbon-2.2.3-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/174.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets[vision]~=2.14.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate==0.3.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipadic==1.0.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jiwer==3.0.2 (from autotrain-advanced==0.6.37)\n",
            "  Downloading jiwer-3.0.2-py3-none-any.whl (21 kB)\n",
            "Collecting joblib==1.3.1 (from autotrain-advanced==0.6.37)\n",
            "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru==0.7.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced==0.6.37) (1.5.3)\n",
            "Collecting optuna==3.3.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading optuna-3.3.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==10.0.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==4.23.4 (from autotrain-advanced==0.6.37)\n",
            "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.11 (from autotrain-advanced==0.6.37)\n",
            "  Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses==0.0.53 (from autotrain-advanced==0.6.37)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==1.3.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.99 (from autotrain-advanced==0.6.37)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.65.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug==2.3.6 (from autotrain-advanced==0.6.37)\n",
            "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xgboost==1.7.6 (from autotrain-advanced==0.6.37)\n",
            "  Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced==0.6.37) (0.20.2)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced==0.6.37) (2.31.0)\n",
            "Collecting gradio==3.41.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading gradio-3.41.0-py3-none-any.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.6.1 (from autotrain-advanced==0.6.37)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting invisible-watermark==0.2.0 (from autotrain-advanced==0.6.37)\n",
            "  Downloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==23.1 (from autotrain-advanced==0.6.37)\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced==0.6.37) (2.15.1)\n",
            "Collecting peft (from autotrain-advanced==0.6.37)\n",
            "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl (from autotrain-advanced==0.6.37)\n",
            "  Downloading trl-0.7.9-py3-none-any.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from autotrain-advanced==0.6.37)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced==0.6.37) (4.35.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced==0.6.37) (0.21.0)\n",
            "Collecting diffusers==0.21.4 (from autotrain-advanced==0.6.37)\n",
            "  Downloading diffusers-0.21.4-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from autotrain-advanced==0.6.37)\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced==0.6.37) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced==0.6.37) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced==0.6.37) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced==0.6.37) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced==0.6.37) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced==0.6.37) (4.9.0.80)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3->autotrain-advanced==0.6.37) (1.3.0)\n",
            "Collecting pynvml (from codecarbon==2.2.3->autotrain-advanced==0.6.37)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3->autotrain-advanced==0.6.37) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3->autotrain-advanced==0.6.37) (9.0.0)\n",
            "Collecting fuzzywuzzy (from codecarbon==2.2.3->autotrain-advanced==0.6.37)\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3->autotrain-advanced==0.6.37) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4->autotrain-advanced==0.6.37) (3.13.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4->autotrain-advanced==0.6.37) (7.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4->autotrain-advanced==0.6.37) (2023.6.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4->autotrain-advanced==0.6.37) (0.4.1)\n",
            "Collecting datasets>=2.0.0 (from evaluate==0.3.0->autotrain-advanced==0.6.37)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from evaluate==0.3.0->autotrain-advanced==0.6.37)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0->autotrain-advanced==0.6.37) (3.4.1)\n",
            "Collecting multiprocess (from evaluate==0.3.0->autotrain-advanced==0.6.37)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0->autotrain-advanced==0.6.37) (2023.6.0)\n",
            "Collecting responses<0.19 (from evaluate==0.3.0->autotrain-advanced==0.6.37)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0->autotrain-advanced==0.6.37) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0->autotrain-advanced==0.6.37) (5.1.2)\n",
            "Collecting fastapi (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.5.0 (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading gradio_client-0.5.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0->autotrain-advanced==0.6.37) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0->autotrain-advanced==0.6.37) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0->autotrain-advanced==0.6.37) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0->autotrain-advanced==0.6.37) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m510.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0->autotrain-advanced==0.6.37) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0->autotrain-advanced==0.6.37) (1.5.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0->autotrain-advanced==0.6.37) (4.8.0.76)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0->autotrain-advanced==0.6.37) (2.1.0+cu121)\n",
            "Collecting rapidfuzz==2.13.7 (from jiwer==3.0.2->autotrain-advanced==0.6.37)\n",
            "  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna==3.3.0->autotrain-advanced==0.6.37) (1.12.0)\n",
            "Collecting cmaes>=0.10.0 (from optuna==3.3.0->autotrain-advanced==0.6.37)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Collecting colorlog (from optuna==3.3.0->autotrain-advanced==0.6.37)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna==3.3.0->autotrain-advanced==0.6.37) (2.0.24)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->autotrain-advanced==0.6.37) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->autotrain-advanced==0.6.37) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->autotrain-advanced==0.6.37) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->autotrain-advanced==0.6.37) (2023.11.17)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.53->autotrain-advanced==0.6.37) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0->autotrain-advanced==0.6.37) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]~=2.14.0->autotrain-advanced==0.6.37) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets[vision]~=2.14.0->autotrain-advanced==0.6.37) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets[vision]~=2.14.0->autotrain-advanced==0.6.37) (3.8.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->autotrain-advanced==0.6.37) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->autotrain-advanced==0.6.37) (2023.3.post1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced==0.6.37) (2.0.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced==0.6.37) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced==0.6.37) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced==0.6.37) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced==0.6.37) (3.5.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced==0.6.37) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced==0.6.37) (0.7.2)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->autotrain-advanced==0.6.37) (0.15.0)\n",
            "Collecting tyro>=0.5.11 (from trl->autotrain-advanced==0.6.37)\n",
            "  Downloading tyro-0.6.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna==3.3.0->autotrain-advanced==0.6.37) (1.3.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.41.0->autotrain-advanced==0.6.37) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.41.0->autotrain-advanced==0.6.37) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]~=2.14.0->autotrain-advanced==0.6.37) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]~=2.14.0->autotrain-advanced==0.6.37) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]~=2.14.0->autotrain-advanced==0.6.37) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]~=2.14.0->autotrain-advanced==0.6.37) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]~=2.14.0->autotrain-advanced==0.6.37) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]~=2.14.0->autotrain-advanced==0.6.37) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->autotrain-advanced==0.6.37) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->autotrain-advanced==0.6.37) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->autotrain-advanced==0.6.37) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->autotrain-advanced==0.6.37) (1.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0->autotrain-advanced==0.6.37) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0->autotrain-advanced==0.6.37) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0->autotrain-advanced==0.6.37) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0->autotrain-advanced==0.6.37) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0->autotrain-advanced==0.6.37) (3.1.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->autotrain-advanced==0.6.37) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->autotrain-advanced==0.6.37) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->autotrain-advanced==0.6.37) (2023.12.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna==3.3.0->autotrain-advanced==0.6.37) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0->autotrain-advanced==0.6.37) (1.12)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0->autotrain-advanced==0.6.37) (2.1.0)\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl->autotrain-advanced==0.6.37)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl->autotrain-advanced==0.6.37) (13.7.0)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl->autotrain-advanced==0.6.37)\n",
            "  Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon==2.2.3->autotrain-advanced==0.6.37) (2.8.19.20240106)\n",
            "Collecting starlette<0.36.0,>=0.35.0 (from fastapi->gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.41.0->autotrain-advanced==0.6.37) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->gradio==3.41.0->autotrain-advanced==0.6.37)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.41.0->autotrain-advanced==0.6.37) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.21.4->autotrain-advanced==0.6.37) (3.17.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.0->autotrain-advanced==0.6.37) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.0->autotrain-advanced==0.6.37) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.0->autotrain-advanced==0.6.37) (0.16.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->autotrain-advanced==0.6.37) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->autotrain-advanced==0.6.37) (3.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl->autotrain-advanced==0.6.37) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl->autotrain-advanced==0.6.37) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.41.0->autotrain-advanced==0.6.37) (1.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->invisible-watermark==0.2.0->autotrain-advanced==0.6.37) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->autotrain-advanced==0.6.37) (0.1.2)\n",
            "Building wheels for collected packages: ipadic, sacremoses, ffmpy\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=5c9ad34fe37df14d37d8e2bf177bda65f08f57011c440e8dabc88689de2bbcf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895239 sha256=63945f959053021ce97cbc9ed7d3eaa9718e2f1babc55b0692f38492708c5eea\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=6fbffb1a00f6d7147c299b476258e00ffd0f1df6f0598cbaf86c80fe445f666e\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ipadic sacremoses ffmpy\n",
            "Installing collected packages: sentencepiece, pydub, ipadic, fuzzywuzzy, ffmpy, werkzeug, websockets, typing-extensions, tqdm, shtab, semantic-version, rapidfuzz, python-multipart, pynvml, protobuf, Pillow, packaging, orjson, loguru, joblib, h11, einops, docstring-parser, dill, colorlog, cmaes, xgboost, uvicorn, tiktoken, starlette, scikit-learn, sacremoses, responses, pydantic, multiprocess, jiwer, httpcore, bitsandbytes, tyro, invisible-watermark, httpx, fastapi, diffusers, codecarbon, optuna, gradio-client, datasets, trl, peft, gradio, evaluate, autotrain-advanced\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.0.1\n",
            "    Uninstalling Werkzeug-3.0.1:\n",
            "      Successfully uninstalled Werkzeug-3.0.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.3.2\n",
            "    Uninstalling joblib-1.3.2:\n",
            "      Successfully uninstalled joblib-1.3.2\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.0.3\n",
            "    Uninstalling xgboost-2.0.3:\n",
            "      Successfully uninstalled xgboost-2.0.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "tensorflow-metadata 1.14.0 requires absl-py<2.0.0,>=0.9, but you have absl-py 2.0.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.23.4 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.0.0 autotrain-advanced-0.6.37 bitsandbytes-0.42.0 cmaes-0.10.0 codecarbon-2.2.3 colorlog-6.8.0 datasets-2.14.7 diffusers-0.21.4 dill-0.3.7 docstring-parser-0.15 einops-0.6.1 evaluate-0.3.0 fastapi-0.109.0 ffmpy-0.3.1 fuzzywuzzy-0.18.0 gradio-3.41.0 gradio-client-0.5.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 invisible-watermark-0.2.0 ipadic-1.0.0 jiwer-3.0.2 joblib-1.3.1 loguru-0.7.0 multiprocess-0.70.15 optuna-3.3.0 orjson-3.9.10 packaging-23.1 peft-0.7.1 protobuf-4.23.4 pydantic-1.10.11 pydub-0.25.1 pynvml-11.5.0 python-multipart-0.0.6 rapidfuzz-2.13.7 responses-0.18.0 sacremoses-0.0.53 scikit-learn-1.3.0 semantic-version-2.10.0 sentencepiece-0.1.99 shtab-1.6.5 starlette-0.35.1 tiktoken-0.5.2 tqdm-4.65.0 trl-0.7.9 typing-extensions-4.9.0 tyro-0.6.4 uvicorn-0.25.0 websockets-11.0.3 werkzeug-2.3.6 xgboost-1.7.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes==0.40.2\n",
            "  Downloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "  Attempting uninstall: bitsandbytes\n",
            "    Found existing installation: bitsandbytes 0.42.0\n",
            "    Uninstalling bitsandbytes-0.42.0:\n",
            "      Successfully uninstalled bitsandbytes-0.42.0\n",
            "Successfully installed bitsandbytes-0.40.2\n",
            "Collecting cachetools==5.3.1\n",
            "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
            "Installing collected packages: cachetools\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.2\n",
            "    Uninstalling cachetools-5.3.2:\n",
            "      Successfully uninstalled cachetools-5.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cachetools-5.3.1\n",
            "Collecting certifi==2023.7.22\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: certifi\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.11.17\n",
            "    Uninstalling certifi-2023.11.17:\n",
            "      Successfully uninstalled certifi-2023.11.17\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2023.7.22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset-normalizer==3.3.0\n",
            "  Downloading charset_normalizer-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: charset-normalizer\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "Successfully installed charset-normalizer-3.3.0\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (8.1.7)\n",
            "Requirement already satisfied: cmaes==0.10.0 in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cmaes==0.10.0) (1.23.5)\n",
            "Requirement already satisfied: codecarbon==2.2.3 in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3) (1.5.3)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3) (11.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3) (2.31.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3) (9.0.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3) (0.18.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon==2.2.3) (2.8.2)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon==2.2.3) (2.8.19.20240106)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon==2.2.3) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon==2.2.3) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon==2.2.3) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon==2.2.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon==2.2.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon==2.2.3) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon==2.2.3) (1.16.0)\n",
            "Collecting colorlog==6.7.0\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog\n",
            "  Attempting uninstall: colorlog\n",
            "    Found existing installation: colorlog 6.8.0\n",
            "    Uninstalling colorlog-6.8.0:\n",
            "      Successfully uninstalled colorlog-6.8.0\n",
            "Successfully installed colorlog-6.7.0\n",
            "Collecting contourpy==1.1.1\n",
            "  Downloading contourpy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.7/301.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16 in /usr/local/lib/python3.10/dist-packages (from contourpy==1.1.1) (1.23.5)\n",
            "Installing collected packages: contourpy\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.2.0\n",
            "    Uninstalling contourpy-1.2.0:\n",
            "      Successfully uninstalled contourpy-1.2.0\n",
            "Successfully installed contourpy-1.1.1\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Collecting datasets==2.14.5\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (10.0.1)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (4.9.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.5) (1.16.0)\n",
            "Installing collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.7\n",
            "    Uninstalling datasets-2.14.7:\n",
            "      Successfully uninstalled datasets-2.14.7\n",
            "Successfully installed datasets-2.14.5\n",
            "Requirement already satisfied: diffusers==0.21.4 in /usr/local/lib/python3.10/dist-packages (0.21.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4) (10.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4) (0.20.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4) (7.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.21.4) (0.4.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (23.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.21.4) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.21.4) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.21.4) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.21.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.21.4) (2023.7.22)\n",
            "Requirement already satisfied: dill==0.3.7 in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: docstring-parser==0.15 in /usr/local/lib/python3.10/dist-packages (0.15)\n",
            "Requirement already satisfied: einops==0.6.1 in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: evaluate==0.3.0 in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.3.0) (10.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.3.0) (3.8.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.3.0) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.3.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.3.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.3.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.3.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.3.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.3.0) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.3.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.3.0) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate==0.3.0) (1.16.0)\n",
            "Collecting fastapi==0.104.0\n",
            "  Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.104.0) (3.7.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.104.0) (1.10.11)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi==0.104.0)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.104.0) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.104.0) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.104.0) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.104.0) (1.2.0)\n",
            "Installing collected packages: starlette, fastapi\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.35.1\n",
            "    Uninstalling starlette-0.35.1:\n",
            "      Successfully uninstalled starlette-0.35.1\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.109.0\n",
            "    Uninstalling fastapi-0.109.0:\n",
            "      Successfully uninstalled fastapi-0.109.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastapi-0.104.0 starlette-0.27.0\n",
            "Requirement already satisfied: ffmpy==0.3.1 in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Collecting filelock==3.12.4\n",
            "  Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: filelock\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "Successfully installed filelock-3.12.4\n",
            "Collecting flash-attn==2.3.3\n",
            "  Downloading flash_attn-2.3.3.tar.gz (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.3.3) (2.1.0+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.3.3) (0.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.3.3) (23.1)\n",
            "Collecting ninja (from flash-attn==2.3.3)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.3.3) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.3.3) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.3.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.3.3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.3.3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.3.3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.3.3) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn==2.3.3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn==2.3.3) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.3.3-cp310-cp310-linux_x86_64.whl size=57042553 sha256=b1df92cb5bd7657d38b789dd48e907aa3e0bd2715c817eb85f3c4320bb11fb3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/e6/fa/941802ec61d1afd320d27160ab1db98e6dba65381f84b76d4a\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: ninja, flash-attn\n",
            "Successfully installed flash-attn-2.3.3 ninja-1.11.1.1\n",
            "Collecting fonttools==4.43.1\n",
            "  Downloading fonttools-4.43.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fonttools\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.47.0\n",
            "    Uninstalling fonttools-4.47.0:\n",
            "      Successfully uninstalled fonttools-4.47.0\n",
            "Successfully installed fonttools-4.43.1\n",
            "Collecting frozenlist==1.4.0\n",
            "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: frozenlist\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.4.1\n",
            "    Uninstalling frozenlist-1.4.1:\n",
            "      Successfully uninstalled frozenlist-1.4.1\n",
            "Successfully installed frozenlist-1.4.0\n",
            "Requirement already satisfied: fsspec==2023.6.0 in /usr/local/lib/python3.10/dist-packages (2023.6.0)\n",
            "Requirement already satisfied: fuzzywuzzy==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Collecting google-auth==2.23.3\n",
            "  Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.23.3) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.23.3) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.23.3) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.23.3) (0.5.1)\n",
            "Installing collected packages: google-auth\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.17.3\n",
            "    Uninstalling google-auth-2.17.3:\n",
            "      Successfully uninstalled google-auth-2.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 2.23.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-2.23.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-auth-oauthlib==1.1.0\n",
            "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib==1.1.0) (2.23.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib==1.1.0) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib==1.1.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib==1.1.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib==1.1.0) (4.9)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib==1.1.0) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib==1.1.0) (2.31.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-auth-oauthlib==1.1.0) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib==1.1.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib==1.1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib==1.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib==1.1.0) (2023.7.22)\n",
            "Installing collected packages: google-auth-oauthlib\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "Successfully installed google-auth-oauthlib-1.1.0\n",
            "Requirement already satisfied: gradio==3.41.0 in /usr/local/lib/python3.10/dist-packages (3.41.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (5.1.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (0.104.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (0.5.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (0.26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (0.20.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (3.9.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (10.0.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (1.10.11)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (4.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (0.25.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.41.0) (11.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.0->gradio==3.41.0) (2023.6.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.41.0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.41.0) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.41.0) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.41.0) (4.65.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.41.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.41.0) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.41.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.41.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.41.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.41.0) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.41.0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.41.0) (0.14.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.41.0) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.41.0) (0.27.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.41.0) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.41.0) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.41.0) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.0) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.0) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.0) (0.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.41.0) (1.16.0)\n",
            "Requirement already satisfied: gradio-client==0.5.0 in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.0) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.0) (0.26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.0) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.0) (23.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.0) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.0) (4.9.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.0) (11.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio-client==0.5.0) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio-client==0.5.0) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio-client==0.5.0) (6.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio-client==0.5.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio-client==0.5.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio-client==0.5.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio-client==0.5.0) (2023.7.22)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio-client==0.5.0) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio-client==0.5.0) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio-client==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->gradio-client==0.5.0) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio-client==0.5.0) (1.2.0)\n",
            "Collecting greenlet==3.0.0\n",
            "  Downloading greenlet-3.0.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.9/612.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: greenlet\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 3.0.3\n",
            "    Uninstalling greenlet-3.0.3:\n",
            "      Successfully uninstalled greenlet-3.0.3\n",
            "Successfully installed greenlet-3.0.0\n",
            "Collecting grpcio==1.59.0\n",
            "  Downloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: grpcio\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.60.0\n",
            "    Uninstalling grpcio-1.60.0:\n",
            "      Successfully uninstalled grpcio-1.60.0\n",
            "Successfully installed grpcio-1.59.0\n",
            "Requirement already satisfied: h11==0.14.0 in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Collecting httpcore==0.18.0\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.18.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpcore==0.18.0) (2023.7.22)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.18.0) (0.14.0)\n",
            "Requirement already satisfied: sniffio==1.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.18.0) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore==0.18.0) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore==0.18.0) (1.2.0)\n",
            "Installing collected packages: httpcore\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.2\n",
            "    Uninstalling httpcore-1.0.2:\n",
            "      Successfully uninstalled httpcore-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "httpx 0.26.0 requires httpcore==1.*, but you have httpcore 0.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed httpcore-0.18.0\n",
            "Collecting httpx==0.25.0\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.25.0) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from httpx==0.25.0) (0.18.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.25.0) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.25.0) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx==0.25.0) (3.7.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx==0.25.0) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx==0.25.0) (1.2.0)\n",
            "Installing collected packages: httpx\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.26.0\n",
            "    Uninstalling httpx-0.26.0:\n",
            "      Successfully uninstalled httpx-0.26.0\n",
            "Successfully installed httpx-0.25.0\n",
            "Collecting huggingface-hub==0.17.3\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.3) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.3) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.3) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.3) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.3) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.3) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.3) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.17.3) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.17.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.17.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.17.3) (2023.7.22)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.2\n",
            "    Uninstalling huggingface-hub-0.20.2:\n",
            "      Successfully uninstalled huggingface-hub-0.20.2\n",
            "Successfully installed huggingface-hub-0.17.3\n",
            "Collecting idna==3.4\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: idna\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed idna-3.4\n",
            "Collecting imageio==2.31.5\n",
            "  Downloading imageio-2.31.5-py3-none-any.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.31.5) (1.23.5)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio==2.31.5) (10.0.0)\n",
            "Installing collected packages: imageio\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.31.6\n",
            "    Uninstalling imageio-2.31.6:\n",
            "      Successfully uninstalled imageio-2.31.6\n",
            "Successfully installed imageio-2.31.5\n",
            "Collecting importlib-metadata==6.8.0\n",
            "  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata==6.8.0) (3.17.0)\n",
            "Installing collected packages: importlib-metadata\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed importlib-metadata-6.8.0\n",
            "Collecting importlib-resources==6.1.0\n",
            "  Downloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: importlib-resources\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 6.1.1\n",
            "    Uninstalling importlib-resources-6.1.1:\n",
            "      Successfully uninstalled importlib-resources-6.1.1\n",
            "Successfully installed importlib-resources-6.1.0\n",
            "Collecting inquirerpy==0.3.4\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pfzy<0.4.0,>=0.3.1 (from inquirerpy==0.3.4)\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from inquirerpy==0.3.4) (3.0.43)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->inquirerpy==0.3.4) (0.2.12)\n",
            "Installing collected packages: pfzy, inquirerpy\n",
            "Successfully installed inquirerpy-0.3.4 pfzy-0.3.4\n",
            "Requirement already satisfied: invisible-watermark==0.2.0 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: Pillow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0) (10.0.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0) (4.8.0.76)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->invisible-watermark==0.2.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->invisible-watermark==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: ipadic==1.0.0 in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: jinja2==3.1.2 in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2==3.1.2) (2.1.3)\n",
            "Requirement already satisfied: jiwer==3.0.2 in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer==3.0.2) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz==2.13.7 in /usr/local/lib/python3.10/dist-packages (from jiwer==3.0.2) (2.13.7)\n",
            "Requirement already satisfied: joblib==1.3.1 in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Collecting jsonschema==4.19.1\n",
            "  Downloading jsonschema-4.19.1-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.3/83.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.19.1) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.19.1) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.19.1) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.19.1) (0.16.2)\n",
            "Installing collected packages: jsonschema\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.19.2\n",
            "    Uninstalling jsonschema-4.19.2:\n",
            "      Successfully uninstalled jsonschema-4.19.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jsonschema-4.19.1\n",
            "Collecting jsonschema-specifications==2023.7.1\n",
            "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: referencing>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema-specifications==2023.7.1) (0.32.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from referencing>=0.28.0->jsonschema-specifications==2023.7.1) (23.1.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from referencing>=0.28.0->jsonschema-specifications==2023.7.1) (0.16.2)\n",
            "Installing collected packages: jsonschema-specifications\n",
            "  Attempting uninstall: jsonschema-specifications\n",
            "    Found existing installation: jsonschema-specifications 2023.12.1\n",
            "    Uninstalling jsonschema-specifications-2023.12.1:\n",
            "      Successfully uninstalled jsonschema-specifications-2023.12.1\n",
            "Successfully installed jsonschema-specifications-2023.7.1\n",
            "Requirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (1.4.5)\n",
            "Requirement already satisfied: lazy-loader==0.3 in /usr/local/lib/python3.10/dist-packages (0.3)\n",
            "Requirement already satisfied: loguru==0.7.0 in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Collecting mako==1.2.4\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako==1.2.4) (2.1.3)\n",
            "Installing collected packages: mako\n",
            "  Attempting uninstall: mako\n",
            "    Found existing installation: Mako 1.3.0\n",
            "    Uninstalling Mako-1.3.0:\n",
            "      Successfully uninstalled Mako-1.3.0\n",
            "Successfully installed mako-1.2.4\n",
            "Collecting markdown==3.5\n",
            "  Downloading Markdown-3.5-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: markdown\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.5.1\n",
            "    Uninstalling Markdown-3.5.1:\n",
            "      Successfully uninstalled Markdown-3.5.1\n",
            "Successfully installed markdown-3.5\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py==3.0.0) (0.1.2)\n",
            "Requirement already satisfied: markupsafe==2.1.3 in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Collecting matplotlib==3.8.0\n",
            "  Downloading matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0) (1.4.5)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0) (10.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.0) (1.16.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: multidict==6.0.4 in /usr/local/lib/python3.10/dist-packages (6.0.4)\n",
            "Requirement already satisfied: multiprocess==0.70.15 in /usr/local/lib/python3.10/dist-packages (0.70.15)\n",
            "Requirement already satisfied: dill>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from multiprocess==0.70.15) (0.3.7)\n",
            "Collecting networkx==3.2\n",
            "  Downloading networkx-3.2-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: networkx\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2.1\n",
            "    Uninstalling networkx-3.2.1:\n",
            "      Successfully uninstalled networkx-3.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed networkx-3.2\n",
            "Requirement already satisfied: ninja==1.11.1.1 in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\n",
            "Collecting numpy==1.26.1\n",
            "  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.1\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cublas-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-cupti-cu12\n",
            "Successfully installed nvidia-cuda-cupti-cu12-12.1.105\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-nvrtc-cu12\n",
            "Successfully installed nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu12\n",
            "Successfully installed nvidia-cuda-runtime-cu12-12.1.105\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cudnn-cu12==8.9.2.26) (12.1.3.1)\n",
            "Installing collected packages: nvidia-cudnn-cu12\n",
            "Successfully installed nvidia-cudnn-cu12-8.9.2.26\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cufft-cu12\n",
            "Successfully installed nvidia-cufft-cu12-11.0.2.54\n",
            "Collecting nvidia-curand-cu12\n",
            "  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-curand-cu12\n",
            "Successfully installed nvidia-curand-cu12-10.3.4.107\n",
            "Collecting protobuf==3.20.1\n",
            "  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.23.4\n",
            "    Uninstalling protobuf-4.23.4:\n",
            "      Successfully uninstalled protobuf-4.23.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "autotrain-advanced 0.6.37 requires protobuf==4.23.4, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.4.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.39.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery 3.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-iam 2.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.62.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires absl-py<2.0.0,>=0.9, but you have absl-py 2.0.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install absl-py==2.0.0\n",
        "!pip install accelerate==0.21.0\n",
        "!pip install aiofiles==23.2.1\n",
        "!pip install aiohttp==3.8.6\n",
        "!pip install aiosignal==1.3.1\n",
        "!pip install albumentations==1.3.1\n",
        "!pip install alembic==1.12.0\n",
        "!pip install altair==5.1.2\n",
        "!pip install anyio==3.7.1\n",
        "!pip install arrow==1.3.0\n",
        "!pip install async-timeout==4.0.3\n",
        "!pip install attrs==23.1.0\n",
        "!pip install autotrain-advanced==0.6.37\n",
        "!pip install bitsandbytes==0.40.2\n",
        "!pip install cachetools==5.3.1\n",
        "!pip install certifi==2023.7.22\n",
        "!pip install charset-normalizer==3.3.0\n",
        "!pip install click==8.1.7\n",
        "!pip install cmaes==0.10.0\n",
        "!pip install codecarbon==2.2.3\n",
        "!pip install colorlog==6.7.0\n",
        "!pip install contourpy==1.1.1\n",
        "!pip install cycler==0.12.1\n",
        "!pip install datasets==2.15.0\n",
        "!pip install diffusers==0.21.4\n",
        "!pip install dill==0.3.7\n",
        "!pip install docstring-parser==0.15\n",
        "!pip install einops==0.6.1\n",
        "!pip install evaluate==0.3.0\n",
        "!pip install fastapi==0.104.0\n",
        "!pip install ffmpy==0.3.1\n",
        "!pip install filelock==3.12.4\n",
        "!pip install flash-attn==2.3.3\n",
        "!pip install fonttools==4.43.1\n",
        "!pip install frozenlist==1.4.0\n",
        "!pip install fsspec==2023.6.0\n",
        "!pip install fuzzywuzzy==0.18.0\n",
        "!pip install google-auth==2.23.3\n",
        "!pip install google-auth-oauthlib==1.1.0\n",
        "!pip install gradio==3.41.0\n",
        "!pip install gradio-client==0.5.0\n",
        "!pip install greenlet==3.0.0\n",
        "!pip install grpcio==1.59.0\n",
        "!pip install h11==0.14.0\n",
        "!pip install httpcore==0.18.0\n",
        "!pip install httpx==0.25.0\n",
        "!pip install huggingface-hub==0.17.3\n",
        "!pip install idna==3.4\n",
        "!pip install imageio==2.31.5\n",
        "!pip install importlib-metadata==6.8.0\n",
        "!pip install importlib-resources==6.1.0\n",
        "!pip install inquirerpy==0.3.4\n",
        "!pip install invisible-watermark==0.2.0\n",
        "!pip install ipadic==1.0.0\n",
        "!pip install jinja2==3.1.2\n",
        "!pip install jiwer==3.0.2\n",
        "!pip install joblib==1.3.1\n",
        "!pip install jsonschema==4.19.1\n",
        "!pip install jsonschema-specifications==2023.7.1\n",
        "!pip install kiwisolver==1.4.5\n",
        "!pip install lazy-loader==0.3\n",
        "!pip install loguru==0.7.0\n",
        "!pip install mako==1.2.4\n",
        "!pip install markdown==3.5\n",
        "!pip install markdown-it-py==3.0.0\n",
        "!pip install markupsafe==2.1.3\n",
        "!pip install matplotlib==3.8.0\n",
        "!pip install mdurl==0.1.2\n",
        "!pip install mpmath==1.3.0\n",
        "!pip install multidict==6.0.4\n",
        "!pip install multiprocess==0.70.15\n",
        "!pip install networkx==3.2\n",
        "!pip install ninja==1.11.1.1\n",
        "!pip install numpy==1.26.1\n",
        "!pip install nvidia-cublas-cu12==12.1.3.1\n",
        "!pip install nvidia-cuda-cupti-cu12==12.1.105\n",
        "!pip install nvidia-cuda-nvrtc-cu12==12.1.105\n",
        "!pip install nvidia-cuda-runtime-cu12==12.1.105\n",
        "!pip install nvidia-cudnn-cu12==8.9.2.26\n",
        "!pip install nvidia-cufft-cu12==11.0.2.54\n",
        "!pip install nvidia-curand-cu12\n",
        "!pip install protobuf==3.20.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6Jlbq9mjmgq",
        "outputId": "ccdfacee-eac1-4802-eb8a-f75606985459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
            "Requirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: datasets==2.14.5 in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (1.26.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (10.0.1)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (4.9.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.5) (1.16.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install transformers==4.31.0\n",
        "!pip install sentencepiece==0.1.99\n",
        "!pip install datasets==2.14.5\n",
        "!!pip install -q bitsandbytes einops wand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVkGd4wLrOwR",
        "outputId": "988ed6f9-2670-487f-e61b-675da32e60e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Projects/bangla-llama/data\n"
          ]
        }
      ],
      "source": [
        "# Train with SentencePiece\n",
        "\n",
        "%cd data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqgEzZxcrSdY",
        "outputId": "2de6597b-cc23-4b6a-db57-2f059c777b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mbangla_corpus\u001b[0m/           \u001b[01;34mbn_part_00005_512\u001b[0m/       \u001b[01;34mbn_part_00010_text_512\u001b[0m/  \u001b[01;34mbn_part_00016_512\u001b[0m/\n",
            "\u001b[01;34mbn_part_00000_512\u001b[0m/       \u001b[01;34mbn_part_00005_text_512\u001b[0m/  \u001b[01;34mbn_part_00011_512\u001b[0m/       \u001b[01;34mbn_part_00016_text_512\u001b[0m/\n",
            "\u001b[01;34mbn_part_00000_text_512\u001b[0m/  \u001b[01;34mbn_part_00006_512\u001b[0m/       \u001b[01;34mbn_part_00011_text_512\u001b[0m/  \u001b[01;34mbn_part_00017_512\u001b[0m/\n",
            "\u001b[01;34mbn_part_00001_512\u001b[0m/       \u001b[01;34mbn_part_00006_text_512\u001b[0m/  \u001b[01;34mbn_part_00012_512\u001b[0m/       \u001b[01;34mbn_part_00017_text_512\u001b[0m/\n",
            "\u001b[01;34mbn_part_00001_text_512\u001b[0m/  \u001b[01;34mbn_part_00007_512\u001b[0m/       \u001b[01;34mbn_part_00012_text_512\u001b[0m/  \u001b[01;34mcache\u001b[0m/\n",
            "\u001b[01;34mbn_part_00002_512\u001b[0m/       \u001b[01;34mbn_part_00007_text_512\u001b[0m/  \u001b[01;34mbn_part_00013_512\u001b[0m/       \u001b[01;34mmodels\u001b[0m/\n",
            "\u001b[01;34mbn_part_00002_text_512\u001b[0m/  \u001b[01;34mbn_part_00008_512\u001b[0m/       \u001b[01;34mbn_part_00013_text_512\u001b[0m/  \u001b[01;34moutput\u001b[0m/\n",
            "\u001b[01;34mbn_part_00003_512\u001b[0m/       \u001b[01;34mbn_part_00008_text_512\u001b[0m/  \u001b[01;34mbn_part_00014_512\u001b[0m/       \u001b[01;34muonlp\u001b[0m/\n",
            "\u001b[01;34mbn_part_00003_text_512\u001b[0m/  \u001b[01;34mbn_part_00009_512\u001b[0m/       \u001b[01;34mbn_part_00014_text_512\u001b[0m/\n",
            "\u001b[01;34mbn_part_00004_512\u001b[0m/       \u001b[01;34mbn_part_00009_text_512\u001b[0m/  \u001b[01;34mbn_part_00015_512\u001b[0m/\n",
            "\u001b[01;34mbn_part_00004_text_512\u001b[0m/  \u001b[01;34mbn_part_00010_512\u001b[0m/       \u001b[01;34mbn_part_00015_text_512\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls /content/drive/MyDrive/Projects/bangla-llama/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgDCcrydrZpa",
        "outputId": "b5241109-ad57-4fa0-a9d6-4c80b45894e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,051 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,635 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,263 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,621 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,309 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,340 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,161 kB]\n",
            "Fetched 10.7 MB in 3s (3,607 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAQWCYtmr5pg",
        "outputId": "70c3ed73-9057-4bbc-9e17-34a191e67514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "zip is already the newest version (3.0-12build2).\n",
            "unzip is already the newest version (6.0-26ubuntu3.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 26 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y zip unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMnghXVPsAbf",
        "outputId": "6b07acc7-1648-48cb-89dc-ac640e5db772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open bangla_corpus.zip, bangla_corpus.zip.zip or bangla_corpus.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "#unzip bangla corpus\n",
        "#!unzip bangla_corpus.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMoNiNbtsIyr",
        "outputId": "d1fada95-1c87-4757-e472-4643c98173ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ben-bd_web_2017_1M.tar.gz  ben_news_2020_300K.tar.gz\t ben_wikipedia_2021_1M\t       corpus\n",
            "ben_community_2022.tar.gz  ben_newscrawl_2017_1M.tar.gz  ben_wikipedia_2021_1M.tar.gz\n"
          ]
        }
      ],
      "source": [
        "!ls bangla_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX9GvZ7HsaSR",
        "outputId": "53b6ece5-2410-459f-e721-3c9f96e6dabc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus\n"
          ]
        }
      ],
      "source": [
        "%cd bangla_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVn-RDcZsd9y",
        "outputId": "3c1a56fd-fc30-42f4-d0e7-73f697694aa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ben_wikipedia_2021_1M/\n",
            "ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-co_n.txt\n",
            "ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-import.sql\n",
            "ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-sentences.txt\n",
            "ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-sources.txt\n",
            "ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-co_s.txt\n",
            "ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-inv_w.txt\n",
            "ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-inv_so.txt\n",
            "ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-words.txt\n"
          ]
        }
      ],
      "source": [
        "#!tar zxvf ben_wikipedia_2021_1M.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AyES0besOKT",
        "outputId": "096342fb-4179-4626-f727-3f251de67aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tail: cannot open 'ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-sentences.txt' for reading: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!tail ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-sentences.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfFGQTLFtRTN"
      },
      "outputs": [],
      "source": [
        "#!mkdir /content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f0uMaXptoJZ"
      },
      "outputs": [],
      "source": [
        "#!mkdir /content/drive/MyDrive/Projects/bangla-llama/data/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0vdkIPvtUv2",
        "outputId": "dd06cd27-b714-48e3-b8e4-80557b83f0d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 227825\n",
            "-rw------- 1 root root 233292475 Jan 15 23:15 ben_wikipedia_2021_1M-sentences.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lrt /content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiurLO6htKBM"
      },
      "outputs": [],
      "source": [
        "#!cp /content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/ben_wikipedia_2021_1M/ben_wikipedia_2021_1M-sentences.txt /content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/corpus/ben_wikipedia_2021_1M-sentences.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXcF36mWsuoc",
        "outputId": "6b20af9a-8a55-44a5-85cc-bda08850a3c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Projects/bangla-llama\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Projects/bangla-llama/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFpQQPy5tAXb",
        "outputId": "8c455b7f-a175-4f93-e562-f07188d4ac23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ben-bd_web_2017_1M.tar.gz  ben_news_2020_300K.tar.gz\t ben_wikipedia_2021_1M\t       corpus\n",
            "ben_community_2022.tar.gz  ben_newscrawl_2017_1M.tar.gz  ben_wikipedia_2021_1M.tar.gz\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v-FFDXVPEzma"
      },
      "outputs": [],
      "source": [
        "# Create bangla corpus from huggingface dataset\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "# )\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class CorpusCreator:\n",
        "    def __init__(self):\n",
        "        self.hf_dataset = 'uonlp/CulturaX'\n",
        "        self.text_col = 'text'\n",
        "        self.output_dir = '/content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/corpus'\n",
        "        self.dataset_split = 'train'\n",
        "        self.output_file_name = 'bangla_sentence_corpus_from_CulturaX.txt'\n",
        "\n",
        "    def create_sentence_corpus(\n",
        "        self,\n",
        "        hf_dataset, #='/content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/corpus',\n",
        "        text_col, #='text',\n",
        "        dataset_split=\"train\",\n",
        "        output_file_name=\"bangla_sentence_corpus.txt\",\n",
        "    ):\n",
        "        try:\n",
        "            dataset = load_dataset(hf_dataset, split=dataset_split)\n",
        "            train_df = pd.DataFrame(dataset)\n",
        "\n",
        "            os.makedirs(self.output_dir, exist_ok=True)\n",
        "            corpus_path = os.path.join(self.output_dir, output_file_name)\n",
        "\n",
        "            with open(corpus_path, \"w\") as file:\n",
        "                for index, value in tqdm(\n",
        "                    train_df[text_col].iteritems(), total=len(train_df)\n",
        "                ):\n",
        "                    file.write(str(value) + \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # logger.error(f\"Error creating the text corpus -> {e}\")\n",
        "            pass\n",
        "\n",
        "        return corpus_path\n",
        "\n",
        "    def run(self):\n",
        "        parser = argparse.ArgumentParser(\n",
        "            description=\"Create a sentence corpus from a Hugging Face dataset.\"\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--hf-dataset\",\n",
        "            # required=True,\n",
        "            help=\"Name of the Hugging Face dataset (e.g., 'imdb').\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--text-col\",\n",
        "            # required=True,\n",
        "            help=\"Name of the text column in the dataset.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--dataset-split\",\n",
        "            default=\"train\",\n",
        "            help=\"Dataset split to use (default: 'train').\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--output-file-name\",\n",
        "            default=\"tamil_sentence_corpus.txt\",\n",
        "            help=\"Name of the output corpus file (default: 'tamil_sentence_corpus.txt').\",\n",
        "        )\n",
        "\n",
        "        arg_str = '--hf-dataset ' + self.hf_dataset + ' --text-col ' + self.text_col + ' --dataset-split ' + self.dataset_split + ' --output-file-name ' + self.output_file_name\n",
        "        args = parser.parse_args(arg_str.split())\n",
        "        print(args)\n",
        "        self.create_sentence_corpus(\n",
        "            args.hf_dataset, args.text_col, args.dataset_split, args.output_file_name\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ocf0q6i8GMiC"
      },
      "outputs": [],
      "source": [
        "corpus_creator = CorpusCreator()\n",
        "#corpus_creator.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3zc15vTYUOAK"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ic_FoWIFH41G"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = 'uonlp/CulturaX' #french novels\n",
        "#dataset = load_dataset(dataset_name, \"bn\", use_auth_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fjpT0ReXRtZ"
      },
      "outputs": [],
      "source": [
        "#!mkdir /content/drive/MyDrive/Projects/bangla-llama/data/cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2EYsf3JXctJ"
      },
      "outputs": [],
      "source": [
        "#!mkdir /content/drive/MyDrive/Projects/bangla-llama/data/output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1vGA2BA9TRL",
        "outputId": "c8639926-5db2-467a-84d1-084395d0bedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Projects/bangla-llama/data/uonlp/culturaX/bn:\n",
            "bn_part_00000.parquet  bn_part_00005.parquet  bn_part_00010.parquet  bn_part_00015.parquet\n",
            "bn_part_00001.parquet  bn_part_00006.parquet  bn_part_00011.parquet  bn_part_00016.parquet\n",
            "bn_part_00002.parquet  bn_part_00007.parquet  bn_part_00012.parquet  bn_part_00017.parquet\n",
            "bn_part_00003.parquet  bn_part_00008.parquet  bn_part_00013.parquet\n",
            "bn_part_00004.parquet  bn_part_00009.parquet  bn_part_00014.parquet\n"
          ]
        }
      ],
      "source": [
        "!ls -R /content/drive/MyDrive/Projects/bangla-llama/data/uonlp/culturaX/bn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "720e021855e143a2a27ce91adf234209",
            "82b5c59d829b4b3483531678173c4ae4",
            "0779842cd9b64198ae1517da1ff8f405",
            "a6e9621d424340e693a301d1de7e1435",
            "e997a293549f41be8ff8bdfa511cad54",
            "0ed211bdefd748dfa14535ed99891238",
            "f44b6f3251784c148a27042f5c982950",
            "0f880e16905a4be1b718273daceac680",
            "6a0cab4a12aa4387856747ba1b5a9c3c",
            "d77d5b8bb4b04b0b9a8c1d7b7bff4082",
            "a8456d230c704ab9b7264e5b815c4c12",
            "991c844cbeba4bc5a9c0c628fb9d750c",
            "f571015852eb4df0a356f98edff00646",
            "42d4a74ea6aa430bb0207309448c984f",
            "8c128584f6d94fa5826f9e51ab4dd769",
            "68736d7bef8f464a9ac99bf3a37c0a61",
            "1f192500bcd34ffd851504aa7f5ebafc",
            "92346b3e0d964e349a990b28b913b764",
            "36c4a3510d204df0bcf8fd8e489335b8",
            "eb8014a53cb84abcb0ace6864bd2ef96",
            "33ea235f0a85492d8f723a7dcb532b66",
            "04d0f620e7d54a76b88998dd1fff5bdb",
            "8a66255895fe4a84abea2bd29b980bef",
            "7f9b10f32ac04c909e9218a146cb0ee6",
            "f8e736390872479dac25f52404947bda",
            "d1da4e3908bf468c96a692edd69c06d3",
            "fcfed57b5f2148f0b56d5857dd6056f1",
            "36c9d990123147eaa12bae188abb1fde",
            "96cebf3d2dec4d178d4ad7030cbc91a6",
            "c8388c595da1420ba7fbfedb5196b09f",
            "49359dbf5f544bb3a5ba8d0d4088e2a2",
            "ac4b4b923a3e45dd9e31c065aa4df0ea",
            "a30f800461174e0993a2a60a48b2ac98",
            "e6e9c03e9e5c45ea8ffcf43cb34b85d2",
            "3fbd883cd0344882a02f5dc78041e0d2",
            "fb94cdd865b94ae2bde1189333412698",
            "95d0e0b8ef04423fb50cf257b5f002fd",
            "95e483355a7c41adbc8e98fa860f4c2a",
            "8a46818fb4fc4587aca1b11e1d71eace",
            "7811b700c799497486ae056ffc362fde",
            "d4286a71fc8544bbaad3f2a9b779e888",
            "c3e390c62c2c4652ae245cd946d4dfe8",
            "aecbf1f7d27f48da88d60589547820d1",
            "5c728be6054b4137a0dfdb2142c67cd4"
          ]
        },
        "id": "6lWMt0ne8_GZ",
        "outputId": "d725398d-3ed5-4dd3-88c7-69cbcb76038c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "720e021855e143a2a27ce91adf234209",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "991c844cbeba4bc5a9c0c628fb9d750c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a66255895fe4a84abea2bd29b980bef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6e9c03e9e5c45ea8ffcf43cb34b85d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# # Path to the directory containing the Parquet files\n",
        "# data_dir = '/content/drive/MyDrive/Projects/bangla-llama/data/uonlp/culturaX/bn'\n",
        "\n",
        "# # List of all Parquet files\n",
        "# parquet_files = [f'{data_dir}/bn_part_00000.parquet',\n",
        "#                  f'{data_dir}/bn_part_00001.parquet',\n",
        "#                  f'{data_dir}/bn_part_00002.parquet',\n",
        "#                  f'{data_dir}/bn_part_00003.parquet',\n",
        "#                  f'{data_dir}/bn_part_00004.parquet',\n",
        "#                  f'{data_dir}/bn_part_00005.parquet',\n",
        "#                  f'{data_dir}/bn_part_00006.parquet',\n",
        "#                  f'{data_dir}/bn_part_00007.parquet',\n",
        "#                  f'{data_dir}/bn_part_00008.parquet',\n",
        "#                  f'{data_dir}/bn_part_00009.parquet',\n",
        "#                  f'{data_dir}/bn_part_00010.parquet',\n",
        "#                  f'{data_dir}/bn_part_00011.parquet',\n",
        "#                  f'{data_dir}/bn_part_00012.parquet',\n",
        "#                  f'{data_dir}/bn_part_00013.parquet',\n",
        "#                  f'{data_dir}/bn_part_00014.parquet',\n",
        "#                  f'{data_dir}/bn_part_00015.parquet',\n",
        "#                  f'{data_dir}/bn_part_00016.parquet',\n",
        "#                  f'{data_dir}/bn_part_00017.parquet']\n",
        "\n",
        "# # Load the dataset\n",
        "# dataset = load_dataset('parquet', data_files=parquet_files, split='train', keep_in_memory=False)\n",
        "\n",
        "# Display the dataset structure\n",
        "#print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtUDniumS0b3"
      },
      "outputs": [],
      "source": [
        "#dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY636txXKcvZ"
      },
      "outputs": [],
      "source": [
        "batch = dataset.select(range(10000, 20001))\n",
        "tenth_thousandth_text = batch['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "TjgLIelDUrp3",
        "outputId": "2e737a83-6ccb-4c6a-bb13-a38a8354774e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ডাকসুর ভিপি নুরের ওপরও ডিম নিক্ষেপ | uttorbangla.com\\nআজ- বৃহস্পতিবার, ৪ জুন, ২০২০ :: ২১ জ্যৈষ্ঠ ১৪২৭ :: সময়- ২ : ০৭ পুর্বাহ্ন\\nHome / ক্যাম্পাস / ডাকসুর ভিপি নুরের ওপরও ডিম নিক্ষেপ\\nডাকসুর ভিপি নুরের ওপরও ডিম নিক্ষেপ\\nডেস্ক: ঢাকা বিশ্ববিদ্যালয়ের সলিমুল্লাহ মুসলিম (এসএম) হলে দুই ঘণ্টা অবরুদ্ধ থাকার পর মুক্ত হওয়া ডাকসুর ভিপি নুরুল হক নুরের ওপর ডিম নিক্ষেপ করা হয়েছে। এ সময় তার সঙ্গে থাকা অন্য ছাত্র নেতাদের দিকেও ডিম ছোড়া হয়। ছাত্রলীগের নেতাকর্মীরা এই ডিম নিক্ষেপ করেছেন বলে অভিযোগ উঠেছে।\\nএর আগে আজ মঙ্গলবার বিকেল ৫টার দিকে এসএম হলে গেলে নুকরে লাঞ্ছিত ও অবরুদ্ধ করে রাখেন ছাত্রলীগের নেতাকর্মীরা। দুই ঘণ্টা পর সন্ধ্যা ৭টার দিকে প্রাধ্যক্ষ হলে গেলে বেরিয়ে আসেন নুর। এ সময় তার ও সঙ্গীদের ওপর ডিম নিক্ষেপের ঘটনা ঘটে।\\nহলের শিক্ষার্থীদের সঙ্গে কথা বলে জানা যায়, গতকাল সোমবার দিবাগত রাতে এসএম হলের আবাসিক ছাত্র মো. ফরিদ হাসানকে ছাত্রলীগের নেতাকর্মীরা মারধর করে রক্তাক্ত করেন। ওই ঘটনার জের ধরে আজ বিকেল ৫টার দিকে অন্যান্য ছাত্রনেতাদের নিয়ে এসএম হলে যান ভিপি নুর। এ সময় তাকে লাঞ্ছিত করেন ছাত্রলীগ নেতারা। হল অফিসে নুরের থেকে মুচলেকা চাওয়ার একটি ভিডিও ফেসবুকে প্রকাশ পেয়েছে।\\nএসএম হলে শামসুন্নাহার হল সংসদের ভিপি শেখ তাসনিম আফরোজ ইমির শরীরে ছাত্রলীগের নেতারা হাত দিয়েছেন বলে অভিযোগ ওঠে। এ সময় তাদের দিকে ছাত্রলীগের নেতাকর্মীরা ডিম নিক্ষেপ করেন। সন্ধ্যা ৭টার দিকে অবরুদ্ধ থেকে মুক্ত হয়ে ক্যাম্পাসে ভিপি নুরের নেতৃত্বে বিক্ষোভ মিছিল করেন ছাত্রনেতারা। পরে নুর গণমাধ্যমকর্মীদের সঙ্গে কথা বলেন।'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tenth_thousandth_text[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt4yaWheGr-O"
      },
      "outputs": [],
      "source": [
        "corpus_creator.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUbqIa10aBQr"
      },
      "outputs": [],
      "source": [
        "!cat /content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/corpus/ben_wikipedia_2021_1M-sentences.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a29d0tCabD03",
        "outputId": "eaba20f1-891f-492c-8adf-08f1c81b9589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ben_wikipedia_2021_1M-sentences.txt\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT4xcXOcjrbp"
      },
      "outputs": [],
      "source": [
        "#!rm -rf /content/drive/MyDrive/Projects/bangla-llama/data/models/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bJgCmqXns2Sg"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "class SentencePieceTrainer:\n",
        "    def __init__(self):\n",
        "        self.corpus_dir = \"/content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/corpus\"\n",
        "        self.input_file = self.corpus_dir + \"/ben_wikipedia_2021_1M-sentences.txt\"\n",
        "        self.output_dir = \"/content/drive/MyDrive/Projects/bangla-llama/data/models\"\n",
        "        self.model_prefix = \"bangla_sp\"\n",
        "        self.vocab_size = 20000\n",
        "        self.character_coverage = 1.0\n",
        "        self.model_type = \"unigram\"\n",
        "\n",
        "    def train_sentencepiece_model(self, input_file):\n",
        "        output_model_path = os.path.join(self.output_dir, f\"{self.model_prefix}.model\")\n",
        "\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=input_file,\n",
        "            model_prefix=self.model_prefix,\n",
        "            vocab_size=self.vocab_size,\n",
        "            character_coverage=self.character_coverage,\n",
        "            model_type=self.model_type,\n",
        "        )\n",
        "\n",
        "        # os.rename(\n",
        "        #     f\"{self.model_prefix}.vocab\",\n",
        "        #     os.path.join(self.output_dir, f\"{self.model_prefix}.vocab\"),\n",
        "        # )\n",
        "\n",
        "        shutil.move(\n",
        "          f\"{self.model_prefix}.vocab\",\n",
        "           os.path.join(self.output_dir, f\"{self.model_prefix}.vocab\"),\n",
        "        )\n",
        "\n",
        "        # os.rename(\n",
        "        #     f\"{self.model_prefix}.model\",\n",
        "        #     os.path.join(self.output_dir, f\"{self.model_prefix}.model\"),\n",
        "        # )\n",
        "\n",
        "        shutil.move(\n",
        "          f\"{self.model_prefix}.model\",\n",
        "           os.path.join(self.output_dir, f\"{self.model_prefix}.model\"),\n",
        "        )\n",
        "\n",
        "        return output_model_path\n",
        "\n",
        "    def run(self):\n",
        "        parser = argparse.ArgumentParser(description=\"Train a SentencePiece model.\")\n",
        "        parser.add_argument(\n",
        "            \"--input-file\",\n",
        "            required=True,\n",
        "            help=\"Path to the input text corpus file.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--output-dir\",\n",
        "            default=self.output_dir,\n",
        "            help=\"Directory where the trained model and vocabulary will be saved.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--model-prefix\",\n",
        "            default=self.model_prefix,\n",
        "            help=\"Prefix for the model and vocabulary filenames.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--vocab-size\",\n",
        "            type=int,\n",
        "            default=self.vocab_size,\n",
        "            help=\"Size of the vocabulary.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--character-coverage\",\n",
        "            type=float,\n",
        "            default=self.character_coverage,\n",
        "            help=\"Character coverage for the model.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--model-type\",\n",
        "            default=self.model_type,\n",
        "            choices=[\"bpe\", \"unigram\", \"char\", \"word\"],\n",
        "            help=\"Type of SentencePiece model.\",\n",
        "        )\n",
        "\n",
        "        #args = parser.parse_args()\n",
        "\n",
        "        arg_str = ' --input-file ' + self.input_file + ' --output-dir ' + self.output_dir + ' --model-prefix ' +  self.model_prefix + ' --vocab-size ' + str(self.vocab_size) + ' --character-coverage ' + str(self.character_coverage) + ' --model-type ' + self.model_type\n",
        "\n",
        "        args = parser.parse_args(arg_str.split())\n",
        "\n",
        "        self.output_dir = args.output_dir\n",
        "        self.model_prefix = args.model_prefix\n",
        "        self.vocab_size = args.vocab_size\n",
        "        self.character_coverage = args.character_coverage\n",
        "        self.model_type = args.model_type\n",
        "\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        self.train_sentencepiece_model(args.input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NhJBqOFUuIxa"
      },
      "outputs": [],
      "source": [
        "sp_trainer = SentencePieceTrainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRjMGMC5uLnC",
        "outputId": "bb38fb6c-5ae7-4880-8b36-399d688c91be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " --input-file /content/drive/MyDrive/Projects/bangla-llama/data/bangla_corpus/corpus/ben_wikipedia_2021_1M-sentences.txt --output-dir /content/drive/MyDrive/Projects/bangla-llama/data/models --model-prefix bangla_sp --vocab-size 20000 --character-coverage 1.0 --model-type unigram\n"
          ]
        }
      ],
      "source": [
        "#sp_trainer.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tGQLf8Y3cmy",
        "outputId": "2ce2d754-8c6c-4f41-d0c4-56b23390c8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bangla_sp.model  bangla_sp.vocab  merged_tokenizer_hf  merged_tokenizer_sp\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg7zJhqH9a59",
        "outputId": "d72ff56d-88bd-4b8e-a9ae-6f52b29c1e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bangla_sp.model  merged_tokenizer_hf  merged_tokenizer_spbangla_llama.model\n",
            "bangla_sp.vocab  merged_tokenizer_sp\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHoNuRtALned",
        "outputId": "c27f6f71-5fac-4ae1-de53-019e82662e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access 'scripts': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RES6-F4He15C",
        "outputId": "4bd7739d-d26c-4f6f-cda3-918779eb659a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Projects/bangla-llama\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Projects/bangla-llama/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "g4_esQYHMXyZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "APXDVOj4MZuh"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "from transformers import LlamaTokenizer\n",
        "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsLwIAvdUh_i"
      },
      "outputs": [],
      "source": [
        "#!mkdir bangla_sp.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJr4nkSLUwTv"
      },
      "outputs": [],
      "source": [
        "#!mkdir ./llama-tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "D8c5O2W4McPf"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--llama_tokenizer_dir\", default=None, type=str, required=True)\n",
        "parser.add_argument(\"--bangla_sp_model_file\", default=\"./tamil_sp.model\", type=str)\n",
        "args = parser.parse_args('--llama_tokenizer_dir /content/drive/MyDrive/Projects/bangla-llama/data/llamatokenizer --bangla_sp_model_file /content/drive/MyDrive/Projects/bangla-llama/data/models/bangla_sp.model'.split())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pSmnuCwtX66t"
      },
      "outputs": [],
      "source": [
        "llama_tokenizer_dir = args.llama_tokenizer_dir\n",
        "bangla_sp_model_file = args.bangla_sp_model_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fdqJDB1hlQZY",
        "outputId": "a5b4521c-6a76-40ce-a97c-b91410a16808"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Projects/bangla-llama/data/llamatokenizer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "llama_tokenizer_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nhLn-AR-htaZ",
        "outputId": "760d3b6b-a95d-4631-dab3-de7d28607d68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Projects/bangla-llama/data/models/bangla_sp.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "bangla_sp_model_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "17d2669800954ad6bf0040b3c467acd7",
            "8fce72fb31fd41e781d11e49e56fa8cd",
            "60d2fe11893442d1836bdf509ef60176",
            "e716e77347c94f42b005d9e1fbb1b866",
            "f25e025559b2417595d4b289b051c29e",
            "1316593626794ae5b1b5af75d4e54f43",
            "fcd983a188584167bcadce44df8fda81",
            "49c05019017e43cebf4799995344910b",
            "d0c9ca6a572f4b59a6ba442fe340a5ce",
            "a1b50b86ffc046ea8da8d440e14f01d5",
            "6c3b16d1e63b4e0fa247c070cf051b57",
            "11995d80b6b8456d8ee55bb77d1fd2a4",
            "27ad8d21884d42c7842b851d67f40628",
            "dd7ada0c3a4542569969bc637751d6dc",
            "ca7baa9771bd4eda817a46fb5c568a6f",
            "24e6de95491a4b78ad74dd35ee9f8383",
            "c48f0a0011014b458aec84f9590c9509",
            "54efa63a0e5d408c90f47af1c28bf21b",
            "513035c17d1b4732a8383a852da1e225",
            "ebe4a6eadb6b4a8dae958ced695c3406",
            "fd3cce7b50a14274a25f58d4399018ec",
            "0634e8bd40134ab4b51aeccf75398940",
            "5fc2aae072a349d08aca9ad67160728d",
            "19b9957703a24b17bfae5b49bb9652be",
            "9c43efcbf7ce4e87909d3d9efe31221a",
            "5336441146864144a3720f1bd0d4684f",
            "9d02391121e34cffbaed2bce280bbda6",
            "ae2f2065457e425ea1b873332b3112e8",
            "33a6291177a24688afc476a20f26711d",
            "1fbf2060e60a41aeb15553f0ea0b9cde",
            "35da2736df6847eeaa13e2dbf1b04c75",
            "494e4ccc640d442eba994d5092bb2430"
          ]
        },
        "id": "CGrZK-ZOYm9d",
        "outputId": "e6c669dd-1b31-4b83-defe-b447c5a773ae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17d2669800954ad6bf0040b3c467acd7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJWLMZOtajwv",
        "outputId": "0cf971a6-639a-41a2-e167-131ef8506c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gELArSY2c6I5",
        "outputId": "0773f18e-1171-44bf-bea7-60f6a1cee347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lf0LttlIck2r"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tS7VDH-Acnyg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Manually set the token\n",
        "hf_token = \"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\"\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "\n",
        "# Use the token to authenticate\n",
        "api = HfApi()\n",
        "#api.set_access_token(hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5Jp7Ybv9dGO4",
        "outputId": "b621ee45-778f-4bb4-c528-d51bbb4edd44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "os.getenv(\"HF_TOKEN\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2jJQAet9DezW"
      },
      "outputs": [],
      "source": [
        "llama_tokenizer_dir = \"TinyPixel/Llama-2-7B-bf16-sharded\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CNPadJsTVqVc"
      },
      "outputs": [],
      "source": [
        "llama_tokenizer = LlamaTokenizer.from_pretrained(\"TinyPixel/Llama-2-7B-bf16-sharded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PczkkojTXcYB"
      },
      "outputs": [],
      "source": [
        "bangla_sp_model = spm.SentencePieceProcessor()\n",
        "#bangla_sp_model.Load(bangla_sp_model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fT8lPpgK_JRz",
        "outputId": "b32dc712-f4e1-4840-939c-3dc9bf888dff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Projects/bangla-llama/data/models/bangla_sp.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "bangla_sp_model_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6zbK1DYVe_O",
        "outputId": "6eca5904-5d69-482f-fc4c-e01e474a00bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "bangla_sp_model.Load(bangla_sp_model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FweCXfUrVeBE",
        "outputId": "0e8fd500-90f0-4bd8-994c-2816d1be9fda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "499723"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "llama_spm = sp_pb2_model.ModelProto()\n",
        "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAXPnz1O_REO",
        "outputId": "b48fa0ba-f6af-4e29-cd6a-2fd91b8a65b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "690477"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "bangla_spm = sp_pb2_model.ModelProto()\n",
        "bangla_spm.ParseFromString(bangla_sp_model.serialized_model_proto())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CG1rdWr_cv3",
        "outputId": "24c07ab5-feda-4e08-d06c-c3d5233ee36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000 20000\n",
            "['<s>', '</s>', '<unk>']\n",
            "[1, 2, 0]\n",
            "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
          ]
        }
      ],
      "source": [
        "print(len(llama_tokenizer), len(bangla_sp_model))\n",
        "print(llama_tokenizer.all_special_tokens)\n",
        "print(llama_tokenizer.all_special_ids)\n",
        "print(llama_tokenizer.special_tokens_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUZ_HMYwAKjw",
        "outputId": "68f38d1c-c3ff-461d-e59f-216903e8634b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000\n",
            "Before:32000\n",
            "New model pieces: 50437\n"
          ]
        }
      ],
      "source": [
        "## Add Bangla tokens to LLaMA tokenizer\n",
        "llama_spm_tokens_set = set(p.piece for p in llama_spm.pieces)\n",
        "print(len(llama_spm_tokens_set))\n",
        "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
        "for p in bangla_spm.pieces:\n",
        "    piece = p.piece\n",
        "    if piece not in llama_spm_tokens_set:\n",
        "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
        "        new_p.piece = piece\n",
        "        new_p.score = 0\n",
        "        llama_spm.pieces.append(new_p)\n",
        "print(f\"New model pieces: {len(llama_spm.pieces)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEM36bh4A_fo"
      },
      "outputs": [],
      "source": [
        "#%mkdir /content/drive/MyDrive/Projects/bangla-llama/data/models/merged_tokenizer_sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6DoLx3PBCwO"
      },
      "outputs": [],
      "source": [
        "#%mkdir /content/drive/MyDrive/Projects/bangla-llama/data/models/merged_tokenizer_hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcS_YGAOBXPA",
        "outputId": "a5cc6341-b5e4-47c2-f7a9-1ace44739555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bangla_sp.model  bangla_sp.vocab  merged_tokenizer_hf  merged_tokenizer_sp\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcszcvorBq4i",
        "outputId": "6ba76b71-9f47-4f6b-bdb5-7c89ce5c33b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bangla_llama.model  special_tokens_map.json  tokenizer_config.json  tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/models/merged_tokenizer_sp/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "wJK15mT9AgGo"
      },
      "outputs": [],
      "source": [
        "## Save\n",
        "output_sp_dir = \"/content/drive/MyDrive/Projects/bangla-llama/data/models/merged_tokenizer_sp\"\n",
        "output_hf_dir = \"/content/drive/MyDrive/Projects/bangla-llama/data/models/merged_tokenizer_sp\"  # the path to save Tamil-LLaMA tokenizer\n",
        "os.makedirs(output_sp_dir, exist_ok=True)\n",
        "with open(output_sp_dir + \"/bangla_llama.model\", \"wb\") as f:\n",
        "    f.write(llama_spm.SerializeToString())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h01YI14BCeS-",
        "outputId": "caa46008-6a9e-4f04-e7f7-35ebde999a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bangla-LLaMA tokenizer has been saved to /content/drive/MyDrive/Projects/bangla-llama/data/models/merged_tokenizer_sp\n"
          ]
        }
      ],
      "source": [
        "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir + \"/bangla_llama.model\")\n",
        "\n",
        "tokenizer.save_pretrained(output_hf_dir)\n",
        "print(f\"Bangla-LLaMA tokenizer has been saved to {output_hf_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uznXC6oMC5w1",
        "outputId": "63f7ebec-fc11-44e4-8a12-9468b5393bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '</s>', '<unk>']\n",
            "[1, 2, 0]\n",
            "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
        "bangla_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(tokenizer.special_tokens_map)\n",
        "\n",
        "text = \"\"\"আমার জন্ম বাংলাদেশে\n",
        "I was born in Bangladesh\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPEaKnCNDUyh",
        "outputId": "9b06b512-0f46-4f5d-b207-284d92653a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test text:\n",
            " আমার জন্ম বাংলাদেশে\n",
            "I was born in Bangladesh\n",
            "Tokenized by LLaMA tokenizer:['▁', '<0xE0>', '<0xA6>', '<0x86>', 'ম', 'া', 'র', '▁', 'জ', 'ন', '্', 'ম', '▁', 'ব', 'া', '<0xE0>', '<0xA6>', '<0x82>', 'ল', 'া', 'দ', 'ে', 'শ', 'ে', '<0x0A>', 'I', '▁was', '▁born', '▁in', '▁Bang', 'l', 'adesh']\n",
            "LLaMA tokenizer n_tokens=32\n",
            "Tokenized by Tamil-LLaMA tokenizer:['▁আমার', '▁জন্ম', '▁বা', 'ং', 'লা', 'দেশ', 'ে', '<0x0A>', 'I', '▁was', '▁born', '▁in', '▁Bang', 'l', 'adesh']\n",
            "Bangla LLaMA tokenizer n_tokens=15\n"
          ]
        }
      ],
      "source": [
        "print(\"Test text:\\n\", text)\n",
        "llama_tokenized = llama_tokenizer.tokenize(text)\n",
        "bangla_llama_tokenized = bangla_llama_tokenizer.tokenize(text)\n",
        "print(f\"Tokenized by LLaMA tokenizer:{llama_tokenized}\")\n",
        "print(f\"LLaMA tokenizer n_tokens={len(llama_tokenized)}\")\n",
        "print(f\"Tokenized by Tamil-LLaMA tokenizer:{bangla_llama_tokenized}\")\n",
        "print(f\"Bangla LLaMA tokenizer n_tokens={len(bangla_llama_tokenized)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretraining**"
      ],
      "metadata": {
        "id": "C15i5RNc8Cci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EHQGPgg4cY_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e854d9b-4b9e-4d47-ed7f-30e9e49cf54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12020, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import concatenate_datasets, load_dataset\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        ")\n",
        "from peft.tuners.lora import LoraLayer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    is_torch_tpu_available,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.testing_utils import CaptureLogger\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, get_last_checkpoint\n",
        "from transformers.utils import send_example_telemetry\n",
        "from transformers.utils.versions import require_version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AZgzgASnchZi"
      },
      "outputs": [],
      "source": [
        "class SavePeftModelCallback(transformers.TrainerCallback):\n",
        "    def save_model(self, args, state, kwargs):\n",
        "        if state.best_model_checkpoint is not None:\n",
        "            checkpoint_folder = os.path.join(\n",
        "                state.best_model_checkpoint, \"pt_lora_model\"\n",
        "            )\n",
        "        else:\n",
        "            checkpoint_folder = os.path.join(\n",
        "                args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\"\n",
        "            )\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"pt_lora_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "        kwargs[\"tokenizer\"].save_pretrained(peft_model_path)\n",
        "\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        self.save_model(args, state, kwargs)\n",
        "        return control\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        peft_model_path = os.path.join(args.output_dir, \"pt_lora_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "        kwargs[\"tokenizer\"].save_pretrained(peft_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IMZ6ai4lcl-d"
      },
      "outputs": [],
      "source": [
        "def prepare_model_for_kbit_training(model, use_gradient_checkpointing=True):\n",
        "    r\"\"\"\n",
        "    This method wraps the entire protocol for preparing a model before running a training. This includes:\n",
        "        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm\n",
        "        head to fp32\n",
        "\n",
        "    Args:\n",
        "        model, (`transformers.PreTrainedModel`):\n",
        "            The loaded model from `transformers`\n",
        "    \"\"\"\n",
        "    loaded_in_kbit = getattr(model, \"is_loaded_in_8bit\", False) or getattr(\n",
        "        model, \"is_loaded_in_4bit\", False\n",
        "    )\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        # freeze base model's layers\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # cast all non INT8/INT4 parameters to fp32\n",
        "    for param in model.parameters():\n",
        "        if (\n",
        "            (param.dtype == torch.float16) or (param.dtype == torch.bfloat16)\n",
        "        ) and loaded_in_kbit:\n",
        "            param.data = param.data.to(torch.float32)\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if \"norm\" in name:\n",
        "            module = module.to(torch.float32)\n",
        "\n",
        "    if loaded_in_kbit and use_gradient_checkpointing:\n",
        "        # For backward compatibility\n",
        "        if hasattr(model, \"enable_input_require_grads\"):\n",
        "            model.enable_input_require_grads()\n",
        "        else:\n",
        "\n",
        "            def make_inputs_require_grad(module, _input, output):\n",
        "                output.requires_grad_(True)\n",
        "\n",
        "            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "        # enable gradient checkpointing for memory efficiency\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-ke1S-mGcveA"
      },
      "outputs": [],
      "source": [
        "def accuracy(predictions, references, normalize=True, sample_weight=None):\n",
        "    return {\n",
        "        \"accuracy\": float(\n",
        "            accuracy_score(\n",
        "                references,\n",
        "                predictions,\n",
        "                normalize=normalize,\n",
        "                sample_weight=sample_weight,\n",
        "            )\n",
        "        )\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
        "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
        "    labels = labels[:, 1:].reshape(-1)\n",
        "    preds = preds[:, :-1].reshape(-1)\n",
        "    return accuracy(predictions=preds, references=labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PUMarGlfczSe"
      },
      "outputs": [],
      "source": [
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    if isinstance(logits, tuple):\n",
        "        # Depending on the model and config, logits may contain extra tensors,\n",
        "        # like past_key_values, but logits always come first\n",
        "        logits = logits[0]\n",
        "    return logits.argmax(dim=-1)\n",
        "\n",
        "\n",
        "def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:\n",
        "    if not isinstance(features[0], Mapping):\n",
        "        features = [vars(f) for f in features]\n",
        "    first = features[0]\n",
        "    batch = {}\n",
        "\n",
        "    # Special handling for labels.\n",
        "    # Ensure that tensor is created with the correct type\n",
        "    # (it should be automatically the case, but let's make sure of it.)\n",
        "    if \"label\" in first and first[\"label\"] is not None:\n",
        "        label = (\n",
        "            first[\"label\"].item()\n",
        "            if isinstance(first[\"label\"], torch.Tensor)\n",
        "            else first[\"label\"]\n",
        "        )\n",
        "        dtype = torch.long if isinstance(label, int) else torch.float\n",
        "        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n",
        "    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n",
        "        if isinstance(first[\"label_ids\"], torch.Tensor):\n",
        "            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n",
        "        else:\n",
        "            dtype = (\n",
        "                torch.long if isinstance(first[\"label_ids\"][0], int) else torch.float\n",
        "            )\n",
        "            batch[\"labels\"] = torch.tensor(\n",
        "                [f[\"label_ids\"] for f in features], dtype=dtype\n",
        "            )\n",
        "\n",
        "    # Handling of all other possible keys.\n",
        "    # Again, we will use the first element to figure out which key/values are not None for this model.\n",
        "\n",
        "    try:\n",
        "        for k, v in first.items():\n",
        "            if (\n",
        "                k not in (\"label\", \"label_ids\")\n",
        "                and v is not None\n",
        "                and not isinstance(v, str)\n",
        "            ):\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    batch[k] = torch.stack([f[k] for f in features])\n",
        "                elif isinstance(v, np.ndarray):\n",
        "                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n",
        "                else:\n",
        "                    batch[k] = torch.tensor([f[k] for f in features])\n",
        "    except ValueError:  # quick fix by simply take the first example\n",
        "        for k, v in first.items():\n",
        "            if (\n",
        "                k not in (\"label\", \"label_ids\")\n",
        "                and v is not None\n",
        "                and not isinstance(v, str)\n",
        "            ):\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    batch[k] = torch.stack([features[0][k]] * len(features))\n",
        "                elif isinstance(v, np.ndarray):\n",
        "                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))\n",
        "                else:\n",
        "                    batch[k] = torch.tensor([features[0][k]] * len(features))\n",
        "\n",
        "    return batch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mKEn3W_wc82T"
      },
      "outputs": [],
      "source": [
        "MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KIELaWINc-0f"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    tokenizer_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"If training from scratch, pass a model type from the list: \"\n",
        "            + \", \".join(MODEL_TYPES)\n",
        "        },\n",
        "    )\n",
        "    config_overrides: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Override some existing default config settings when a model is trained from scratch. Example: \"\n",
        "                \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Pretrained config name or path if not the same as model_name\"\n",
        "        },\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
        "        },\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"\n",
        "        },\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\n",
        "            \"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"\n",
        "        },\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\n",
        "            \"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"\n",
        "        },\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    torch_dtype: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
        "                \"dtype will be automatically derived from the model's weights.\"\n",
        "            ),\n",
        "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.config_overrides is not None and (\n",
        "            self.config_name is not None or self.model_name_or_path is not None\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"--config_overrides can't be used in combination with --config_name or --model_name_or_path\"\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"The configuration name of the dataset to use (via the datasets library).\"\n",
        "        },\n",
        "    )\n",
        "    train_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
        "    )\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n",
        "    block_size: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Optional input sequence length after tokenization. \"\n",
        "                \"The training dataset will be truncated in block of this size for training. \"\n",
        "                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n",
        "    )\n",
        "    validation_split_percentage: Optional[float] = field(\n",
        "        default=0.05,\n",
        "        metadata={\n",
        "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
        "        },\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    keep_linebreaks: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"},\n",
        "    )\n",
        "    data_cache_dir: Optional[str] = field(\n",
        "        default=\"./\", metadata={\"help\": \"The datasets processed stored\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.streaming:\n",
        "            require_version(\n",
        "                \"datasets>=2.0.0\", \"The streaming feature requires `datasets>=2.0.0`\"\n",
        "            )"
      ],
      "metadata": {
        "id": "qfmpsmgX_Q7-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class MyTrainingArguments(TrainingArguments):\n",
        "    trainable: Optional[str] = field(default=\"q_proj,v_proj\")\n",
        "    lora_rank: Optional[int] = field(default=8)\n",
        "    lora_dropout: Optional[float] = field(default=0.1)\n",
        "    lora_alpha: Optional[float] = field(default=32.0)\n",
        "    modules_to_save: Optional[str] = field(default=None)\n",
        "    debug_mode: Optional[bool] = field(default=False)\n",
        "    peft_path: Optional[str] = field(default=None)\n",
        "    flash_attn: Optional[bool] = field(default=False)\n",
        "    double_quant: Optional[bool] = field(default=True)\n",
        "    quant_type: Optional[str] = field(default=\"nf4\")\n",
        "    load_in_kbits: Optional[int] = field(default=16)\n",
        "    use_mixed_precision: Optional[bool] = field(default=False)  # Add this line\n",
        "    per_device_train_batch_size: Optional[int] = field(default=8) # Default batch size\n",
        "    per_device_eval_batch_size: Optional[int] = field(default=8) # Default batch size\n",
        "    gradient_accumulation_steps: Optional[int] = field(default=8) # Default gradient accumulation steps\n"
      ],
      "metadata": {
        "id": "g23SfIo2_TB0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "61gt89vAoLdf"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6D0ZVICrQdy",
        "outputId": "289192fe-f529-4eb4-9f0c-21ceefcebf2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bn_part_00000.parquet  bn_part_00005.parquet  bn_part_00010.parquet  bn_part_00015.parquet\n",
            "bn_part_00001.parquet  bn_part_00006.parquet  bn_part_00011.parquet  bn_part_00016.parquet\n",
            "bn_part_00002.parquet  bn_part_00007.parquet  bn_part_00012.parquet  bn_part_00017.parquet\n",
            "bn_part_00003.parquet  bn_part_00008.parquet  bn_part_00013.parquet\n",
            "bn_part_00004.parquet  bn_part_00009.parquet  bn_part_00014.parquet\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/uonlp/culturaX/bn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuXeoIJIrTHE",
        "outputId": "a1e11a6f-9732-4e75-a8ba-047ec366e4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bangla_llama.model  special_tokens_map.json  tokenizer_config.json  tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/models/merged_tokenizer_sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JBq3x7G-rYnN"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bTVW5U4yrcy2"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/Projects/bangla-llama/data/output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xdo0QG_7sgMq"
      },
      "outputs": [],
      "source": [
        "# Below code is based on https://github.com/lm-sys/FastChat/blob/main/fastchat/train/llama_flash_attn_monkey_patch.py.\n",
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
        "\n",
        "from einops import rearrange\n",
        "try:\n",
        "    from flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func\n",
        "    from flash_attn.bert_padding import unpad_input, pad_input\n",
        "except ImportError:\n",
        "    raise ImportError(\n",
        "        \"FlashAttention-2 is not installed correctly. Please check the usage in https://github.com/Dao-AILab/flash-attention for more details.\"\n",
        "    )\n",
        "\n",
        "def forward(\n",
        "    self,\n",
        "    hidden_states: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor] = None,\n",
        "    position_ids: Optional[torch.Tensor] = None,\n",
        "    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "    output_attentions: bool = False,\n",
        "    use_cache: bool = False,\n",
        ") -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "    \"\"\"Input shape: Batch x Time x Channel\n",
        "\n",
        "    attention_mask: [bsz, q_len]\n",
        "    \"\"\"\n",
        "    bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "    query_states = (\n",
        "        self.q_proj(hidden_states)\n",
        "        .view(bsz, q_len, self.num_heads, self.head_dim)\n",
        "        .transpose(1, 2)\n",
        "    )\n",
        "    key_states = (\n",
        "        self.k_proj(hidden_states)\n",
        "        .view(bsz, q_len, self.num_heads, self.head_dim)\n",
        "        .transpose(1, 2)\n",
        "    )\n",
        "    value_states = (\n",
        "        self.v_proj(hidden_states)\n",
        "        .view(bsz, q_len, self.num_heads, self.head_dim)\n",
        "        .transpose(1, 2)\n",
        "    )\n",
        "    # [bsz, q_len, nh, hd]\n",
        "    # [bsz, nh, q_len, hd]\n",
        "\n",
        "    kv_seq_len = key_states.shape[-2]\n",
        "    assert past_key_value is None, \"past_key_value is not supported\"\n",
        "\n",
        "    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
        "    query_states, key_states = apply_rotary_pos_emb(\n",
        "        query_states, key_states, cos, sin, position_ids\n",
        "    )\n",
        "    # [bsz, nh, t, hd]\n",
        "    assert not output_attentions, \"output_attentions is not supported\"\n",
        "    assert not use_cache, \"use_cache is not supported\"\n",
        "\n",
        "    # Flash attention codes from\n",
        "    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n",
        "\n",
        "    # transform the data into the format required by flash attention\n",
        "    qkv = torch.stack(\n",
        "        [query_states, key_states, value_states], dim=2\n",
        "    )  # [bsz, nh, 3, q_len, hd]\n",
        "    qkv = qkv.transpose(1, 3)  # [bsz, q_len, 3, nh, hd]\n",
        "    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n",
        "    # the attention_mask should be the same as the key_padding_mask\n",
        "    key_padding_mask = attention_mask\n",
        "\n",
        "    if key_padding_mask is None:\n",
        "        qkv = rearrange(qkv, \"b s ... -> (b s) ...\")\n",
        "        max_s = q_len\n",
        "        cu_q_lens = torch.arange(\n",
        "            0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32, device=qkv.device\n",
        "        )\n",
        "        output = flash_attn_varlen_qkvpacked_func(\n",
        "            qkv, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n",
        "        )\n",
        "        output = rearrange(output, \"(b s) ... -> b s ...\", b=bsz)\n",
        "    else:\n",
        "        nheads = qkv.shape[-2]\n",
        "        x = rearrange(qkv, \"b s three h d -> b s (three h d)\")\n",
        "        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n",
        "        x_unpad = rearrange(\n",
        "            x_unpad, \"nnz (three h d) -> nnz three h d\", three=3, h=nheads\n",
        "        )\n",
        "        output_unpad = flash_attn_varlen_qkvpacked_func(\n",
        "            x_unpad, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n",
        "        )\n",
        "        output = rearrange(\n",
        "            pad_input(\n",
        "                rearrange(output_unpad, \"nnz h d -> nnz (h d)\"), indices, bsz, q_len\n",
        "            ),\n",
        "            \"b s (h d) -> b s h d\",\n",
        "            h=nheads,\n",
        "        )\n",
        "    return self.o_proj(rearrange(output, \"b s h d -> b s (h d)\")), None, None\n",
        "\n",
        "\n",
        "# Disable the transformation of the attention mask in LlamaModel as the flash attention\n",
        "# requires the attention mask to be the same as the key_padding_mask\n",
        "def _prepare_decoder_attention_mask(\n",
        "    self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "):\n",
        "    # [bsz, seq_len]\n",
        "    return attention_mask\n",
        "\n",
        "\n",
        "def replace_llama_attn_with_flash_attn():\n",
        "    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = (\n",
        "        _prepare_decoder_attention_mask\n",
        "    )\n",
        "    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = HfArgumentParser(\n",
        "        (ModelArguments, DataTrainingArguments, MyTrainingArguments)\n",
        "    )\n",
        "if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "    # If we pass only one argument to the script and it's the path to a json file,\n",
        "    # let's parse it to get our arguments.\n",
        "    model_args, data_args, training_args = parser.parse_json_file(\n",
        "        json_file=os.path.abspath(sys.argv[1])\n",
        "    )\n",
        "else:\n",
        "    random_seed = 1357\n",
        "    cmd_line_args = [\n",
        "      \"--model_name_or_path\", \"TinyPixel/Llama-2-7B-bf16-sharded\",\n",
        "      \"--tokenizer_name_or_path\", \"/content/drive/MyDrive/Projects/bangla-llama/data/models/merged_tokenizer_sp\",\n",
        "      \"--dataset_dir\", \"/content/drive/MyDrive/Projects/bangla-llama/data/uonlp/culturaX/bn\",\n",
        "      \"--data_cache_dir\", \"/content/drive/MyDrive/Projects/bangla-llama/data\",\n",
        "      \"--validation_split_percentage\", \"0.1\",\n",
        "      \"--per_device_train_batch_size\", \"8\",\n",
        "      \"--do_train\",\n",
        "      \"--seed\", str(random_seed),\n",
        "      #\"--fp16\",\n",
        "      \"--num_train_epochs\", \"1\",\n",
        "      \"--lr_scheduler_type\", \"cosine\",\n",
        "      \"--learning_rate\", \"2e-4\",\n",
        "      \"--warmup_ratio\", \"0.05\",\n",
        "      \"--weight_decay\", \"0.01\",\n",
        "      \"--logging_strategy\", \"steps\",\n",
        "      \"--logging_steps\", \"10\",\n",
        "      \"--save_strategy\", \"steps\",\n",
        "      \"--save_total_limit\", \"1\",\n",
        "      \"--save_steps\", \"50\",\n",
        "      \"--gradient_accumulation_steps\", \"2\",\n",
        "      \"--preprocessing_num_workers\", \"8\",\n",
        "      \"--block_size\", \"512\",\n",
        "      \"--output_dir\", \"/content/drive/MyDrive/Projects/bangla-llama/data/output\",\n",
        "      \"--overwrite_output_dir\",\n",
        "      \"--ddp_timeout\", \"30000\",\n",
        "      \"--logging_first_step\", \"True\",\n",
        "      \"--lora_rank\", \"4\",\n",
        "      \"--double_quant\", \"True\",\n",
        "      \"--lora_alpha\", \"128.0\",\n",
        "      \"--trainable\", \"q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj\",\n",
        "      \"--lora_dropout\", \"0.05\",\n",
        "      \"--modules_to_save\", \"embed_tokens,lm_head\",\n",
        "      \"--torch_dtype\", \"float16\",\n",
        "      \"--gradient_checkpointing\",\n",
        "      \"--ddp_find_unused_parameters\", \"False\",\n",
        "      \"--flash_attn\", \"True\"\n",
        "\n",
        "      #\"--quant_type\", \"nf4\",\n",
        "      #\"--load_in_kbits\", \"16\",\n",
        "    # Add other command line arguments as needed\n",
        "  ]\n",
        "\n",
        "model_args, data_args, training_args = parser.parse_args_into_dataclasses(cmd_line_args)\n"
      ],
      "metadata": {
        "id": "L1hAWAvMqWzZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if training_args.flash_attn:\n",
        "    #from flash_attn_patch import replace_llama_attn_with_flash_attn\n",
        "\n",
        "    replace_llama_attn_with_flash_attn()\n",
        "\n",
        "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "send_example_telemetry(\"run_clm\", model_args, data_args)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,  # if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")"
      ],
      "metadata": {
        "id": "ZnC9wSAnqfch"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
        "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
        "send_example_telemetry(\"run_clm\", model_args, data_args)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,  # if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")\n",
        "\n",
        "if training_args.should_log:\n",
        "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
        "    transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "log_level = training_args.get_process_log_level()\n",
        "logger.setLevel(log_level)\n",
        "datasets.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.enable_default_handler()\n",
        "transformers.utils.logging.enable_explicit_format()\n",
        "# transformers.tokenization_utils.logging.set_verbosity_warning()\n",
        "\n",
        "if training_args.flash_attn:\n",
        "    logger.info(\"Using Flash Attention!\")\n",
        "\n",
        "# Log on each process the small summary:\n",
        "logger.warning(\n",
        "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7daiCWRmqv4H",
        "outputId": "e6d588cb-b3b1-4649-bce7-6ef644baf61d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Using Flash Attention!\n",
            "WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log on each process the small summary:\n",
        "logger.warning(\n",
        "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        ")\n",
        "\n",
        "# Detecting last checkpoint.\n",
        "last_checkpoint = None\n",
        "if (\n",
        "    os.path.isdir(training_args.output_dir)\n",
        "    and training_args.do_train\n",
        "    and not training_args.overwrite_output_dir\n",
        "):\n",
        "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
        "            \"Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "    elif (\n",
        "        last_checkpoint is not None and training_args.resume_from_checkpoint is None\n",
        "    ):\n",
        "        logger.info(\n",
        "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
        "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL9E1JKNrGjz",
        "outputId": "fa96a717-aa4d-4bba-8c59-b422b53d0bf2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed before initializing model.\n",
        "set_seed(training_args.seed)\n",
        "\n",
        "config_kwargs = {\n",
        "    \"cache_dir\": model_args.cache_dir,\n",
        "    \"revision\": model_args.model_revision,\n",
        "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
        "}\n",
        "if model_args.config_name:\n",
        "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
        "elif model_args.model_name_or_path:\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path, **config_kwargs\n",
        "    )\n",
        "else:\n",
        "    config = CONFIG_MAPPING[model_args.model_type]()\n",
        "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "    if model_args.config_overrides is not None:\n",
        "        logger.info(f\"Overriding config: {model_args.config_overrides}\")\n",
        "        config.update_from_string(model_args.config_overrides)\n",
        "        logger.info(f\"New config: {config}\")\n",
        "\n",
        "tokenizer_kwargs = {\n",
        "    \"cache_dir\": model_args.cache_dir,\n",
        "    \"use_fast\": model_args.use_fast_tokenizer,\n",
        "    \"revision\": model_args.model_revision,\n",
        "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
        "}\n",
        "if model_args.tokenizer_name:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name, **tokenizer_kwargs\n",
        "    )\n",
        "elif model_args.tokenizer_name_or_path:\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name_or_path, **tokenizer_kwargs\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "    )\n",
        "tokenizer.add_eos_token = True\n",
        "\n",
        "# Preprocessing the datasets.\n",
        "# First we tokenize all the texts.\n",
        "# since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
        "tok_logger = transformers.utils.logging.get_logger(\n",
        "    \"transformers.tokenization_utils_base\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrwieFtdrYb8",
        "outputId": "dfc54ee9-f37a-4601-dc44-ddca28020527"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:712] 2024-01-16 16:13:52,368 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyPixel--Llama-2-7B-bf16-sharded/snapshots/3f5d08bf8c31192686e3e88d0b9d2cdeff4115e4/config.json\n",
            "[INFO|configuration_utils.py:768] 2024-01-16 16:13:52,371 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"TinyPixel/Llama-2-7B-bf16-sharded\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1837] 2024-01-16 16:13:52,375 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:1837] 2024-01-16 16:13:52,376 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2024-01-16 16:13:52,377 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1837] 2024-01-16 16:13:52,378 >> loading file tokenizer_config.json\n",
            "[WARNING|logging.py:295] 2024-01-16 16:13:52,382 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    with CaptureLogger(tok_logger) as cl:\n",
        "        output = tokenizer(examples[\"text\"])\n",
        "    # clm input could be much much longer than block_size\n",
        "    if \"Token indices sequence length is longer than the\" in cl.out:\n",
        "        tok_logger.warning(\n",
        "            \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
        "            \" before being passed to the model.\"\n",
        "        )\n",
        "    return output"
      ],
      "metadata": {
        "id": "yAD_YCZTx3kz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if data_args.block_size is None:\n",
        "    block_size = tokenizer.model_max_length\n",
        "    if block_size > 1024:\n",
        "        logger.warning(\n",
        "            \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n",
        "            \" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n",
        "            \" override this default with `--block_size xxx`.\"\n",
        "        )\n",
        "        block_size = 1024\n",
        "else:\n",
        "    if data_args.block_size > tokenizer.model_max_length:\n",
        "        logger.warning(\n",
        "            f\"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model\"\n",
        "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
        "        )\n",
        "    block_size = min(data_args.block_size, tokenizer.model_max_length)"
      ],
      "metadata": {
        "id": "PdkGNcarx6Kk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "9b1JPwCwrjDP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_record(record):\n",
        "    record['timestamp'] = [record['timestamp']] if record['timestamp'] is not None else []\n",
        "    record['url'] = [record['url']] if record['url'] is not None else []\n",
        "    record['source'] = [record['source']] if record['source'] is not None else []\n",
        "    record['labels'] = record.get('labels', [])\n",
        "    return record"
      ],
      "metadata": {
        "id": "3Bm7suFV31CA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with training_args.main_process_first(desc=\"dataset map tokenization and grouping\"):\n",
        "    lm_datasets = []\n",
        "    path = Path(data_args.dataset_dir)\n",
        "    files = [file.name for file in path.glob(\"*.parquet\")]\n",
        "    if training_args.debug_mode is True:\n",
        "        files = [files[0]]\n",
        "    for idx, file in enumerate(files):\n",
        "        data_file = os.path.join(path, file)\n",
        "        filename = \"\".join(file.split(\".\")[:-1])\n",
        "        print(filename)\n",
        "        cache_path = os.path.join(\n",
        "            data_args.data_cache_dir, filename + f\"_{block_size}\"\n",
        "        )\n",
        "        print(cache_path)\n",
        "        os.makedirs(cache_path, exist_ok=True)\n",
        "        try:\n",
        "            processed_dataset = datasets.load_from_disk(\n",
        "                cache_path, keep_in_memory=False\n",
        "            )\n",
        "            logger.info(f\"training datasets-{filename} has been loaded from disk\")\n",
        "        except Exception:\n",
        "            cache_dir = os.path.join(\n",
        "                data_args.data_cache_dir, filename + f\"_text_{block_size}\"\n",
        "            )\n",
        "            os.makedirs(cache_dir, exist_ok=True)\n",
        "            raw_dataset = load_dataset(\n",
        "                \"parquet\",\n",
        "                data_files=data_file,\n",
        "                cache_dir=cache_dir,\n",
        "                keep_in_memory=False,\n",
        "            )\n",
        "            logger.info(f\"{file} has been loaded\")\n",
        "            tokenized_dataset = raw_dataset.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                num_proc=data_args.preprocessing_num_workers,\n",
        "                remove_columns=\"text\",\n",
        "                load_from_cache_file=True,\n",
        "                keep_in_memory=False,\n",
        "                cache_file_names={\n",
        "                    k: os.path.join(cache_dir, \"tokenized.arrow\")\n",
        "                    for k in raw_dataset\n",
        "                },\n",
        "                desc=\"Running tokenizer on dataset\",\n",
        "            )\n",
        "            grouped_datasets = tokenized_dataset.map(\n",
        "                group_texts,\n",
        "                batched=True,\n",
        "                num_proc=data_args.preprocessing_num_workers,\n",
        "                load_from_cache_file=True,\n",
        "                keep_in_memory=False,\n",
        "                cache_file_names={\n",
        "                    k: os.path.join(cache_dir, \"grouped.arrow\")\n",
        "                    for k in tokenized_dataset\n",
        "                },\n",
        "                desc=f\"Grouping texts in chunks of {block_size}\",\n",
        "            )\n",
        "            processed_dataset = grouped_datasets\n",
        "            processed_dataset.save_to_disk(cache_path)\n",
        "        if idx == 0:\n",
        "            lm_datasets = processed_dataset[\"train\"]\n",
        "        else:\n",
        "            if lm_datasets.features.type != processed_dataset[\"train\"].features.type:\n",
        "              print(lm_datasets.features.type)\n",
        "              print(processed_dataset[\"train\"].features.type)\n",
        "\n",
        "              print(\"Converting to match types\")\n",
        "\n",
        "\n",
        "              print(\"Before transformation:\", processed_dataset[\"train\"].features)\n",
        "              processed_dataset['train'] = processed_dataset['train'].map(transform_record)\n",
        "              print(\"After transformation:\", processed_dataset[\"train\"].features)\n",
        "\n",
        "              continue\n",
        "              # assert (\n",
        "              #     lm_datasets.features.type\n",
        "              #     == processed_dataset[\"train\"].features.type\n",
        "              # )\n",
        "            lm_datasets = concatenate_datasets(\n",
        "                [lm_datasets, processed_dataset[\"train\"]]\n",
        "            )\n",
        "    lm_datasets = lm_datasets.train_test_split(\n",
        "        test_size=data_args.validation_split_percentage\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7J9QdyQru2W",
        "outputId": "2cce5ddf-c6f7-48bf-b0aa-25cd2659fb8b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:training datasets-bn_part_00007 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00003 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00013 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00006 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00014 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00016 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00009 has been loaded from disk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bn_part_00007\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00007_512\n",
            "bn_part_00003\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00003_512\n",
            "bn_part_00013\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00013_512\n",
            "bn_part_00006\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00006_512\n",
            "bn_part_00014\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00014_512\n",
            "bn_part_00016\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00016_512\n",
            "bn_part_00009\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00009_512\n",
            "bn_part_00002\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00002_512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:training datasets-bn_part_00002 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00004 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00012 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00005 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00008 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00001 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00000 has been loaded from disk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bn_part_00004\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00004_512\n",
            "bn_part_00012\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00012_512\n",
            "bn_part_00005\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00005_512\n",
            "bn_part_00008\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00008_512\n",
            "bn_part_00001\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00001_512\n",
            "bn_part_00000\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00000_512\n",
            "bn_part_00011\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00011_512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:training datasets-bn_part_00011 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00010 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00015 has been loaded from disk\n",
            "INFO:__main__:training datasets-bn_part_00017 has been loaded from disk\n",
            "Loading cached split indices for dataset at /content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00007_512/train/cache-1d62d82181d3540a.arrow and /content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00007_512/train/cache-827737c30f255c9c.arrow\n",
            "INFO:datasets.arrow_dataset:Loading cached split indices for dataset at /content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00007_512/train/cache-1d62d82181d3540a.arrow and /content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00007_512/train/cache-827737c30f255c9c.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bn_part_00010\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00010_512\n",
            "bn_part_00015\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00015_512\n",
            "bn_part_00017\n",
            "/content/drive/MyDrive/Projects/bangla-llama/data/bn_part_00017_512\n",
            "struct<timestamp: list<item: string>, url: list<item: string>, source: list<item: string>, input_ids: list<item: int32>, attention_mask: list<item: int8>, labels: list<item: int64>>\n",
            "struct<timestamp: string, url: string, source: string, input_ids: list<item: int32>, attention_mask: list<item: int8>>\n",
            "Converting to match types\n",
            "Before transformation: {'timestamp': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
            "After transformation: {'timestamp': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if training_args.do_train:\n",
        "    train_dataset = lm_datasets[\"train\"]\n",
        "    if data_args.max_train_samples is not None:\n",
        "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "        train_dataset = train_dataset.select(range(max_train_samples))\n",
        "    logger.info(f\"Num train_samples  {len(train_dataset)}\")\n",
        "    logger.info(\"Training example:\")\n",
        "    logger.info(tokenizer.decode(train_dataset[0][\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLpGSlDGyFb0",
        "outputId": "a7ec1f60-f268-400f-c7aa-9d0f6983fa06"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Num train_samples  393071\n",
            "INFO:__main__:Training example:\n",
            "INFO:__main__:কিশোরদের জন্য রচিত একটি বই। একে থ্রিলার কিংবা কিশোর উপন্যাস অথবা এডভেঞ্চার জনরাতে ফেলতে পারেন। ১৯৩৭ সালে রচিত এই বইটি ১৯৫৯ সালে আসে সিনেমার পর্দায়। ঋত্বিক ঘটকের চিত্রনাট্যে এই সিনেমায় বেশ কিছু পরিবর্তন আসলেও মূলভাব একই ছিল।\n",
            "বইটিতে কাঞ্চনের বয়স ১৩ বা ১৪ দেখানো হয়, সিনেমার গল্প অনুযায়ী সেটা ৮ বা ৯ বছর। দুটো ক্ষেত্রেই বাবার ভয়ে বাড়ি ছেড়ে পালায় কাঞ্চন। কলকাতায় তার সাথে ঘটা ঘটনাগুলো কম-বেশি প্রায় একই ছিল। তবে বই আর সিনেমা যদি আলাদা করে পড়েন আর দেখেন, বেশ কিছু পার্থক্য চোখে পড়বে। এত বছর পর সেই সিনেমার খুব ভালো সংস্করণ মেলে না। ইউটিউবে মিলবে এই অদ্ভুত সুন্দর সিনেমাটি। দেড় ঘন্টা সময় কীভাবে চলে যাবে টেরও পাবেন না।\n",
            "মিনি আর কাঞ্চন; Image Courtesy: Bari Thekey Paliye [Runaway] (1959) | Ritwik Ghatak | Restored\n",
            "দুটো ক্ষেত্রেই কাঞ্চন কলকাতায় গিয়ে নিজের জীবনকে বুঝতে শিখেছে। উঁচুতলার মানুষ থেকে একেবারে নিচুতলার মানুষ, স্বদেশী আন্দোলনের কর্মীদের মতো কিছু মানুষের জীবন, রেসকোর্স ময়দানে কীভাবে ঘোড়দৌড় হয়, কীভাবে একজন অসহায় মানুষের দিন কাটে- সেসব কাঞ্চন দেখেছে। তাকে পুরো সময় এক বুলবুল ভাজাওয়ালা সাহায্য করেছিল। তবে তা বইয়ে নাকি সিনেমায় সেটা আমরা রহস্যই রেখে দিচ্ছি।\n",
            "বইটি রচিত ত্রিশ বা চল্লিশের দশকের দিকে। সেখানে কলকাতার কিছু চিত্র এমনভাবে দেখানো হয়েছে যে মনে হচ্ছিল এটা ২০২১-এর এক গল্প। শিবরাম চক্রবর্তীর লেখায় যেমন জাদু আছে, তেমনই এক সম্মোহনী ক্ষমতা আছে ঋত্বিক ঘটকেরও।\n",
            "সলিল চৌধুরী ছিলেন সুরকার। এই সিনেমার প্রতিটি গান, সুর এত অদ্ভুত রকমের, শোনার সাথে সাথে একটা মন খারাপ অনুভূতি কাজ করে। প্রত্যেকে এত সুন্দর অভিনয় করেছে। সে কাঞ্চন হোক, মিনি, বিনোদ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if training_args.do_eval:\n",
        "    eval_dataset = lm_datasets[\"test\"]\n",
        "    if data_args.max_eval_samples is not None:\n",
        "        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "        eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "    logger.info(f\"Num eval_samples  {len(eval_dataset)}\")\n",
        "    logger.info(\"Evaluation example:\")\n",
        "    logger.info(tokenizer.decode(eval_dataset[0][\"input_ids\"]))\n",
        "compute_dtype = (\n",
        "    torch.float16\n",
        "    if training_args.fp16\n",
        "    else (torch.bfloat16 if training_args.bf16 else torch.float32)\n",
        ")\n"
      ],
      "metadata": {
        "id": "j8V15sdMsBVL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if training_args.load_in_kbits in [4, 8]:\n",
        "    load_in_4bit = training_args.load_in_kbits == 4\n",
        "    load_in_8bit = training_args.load_in_kbits == 8\n",
        "    if training_args.modules_to_save is not None:\n",
        "        load_in_8bit_skip_modules = training_args.modules_to_save.split(\",\")\n",
        "    else:\n",
        "        load_in_8bit_skip_modules = None\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=training_args.load_in_kbits == 4,\n",
        "        load_in_8bit=training_args.load_in_kbits == 8,\n",
        "        llm_int8_threshold=6.0,\n",
        "        load_in_8bit_skip_modules=load_in_8bit_skip_modules,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=training_args.double_quant,\n",
        "        bnb_4bit_quant_type=training_args.quant_type,  # {'fp4', 'nf4'}\n",
        "    )\n",
        "else:\n",
        "    load_in_4bit = False\n",
        "    load_in_8bit = False\n",
        "    quantization_config = None\n",
        "\n",
        "if quantization_config is not None:\n",
        "    logger.info(f\"quantization_config:{quantization_config.to_dict()}\")\n"
      ],
      "metadata": {
        "id": "u7aALS6O66cr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_args.model_name_or_path:\n",
        "    torch_dtype = (\n",
        "        model_args.torch_dtype\n",
        "        if model_args.torch_dtype in [\"auto\", None]\n",
        "        else getattr(torch, model_args.torch_dtype)\n",
        "    )\n",
        "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "        torch_dtype=torch_dtype,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=device_map,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        load_in_8bit=load_in_8bit,\n",
        "        quantization_config=quantization_config,\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_config(config)\n",
        "    n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
        "    logger.info(\n",
        "        f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480,
          "referenced_widgets": [
            "58a22a03a8a24d47b54495fddf43f6ee",
            "fb39fd256c1c411a868a69a9265fd7ea",
            "7621e18f25d147cfb6178a974a405857",
            "1b4db8be7c034e8186ef10807c26efef",
            "43f0ceea73e646d9968a6236739c5118",
            "ca7bdcb1ee0c431fbbd74c9c93349d53",
            "098866be491543968a6086d4bae19029",
            "51798017610640e490331f20a059776f",
            "0ef577fcc6724b92b157fb3b86242ff4",
            "d4fab19eea894e66b56afa5e7fc44428",
            "a4bf10a93e7042919112801662b53ec4"
          ]
        },
        "id": "EUixK2rV7BhR",
        "outputId": "3ad9af97-3cbe-4243-c97b-cf1cab21efe3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modeling_utils.py:2603] 2024-01-16 16:14:21,011 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TinyPixel--Llama-2-7B-bf16-sharded/snapshots/3f5d08bf8c31192686e3e88d0b9d2cdeff4115e4/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1172] 2024-01-16 16:14:21,014 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:599] 2024-01-16 16:14:21,016 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58a22a03a8a24d47b54495fddf43f6ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|modeling_utils.py:3329] 2024-01-16 16:14:31,977 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:3337] 2024-01-16 16:14:31,978 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyPixel/Llama-2-7B-bf16-sharded.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:561] 2024-01-16 16:14:32,252 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TinyPixel--Llama-2-7B-bf16-sharded/snapshots/3f5d08bf8c31192686e3e88d0b9d2cdeff4115e4/generation_config.json\n",
            "[INFO|configuration_utils.py:599] 2024-01-16 16:14:32,253 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if training_args.load_in_kbits in [4, 8]:\n",
        "    model = prepare_model_for_kbit_training(\n",
        "        model, use_gradient_checkpointing=training_args.gradient_checkpointing\n",
        "    )\n",
        "model.config.use_cache = False\n",
        "model_vocab_size = model.get_output_embeddings().weight.size(0)\n",
        "tokenizer_vocab_size = len(tokenizer)\n",
        "\n",
        "logger.info(f\"Model vocab size: {model_vocab_size}\")\n",
        "logger.info(f\"Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
        "\n",
        "if model_vocab_size != tokenizer_vocab_size:\n",
        "    logger.info(f\"Resize model vocab size to {tokenizer_vocab_size}\")\n",
        "    model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGzhFv7u7I2y",
        "outputId": "66ab68fa-f54c-4d02-a609-4558ac79d1c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Model vocab size: 32000\n",
            "INFO:__main__:Tokenizer vocab size: 50437\n",
            "INFO:__main__:Resize model vocab size to 50437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if training_args.peft_path is not None:\n",
        "    logger.info(\"Peft from pre-trained model\")\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model, training_args.peft_path, device_map=device_map\n",
        "    )\n",
        "else:\n",
        "    logger.info(\"Init new peft model\")\n",
        "    target_modules = training_args.trainable.split(\",\")\n",
        "    modules_to_save = training_args.modules_to_save\n",
        "    if modules_to_save is not None:\n",
        "        modules_to_save = modules_to_save.split(\",\")\n",
        "    lora_rank = training_args.lora_rank\n",
        "    lora_dropout = training_args.lora_dropout\n",
        "    lora_alpha = training_args.lora_alpha\n",
        "    logger.info(f\"target_modules: {target_modules}\")\n",
        "    logger.info(f\"lora_rank: {lora_rank}\")\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        target_modules=target_modules,\n",
        "        inference_mode=False,\n",
        "        r=lora_rank,\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=lora_dropout,\n",
        "        modules_to_save=modules_to_save,\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT8pGCpK7Ord",
        "outputId": "e3da795d-ce05-41ee-9643-ce784ccf947e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Init new peft model\n",
            "INFO:__main__:target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
            "INFO:__main__:lora_rank: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if training_args.gradient_checkpointing and (\n",
        "    not model.modules_to_save or \"embed_tokens\" not in model.modules_to_save\n",
        "):\n",
        "    # enable requires_grad to avoid exception during backward pass when using gradient_checkpoint without tuning embed.\n",
        "    if hasattr(model.base_model, \"enable_input_require_grads\"):\n",
        "        model.base_model.enable_input_require_grads()\n",
        "    elif hasattr(model.base_model, \"get_input_embeddings\"):\n",
        "\n",
        "        def make_inputs_require_grad(_module, _input, _output):\n",
        "            _output.requires_grad_(True)\n",
        "\n",
        "        model.base_model.get_input_embeddings().register_forward_hook(\n",
        "            make_inputs_require_grad\n",
        "        )\n"
      ],
      "metadata": {
        "id": "UMc8Q6Sh7RP5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, LoraLayer):\n",
        "        if training_args.bf16:\n",
        "            module = module.to(torch.bfloat16)\n",
        "        if training_args.fp16:\n",
        "            module = module.to(torch.float16)\n",
        "    if \"norm\" in name:\n",
        "        module = module.to(torch.float16)\n",
        "    if \"lm_head\" in name or \"embed_tokens\" in name:\n",
        "        if hasattr(module, \"weight\"):\n",
        "            if training_args.bf16 and module.weight.dtype == torch.float32:\n",
        "                module = module.to(torch.bfloat16)\n",
        "            if training_args.fp16 and module.weight.dtype == torch.float32:\n",
        "                module = module.to(torch.float16)\n",
        "model.print_trainable_parameters()\n",
        "logger.info(f\"model.modules_to_save: {model.modules_to_save}\")\n",
        "old_state_dict = model.state_dict\n",
        "model.state_dict = (\n",
        "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
        ").__get__(model, type(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So5uOdow7TNz",
        "outputId": "29a8c7c3-534b-4315-805b-81c3e18ced6b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:model.modules_to_save: {'lm_head', 'embed_tokens'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 423,174,144 || all params: 7,312,625,664 || trainable%: 5.786897394232595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=fault_tolerance_data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        "    if training_args.do_eval and not is_torch_tpu_available()\n",
        "    else None,\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
        "    if training_args.do_eval and not is_torch_tpu_available()\n",
        "    else None,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsnwd2ZYsM7o",
        "outputId": "815cb25a-68a4-4e02-9f44-b53afe24446a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|trainer.py:386] 2024-01-16 16:15:19,134 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
            "[codecarbon INFO @ 16:15:19] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 16:15:19] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 16:15:19] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 16:15:19] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 16:15:19] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon WARNING @ 16:15:20] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 16:15:20] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 16:15:20] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 16:15:20]   Platform system: Linux-6.1.58+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 16:15:20]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 16:15:20]   CodeCarbon version: 2.2.3\n",
            "[codecarbon INFO @ 16:15:20]   Available RAM : 83.477 GB\n",
            "[codecarbon INFO @ 16:15:20]   CPU count: 12\n",
            "[codecarbon INFO @ 16:15:20]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 16:15:20]   GPU count: 1\n",
            "[codecarbon INFO @ 16:15:20]   GPU model: 1 x NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args.per_device_train_batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54fKn04pAXqH",
        "outputId": "d39fcc1f-9fa6-4282-beec-0ee7bc7e2ce8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.add_callback(SavePeftModelCallback)\n",
        "# Training\n",
        "if training_args.do_train:\n",
        "    print(\"Starting training\")\n",
        "\n",
        "    checkpoint = None\n",
        "    if training_args.resume_from_checkpoint is not None:\n",
        "        checkpoint = training_args.resume_from_checkpoint\n",
        "    elif last_checkpoint is not None:\n",
        "        checkpoint = last_checkpoint\n",
        "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "\n",
        "    metrics = train_result.metrics\n",
        "\n",
        "    max_train_samples = (\n",
        "        data_args.max_train_samples\n",
        "        if data_args.max_train_samples is not None\n",
        "        else len(train_dataset)\n",
        "    )\n",
        "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "    trainer.save_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vaTOF4X_7mhe",
        "outputId": "025a4cec-67ec-4501-8b17-782f0b05f3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|trainer.py:763] 2024-01-16 16:15:28,126 >> The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: source, timestamp, url. If source, timestamp, url are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1686] 2024-01-16 16:15:28,180 >> ***** Running training *****\n",
            "[INFO|trainer.py:1687] 2024-01-16 16:15:28,181 >>   Num examples = 393,071\n",
            "[INFO|trainer.py:1688] 2024-01-16 16:15:28,182 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1689] 2024-01-16 16:15:28,183 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1692] 2024-01-16 16:15:28,185 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1693] 2024-01-16 16:15:28,186 >>   Gradient Accumulation steps = 2\n",
            "[INFO|trainer.py:1694] 2024-01-16 16:15:28,187 >>   Total optimization steps = 24,567\n",
            "[INFO|trainer.py:1695] 2024-01-16 16:15:28,197 >>   Number of trainable parameters = 423,174,144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='325' max='24567' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  325/24567 16:42 < 20:53:50, 0.32 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.689300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 16:15:43] Energy consumed for RAM : 0.000130 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:15:43] Energy consumed for all GPUs : 0.001105 kWh. Total GPU Power : 265.002 W\n",
            "[codecarbon INFO @ 16:15:43] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:15:43] 0.001412 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:15:58] Energy consumed for RAM : 0.000261 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:15:58] Energy consumed for all GPUs : 0.002197 kWh. Total GPU Power : 262.395 W\n",
            "[codecarbon INFO @ 16:15:58] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:15:58] 0.002812 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:16:13] Energy consumed for RAM : 0.000391 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:16:13] Energy consumed for all GPUs : 0.003337 kWh. Total GPU Power : 273.716 W\n",
            "[codecarbon INFO @ 16:16:13] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:16:13] 0.004259 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:16:28] Energy consumed for RAM : 0.000521 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:16:28] Energy consumed for all GPUs : 0.004402 kWh. Total GPU Power : 255.942 W\n",
            "[codecarbon INFO @ 16:16:28] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:16:28] 0.005632 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:16:43] Energy consumed for RAM : 0.000652 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:16:43] Energy consumed for all GPUs : 0.005513 kWh. Total GPU Power : 266.728 W\n",
            "[codecarbon INFO @ 16:16:43] Energy consumed for all CPUs : 0.000885 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:16:43] 0.007050 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:16:58] Energy consumed for RAM : 0.000782 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:16:58] Energy consumed for all GPUs : 0.006686 kWh. Total GPU Power : 281.597 W\n",
            "[codecarbon INFO @ 16:16:58] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:16:58] 0.008530 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:17:13] Energy consumed for RAM : 0.000912 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:17:13] Energy consumed for all GPUs : 0.007791 kWh. Total GPU Power : 265.54100000000005 W\n",
            "[codecarbon INFO @ 16:17:13] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:17:13] 0.009943 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:17:28] Energy consumed for RAM : 0.001043 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:17:28] Energy consumed for all GPUs : 0.008897 kWh. Total GPU Power : 265.589 W\n",
            "[codecarbon INFO @ 16:17:28] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:17:28] 0.011357 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:17:43] Energy consumed for RAM : 0.001173 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:17:43] Energy consumed for all GPUs : 0.009820 kWh. Total GPU Power : 221.559 W\n",
            "[codecarbon INFO @ 16:17:43] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:17:43] 0.012587 kWh of electricity used since the beginning.\n",
            "[INFO|trainer.py:2807] 2024-01-16 16:17:54,745 >> Saving model checkpoint to /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-50\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:17:54,781 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-50/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:17:54,786 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-50/special_tokens_map.json\n",
            "[codecarbon INFO @ 16:17:59] Energy consumed for RAM : 0.001307 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:18:03] Energy consumed for all GPUs : 0.010103 kWh. Total GPU Power : 50.167 W\n",
            "[codecarbon INFO @ 16:18:03] Energy consumed for all CPUs : 0.001834 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:18:03] 0.013244 kWh of electricity used since the beginning.\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:18:04,171 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-50/pt_lora_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:18:04,176 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-50/pt_lora_model/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 16:18:13] Energy consumed for RAM : 0.001395 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:18:13] Energy consumed for all GPUs : 0.010852 kWh. Total GPU Power : 266.466 W\n",
            "[codecarbon INFO @ 16:18:13] Energy consumed for all CPUs : 0.001953 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:18:13] 0.014200 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:18:28] Energy consumed for RAM : 0.001526 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:18:28] Energy consumed for all GPUs : 0.011942 kWh. Total GPU Power : 261.7970000000001 W\n",
            "[codecarbon INFO @ 16:18:28] Energy consumed for all CPUs : 0.002130 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:18:28] 0.015598 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:18:43] Energy consumed for RAM : 0.001656 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:18:43] Energy consumed for all GPUs : 0.013054 kWh. Total GPU Power : 267.051 W\n",
            "[codecarbon INFO @ 16:18:43] Energy consumed for all CPUs : 0.002307 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:18:43] 0.017017 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:18:58] Energy consumed for RAM : 0.001786 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:18:58] Energy consumed for all GPUs : 0.014112 kWh. Total GPU Power : 254.20900000000003 W\n",
            "[codecarbon INFO @ 16:18:58] Energy consumed for all CPUs : 0.002484 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:18:58] 0.018383 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:19:13] Energy consumed for RAM : 0.001917 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:19:13] Energy consumed for all GPUs : 0.015224 kWh. Total GPU Power : 267.005 W\n",
            "[codecarbon INFO @ 16:19:13] Energy consumed for all CPUs : 0.002661 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:19:13] 0.019802 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:19:28] Energy consumed for RAM : 0.002047 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:19:28] Energy consumed for all GPUs : 0.016345 kWh. Total GPU Power : 269.06 W\n",
            "[codecarbon INFO @ 16:19:28] Energy consumed for all CPUs : 0.002839 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:19:28] 0.021230 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:19:43] Energy consumed for RAM : 0.002177 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:19:43] Energy consumed for all GPUs : 0.017442 kWh. Total GPU Power : 263.538 W\n",
            "[codecarbon INFO @ 16:19:43] Energy consumed for all CPUs : 0.003016 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:19:43] 0.022635 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:19:58] Energy consumed for RAM : 0.002308 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:19:58] Energy consumed for all GPUs : 0.018544 kWh. Total GPU Power : 264.671 W\n",
            "[codecarbon INFO @ 16:19:58] Energy consumed for all CPUs : 0.003193 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:19:58] 0.024044 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:20:13] Energy consumed for RAM : 0.002438 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:20:13] Energy consumed for all GPUs : 0.019685 kWh. Total GPU Power : 274.047 W\n",
            "[codecarbon INFO @ 16:20:13] Energy consumed for all CPUs : 0.003370 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:20:13] 0.025493 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:20:28] Energy consumed for RAM : 0.002568 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:20:28] Energy consumed for all GPUs : 0.020738 kWh. Total GPU Power : 253.01500000000004 W\n",
            "[codecarbon INFO @ 16:20:28] Energy consumed for all CPUs : 0.003547 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:20:28] 0.026853 kWh of electricity used since the beginning.\n",
            "[INFO|trainer.py:2807] 2024-01-16 16:20:29,538 >> Saving model checkpoint to /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-100\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:20:29,584 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:20:29,603 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-100/special_tokens_map.json\n",
            "[INFO|trainer.py:2894] 2024-01-16 16:20:34,768 >> Deleting older checkpoint [/content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-50] due to args.save_total_limit\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:20:39,406 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-100/pt_lora_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:20:39,417 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-100/pt_lora_model/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 16:20:43] Energy consumed for RAM : 0.002699 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:20:43] Energy consumed for all GPUs : 0.021821 kWh. Total GPU Power : 260.01 W\n",
            "[codecarbon INFO @ 16:20:43] Energy consumed for all CPUs : 0.003724 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:20:43] 0.028243 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:20:58] Energy consumed for RAM : 0.002829 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:20:58] Energy consumed for all GPUs : 0.022917 kWh. Total GPU Power : 263.20700000000005 W\n",
            "[codecarbon INFO @ 16:20:58] Energy consumed for all CPUs : 0.003901 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:20:58] 0.029647 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:21:13] Energy consumed for RAM : 0.002959 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:21:13] Energy consumed for all GPUs : 0.024039 kWh. Total GPU Power : 269.34000000000003 W\n",
            "[codecarbon INFO @ 16:21:13] Energy consumed for all CPUs : 0.004078 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:21:13] 0.031076 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:21:28] Energy consumed for RAM : 0.003090 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:21:28] Energy consumed for all GPUs : 0.025176 kWh. Total GPU Power : 273.177 W\n",
            "[codecarbon INFO @ 16:21:28] Energy consumed for all CPUs : 0.004255 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:21:28] 0.032520 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:21:43] Energy consumed for RAM : 0.003220 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:21:43] Energy consumed for all GPUs : 0.026312 kWh. Total GPU Power : 272.86000000000007 W\n",
            "[codecarbon INFO @ 16:21:43] Energy consumed for all CPUs : 0.004432 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:21:43] 0.033964 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:21:58] Energy consumed for RAM : 0.003350 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:21:58] Energy consumed for all GPUs : 0.027396 kWh. Total GPU Power : 260.334 W\n",
            "[codecarbon INFO @ 16:21:58] Energy consumed for all CPUs : 0.004609 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:21:58] 0.035355 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:22:13] Energy consumed for RAM : 0.003480 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:22:13] Energy consumed for all GPUs : 0.028531 kWh. Total GPU Power : 272.537 W\n",
            "[codecarbon INFO @ 16:22:13] Energy consumed for all CPUs : 0.004786 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:22:13] 0.036798 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:22:28] Energy consumed for RAM : 0.003611 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:22:28] Energy consumed for all GPUs : 0.029679 kWh. Total GPU Power : 275.795 W\n",
            "[codecarbon INFO @ 16:22:28] Energy consumed for all CPUs : 0.004963 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:22:28] 0.038253 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:22:43] Energy consumed for RAM : 0.003741 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:22:43] Energy consumed for all GPUs : 0.030767 kWh. Total GPU Power : 261.202 W\n",
            "[codecarbon INFO @ 16:22:43] Energy consumed for all CPUs : 0.005140 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:22:43] 0.039648 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:22:58] Energy consumed for RAM : 0.003871 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:22:58] Energy consumed for all GPUs : 0.031857 kWh. Total GPU Power : 261.7970000000001 W\n",
            "[codecarbon INFO @ 16:22:58] Energy consumed for all CPUs : 0.005317 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:22:58] 0.041046 kWh of electricity used since the beginning.\n",
            "[INFO|trainer.py:2807] 2024-01-16 16:23:07,729 >> Saving model checkpoint to /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-150\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:23:07,776 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-150/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:23:07,781 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-150/special_tokens_map.json\n",
            "[codecarbon INFO @ 16:23:19] Energy consumed for RAM : 0.004009 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:23:19] Energy consumed for all GPUs : 0.032144 kWh. Total GPU Power : 50.219 W\n",
            "[INFO|trainer.py:2894] 2024-01-16 16:23:19,311 >> Deleting older checkpoint [/content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-100] due to args.save_total_limit\n",
            "[codecarbon INFO @ 16:23:19] Energy consumed for all CPUs : 0.005561 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:23:19] 0.041715 kWh of electricity used since the beginning.\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:23:19,642 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-150/pt_lora_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:23:19,648 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-150/pt_lora_model/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 16:23:29] Energy consumed for RAM : 0.004099 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:23:29] Energy consumed for all GPUs : 0.032894 kWh. Total GPU Power : 263.211 W\n",
            "[codecarbon INFO @ 16:23:29] Energy consumed for all CPUs : 0.005682 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:23:29] 0.042675 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:23:44] Energy consumed for RAM : 0.004229 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:23:44] Energy consumed for all GPUs : 0.034015 kWh. Total GPU Power : 269.07 W\n",
            "[codecarbon INFO @ 16:23:44] Energy consumed for all CPUs : 0.005859 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:23:44] 0.044102 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:23:59] Energy consumed for RAM : 0.004359 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:23:59] Energy consumed for all GPUs : 0.035081 kWh. Total GPU Power : 256.212 W\n",
            "[codecarbon INFO @ 16:23:59] Energy consumed for all CPUs : 0.006036 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:23:59] 0.045476 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:24:14] Energy consumed for RAM : 0.004490 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:24:14] Energy consumed for all GPUs : 0.036181 kWh. Total GPU Power : 264.13200000000006 W\n",
            "[codecarbon INFO @ 16:24:14] Energy consumed for all CPUs : 0.006213 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:24:14] 0.046884 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:24:29] Energy consumed for RAM : 0.004620 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:24:29] Energy consumed for all GPUs : 0.037278 kWh. Total GPU Power : 263.534 W\n",
            "[codecarbon INFO @ 16:24:29] Energy consumed for all CPUs : 0.006390 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:24:29] 0.048288 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:24:44] Energy consumed for RAM : 0.004750 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:24:44] Energy consumed for all GPUs : 0.038224 kWh. Total GPU Power : 227.091 W\n",
            "[codecarbon INFO @ 16:24:44] Energy consumed for all CPUs : 0.006567 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:24:44] 0.049541 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:24:59] Energy consumed for RAM : 0.004881 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:24:59] Energy consumed for all GPUs : 0.039040 kWh. Total GPU Power : 195.905 W\n",
            "[codecarbon INFO @ 16:24:59] Energy consumed for all CPUs : 0.006744 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:24:59] 0.050665 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:25:14] Energy consumed for RAM : 0.005011 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:25:14] Energy consumed for all GPUs : 0.040132 kWh. Total GPU Power : 262.395 W\n",
            "[codecarbon INFO @ 16:25:14] Energy consumed for all CPUs : 0.006921 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:25:14] 0.052064 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:25:29] Energy consumed for RAM : 0.005141 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:25:29] Energy consumed for all GPUs : 0.041231 kWh. Total GPU Power : 263.857 W\n",
            "[codecarbon INFO @ 16:25:29] Energy consumed for all CPUs : 0.007098 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:25:29] 0.053471 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:25:44] Energy consumed for RAM : 0.005272 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:25:44] Energy consumed for all GPUs : 0.042340 kWh. Total GPU Power : 266.459 W\n",
            "[codecarbon INFO @ 16:25:44] Energy consumed for all CPUs : 0.007275 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:25:44] 0.054887 kWh of electricity used since the beginning.\n",
            "[INFO|trainer.py:2807] 2024-01-16 16:25:46,156 >> Saving model checkpoint to /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-200\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:25:46,215 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:25:46,220 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-200/special_tokens_map.json\n",
            "[INFO|trainer.py:2894] 2024-01-16 16:25:51,383 >> Deleting older checkpoint [/content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-150] due to args.save_total_limit\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:25:56,006 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-200/pt_lora_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:25:56,012 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-200/pt_lora_model/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 16:25:59] Energy consumed for RAM : 0.005402 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:25:59] Energy consumed for all GPUs : 0.043411 kWh. Total GPU Power : 257.139 W\n",
            "[codecarbon INFO @ 16:25:59] Energy consumed for all CPUs : 0.007452 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:25:59] 0.056265 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:26:14] Energy consumed for RAM : 0.005532 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:26:14] Energy consumed for all GPUs : 0.044504 kWh. Total GPU Power : 262.395 W\n",
            "[codecarbon INFO @ 16:26:14] Energy consumed for all CPUs : 0.007630 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:26:14] 0.057665 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:26:29] Energy consumed for RAM : 0.005663 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:26:29] Energy consumed for all GPUs : 0.045587 kWh. Total GPU Power : 260.063 W\n",
            "[codecarbon INFO @ 16:26:29] Energy consumed for all CPUs : 0.007807 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:26:29] 0.059056 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:26:44] Energy consumed for RAM : 0.005793 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:26:44] Energy consumed for all GPUs : 0.046408 kWh. Total GPU Power : 197.16400000000002 W\n",
            "[codecarbon INFO @ 16:26:44] Energy consumed for all CPUs : 0.007984 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:26:44] 0.060184 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:26:59] Energy consumed for RAM : 0.005923 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:26:59] Energy consumed for all GPUs : 0.047505 kWh. Total GPU Power : 263.534 W\n",
            "[codecarbon INFO @ 16:26:59] Energy consumed for all CPUs : 0.008161 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:26:59] 0.061589 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:27:14] Energy consumed for RAM : 0.006053 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:27:14] Energy consumed for all GPUs : 0.048598 kWh. Total GPU Power : 262.395 W\n",
            "[codecarbon INFO @ 16:27:14] Energy consumed for all CPUs : 0.008338 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:27:14] 0.062989 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:27:29] Energy consumed for RAM : 0.006184 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:27:29] Energy consumed for all GPUs : 0.049435 kWh. Total GPU Power : 201.10500000000005 W\n",
            "[codecarbon INFO @ 16:27:29] Energy consumed for all CPUs : 0.008515 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:27:29] 0.064134 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:27:44] Energy consumed for RAM : 0.006314 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:27:44] Energy consumed for all GPUs : 0.050525 kWh. Total GPU Power : 261.799 W\n",
            "[codecarbon INFO @ 16:27:44] Energy consumed for all CPUs : 0.008692 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:27:44] 0.065531 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:27:59] Energy consumed for RAM : 0.006444 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:27:59] Energy consumed for all GPUs : 0.051611 kWh. Total GPU Power : 260.88100000000003 W\n",
            "[codecarbon INFO @ 16:27:59] Energy consumed for all CPUs : 0.008869 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:27:59] 0.066925 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:28:14] Energy consumed for RAM : 0.006575 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:28:14] Energy consumed for all GPUs : 0.052774 kWh. Total GPU Power : 279.262 W\n",
            "[codecarbon INFO @ 16:28:14] Energy consumed for all CPUs : 0.009046 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:28:14] 0.068395 kWh of electricity used since the beginning.\n",
            "[INFO|trainer.py:2807] 2024-01-16 16:28:22,973 >> Saving model checkpoint to /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-250\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:28:23,019 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:28:23,024 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-250/special_tokens_map.json\n",
            "[INFO|trainer.py:2894] 2024-01-16 16:28:28,274 >> Deleting older checkpoint [/content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-200] due to args.save_total_limit\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:28:28,509 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-250/pt_lora_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:28:28,514 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-250/pt_lora_model/special_tokens_map.json\n",
            "[codecarbon INFO @ 16:28:29] Energy consumed for RAM : 0.006705 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:28:29] Energy consumed for all GPUs : 0.052986 kWh. Total GPU Power : 50.76 W\n",
            "[codecarbon INFO @ 16:28:29] Energy consumed for all CPUs : 0.009223 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:28:29] 0.068914 kWh of electricity used since the beginning.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 16:28:44] Energy consumed for RAM : 0.006835 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:28:44] Energy consumed for all GPUs : 0.054096 kWh. Total GPU Power : 266.728 W\n",
            "[codecarbon INFO @ 16:28:44] Energy consumed for all CPUs : 0.009400 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:28:44] 0.070332 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:28:59] Energy consumed for RAM : 0.006966 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:28:59] Energy consumed for all GPUs : 0.055191 kWh. Total GPU Power : 262.9340000000001 W\n",
            "[codecarbon INFO @ 16:28:59] Energy consumed for all CPUs : 0.009577 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:28:59] 0.071734 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:29:14] Energy consumed for RAM : 0.007096 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:29:14] Energy consumed for all GPUs : 0.056304 kWh. Total GPU Power : 267.32800000000003 W\n",
            "[codecarbon INFO @ 16:29:14] Energy consumed for all CPUs : 0.009754 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:29:14] 0.073154 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:29:29] Energy consumed for RAM : 0.007226 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:29:29] Energy consumed for all GPUs : 0.056754 kWh. Total GPU Power : 108.14900000000002 W\n",
            "[codecarbon INFO @ 16:29:29] Energy consumed for all CPUs : 0.009931 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:29:29] 0.073912 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:29:44] Energy consumed for RAM : 0.007357 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:29:44] Energy consumed for all GPUs : 0.057834 kWh. Total GPU Power : 259.201 W\n",
            "[codecarbon INFO @ 16:29:44] Energy consumed for all CPUs : 0.010108 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:29:44] 0.075299 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:29:59] Energy consumed for RAM : 0.007487 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:29:59] Energy consumed for all GPUs : 0.058919 kWh. Total GPU Power : 260.61 W\n",
            "[codecarbon INFO @ 16:29:59] Energy consumed for all CPUs : 0.010285 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:29:59] 0.076691 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:30:14] Energy consumed for RAM : 0.007617 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:30:14] Energy consumed for all GPUs : 0.060027 kWh. Total GPU Power : 265.858 W\n",
            "[codecarbon INFO @ 16:30:14] Energy consumed for all CPUs : 0.010463 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:30:14] 0.078107 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:30:29] Energy consumed for RAM : 0.007747 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:30:29] Energy consumed for all GPUs : 0.061190 kWh. Total GPU Power : 279.874 W\n",
            "[codecarbon INFO @ 16:30:29] Energy consumed for all CPUs : 0.010640 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:30:29] 0.079577 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:30:44] Energy consumed for RAM : 0.007878 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:30:44] Energy consumed for all GPUs : 0.062347 kWh. Total GPU Power : 277.799 W\n",
            "[codecarbon INFO @ 16:30:44] Energy consumed for all CPUs : 0.010817 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:30:44] 0.081041 kWh of electricity used since the beginning.\n",
            "[INFO|trainer.py:2807] 2024-01-16 16:30:56,807 >> Saving model checkpoint to /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-300\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:30:56,855 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:30:56,860 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-300/special_tokens_map.json\n",
            "[codecarbon INFO @ 16:31:00] Energy consumed for RAM : 0.008009 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:31:01] Energy consumed for all GPUs : 0.062591 kWh. Total GPU Power : 50.985 W\n",
            "[INFO|trainer.py:2894] 2024-01-16 16:31:01,846 >> Deleting older checkpoint [/content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-250] due to args.save_total_limit\n",
            "[codecarbon INFO @ 16:31:01] Energy consumed for all CPUs : 0.011020 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:31:01] 0.081620 kWh of electricity used since the beginning.\n",
            "[INFO|tokenization_utils_base.py:2210] 2024-01-16 16:31:04,647 >> tokenizer config file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-300/pt_lora_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2217] 2024-01-16 16:31:04,652 >> Special tokens file saved in /content/drive/MyDrive/Projects/bangla-llama/data/output/checkpoint-300/pt_lora_model/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 16:31:14] Energy consumed for RAM : 0.008121 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:31:14] Energy consumed for all GPUs : 0.063524 kWh. Total GPU Power : 261.795 W\n",
            "[codecarbon INFO @ 16:31:14] Energy consumed for all CPUs : 0.011172 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:31:14] 0.082817 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:31:29] Energy consumed for RAM : 0.008251 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:31:29] Energy consumed for all GPUs : 0.064681 kWh. Total GPU Power : 277.833 W\n",
            "[codecarbon INFO @ 16:31:29] Energy consumed for all CPUs : 0.011349 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:31:29] 0.084281 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:31:44] Energy consumed for RAM : 0.008381 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:31:44] Energy consumed for all GPUs : 0.065786 kWh. Total GPU Power : 265.266 W\n",
            "[codecarbon INFO @ 16:31:44] Energy consumed for all CPUs : 0.011526 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:31:44] 0.085693 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 16:31:59] Energy consumed for RAM : 0.008512 kWh. RAM Power : 31.303916931152344 W\n",
            "[codecarbon INFO @ 16:31:59] Energy consumed for all GPUs : 0.066879 kWh. Total GPU Power : 262.664 W\n",
            "[codecarbon INFO @ 16:31:59] Energy consumed for all CPUs : 0.011703 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 16:31:59] 0.087094 kWh of electricity used since the beginning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "if training_args.do_eval:\n",
        "    logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    max_eval_samples = (\n",
        "        data_args.max_eval_samples\n",
        "        if data_args.max_eval_samples is not None\n",
        "        else len(eval_dataset)\n",
        "    )\n",
        "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
        "    try:\n",
        "        perplexity = math.exp(metrics[\"eval_loss\"])\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "    metrics[\"perplexity\"] = perplexity\n",
        "\n",
        "    trainer.log_metrics(\"eval\", metrics)\n",
        "    trainer.save_metrics(\"eval\", metrics)"
      ],
      "metadata": {
        "id": "Il18p7Hd7oSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNp0v8Zczi6ETfXDD+8GKj2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04d0f620e7d54a76b88998dd1fff5bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0779842cd9b64198ae1517da1ff8f405": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f880e16905a4be1b718273daceac680",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a0cab4a12aa4387856747ba1b5a9c3c",
            "value": 18
          }
        },
        "0ed211bdefd748dfa14535ed99891238": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f880e16905a4be1b718273daceac680": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f192500bcd34ffd851504aa7f5ebafc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33ea235f0a85492d8f723a7dcb532b66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c4a3510d204df0bcf8fd8e489335b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c9d990123147eaa12bae188abb1fde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fbd883cd0344882a02f5dc78041e0d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a46818fb4fc4587aca1b11e1d71eace",
            "placeholder": "​",
            "style": "IPY_MODEL_7811b700c799497486ae056ffc362fde",
            "value": "Generating train split: "
          }
        },
        "42d4a74ea6aa430bb0207309448c984f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36c4a3510d204df0bcf8fd8e489335b8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb8014a53cb84abcb0ace6864bd2ef96",
            "value": 1
          }
        },
        "49359dbf5f544bb3a5ba8d0d4088e2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c728be6054b4137a0dfdb2142c67cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68736d7bef8f464a9ac99bf3a37c0a61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a0cab4a12aa4387856747ba1b5a9c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "720e021855e143a2a27ce91adf234209": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82b5c59d829b4b3483531678173c4ae4",
              "IPY_MODEL_0779842cd9b64198ae1517da1ff8f405",
              "IPY_MODEL_a6e9621d424340e693a301d1de7e1435"
            ],
            "layout": "IPY_MODEL_e997a293549f41be8ff8bdfa511cad54"
          }
        },
        "7811b700c799497486ae056ffc362fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f9b10f32ac04c909e9218a146cb0ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36c9d990123147eaa12bae188abb1fde",
            "placeholder": "​",
            "style": "IPY_MODEL_96cebf3d2dec4d178d4ad7030cbc91a6",
            "value": "Extracting data files: 100%"
          }
        },
        "82b5c59d829b4b3483531678173c4ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ed211bdefd748dfa14535ed99891238",
            "placeholder": "​",
            "style": "IPY_MODEL_f44b6f3251784c148a27042f5c982950",
            "value": "Resolving data files: 100%"
          }
        },
        "8a46818fb4fc4587aca1b11e1d71eace": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a66255895fe4a84abea2bd29b980bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f9b10f32ac04c909e9218a146cb0ee6",
              "IPY_MODEL_f8e736390872479dac25f52404947bda",
              "IPY_MODEL_d1da4e3908bf468c96a692edd69c06d3"
            ],
            "layout": "IPY_MODEL_fcfed57b5f2148f0b56d5857dd6056f1"
          }
        },
        "8c128584f6d94fa5826f9e51ab4dd769": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ea235f0a85492d8f723a7dcb532b66",
            "placeholder": "​",
            "style": "IPY_MODEL_04d0f620e7d54a76b88998dd1fff5bdb",
            "value": " 1/1 [00:00&lt;00:00, 45.76it/s]"
          }
        },
        "92346b3e0d964e349a990b28b913b764": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95d0e0b8ef04423fb50cf257b5f002fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aecbf1f7d27f48da88d60589547820d1",
            "placeholder": "​",
            "style": "IPY_MODEL_5c728be6054b4137a0dfdb2142c67cd4",
            "value": " 12436596/0 [08:13&lt;00:00, 31440.88 examples/s]"
          }
        },
        "95e483355a7c41adbc8e98fa860f4c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96cebf3d2dec4d178d4ad7030cbc91a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "991c844cbeba4bc5a9c0c628fb9d750c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f571015852eb4df0a356f98edff00646",
              "IPY_MODEL_42d4a74ea6aa430bb0207309448c984f",
              "IPY_MODEL_8c128584f6d94fa5826f9e51ab4dd769"
            ],
            "layout": "IPY_MODEL_68736d7bef8f464a9ac99bf3a37c0a61"
          }
        },
        "a30f800461174e0993a2a60a48b2ac98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6e9621d424340e693a301d1de7e1435": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d77d5b8bb4b04b0b9a8c1d7b7bff4082",
            "placeholder": "​",
            "style": "IPY_MODEL_a8456d230c704ab9b7264e5b815c4c12",
            "value": " 18/18 [00:00&lt;00:00, 1567.48it/s]"
          }
        },
        "a8456d230c704ab9b7264e5b815c4c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac4b4b923a3e45dd9e31c065aa4df0ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aecbf1f7d27f48da88d60589547820d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3e390c62c2c4652ae245cd946d4dfe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8388c595da1420ba7fbfedb5196b09f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1da4e3908bf468c96a692edd69c06d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac4b4b923a3e45dd9e31c065aa4df0ea",
            "placeholder": "​",
            "style": "IPY_MODEL_a30f800461174e0993a2a60a48b2ac98",
            "value": " 1/1 [00:50&lt;00:00, 50.56s/it]"
          }
        },
        "d4286a71fc8544bbaad3f2a9b779e888": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d77d5b8bb4b04b0b9a8c1d7b7bff4082": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6e9c03e9e5c45ea8ffcf43cb34b85d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3fbd883cd0344882a02f5dc78041e0d2",
              "IPY_MODEL_fb94cdd865b94ae2bde1189333412698",
              "IPY_MODEL_95d0e0b8ef04423fb50cf257b5f002fd"
            ],
            "layout": "IPY_MODEL_95e483355a7c41adbc8e98fa860f4c2a"
          }
        },
        "e997a293549f41be8ff8bdfa511cad54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb8014a53cb84abcb0ace6864bd2ef96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f44b6f3251784c148a27042f5c982950": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f571015852eb4df0a356f98edff00646": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f192500bcd34ffd851504aa7f5ebafc",
            "placeholder": "​",
            "style": "IPY_MODEL_92346b3e0d964e349a990b28b913b764",
            "value": "Downloading data files: 100%"
          }
        },
        "f8e736390872479dac25f52404947bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8388c595da1420ba7fbfedb5196b09f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49359dbf5f544bb3a5ba8d0d4088e2a2",
            "value": 1
          }
        },
        "fb94cdd865b94ae2bde1189333412698": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4286a71fc8544bbaad3f2a9b779e888",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3e390c62c2c4652ae245cd946d4dfe8",
            "value": 1
          }
        },
        "fcfed57b5f2148f0b56d5857dd6056f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d2669800954ad6bf0040b3c467acd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd3cce7b50a14274a25f58d4399018ec",
              "IPY_MODEL_0634e8bd40134ab4b51aeccf75398940",
              "IPY_MODEL_5fc2aae072a349d08aca9ad67160728d",
              "IPY_MODEL_19b9957703a24b17bfae5b49bb9652be"
            ],
            "layout": "IPY_MODEL_fcd983a188584167bcadce44df8fda81"
          }
        },
        "8fce72fb31fd41e781d11e49e56fa8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49c05019017e43cebf4799995344910b",
            "placeholder": "​",
            "style": "IPY_MODEL_d0c9ca6a572f4b59a6ba442fe340a5ce",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "60d2fe11893442d1836bdf509ef60176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_a1b50b86ffc046ea8da8d440e14f01d5",
            "placeholder": "​",
            "style": "IPY_MODEL_6c3b16d1e63b4e0fa247c070cf051b57",
            "value": ""
          }
        },
        "e716e77347c94f42b005d9e1fbb1b866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_11995d80b6b8456d8ee55bb77d1fd2a4",
            "style": "IPY_MODEL_27ad8d21884d42c7842b851d67f40628",
            "value": true
          }
        },
        "f25e025559b2417595d4b289b051c29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_dd7ada0c3a4542569969bc637751d6dc",
            "style": "IPY_MODEL_ca7baa9771bd4eda817a46fb5c568a6f",
            "tooltip": ""
          }
        },
        "1316593626794ae5b1b5af75d4e54f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24e6de95491a4b78ad74dd35ee9f8383",
            "placeholder": "​",
            "style": "IPY_MODEL_c48f0a0011014b458aec84f9590c9509",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "fcd983a188584167bcadce44df8fda81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "49c05019017e43cebf4799995344910b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0c9ca6a572f4b59a6ba442fe340a5ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1b50b86ffc046ea8da8d440e14f01d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c3b16d1e63b4e0fa247c070cf051b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11995d80b6b8456d8ee55bb77d1fd2a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ad8d21884d42c7842b851d67f40628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd7ada0c3a4542569969bc637751d6dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7baa9771bd4eda817a46fb5c568a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "24e6de95491a4b78ad74dd35ee9f8383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48f0a0011014b458aec84f9590c9509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54efa63a0e5d408c90f47af1c28bf21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_513035c17d1b4732a8383a852da1e225",
            "placeholder": "​",
            "style": "IPY_MODEL_ebe4a6eadb6b4a8dae958ced695c3406",
            "value": "Connecting..."
          }
        },
        "513035c17d1b4732a8383a852da1e225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebe4a6eadb6b4a8dae958ced695c3406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd3cce7b50a14274a25f58d4399018ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c43efcbf7ce4e87909d3d9efe31221a",
            "placeholder": "​",
            "style": "IPY_MODEL_5336441146864144a3720f1bd0d4684f",
            "value": "Token is valid (permission: write)."
          }
        },
        "0634e8bd40134ab4b51aeccf75398940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d02391121e34cffbaed2bce280bbda6",
            "placeholder": "​",
            "style": "IPY_MODEL_ae2f2065457e425ea1b873332b3112e8",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "5fc2aae072a349d08aca9ad67160728d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33a6291177a24688afc476a20f26711d",
            "placeholder": "​",
            "style": "IPY_MODEL_1fbf2060e60a41aeb15553f0ea0b9cde",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "19b9957703a24b17bfae5b49bb9652be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35da2736df6847eeaa13e2dbf1b04c75",
            "placeholder": "​",
            "style": "IPY_MODEL_494e4ccc640d442eba994d5092bb2430",
            "value": "Login successful"
          }
        },
        "9c43efcbf7ce4e87909d3d9efe31221a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5336441146864144a3720f1bd0d4684f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d02391121e34cffbaed2bce280bbda6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae2f2065457e425ea1b873332b3112e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33a6291177a24688afc476a20f26711d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fbf2060e60a41aeb15553f0ea0b9cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35da2736df6847eeaa13e2dbf1b04c75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494e4ccc640d442eba994d5092bb2430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58a22a03a8a24d47b54495fddf43f6ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb39fd256c1c411a868a69a9265fd7ea",
              "IPY_MODEL_7621e18f25d147cfb6178a974a405857",
              "IPY_MODEL_1b4db8be7c034e8186ef10807c26efef"
            ],
            "layout": "IPY_MODEL_43f0ceea73e646d9968a6236739c5118"
          }
        },
        "fb39fd256c1c411a868a69a9265fd7ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca7bdcb1ee0c431fbbd74c9c93349d53",
            "placeholder": "​",
            "style": "IPY_MODEL_098866be491543968a6086d4bae19029",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7621e18f25d147cfb6178a974a405857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51798017610640e490331f20a059776f",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ef577fcc6724b92b157fb3b86242ff4",
            "value": 14
          }
        },
        "1b4db8be7c034e8186ef10807c26efef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4fab19eea894e66b56afa5e7fc44428",
            "placeholder": "​",
            "style": "IPY_MODEL_a4bf10a93e7042919112801662b53ec4",
            "value": " 14/14 [00:10&lt;00:00,  1.33it/s]"
          }
        },
        "43f0ceea73e646d9968a6236739c5118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7bdcb1ee0c431fbbd74c9c93349d53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "098866be491543968a6086d4bae19029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51798017610640e490331f20a059776f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ef577fcc6724b92b157fb3b86242ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4fab19eea894e66b56afa5e7fc44428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4bf10a93e7042919112801662b53ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}